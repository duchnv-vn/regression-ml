{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T14:46:49.484772Z",
     "start_time": "2025-06-23T14:46:46.711704Z"
    }
   },
   "source": [
    "from load_image import load_images, load_label, one_hot_encode\n",
    "\n",
    "X_train = load_images(\"train-images-idx3-ubyte.gz\")\n",
    "Y_train_unencoded = load_label(\"train-labels-idx1-ubyte.gz\")\n",
    "Y_train = one_hot_encode(Y_train_unencoded, 10)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:46:50.487028Z",
     "start_time": "2025-06-23T14:46:50.396646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_test = load_images(\"t10k-images-idx3-ubyte.gz\")\n",
    "Y_test = load_label(\"t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ],
   "id": "c2ab2770e05a23ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T15:35:54.280900Z",
     "start_time": "2025-06-23T14:50:40.548548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from neural_network import train\n",
    "\n",
    "w1, w2 = train(X_train, Y_train, X_test, Y_test,\n",
    "               n_hidden_nodes=200, epochs=2, batch_size=32, lr=0.01)"
   ],
   "id": "4bb9d8d2555cccf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0-0 > Loss: 2.43364135, Accuracy: 15.18%\n",
      "    0-1 > Loss: 2.37567270, Accuracy: 17.47%\n",
      "    0-2 > Loss: 2.34633760, Accuracy: 19.87%\n",
      "    0-3 > Loss: 2.29222027, Accuracy: 24.50%\n",
      "    0-4 > Loss: 2.25790576, Accuracy: 24.99%\n",
      "    0-5 > Loss: 2.21945090, Accuracy: 27.67%\n",
      "    0-6 > Loss: 2.18721954, Accuracy: 29.19%\n",
      "    0-7 > Loss: 2.14235100, Accuracy: 30.44%\n",
      "    0-8 > Loss: 2.11939127, Accuracy: 31.28%\n",
      "    0-9 > Loss: 2.07571931, Accuracy: 33.26%\n",
      "    0-10 > Loss: 2.04546458, Accuracy: 34.10%\n",
      "    0-11 > Loss: 2.02829576, Accuracy: 34.81%\n",
      "    0-12 > Loss: 2.01150778, Accuracy: 35.14%\n",
      "    0-13 > Loss: 1.99381912, Accuracy: 36.96%\n",
      "    0-14 > Loss: 1.97262062, Accuracy: 37.10%\n",
      "    0-15 > Loss: 1.95572403, Accuracy: 37.34%\n",
      "    0-16 > Loss: 1.94263152, Accuracy: 37.10%\n",
      "    0-17 > Loss: 1.92459971, Accuracy: 38.07%\n",
      "    0-18 > Loss: 1.91099865, Accuracy: 39.57%\n",
      "    0-19 > Loss: 1.89682909, Accuracy: 40.24%\n",
      "    0-20 > Loss: 1.87824225, Accuracy: 41.75%\n",
      "    0-21 > Loss: 1.86419273, Accuracy: 42.51%\n",
      "    0-22 > Loss: 1.84742591, Accuracy: 43.39%\n",
      "    0-23 > Loss: 1.83181945, Accuracy: 45.28%\n",
      "    0-24 > Loss: 1.81918148, Accuracy: 45.50%\n",
      "    0-25 > Loss: 1.80396826, Accuracy: 47.70%\n",
      "    0-26 > Loss: 1.79127208, Accuracy: 48.18%\n",
      "    0-27 > Loss: 1.77789411, Accuracy: 50.24%\n",
      "    0-28 > Loss: 1.77018376, Accuracy: 49.77%\n",
      "    0-29 > Loss: 1.76196689, Accuracy: 48.47%\n",
      "    0-30 > Loss: 1.75396370, Accuracy: 48.63%\n",
      "    0-31 > Loss: 1.74857878, Accuracy: 47.22%\n",
      "    0-32 > Loss: 1.73532488, Accuracy: 48.90%\n",
      "    0-33 > Loss: 1.73365724, Accuracy: 48.72%\n",
      "    0-34 > Loss: 1.72464826, Accuracy: 49.94%\n",
      "    0-35 > Loss: 1.71076423, Accuracy: 52.31%\n",
      "    0-36 > Loss: 1.70039349, Accuracy: 52.72%\n",
      "    0-37 > Loss: 1.68201856, Accuracy: 53.81%\n",
      "    0-38 > Loss: 1.67721591, Accuracy: 53.88%\n",
      "    0-39 > Loss: 1.66169779, Accuracy: 56.04%\n",
      "    0-40 > Loss: 1.65661226, Accuracy: 56.47%\n",
      "    0-41 > Loss: 1.65547539, Accuracy: 57.03%\n",
      "    0-42 > Loss: 1.64357924, Accuracy: 56.27%\n",
      "    0-43 > Loss: 1.63090238, Accuracy: 57.51%\n",
      "    0-44 > Loss: 1.62213126, Accuracy: 58.04%\n",
      "    0-45 > Loss: 1.61816256, Accuracy: 57.44%\n",
      "    0-46 > Loss: 1.61253670, Accuracy: 57.99%\n",
      "    0-47 > Loss: 1.60457816, Accuracy: 58.50%\n",
      "    0-48 > Loss: 1.59669494, Accuracy: 59.87%\n",
      "    0-49 > Loss: 1.59180739, Accuracy: 60.57%\n",
      "    0-50 > Loss: 1.58231775, Accuracy: 61.21%\n",
      "    0-51 > Loss: 1.57380815, Accuracy: 62.50%\n",
      "    0-52 > Loss: 1.56964934, Accuracy: 61.91%\n",
      "    0-53 > Loss: 1.54272669, Accuracy: 63.36%\n",
      "    0-54 > Loss: 1.53670039, Accuracy: 63.93%\n",
      "    0-55 > Loss: 1.52819469, Accuracy: 63.76%\n",
      "    0-56 > Loss: 1.51839823, Accuracy: 63.98%\n",
      "    0-57 > Loss: 1.50802737, Accuracy: 64.60%\n",
      "    0-58 > Loss: 1.49783697, Accuracy: 65.32%\n",
      "    0-59 > Loss: 1.49533496, Accuracy: 64.48%\n",
      "    0-60 > Loss: 1.48583758, Accuracy: 64.52%\n",
      "    0-61 > Loss: 1.47257081, Accuracy: 66.34%\n",
      "    0-62 > Loss: 1.46372082, Accuracy: 66.57%\n",
      "    0-63 > Loss: 1.45959500, Accuracy: 66.79%\n",
      "    0-64 > Loss: 1.45434764, Accuracy: 67.10%\n",
      "    0-65 > Loss: 1.44689582, Accuracy: 66.64%\n",
      "    0-66 > Loss: 1.44088443, Accuracy: 67.20%\n",
      "    0-67 > Loss: 1.43763057, Accuracy: 65.83%\n",
      "    0-68 > Loss: 1.43456576, Accuracy: 66.28%\n",
      "    0-69 > Loss: 1.43191726, Accuracy: 66.18%\n",
      "    0-70 > Loss: 1.42080833, Accuracy: 66.90%\n",
      "    0-71 > Loss: 1.41155680, Accuracy: 68.16%\n",
      "    0-72 > Loss: 1.40280976, Accuracy: 68.67%\n",
      "    0-73 > Loss: 1.38862496, Accuracy: 69.14%\n",
      "    0-74 > Loss: 1.37865254, Accuracy: 70.56%\n",
      "    0-75 > Loss: 1.37467099, Accuracy: 69.55%\n",
      "    0-76 > Loss: 1.36803432, Accuracy: 69.31%\n",
      "    0-77 > Loss: 1.36440328, Accuracy: 68.23%\n",
      "    0-78 > Loss: 1.35284412, Accuracy: 70.25%\n",
      "    0-79 > Loss: 1.34672362, Accuracy: 70.88%\n",
      "    0-80 > Loss: 1.34170813, Accuracy: 69.79%\n",
      "    0-81 > Loss: 1.33200768, Accuracy: 70.55%\n",
      "    0-82 > Loss: 1.32766748, Accuracy: 71.37%\n",
      "    0-83 > Loss: 1.31944644, Accuracy: 71.63%\n",
      "    0-84 > Loss: 1.31185146, Accuracy: 72.22%\n",
      "    0-85 > Loss: 1.30850046, Accuracy: 71.63%\n",
      "    0-86 > Loss: 1.30374318, Accuracy: 71.85%\n",
      "    0-87 > Loss: 1.29944899, Accuracy: 72.11%\n",
      "    0-88 > Loss: 1.29304177, Accuracy: 72.11%\n",
      "    0-89 > Loss: 1.28885053, Accuracy: 72.20%\n",
      "    0-90 > Loss: 1.27892309, Accuracy: 71.23%\n",
      "    0-91 > Loss: 1.27379481, Accuracy: 71.47%\n",
      "    0-92 > Loss: 1.26638337, Accuracy: 72.22%\n",
      "    0-93 > Loss: 1.26011479, Accuracy: 72.54%\n",
      "    0-94 > Loss: 1.25464320, Accuracy: 72.38%\n",
      "    0-95 > Loss: 1.24703440, Accuracy: 73.04%\n",
      "    0-96 > Loss: 1.24386348, Accuracy: 73.29%\n",
      "    0-97 > Loss: 1.23173424, Accuracy: 74.23%\n",
      "    0-98 > Loss: 1.22366351, Accuracy: 74.66%\n",
      "    0-99 > Loss: 1.21967581, Accuracy: 75.42%\n",
      "    0-100 > Loss: 1.21658745, Accuracy: 75.52%\n",
      "    0-101 > Loss: 1.21043294, Accuracy: 75.45%\n",
      "    0-102 > Loss: 1.20729300, Accuracy: 75.69%\n",
      "    0-103 > Loss: 1.20880274, Accuracy: 75.40%\n",
      "    0-104 > Loss: 1.20994518, Accuracy: 74.79%\n",
      "    0-105 > Loss: 1.20413638, Accuracy: 74.87%\n",
      "    0-106 > Loss: 1.19718820, Accuracy: 74.91%\n",
      "    0-107 > Loss: 1.18777473, Accuracy: 74.62%\n",
      "    0-108 > Loss: 1.19186860, Accuracy: 72.91%\n",
      "    0-109 > Loss: 1.18465034, Accuracy: 74.54%\n",
      "    0-110 > Loss: 1.18056721, Accuracy: 74.40%\n",
      "    0-111 > Loss: 1.17242566, Accuracy: 76.05%\n",
      "    0-112 > Loss: 1.17102369, Accuracy: 75.56%\n",
      "    0-113 > Loss: 1.16644919, Accuracy: 76.27%\n",
      "    0-114 > Loss: 1.16305966, Accuracy: 76.17%\n",
      "    0-115 > Loss: 1.16014619, Accuracy: 76.10%\n",
      "    0-116 > Loss: 1.15512120, Accuracy: 75.79%\n",
      "    0-117 > Loss: 1.14913003, Accuracy: 77.19%\n",
      "    0-118 > Loss: 1.13907590, Accuracy: 77.17%\n",
      "    0-119 > Loss: 1.13253646, Accuracy: 77.45%\n",
      "    0-120 > Loss: 1.13310509, Accuracy: 77.50%\n",
      "    0-121 > Loss: 1.12487508, Accuracy: 77.43%\n",
      "    0-122 > Loss: 1.11855016, Accuracy: 77.90%\n",
      "    0-123 > Loss: 1.11545453, Accuracy: 76.91%\n",
      "    0-124 > Loss: 1.11407137, Accuracy: 77.66%\n",
      "    0-125 > Loss: 1.10877087, Accuracy: 78.16%\n",
      "    0-126 > Loss: 1.11201121, Accuracy: 77.79%\n",
      "    0-127 > Loss: 1.10739129, Accuracy: 77.99%\n",
      "    0-128 > Loss: 1.10404405, Accuracy: 77.61%\n",
      "    0-129 > Loss: 1.09539346, Accuracy: 77.77%\n",
      "    0-130 > Loss: 1.09100692, Accuracy: 78.31%\n",
      "    0-131 > Loss: 1.08544352, Accuracy: 78.50%\n",
      "    0-132 > Loss: 1.08429399, Accuracy: 77.56%\n",
      "    0-133 > Loss: 1.07746444, Accuracy: 77.55%\n",
      "    0-134 > Loss: 1.07556173, Accuracy: 78.10%\n",
      "    0-135 > Loss: 1.06857656, Accuracy: 78.19%\n",
      "    0-136 > Loss: 1.06147495, Accuracy: 78.47%\n",
      "    0-137 > Loss: 1.05740806, Accuracy: 78.64%\n",
      "    0-138 > Loss: 1.05317354, Accuracy: 79.40%\n",
      "    0-139 > Loss: 1.05031837, Accuracy: 79.60%\n",
      "    0-140 > Loss: 1.04613673, Accuracy: 79.16%\n",
      "    0-141 > Loss: 1.03827462, Accuracy: 78.93%\n",
      "    0-142 > Loss: 1.03266767, Accuracy: 79.18%\n",
      "    0-143 > Loss: 1.03102444, Accuracy: 79.78%\n",
      "    0-144 > Loss: 1.02951386, Accuracy: 79.70%\n",
      "    0-145 > Loss: 1.02598562, Accuracy: 80.04%\n",
      "    0-146 > Loss: 1.02083103, Accuracy: 80.49%\n",
      "    0-147 > Loss: 1.01774971, Accuracy: 79.92%\n",
      "    0-148 > Loss: 1.01599253, Accuracy: 79.70%\n",
      "    0-149 > Loss: 1.01608017, Accuracy: 79.11%\n",
      "    0-150 > Loss: 1.01719098, Accuracy: 79.23%\n",
      "    0-151 > Loss: 1.01487284, Accuracy: 79.40%\n",
      "    0-152 > Loss: 1.01161070, Accuracy: 79.33%\n",
      "    0-153 > Loss: 1.01037053, Accuracy: 79.09%\n",
      "    0-154 > Loss: 1.01000165, Accuracy: 79.00%\n",
      "    0-155 > Loss: 1.00719148, Accuracy: 79.03%\n",
      "    0-156 > Loss: 1.00627140, Accuracy: 78.57%\n",
      "    0-157 > Loss: 1.00495162, Accuracy: 78.73%\n",
      "    0-158 > Loss: 1.00243436, Accuracy: 78.95%\n",
      "    0-159 > Loss: 0.99399642, Accuracy: 79.56%\n",
      "    0-160 > Loss: 0.99232683, Accuracy: 79.38%\n",
      "    0-161 > Loss: 0.99472909, Accuracy: 79.10%\n",
      "    0-162 > Loss: 0.99125530, Accuracy: 79.17%\n",
      "    0-163 > Loss: 0.98181820, Accuracy: 79.86%\n",
      "    0-164 > Loss: 0.98060159, Accuracy: 79.46%\n",
      "    0-165 > Loss: 0.97660952, Accuracy: 79.88%\n",
      "    0-166 > Loss: 0.97256234, Accuracy: 79.69%\n",
      "    0-167 > Loss: 0.96867008, Accuracy: 80.02%\n",
      "    0-168 > Loss: 0.96128415, Accuracy: 80.83%\n",
      "    0-169 > Loss: 0.95848213, Accuracy: 81.31%\n",
      "    0-170 > Loss: 0.95506660, Accuracy: 81.16%\n",
      "    0-171 > Loss: 0.95507564, Accuracy: 80.70%\n",
      "    0-172 > Loss: 0.95008825, Accuracy: 80.57%\n",
      "    0-173 > Loss: 0.95285281, Accuracy: 80.27%\n",
      "    0-174 > Loss: 0.95155528, Accuracy: 80.01%\n",
      "    0-175 > Loss: 0.94381863, Accuracy: 80.60%\n",
      "    0-176 > Loss: 0.94286223, Accuracy: 80.56%\n",
      "    0-177 > Loss: 0.94249363, Accuracy: 80.72%\n",
      "    0-178 > Loss: 0.93878008, Accuracy: 80.66%\n",
      "    0-179 > Loss: 0.93982043, Accuracy: 80.09%\n",
      "    0-180 > Loss: 0.93408677, Accuracy: 80.48%\n",
      "    0-181 > Loss: 0.93400003, Accuracy: 79.83%\n",
      "    0-182 > Loss: 0.93184850, Accuracy: 79.72%\n",
      "    0-183 > Loss: 0.93440457, Accuracy: 79.30%\n",
      "    0-184 > Loss: 0.93147153, Accuracy: 79.41%\n",
      "    0-185 > Loss: 0.92819458, Accuracy: 79.51%\n",
      "    0-186 > Loss: 0.92421213, Accuracy: 79.62%\n",
      "    0-187 > Loss: 0.92314548, Accuracy: 79.96%\n",
      "    0-188 > Loss: 0.91615380, Accuracy: 80.40%\n",
      "    0-189 > Loss: 0.91730623, Accuracy: 80.28%\n",
      "    0-190 > Loss: 0.91961376, Accuracy: 80.01%\n",
      "    0-191 > Loss: 0.91521971, Accuracy: 80.45%\n",
      "    0-192 > Loss: 0.91793738, Accuracy: 79.88%\n",
      "    0-193 > Loss: 0.91535858, Accuracy: 80.49%\n",
      "    0-194 > Loss: 0.90984051, Accuracy: 80.55%\n",
      "    0-195 > Loss: 0.90461415, Accuracy: 81.20%\n",
      "    0-196 > Loss: 0.90572576, Accuracy: 80.73%\n",
      "    0-197 > Loss: 0.90761288, Accuracy: 80.89%\n",
      "    0-198 > Loss: 0.90782954, Accuracy: 80.51%\n",
      "    0-199 > Loss: 0.90100669, Accuracy: 81.04%\n",
      "    0-200 > Loss: 0.90226610, Accuracy: 80.86%\n",
      "    0-201 > Loss: 0.89876361, Accuracy: 80.90%\n",
      "    0-202 > Loss: 0.89691298, Accuracy: 81.40%\n",
      "    0-203 > Loss: 0.88992888, Accuracy: 82.19%\n",
      "    0-204 > Loss: 0.88697128, Accuracy: 82.33%\n",
      "    0-205 > Loss: 0.88370849, Accuracy: 82.51%\n",
      "    0-206 > Loss: 0.88105849, Accuracy: 82.61%\n",
      "    0-207 > Loss: 0.88017542, Accuracy: 82.23%\n",
      "    0-208 > Loss: 0.87458875, Accuracy: 82.16%\n",
      "    0-209 > Loss: 0.87287715, Accuracy: 81.94%\n",
      "    0-210 > Loss: 0.87142258, Accuracy: 82.22%\n",
      "    0-211 > Loss: 0.86885880, Accuracy: 82.51%\n",
      "    0-212 > Loss: 0.86584879, Accuracy: 82.56%\n",
      "    0-213 > Loss: 0.86432701, Accuracy: 82.84%\n",
      "    0-214 > Loss: 0.86074757, Accuracy: 82.64%\n",
      "    0-215 > Loss: 0.85820496, Accuracy: 82.56%\n",
      "    0-216 > Loss: 0.85630060, Accuracy: 83.10%\n",
      "    0-217 > Loss: 0.85578876, Accuracy: 82.62%\n",
      "    0-218 > Loss: 0.84985246, Accuracy: 83.49%\n",
      "    0-219 > Loss: 0.84893842, Accuracy: 83.39%\n",
      "    0-220 > Loss: 0.84694029, Accuracy: 83.18%\n",
      "    0-221 > Loss: 0.84359007, Accuracy: 83.26%\n",
      "    0-222 > Loss: 0.84232285, Accuracy: 83.08%\n",
      "    0-223 > Loss: 0.83871034, Accuracy: 83.19%\n",
      "    0-224 > Loss: 0.83862257, Accuracy: 83.48%\n",
      "    0-225 > Loss: 0.83693123, Accuracy: 83.19%\n",
      "    0-226 > Loss: 0.83922905, Accuracy: 82.83%\n",
      "    0-227 > Loss: 0.83990352, Accuracy: 82.54%\n",
      "    0-228 > Loss: 0.83477358, Accuracy: 83.24%\n",
      "    0-229 > Loss: 0.83221012, Accuracy: 83.44%\n",
      "    0-230 > Loss: 0.83398969, Accuracy: 82.89%\n",
      "    0-231 > Loss: 0.82955696, Accuracy: 83.02%\n",
      "    0-232 > Loss: 0.82815183, Accuracy: 83.44%\n",
      "    0-233 > Loss: 0.82687876, Accuracy: 83.51%\n",
      "    0-234 > Loss: 0.82422096, Accuracy: 83.44%\n",
      "    0-235 > Loss: 0.82412354, Accuracy: 83.87%\n",
      "    0-236 > Loss: 0.82122742, Accuracy: 83.77%\n",
      "    0-237 > Loss: 0.81874352, Accuracy: 83.72%\n",
      "    0-238 > Loss: 0.81755168, Accuracy: 83.94%\n",
      "    0-239 > Loss: 0.81418219, Accuracy: 83.80%\n",
      "    0-240 > Loss: 0.81205168, Accuracy: 83.69%\n",
      "    0-241 > Loss: 0.80905635, Accuracy: 83.85%\n",
      "    0-242 > Loss: 0.80574487, Accuracy: 83.62%\n",
      "    0-243 > Loss: 0.80767411, Accuracy: 83.37%\n",
      "    0-244 > Loss: 0.80715208, Accuracy: 83.13%\n",
      "    0-245 > Loss: 0.80146793, Accuracy: 83.62%\n",
      "    0-246 > Loss: 0.79917596, Accuracy: 83.65%\n",
      "    0-247 > Loss: 0.79818712, Accuracy: 83.76%\n",
      "    0-248 > Loss: 0.80221099, Accuracy: 83.71%\n",
      "    0-249 > Loss: 0.79851734, Accuracy: 83.86%\n",
      "    0-250 > Loss: 0.79837531, Accuracy: 83.68%\n",
      "    0-251 > Loss: 0.79436267, Accuracy: 83.48%\n",
      "    0-252 > Loss: 0.78992287, Accuracy: 83.53%\n",
      "    0-253 > Loss: 0.79043701, Accuracy: 83.42%\n",
      "    0-254 > Loss: 0.79363951, Accuracy: 83.28%\n",
      "    0-255 > Loss: 0.79590005, Accuracy: 83.16%\n",
      "    0-256 > Loss: 0.79391635, Accuracy: 83.18%\n",
      "    0-257 > Loss: 0.79400669, Accuracy: 83.19%\n",
      "    0-258 > Loss: 0.79404845, Accuracy: 82.79%\n",
      "    0-259 > Loss: 0.79340781, Accuracy: 82.66%\n",
      "    0-260 > Loss: 0.79125117, Accuracy: 83.00%\n",
      "    0-261 > Loss: 0.78895567, Accuracy: 82.98%\n",
      "    0-262 > Loss: 0.78647840, Accuracy: 82.92%\n",
      "    0-263 > Loss: 0.78243451, Accuracy: 83.20%\n",
      "    0-264 > Loss: 0.78131097, Accuracy: 83.23%\n",
      "    0-265 > Loss: 0.78544280, Accuracy: 83.00%\n",
      "    0-266 > Loss: 0.78402327, Accuracy: 82.90%\n",
      "    0-267 > Loss: 0.78081603, Accuracy: 82.89%\n",
      "    0-268 > Loss: 0.77924162, Accuracy: 83.01%\n",
      "    0-269 > Loss: 0.77826870, Accuracy: 83.23%\n",
      "    0-270 > Loss: 0.77527887, Accuracy: 83.68%\n",
      "    0-271 > Loss: 0.77787650, Accuracy: 83.58%\n",
      "    0-272 > Loss: 0.77787880, Accuracy: 83.96%\n",
      "    0-273 > Loss: 0.77511584, Accuracy: 84.20%\n",
      "    0-274 > Loss: 0.77443741, Accuracy: 83.87%\n",
      "    0-275 > Loss: 0.78101505, Accuracy: 83.30%\n",
      "    0-276 > Loss: 0.78015041, Accuracy: 83.38%\n",
      "    0-277 > Loss: 0.78550532, Accuracy: 82.58%\n",
      "    0-278 > Loss: 0.78415887, Accuracy: 82.84%\n",
      "    0-279 > Loss: 0.78493490, Accuracy: 82.58%\n",
      "    0-280 > Loss: 0.78229267, Accuracy: 83.01%\n",
      "    0-281 > Loss: 0.77573311, Accuracy: 83.17%\n",
      "    0-282 > Loss: 0.77416678, Accuracy: 82.97%\n",
      "    0-283 > Loss: 0.76879407, Accuracy: 83.43%\n",
      "    0-284 > Loss: 0.76769467, Accuracy: 83.36%\n",
      "    0-285 > Loss: 0.76346572, Accuracy: 83.05%\n",
      "    0-286 > Loss: 0.75985379, Accuracy: 83.36%\n",
      "    0-287 > Loss: 0.75713239, Accuracy: 83.38%\n",
      "    0-288 > Loss: 0.75580312, Accuracy: 83.99%\n",
      "    0-289 > Loss: 0.75977822, Accuracy: 83.23%\n",
      "    0-290 > Loss: 0.75781559, Accuracy: 83.21%\n",
      "    0-291 > Loss: 0.76114009, Accuracy: 83.20%\n",
      "    0-292 > Loss: 0.75987881, Accuracy: 83.85%\n",
      "    0-293 > Loss: 0.76458946, Accuracy: 83.40%\n",
      "    0-294 > Loss: 0.76309513, Accuracy: 83.40%\n",
      "    0-295 > Loss: 0.75740805, Accuracy: 83.36%\n",
      "    0-296 > Loss: 0.75167815, Accuracy: 83.87%\n",
      "    0-297 > Loss: 0.75011373, Accuracy: 83.81%\n",
      "    0-298 > Loss: 0.74860481, Accuracy: 84.02%\n",
      "    0-299 > Loss: 0.74978308, Accuracy: 83.67%\n",
      "    0-300 > Loss: 0.74713349, Accuracy: 84.16%\n",
      "    0-301 > Loss: 0.74484061, Accuracy: 84.23%\n",
      "    0-302 > Loss: 0.74494960, Accuracy: 83.53%\n",
      "    0-303 > Loss: 0.74034458, Accuracy: 83.43%\n",
      "    0-304 > Loss: 0.73674721, Accuracy: 83.75%\n",
      "    0-305 > Loss: 0.73441875, Accuracy: 84.19%\n",
      "    0-306 > Loss: 0.73044090, Accuracy: 84.53%\n",
      "    0-307 > Loss: 0.72933425, Accuracy: 84.76%\n",
      "    0-308 > Loss: 0.72893797, Accuracy: 84.87%\n",
      "    0-309 > Loss: 0.72775438, Accuracy: 84.92%\n",
      "    0-310 > Loss: 0.72629242, Accuracy: 84.94%\n",
      "    0-311 > Loss: 0.72783866, Accuracy: 84.67%\n",
      "    0-312 > Loss: 0.72544952, Accuracy: 84.71%\n",
      "    0-313 > Loss: 0.72344243, Accuracy: 84.64%\n",
      "    0-314 > Loss: 0.72134413, Accuracy: 84.99%\n",
      "    0-315 > Loss: 0.72009322, Accuracy: 84.98%\n",
      "    0-316 > Loss: 0.71818312, Accuracy: 84.76%\n",
      "    0-317 > Loss: 0.71514355, Accuracy: 84.81%\n",
      "    0-318 > Loss: 0.71825931, Accuracy: 84.62%\n",
      "    0-319 > Loss: 0.71890084, Accuracy: 84.68%\n",
      "    0-320 > Loss: 0.72246268, Accuracy: 84.22%\n",
      "    0-321 > Loss: 0.72418798, Accuracy: 84.62%\n",
      "    0-322 > Loss: 0.72275841, Accuracy: 84.40%\n",
      "    0-323 > Loss: 0.72234361, Accuracy: 84.26%\n",
      "    0-324 > Loss: 0.71892658, Accuracy: 84.21%\n",
      "    0-325 > Loss: 0.71715184, Accuracy: 84.38%\n",
      "    0-326 > Loss: 0.71491665, Accuracy: 84.70%\n",
      "    0-327 > Loss: 0.71396858, Accuracy: 84.70%\n",
      "    0-328 > Loss: 0.71285813, Accuracy: 85.01%\n",
      "    0-329 > Loss: 0.71043381, Accuracy: 84.98%\n",
      "    0-330 > Loss: 0.71036413, Accuracy: 85.21%\n",
      "    0-331 > Loss: 0.70995428, Accuracy: 85.55%\n",
      "    0-332 > Loss: 0.71044483, Accuracy: 85.32%\n",
      "    0-333 > Loss: 0.70839328, Accuracy: 85.10%\n",
      "    0-334 > Loss: 0.70612440, Accuracy: 85.13%\n",
      "    0-335 > Loss: 0.70559417, Accuracy: 85.29%\n",
      "    0-336 > Loss: 0.70599842, Accuracy: 85.02%\n",
      "    0-337 > Loss: 0.70413179, Accuracy: 85.23%\n",
      "    0-338 > Loss: 0.70081054, Accuracy: 85.26%\n",
      "    0-339 > Loss: 0.69780509, Accuracy: 85.19%\n",
      "    0-340 > Loss: 0.69825535, Accuracy: 85.10%\n",
      "    0-341 > Loss: 0.70168723, Accuracy: 85.19%\n",
      "    0-342 > Loss: 0.69842776, Accuracy: 85.25%\n",
      "    0-343 > Loss: 0.69818231, Accuracy: 85.04%\n",
      "    0-344 > Loss: 0.69906344, Accuracy: 84.45%\n",
      "    0-345 > Loss: 0.69691712, Accuracy: 84.37%\n",
      "    0-346 > Loss: 0.69214582, Accuracy: 84.49%\n",
      "    0-347 > Loss: 0.69014194, Accuracy: 84.38%\n",
      "    0-348 > Loss: 0.69112347, Accuracy: 84.67%\n",
      "    0-349 > Loss: 0.68924754, Accuracy: 85.08%\n",
      "    0-350 > Loss: 0.68851238, Accuracy: 85.05%\n",
      "    0-351 > Loss: 0.68503918, Accuracy: 85.09%\n",
      "    0-352 > Loss: 0.68379522, Accuracy: 85.35%\n",
      "    0-353 > Loss: 0.68288153, Accuracy: 85.34%\n",
      "    0-354 > Loss: 0.67936245, Accuracy: 85.40%\n",
      "    0-355 > Loss: 0.68265277, Accuracy: 84.82%\n",
      "    0-356 > Loss: 0.68350135, Accuracy: 84.99%\n",
      "    0-357 > Loss: 0.68187865, Accuracy: 84.97%\n",
      "    0-358 > Loss: 0.68412358, Accuracy: 85.22%\n",
      "    0-359 > Loss: 0.68327754, Accuracy: 85.14%\n",
      "    0-360 > Loss: 0.68343553, Accuracy: 84.86%\n",
      "    0-361 > Loss: 0.68512699, Accuracy: 84.47%\n",
      "    0-362 > Loss: 0.68345991, Accuracy: 84.97%\n",
      "    0-363 > Loss: 0.68208552, Accuracy: 84.81%\n",
      "    0-364 > Loss: 0.68142456, Accuracy: 85.00%\n",
      "    0-365 > Loss: 0.68495782, Accuracy: 84.96%\n",
      "    0-366 > Loss: 0.68269039, Accuracy: 85.35%\n",
      "    0-367 > Loss: 0.68507460, Accuracy: 85.16%\n",
      "    0-368 > Loss: 0.68532317, Accuracy: 85.02%\n",
      "    0-369 > Loss: 0.68509674, Accuracy: 85.14%\n",
      "    0-370 > Loss: 0.68692080, Accuracy: 85.14%\n",
      "    0-371 > Loss: 0.68836034, Accuracy: 85.00%\n",
      "    0-372 > Loss: 0.69028511, Accuracy: 84.97%\n",
      "    0-373 > Loss: 0.68693361, Accuracy: 84.83%\n",
      "    0-374 > Loss: 0.68818594, Accuracy: 84.74%\n",
      "    0-375 > Loss: 0.68323244, Accuracy: 84.73%\n",
      "    0-376 > Loss: 0.68003083, Accuracy: 84.68%\n",
      "    0-377 > Loss: 0.67855934, Accuracy: 84.59%\n",
      "    0-378 > Loss: 0.67838230, Accuracy: 84.60%\n",
      "    0-379 > Loss: 0.67552935, Accuracy: 84.69%\n",
      "    0-380 > Loss: 0.67643407, Accuracy: 84.61%\n",
      "    0-381 > Loss: 0.67384990, Accuracy: 84.49%\n",
      "    0-382 > Loss: 0.67067083, Accuracy: 84.59%\n",
      "    0-383 > Loss: 0.66980069, Accuracy: 84.47%\n",
      "    0-384 > Loss: 0.66788618, Accuracy: 84.61%\n",
      "    0-385 > Loss: 0.66911853, Accuracy: 84.06%\n",
      "    0-386 > Loss: 0.66770596, Accuracy: 84.43%\n",
      "    0-387 > Loss: 0.66310512, Accuracy: 84.66%\n",
      "    0-388 > Loss: 0.66408252, Accuracy: 84.43%\n",
      "    0-389 > Loss: 0.66382910, Accuracy: 84.61%\n",
      "    0-390 > Loss: 0.66274777, Accuracy: 84.69%\n",
      "    0-391 > Loss: 0.65941946, Accuracy: 85.01%\n",
      "    0-392 > Loss: 0.65711942, Accuracy: 85.38%\n",
      "    0-393 > Loss: 0.65791253, Accuracy: 85.46%\n",
      "    0-394 > Loss: 0.65788056, Accuracy: 85.75%\n",
      "    0-395 > Loss: 0.66067281, Accuracy: 85.77%\n",
      "    0-396 > Loss: 0.66057660, Accuracy: 85.69%\n",
      "    0-397 > Loss: 0.65846756, Accuracy: 85.71%\n",
      "    0-398 > Loss: 0.65622157, Accuracy: 85.59%\n",
      "    0-399 > Loss: 0.65756063, Accuracy: 85.37%\n",
      "    0-400 > Loss: 0.65562963, Accuracy: 85.68%\n",
      "    0-401 > Loss: 0.65540252, Accuracy: 85.64%\n",
      "    0-402 > Loss: 0.65436813, Accuracy: 85.53%\n",
      "    0-403 > Loss: 0.65637108, Accuracy: 85.22%\n",
      "    0-404 > Loss: 0.65346317, Accuracy: 85.58%\n",
      "    0-405 > Loss: 0.65098276, Accuracy: 85.77%\n",
      "    0-406 > Loss: 0.65172372, Accuracy: 85.92%\n",
      "    0-407 > Loss: 0.65095570, Accuracy: 86.00%\n",
      "    0-408 > Loss: 0.65321853, Accuracy: 85.97%\n",
      "    0-409 > Loss: 0.65504985, Accuracy: 85.72%\n",
      "    0-410 > Loss: 0.65745561, Accuracy: 85.72%\n",
      "    0-411 > Loss: 0.65575630, Accuracy: 85.62%\n",
      "    0-412 > Loss: 0.65847440, Accuracy: 85.22%\n",
      "    0-413 > Loss: 0.65860312, Accuracy: 85.11%\n",
      "    0-414 > Loss: 0.65374730, Accuracy: 85.61%\n",
      "    0-415 > Loss: 0.65657417, Accuracy: 85.48%\n",
      "    0-416 > Loss: 0.64941174, Accuracy: 85.91%\n",
      "    0-417 > Loss: 0.64704987, Accuracy: 86.01%\n",
      "    0-418 > Loss: 0.64572094, Accuracy: 86.18%\n",
      "    0-419 > Loss: 0.64021833, Accuracy: 86.42%\n",
      "    0-420 > Loss: 0.63862304, Accuracy: 86.40%\n",
      "    0-421 > Loss: 0.63555269, Accuracy: 86.48%\n",
      "    0-422 > Loss: 0.63421293, Accuracy: 86.77%\n",
      "    0-423 > Loss: 0.63438898, Accuracy: 86.65%\n",
      "    0-424 > Loss: 0.63043559, Accuracy: 86.37%\n",
      "    0-425 > Loss: 0.63105080, Accuracy: 86.26%\n",
      "    0-426 > Loss: 0.63270211, Accuracy: 86.18%\n",
      "    0-427 > Loss: 0.63629631, Accuracy: 86.18%\n",
      "    0-428 > Loss: 0.63934578, Accuracy: 86.04%\n",
      "    0-429 > Loss: 0.63839645, Accuracy: 85.99%\n",
      "    0-430 > Loss: 0.63617876, Accuracy: 85.90%\n",
      "    0-431 > Loss: 0.63179719, Accuracy: 86.34%\n",
      "    0-432 > Loss: 0.63175922, Accuracy: 86.20%\n",
      "    0-433 > Loss: 0.62875475, Accuracy: 86.47%\n",
      "    0-434 > Loss: 0.62405145, Accuracy: 86.52%\n",
      "    0-435 > Loss: 0.62233821, Accuracy: 86.74%\n",
      "    0-436 > Loss: 0.62379515, Accuracy: 86.53%\n",
      "    0-437 > Loss: 0.62371646, Accuracy: 86.61%\n",
      "    0-438 > Loss: 0.62065469, Accuracy: 86.62%\n",
      "    0-439 > Loss: 0.62195432, Accuracy: 86.46%\n",
      "    0-440 > Loss: 0.62206162, Accuracy: 86.78%\n",
      "    0-441 > Loss: 0.62989688, Accuracy: 86.32%\n",
      "    0-442 > Loss: 0.62193719, Accuracy: 86.52%\n",
      "    0-443 > Loss: 0.61695039, Accuracy: 86.69%\n",
      "    0-444 > Loss: 0.61578708, Accuracy: 86.67%\n",
      "    0-445 > Loss: 0.61422538, Accuracy: 86.71%\n",
      "    0-446 > Loss: 0.61325846, Accuracy: 86.59%\n",
      "    0-447 > Loss: 0.61745424, Accuracy: 86.32%\n",
      "    0-448 > Loss: 0.61988812, Accuracy: 86.26%\n",
      "    0-449 > Loss: 0.61813352, Accuracy: 86.29%\n",
      "    0-450 > Loss: 0.61759505, Accuracy: 86.65%\n",
      "    0-451 > Loss: 0.61599751, Accuracy: 86.67%\n",
      "    0-452 > Loss: 0.61469090, Accuracy: 86.90%\n",
      "    0-453 > Loss: 0.61187884, Accuracy: 86.82%\n",
      "    0-454 > Loss: 0.60970702, Accuracy: 87.03%\n",
      "    0-455 > Loss: 0.60907283, Accuracy: 87.08%\n",
      "    0-456 > Loss: 0.60837556, Accuracy: 86.96%\n",
      "    0-457 > Loss: 0.60784922, Accuracy: 86.82%\n",
      "    0-458 > Loss: 0.61118034, Accuracy: 86.96%\n",
      "    0-459 > Loss: 0.61117252, Accuracy: 86.87%\n",
      "    0-460 > Loss: 0.61248142, Accuracy: 86.80%\n",
      "    0-461 > Loss: 0.61385269, Accuracy: 86.89%\n",
      "    0-462 > Loss: 0.61007301, Accuracy: 87.09%\n",
      "    0-463 > Loss: 0.60996863, Accuracy: 87.11%\n",
      "    0-464 > Loss: 0.61049015, Accuracy: 87.15%\n",
      "    0-465 > Loss: 0.60823061, Accuracy: 87.29%\n",
      "    0-466 > Loss: 0.60486382, Accuracy: 87.29%\n",
      "    0-467 > Loss: 0.60272270, Accuracy: 87.31%\n",
      "    0-468 > Loss: 0.60176567, Accuracy: 87.19%\n",
      "    0-469 > Loss: 0.59782666, Accuracy: 87.06%\n",
      "    0-470 > Loss: 0.59831065, Accuracy: 87.10%\n",
      "    0-471 > Loss: 0.59925557, Accuracy: 87.13%\n",
      "    0-472 > Loss: 0.59521610, Accuracy: 87.42%\n",
      "    0-473 > Loss: 0.59655222, Accuracy: 87.39%\n",
      "    0-474 > Loss: 0.59492204, Accuracy: 87.31%\n",
      "    0-475 > Loss: 0.59238658, Accuracy: 87.46%\n",
      "    0-476 > Loss: 0.59212108, Accuracy: 87.05%\n",
      "    0-477 > Loss: 0.59343394, Accuracy: 87.28%\n",
      "    0-478 > Loss: 0.59199719, Accuracy: 87.21%\n",
      "    0-479 > Loss: 0.59043763, Accuracy: 87.15%\n",
      "    0-480 > Loss: 0.58943599, Accuracy: 87.33%\n",
      "    0-481 > Loss: 0.58989016, Accuracy: 87.13%\n",
      "    0-482 > Loss: 0.58910481, Accuracy: 87.07%\n",
      "    0-483 > Loss: 0.59079691, Accuracy: 87.04%\n",
      "    0-484 > Loss: 0.59096728, Accuracy: 87.03%\n",
      "    0-485 > Loss: 0.59048032, Accuracy: 87.17%\n",
      "    0-486 > Loss: 0.59054608, Accuracy: 86.99%\n",
      "    0-487 > Loss: 0.58782179, Accuracy: 87.23%\n",
      "    0-488 > Loss: 0.58567474, Accuracy: 87.51%\n",
      "    0-489 > Loss: 0.58616583, Accuracy: 87.37%\n",
      "    0-490 > Loss: 0.58527116, Accuracy: 87.61%\n",
      "    0-491 > Loss: 0.58571853, Accuracy: 87.50%\n",
      "    0-492 > Loss: 0.58755384, Accuracy: 87.37%\n",
      "    0-493 > Loss: 0.58577918, Accuracy: 87.41%\n",
      "    0-494 > Loss: 0.58680999, Accuracy: 87.37%\n",
      "    0-495 > Loss: 0.58505930, Accuracy: 87.10%\n",
      "    0-496 > Loss: 0.58495305, Accuracy: 87.25%\n",
      "    0-497 > Loss: 0.58676415, Accuracy: 87.37%\n",
      "    0-498 > Loss: 0.58781758, Accuracy: 87.21%\n",
      "    0-499 > Loss: 0.58666023, Accuracy: 87.32%\n",
      "    0-500 > Loss: 0.58777156, Accuracy: 87.14%\n",
      "    0-501 > Loss: 0.58226120, Accuracy: 87.57%\n",
      "    0-502 > Loss: 0.58081174, Accuracy: 87.55%\n",
      "    0-503 > Loss: 0.58027519, Accuracy: 87.53%\n",
      "    0-504 > Loss: 0.57916354, Accuracy: 87.39%\n",
      "    0-505 > Loss: 0.58138847, Accuracy: 87.04%\n",
      "    0-506 > Loss: 0.58343393, Accuracy: 86.96%\n",
      "    0-507 > Loss: 0.58021566, Accuracy: 87.06%\n",
      "    0-508 > Loss: 0.57812765, Accuracy: 87.40%\n",
      "    0-509 > Loss: 0.57931537, Accuracy: 87.53%\n",
      "    0-510 > Loss: 0.57806187, Accuracy: 87.64%\n",
      "    0-511 > Loss: 0.57809335, Accuracy: 87.63%\n",
      "    0-512 > Loss: 0.57456728, Accuracy: 87.68%\n",
      "    0-513 > Loss: 0.57339600, Accuracy: 87.54%\n",
      "    0-514 > Loss: 0.57272607, Accuracy: 87.81%\n",
      "    0-515 > Loss: 0.57510954, Accuracy: 87.59%\n",
      "    0-516 > Loss: 0.57479485, Accuracy: 87.59%\n",
      "    0-517 > Loss: 0.57817332, Accuracy: 87.46%\n",
      "    0-518 > Loss: 0.57687958, Accuracy: 87.45%\n",
      "    0-519 > Loss: 0.57753553, Accuracy: 87.58%\n",
      "    0-520 > Loss: 0.57574614, Accuracy: 87.63%\n",
      "    0-521 > Loss: 0.57682598, Accuracy: 87.83%\n",
      "    0-522 > Loss: 0.57600573, Accuracy: 87.67%\n",
      "    0-523 > Loss: 0.57729564, Accuracy: 87.87%\n",
      "    0-524 > Loss: 0.57504353, Accuracy: 87.80%\n",
      "    0-525 > Loss: 0.57287105, Accuracy: 87.79%\n",
      "    0-526 > Loss: 0.57054637, Accuracy: 87.82%\n",
      "    0-527 > Loss: 0.57041263, Accuracy: 87.87%\n",
      "    0-528 > Loss: 0.56955173, Accuracy: 87.87%\n",
      "    0-529 > Loss: 0.57036489, Accuracy: 87.79%\n",
      "    0-530 > Loss: 0.56891969, Accuracy: 87.85%\n",
      "    0-531 > Loss: 0.57187340, Accuracy: 87.63%\n",
      "    0-532 > Loss: 0.56896108, Accuracy: 87.97%\n",
      "    0-533 > Loss: 0.56874242, Accuracy: 87.93%\n",
      "    0-534 > Loss: 0.57319734, Accuracy: 87.59%\n",
      "    0-535 > Loss: 0.57621751, Accuracy: 87.50%\n",
      "    0-536 > Loss: 0.57642381, Accuracy: 87.53%\n",
      "    0-537 > Loss: 0.57443309, Accuracy: 87.34%\n",
      "    0-538 > Loss: 0.57422576, Accuracy: 87.55%\n",
      "    0-539 > Loss: 0.57284590, Accuracy: 87.55%\n",
      "    0-540 > Loss: 0.57156861, Accuracy: 87.53%\n",
      "    0-541 > Loss: 0.57441539, Accuracy: 87.22%\n",
      "    0-542 > Loss: 0.57174963, Accuracy: 87.34%\n",
      "    0-543 > Loss: 0.57177017, Accuracy: 87.45%\n",
      "    0-544 > Loss: 0.57019792, Accuracy: 87.49%\n",
      "    0-545 > Loss: 0.56929755, Accuracy: 87.47%\n",
      "    0-546 > Loss: 0.56722870, Accuracy: 87.47%\n",
      "    0-547 > Loss: 0.56517744, Accuracy: 87.68%\n",
      "    0-548 > Loss: 0.56598992, Accuracy: 87.98%\n",
      "    0-549 > Loss: 0.57046569, Accuracy: 87.72%\n",
      "    0-550 > Loss: 0.57109722, Accuracy: 87.47%\n",
      "    0-551 > Loss: 0.56824229, Accuracy: 87.62%\n",
      "    0-552 > Loss: 0.56590159, Accuracy: 87.91%\n",
      "    0-553 > Loss: 0.56589013, Accuracy: 87.75%\n",
      "    0-554 > Loss: 0.56341963, Accuracy: 88.04%\n",
      "    0-555 > Loss: 0.56392235, Accuracy: 87.72%\n",
      "    0-556 > Loss: 0.56207319, Accuracy: 87.65%\n",
      "    0-557 > Loss: 0.56170439, Accuracy: 87.74%\n",
      "    0-558 > Loss: 0.55991812, Accuracy: 87.92%\n",
      "    0-559 > Loss: 0.55920574, Accuracy: 87.86%\n",
      "    0-560 > Loss: 0.55786778, Accuracy: 87.93%\n",
      "    0-561 > Loss: 0.55459233, Accuracy: 87.78%\n",
      "    0-562 > Loss: 0.55484408, Accuracy: 87.76%\n",
      "    0-563 > Loss: 0.55560631, Accuracy: 87.74%\n",
      "    0-564 > Loss: 0.55657538, Accuracy: 87.60%\n",
      "    0-565 > Loss: 0.55687320, Accuracy: 87.73%\n",
      "    0-566 > Loss: 0.55574665, Accuracy: 87.86%\n",
      "    0-567 > Loss: 0.55416728, Accuracy: 87.88%\n",
      "    0-568 > Loss: 0.55432601, Accuracy: 87.83%\n",
      "    0-569 > Loss: 0.55189627, Accuracy: 88.03%\n",
      "    0-570 > Loss: 0.55239529, Accuracy: 87.65%\n",
      "    0-571 > Loss: 0.55183612, Accuracy: 87.72%\n",
      "    0-572 > Loss: 0.55039215, Accuracy: 87.63%\n",
      "    0-573 > Loss: 0.55031189, Accuracy: 87.66%\n",
      "    0-574 > Loss: 0.54865150, Accuracy: 87.74%\n",
      "    0-575 > Loss: 0.55054280, Accuracy: 87.86%\n",
      "    0-576 > Loss: 0.54831290, Accuracy: 87.66%\n",
      "    0-577 > Loss: 0.55252178, Accuracy: 87.48%\n",
      "    0-578 > Loss: 0.55013673, Accuracy: 87.31%\n",
      "    0-579 > Loss: 0.54746550, Accuracy: 87.68%\n",
      "    0-580 > Loss: 0.54568879, Accuracy: 87.97%\n",
      "    0-581 > Loss: 0.54617087, Accuracy: 87.84%\n",
      "    0-582 > Loss: 0.54737841, Accuracy: 87.71%\n",
      "    0-583 > Loss: 0.54603456, Accuracy: 87.97%\n",
      "    0-584 > Loss: 0.54557822, Accuracy: 88.07%\n",
      "    0-585 > Loss: 0.54312049, Accuracy: 88.09%\n",
      "    0-586 > Loss: 0.54214197, Accuracy: 87.99%\n",
      "    0-587 > Loss: 0.54129923, Accuracy: 88.03%\n",
      "    0-588 > Loss: 0.53989220, Accuracy: 87.87%\n",
      "    0-589 > Loss: 0.53890572, Accuracy: 87.78%\n",
      "    0-590 > Loss: 0.53571451, Accuracy: 87.88%\n",
      "    0-591 > Loss: 0.53454703, Accuracy: 88.00%\n",
      "    0-592 > Loss: 0.53216657, Accuracy: 87.95%\n",
      "    0-593 > Loss: 0.53145885, Accuracy: 88.06%\n",
      "    0-594 > Loss: 0.53258597, Accuracy: 87.94%\n",
      "    0-595 > Loss: 0.53064685, Accuracy: 88.03%\n",
      "    0-596 > Loss: 0.53112014, Accuracy: 88.03%\n",
      "    0-597 > Loss: 0.53184727, Accuracy: 87.84%\n",
      "    0-598 > Loss: 0.53166322, Accuracy: 88.05%\n",
      "    0-599 > Loss: 0.52989518, Accuracy: 88.05%\n",
      "    0-600 > Loss: 0.52911464, Accuracy: 87.91%\n",
      "    0-601 > Loss: 0.52950893, Accuracy: 87.84%\n",
      "    0-602 > Loss: 0.52964659, Accuracy: 87.95%\n",
      "    0-603 > Loss: 0.52855994, Accuracy: 87.94%\n",
      "    0-604 > Loss: 0.52772698, Accuracy: 87.83%\n",
      "    0-605 > Loss: 0.52848849, Accuracy: 87.99%\n",
      "    0-606 > Loss: 0.52853162, Accuracy: 87.92%\n",
      "    0-607 > Loss: 0.52996946, Accuracy: 87.58%\n",
      "    0-608 > Loss: 0.52734036, Accuracy: 87.82%\n",
      "    0-609 > Loss: 0.52707050, Accuracy: 87.73%\n",
      "    0-610 > Loss: 0.52958305, Accuracy: 87.57%\n",
      "    0-611 > Loss: 0.52966504, Accuracy: 87.46%\n",
      "    0-612 > Loss: 0.52753667, Accuracy: 87.89%\n",
      "    0-613 > Loss: 0.52792831, Accuracy: 87.60%\n",
      "    0-614 > Loss: 0.52456576, Accuracy: 87.98%\n",
      "    0-615 > Loss: 0.52280939, Accuracy: 88.01%\n",
      "    0-616 > Loss: 0.52223162, Accuracy: 87.87%\n",
      "    0-617 > Loss: 0.52239309, Accuracy: 88.13%\n",
      "    0-618 > Loss: 0.52187371, Accuracy: 88.07%\n",
      "    0-619 > Loss: 0.52138245, Accuracy: 88.14%\n",
      "    0-620 > Loss: 0.52264559, Accuracy: 88.27%\n",
      "    0-621 > Loss: 0.52242990, Accuracy: 88.16%\n",
      "    0-622 > Loss: 0.52461599, Accuracy: 88.22%\n",
      "    0-623 > Loss: 0.52558512, Accuracy: 88.14%\n",
      "    0-624 > Loss: 0.52642826, Accuracy: 88.18%\n",
      "    0-625 > Loss: 0.52788660, Accuracy: 88.25%\n",
      "    0-626 > Loss: 0.53072966, Accuracy: 88.16%\n",
      "    0-627 > Loss: 0.53310567, Accuracy: 88.07%\n",
      "    0-628 > Loss: 0.53107636, Accuracy: 88.08%\n",
      "    0-629 > Loss: 0.52940090, Accuracy: 88.04%\n",
      "    0-630 > Loss: 0.52760799, Accuracy: 88.24%\n",
      "    0-631 > Loss: 0.52779564, Accuracy: 88.26%\n",
      "    0-632 > Loss: 0.52549945, Accuracy: 88.50%\n",
      "    0-633 > Loss: 0.52215225, Accuracy: 88.55%\n",
      "    0-634 > Loss: 0.52032212, Accuracy: 88.36%\n",
      "    0-635 > Loss: 0.52131202, Accuracy: 88.44%\n",
      "    0-636 > Loss: 0.51995156, Accuracy: 88.45%\n",
      "    0-637 > Loss: 0.51715198, Accuracy: 88.59%\n",
      "    0-638 > Loss: 0.51576776, Accuracy: 88.47%\n",
      "    0-639 > Loss: 0.51418516, Accuracy: 88.50%\n",
      "    0-640 > Loss: 0.51380976, Accuracy: 88.43%\n",
      "    0-641 > Loss: 0.51401666, Accuracy: 88.51%\n",
      "    0-642 > Loss: 0.51438563, Accuracy: 88.41%\n",
      "    0-643 > Loss: 0.51437135, Accuracy: 88.44%\n",
      "    0-644 > Loss: 0.51229510, Accuracy: 88.39%\n",
      "    0-645 > Loss: 0.51241664, Accuracy: 88.41%\n",
      "    0-646 > Loss: 0.51689395, Accuracy: 88.22%\n",
      "    0-647 > Loss: 0.51635851, Accuracy: 88.31%\n",
      "    0-648 > Loss: 0.51433968, Accuracy: 88.34%\n",
      "    0-649 > Loss: 0.51366175, Accuracy: 88.20%\n",
      "    0-650 > Loss: 0.51381936, Accuracy: 88.27%\n",
      "    0-651 > Loss: 0.51221718, Accuracy: 88.29%\n",
      "    0-652 > Loss: 0.50976428, Accuracy: 88.35%\n",
      "    0-653 > Loss: 0.51052692, Accuracy: 88.12%\n",
      "    0-654 > Loss: 0.51152552, Accuracy: 88.13%\n",
      "    0-655 > Loss: 0.51116645, Accuracy: 88.24%\n",
      "    0-656 > Loss: 0.50887555, Accuracy: 88.38%\n",
      "    0-657 > Loss: 0.50878677, Accuracy: 88.32%\n",
      "    0-658 > Loss: 0.50726723, Accuracy: 88.29%\n",
      "    0-659 > Loss: 0.50666722, Accuracy: 88.09%\n",
      "    0-660 > Loss: 0.50645916, Accuracy: 88.15%\n",
      "    0-661 > Loss: 0.50668897, Accuracy: 88.09%\n",
      "    0-662 > Loss: 0.50885782, Accuracy: 88.12%\n",
      "    0-663 > Loss: 0.50749016, Accuracy: 88.25%\n",
      "    0-664 > Loss: 0.50352097, Accuracy: 88.46%\n",
      "    0-665 > Loss: 0.50435811, Accuracy: 88.40%\n",
      "    0-666 > Loss: 0.50480288, Accuracy: 88.49%\n",
      "    0-667 > Loss: 0.50650256, Accuracy: 88.49%\n",
      "    0-668 > Loss: 0.51095278, Accuracy: 88.23%\n",
      "    0-669 > Loss: 0.50891828, Accuracy: 88.38%\n",
      "    0-670 > Loss: 0.50964732, Accuracy: 88.44%\n",
      "    0-671 > Loss: 0.50837145, Accuracy: 88.39%\n",
      "    0-672 > Loss: 0.50953527, Accuracy: 88.75%\n",
      "    0-673 > Loss: 0.50680334, Accuracy: 88.90%\n",
      "    0-674 > Loss: 0.50636703, Accuracy: 88.86%\n",
      "    0-675 > Loss: 0.50505626, Accuracy: 88.79%\n",
      "    0-676 > Loss: 0.50455937, Accuracy: 88.74%\n",
      "    0-677 > Loss: 0.50265479, Accuracy: 88.80%\n",
      "    0-678 > Loss: 0.50204221, Accuracy: 88.80%\n",
      "    0-679 > Loss: 0.50321627, Accuracy: 89.05%\n",
      "    0-680 > Loss: 0.50238141, Accuracy: 89.07%\n",
      "    0-681 > Loss: 0.50219835, Accuracy: 88.98%\n",
      "    0-682 > Loss: 0.49902537, Accuracy: 88.74%\n",
      "    0-683 > Loss: 0.49891691, Accuracy: 88.91%\n",
      "    0-684 > Loss: 0.50036927, Accuracy: 88.76%\n",
      "    0-685 > Loss: 0.49782623, Accuracy: 88.92%\n",
      "    0-686 > Loss: 0.49518935, Accuracy: 88.90%\n",
      "    0-687 > Loss: 0.49418001, Accuracy: 88.71%\n",
      "    0-688 > Loss: 0.49438464, Accuracy: 88.64%\n",
      "    0-689 > Loss: 0.49299547, Accuracy: 88.66%\n",
      "    0-690 > Loss: 0.49456455, Accuracy: 88.92%\n",
      "    0-691 > Loss: 0.49441551, Accuracy: 88.88%\n",
      "    0-692 > Loss: 0.49400074, Accuracy: 89.05%\n",
      "    0-693 > Loss: 0.49468124, Accuracy: 88.95%\n",
      "    0-694 > Loss: 0.49523968, Accuracy: 88.94%\n",
      "    0-695 > Loss: 0.49645981, Accuracy: 88.74%\n",
      "    0-696 > Loss: 0.49578580, Accuracy: 88.76%\n",
      "    0-697 > Loss: 0.49675603, Accuracy: 88.67%\n",
      "    0-698 > Loss: 0.49562877, Accuracy: 88.59%\n",
      "    0-699 > Loss: 0.49480955, Accuracy: 88.56%\n",
      "    0-700 > Loss: 0.49381151, Accuracy: 88.63%\n",
      "    0-701 > Loss: 0.49312036, Accuracy: 88.65%\n",
      "    0-702 > Loss: 0.49356013, Accuracy: 88.57%\n",
      "    0-703 > Loss: 0.49287919, Accuracy: 88.60%\n",
      "    0-704 > Loss: 0.49212399, Accuracy: 88.64%\n",
      "    0-705 > Loss: 0.49700275, Accuracy: 88.35%\n",
      "    0-706 > Loss: 0.49706270, Accuracy: 88.35%\n",
      "    0-707 > Loss: 0.49880670, Accuracy: 88.32%\n",
      "    0-708 > Loss: 0.49996539, Accuracy: 88.17%\n",
      "    0-709 > Loss: 0.49790476, Accuracy: 88.17%\n",
      "    0-710 > Loss: 0.49606604, Accuracy: 88.38%\n",
      "    0-711 > Loss: 0.49626193, Accuracy: 88.36%\n",
      "    0-712 > Loss: 0.49480709, Accuracy: 88.38%\n",
      "    0-713 > Loss: 0.49645285, Accuracy: 88.30%\n",
      "    0-714 > Loss: 0.49408959, Accuracy: 88.14%\n",
      "    0-715 > Loss: 0.49777736, Accuracy: 87.93%\n",
      "    0-716 > Loss: 0.49433562, Accuracy: 87.96%\n",
      "    0-717 > Loss: 0.49423664, Accuracy: 87.95%\n",
      "    0-718 > Loss: 0.49234015, Accuracy: 87.89%\n",
      "    0-719 > Loss: 0.49446199, Accuracy: 88.05%\n",
      "    0-720 > Loss: 0.49414675, Accuracy: 88.22%\n",
      "    0-721 > Loss: 0.49119481, Accuracy: 88.56%\n",
      "    0-722 > Loss: 0.49087160, Accuracy: 88.60%\n",
      "    0-723 > Loss: 0.48778864, Accuracy: 88.56%\n",
      "    0-724 > Loss: 0.48354935, Accuracy: 88.84%\n",
      "    0-725 > Loss: 0.48235624, Accuracy: 88.91%\n",
      "    0-726 > Loss: 0.48344221, Accuracy: 88.77%\n",
      "    0-727 > Loss: 0.48271724, Accuracy: 88.72%\n",
      "    0-728 > Loss: 0.48435659, Accuracy: 88.57%\n",
      "    0-729 > Loss: 0.48273440, Accuracy: 88.72%\n",
      "    0-730 > Loss: 0.48491057, Accuracy: 88.71%\n",
      "    0-731 > Loss: 0.48532414, Accuracy: 88.91%\n",
      "    0-732 > Loss: 0.48439871, Accuracy: 88.87%\n",
      "    0-733 > Loss: 0.48387613, Accuracy: 88.84%\n",
      "    0-734 > Loss: 0.48461857, Accuracy: 88.87%\n",
      "    0-735 > Loss: 0.48205126, Accuracy: 88.85%\n",
      "    0-736 > Loss: 0.48181336, Accuracy: 88.77%\n",
      "    0-737 > Loss: 0.48031954, Accuracy: 88.99%\n",
      "    0-738 > Loss: 0.48079957, Accuracy: 88.88%\n",
      "    0-739 > Loss: 0.47976536, Accuracy: 89.04%\n",
      "    0-740 > Loss: 0.47804402, Accuracy: 89.05%\n",
      "    0-741 > Loss: 0.47976568, Accuracy: 88.95%\n",
      "    0-742 > Loss: 0.47833490, Accuracy: 88.85%\n",
      "    0-743 > Loss: 0.47687052, Accuracy: 89.05%\n",
      "    0-744 > Loss: 0.48031684, Accuracy: 89.01%\n",
      "    0-745 > Loss: 0.48193358, Accuracy: 89.04%\n",
      "    0-746 > Loss: 0.47985351, Accuracy: 88.97%\n",
      "    0-747 > Loss: 0.48280289, Accuracy: 88.96%\n",
      "    0-748 > Loss: 0.48350758, Accuracy: 88.96%\n",
      "    0-749 > Loss: 0.48151556, Accuracy: 89.10%\n",
      "    0-750 > Loss: 0.48172351, Accuracy: 89.35%\n",
      "    0-751 > Loss: 0.48292926, Accuracy: 89.15%\n",
      "    0-752 > Loss: 0.48186718, Accuracy: 89.34%\n",
      "    0-753 > Loss: 0.48001982, Accuracy: 89.27%\n",
      "    0-754 > Loss: 0.47816743, Accuracy: 89.22%\n",
      "    0-755 > Loss: 0.47823567, Accuracy: 89.06%\n",
      "    0-756 > Loss: 0.47986814, Accuracy: 89.14%\n",
      "    0-757 > Loss: 0.47979504, Accuracy: 89.05%\n",
      "    0-758 > Loss: 0.47961291, Accuracy: 89.05%\n",
      "    0-759 > Loss: 0.47855880, Accuracy: 89.20%\n",
      "    0-760 > Loss: 0.47900724, Accuracy: 89.18%\n",
      "    0-761 > Loss: 0.47684330, Accuracy: 89.34%\n",
      "    0-762 > Loss: 0.47637235, Accuracy: 89.29%\n",
      "    0-763 > Loss: 0.47705482, Accuracy: 89.19%\n",
      "    0-764 > Loss: 0.47779180, Accuracy: 89.05%\n",
      "    0-765 > Loss: 0.47770526, Accuracy: 89.04%\n",
      "    0-766 > Loss: 0.47701936, Accuracy: 88.87%\n",
      "    0-767 > Loss: 0.47659881, Accuracy: 88.85%\n",
      "    0-768 > Loss: 0.47752146, Accuracy: 88.90%\n",
      "    0-769 > Loss: 0.47844481, Accuracy: 89.12%\n",
      "    0-770 > Loss: 0.47706642, Accuracy: 89.22%\n",
      "    0-771 > Loss: 0.47596814, Accuracy: 89.24%\n",
      "    0-772 > Loss: 0.48015716, Accuracy: 89.18%\n",
      "    0-773 > Loss: 0.48048564, Accuracy: 89.11%\n",
      "    0-774 > Loss: 0.48092193, Accuracy: 89.13%\n",
      "    0-775 > Loss: 0.48018632, Accuracy: 89.22%\n",
      "    0-776 > Loss: 0.48016456, Accuracy: 89.23%\n",
      "    0-777 > Loss: 0.48052058, Accuracy: 89.00%\n",
      "    0-778 > Loss: 0.48268972, Accuracy: 89.11%\n",
      "    0-779 > Loss: 0.48013732, Accuracy: 89.14%\n",
      "    0-780 > Loss: 0.48386560, Accuracy: 88.88%\n",
      "    0-781 > Loss: 0.48433991, Accuracy: 88.93%\n",
      "    0-782 > Loss: 0.48477885, Accuracy: 88.88%\n",
      "    0-783 > Loss: 0.48430043, Accuracy: 89.03%\n",
      "    0-784 > Loss: 0.48338357, Accuracy: 89.06%\n",
      "    0-785 > Loss: 0.48219391, Accuracy: 89.14%\n",
      "    0-786 > Loss: 0.47983799, Accuracy: 89.01%\n",
      "    0-787 > Loss: 0.47792367, Accuracy: 89.17%\n",
      "    0-788 > Loss: 0.48053536, Accuracy: 89.02%\n",
      "    0-789 > Loss: 0.47979278, Accuracy: 89.10%\n",
      "    0-790 > Loss: 0.47986074, Accuracy: 89.15%\n",
      "    0-791 > Loss: 0.48146348, Accuracy: 89.26%\n",
      "    0-792 > Loss: 0.47904082, Accuracy: 89.42%\n",
      "    0-793 > Loss: 0.47975600, Accuracy: 89.28%\n",
      "    0-794 > Loss: 0.47949575, Accuracy: 89.25%\n",
      "    0-795 > Loss: 0.47742761, Accuracy: 89.35%\n",
      "    0-796 > Loss: 0.47864076, Accuracy: 89.35%\n",
      "    0-797 > Loss: 0.47736246, Accuracy: 89.45%\n",
      "    0-798 > Loss: 0.47757691, Accuracy: 89.34%\n",
      "    0-799 > Loss: 0.47656630, Accuracy: 89.54%\n",
      "    0-800 > Loss: 0.47557696, Accuracy: 89.47%\n",
      "    0-801 > Loss: 0.47480345, Accuracy: 89.45%\n",
      "    0-802 > Loss: 0.47663145, Accuracy: 89.36%\n",
      "    0-803 > Loss: 0.47546486, Accuracy: 89.42%\n",
      "    0-804 > Loss: 0.47346570, Accuracy: 89.41%\n",
      "    0-805 > Loss: 0.47413962, Accuracy: 89.36%\n",
      "    0-806 > Loss: 0.47355498, Accuracy: 89.38%\n",
      "    0-807 > Loss: 0.47206880, Accuracy: 89.41%\n",
      "    0-808 > Loss: 0.47151522, Accuracy: 89.30%\n",
      "    0-809 > Loss: 0.46840661, Accuracy: 89.36%\n",
      "    0-810 > Loss: 0.46780122, Accuracy: 89.53%\n",
      "    0-811 > Loss: 0.46907397, Accuracy: 89.65%\n",
      "    0-812 > Loss: 0.46785425, Accuracy: 89.55%\n",
      "    0-813 > Loss: 0.47106860, Accuracy: 89.75%\n",
      "    0-814 > Loss: 0.46898433, Accuracy: 89.81%\n",
      "    0-815 > Loss: 0.46808201, Accuracy: 89.86%\n",
      "    0-816 > Loss: 0.46922418, Accuracy: 89.59%\n",
      "    0-817 > Loss: 0.46810381, Accuracy: 89.59%\n",
      "    0-818 > Loss: 0.47002059, Accuracy: 89.70%\n",
      "    0-819 > Loss: 0.46623098, Accuracy: 89.69%\n",
      "    0-820 > Loss: 0.46545618, Accuracy: 89.55%\n",
      "    0-821 > Loss: 0.46449508, Accuracy: 89.32%\n",
      "    0-822 > Loss: 0.46485940, Accuracy: 89.13%\n",
      "    0-823 > Loss: 0.46389123, Accuracy: 89.38%\n",
      "    0-824 > Loss: 0.46465803, Accuracy: 89.39%\n",
      "    0-825 > Loss: 0.46547950, Accuracy: 89.38%\n",
      "    0-826 > Loss: 0.46555422, Accuracy: 89.38%\n",
      "    0-827 > Loss: 0.46700392, Accuracy: 89.26%\n",
      "    0-828 > Loss: 0.46665308, Accuracy: 89.28%\n",
      "    0-829 > Loss: 0.46532975, Accuracy: 89.22%\n",
      "    0-830 > Loss: 0.46580995, Accuracy: 89.05%\n",
      "    0-831 > Loss: 0.46549256, Accuracy: 88.97%\n",
      "    0-832 > Loss: 0.46743973, Accuracy: 89.00%\n",
      "    0-833 > Loss: 0.46467950, Accuracy: 89.06%\n",
      "    0-834 > Loss: 0.46347258, Accuracy: 89.07%\n",
      "    0-835 > Loss: 0.46546412, Accuracy: 89.13%\n",
      "    0-836 > Loss: 0.46739608, Accuracy: 89.28%\n",
      "    0-837 > Loss: 0.46782669, Accuracy: 89.22%\n",
      "    0-838 > Loss: 0.46473013, Accuracy: 89.20%\n",
      "    0-839 > Loss: 0.46363405, Accuracy: 89.03%\n",
      "    0-840 > Loss: 0.46344257, Accuracy: 89.17%\n",
      "    0-841 > Loss: 0.46303794, Accuracy: 89.25%\n",
      "    0-842 > Loss: 0.46113516, Accuracy: 89.44%\n",
      "    0-843 > Loss: 0.46084346, Accuracy: 89.39%\n",
      "    0-844 > Loss: 0.45960373, Accuracy: 89.41%\n",
      "    0-845 > Loss: 0.45974871, Accuracy: 89.39%\n",
      "    0-846 > Loss: 0.46146638, Accuracy: 89.44%\n",
      "    0-847 > Loss: 0.46026283, Accuracy: 89.46%\n",
      "    0-848 > Loss: 0.46202122, Accuracy: 89.35%\n",
      "    0-849 > Loss: 0.46361163, Accuracy: 89.28%\n",
      "    0-850 > Loss: 0.46433232, Accuracy: 89.01%\n",
      "    0-851 > Loss: 0.46295560, Accuracy: 89.29%\n",
      "    0-852 > Loss: 0.46312125, Accuracy: 89.32%\n",
      "    0-853 > Loss: 0.46257074, Accuracy: 89.14%\n",
      "    0-854 > Loss: 0.46286195, Accuracy: 89.17%\n",
      "    0-855 > Loss: 0.46041533, Accuracy: 89.17%\n",
      "    0-856 > Loss: 0.46198755, Accuracy: 89.15%\n",
      "    0-857 > Loss: 0.46572156, Accuracy: 89.05%\n",
      "    0-858 > Loss: 0.46531855, Accuracy: 89.15%\n",
      "    0-859 > Loss: 0.46693284, Accuracy: 89.15%\n",
      "    0-860 > Loss: 0.46750081, Accuracy: 89.26%\n",
      "    0-861 > Loss: 0.46646636, Accuracy: 89.25%\n",
      "    0-862 > Loss: 0.46566956, Accuracy: 89.22%\n",
      "    0-863 > Loss: 0.46477428, Accuracy: 89.28%\n",
      "    0-864 > Loss: 0.46350512, Accuracy: 89.24%\n",
      "    0-865 > Loss: 0.46114858, Accuracy: 89.32%\n",
      "    0-866 > Loss: 0.46041348, Accuracy: 89.22%\n",
      "    0-867 > Loss: 0.45992269, Accuracy: 89.36%\n",
      "    0-868 > Loss: 0.45658024, Accuracy: 89.49%\n",
      "    0-869 > Loss: 0.45856503, Accuracy: 89.37%\n",
      "    0-870 > Loss: 0.45768253, Accuracy: 89.29%\n",
      "    0-871 > Loss: 0.45886419, Accuracy: 89.19%\n",
      "    0-872 > Loss: 0.45797017, Accuracy: 89.33%\n",
      "    0-873 > Loss: 0.45719744, Accuracy: 89.46%\n",
      "    0-874 > Loss: 0.45700354, Accuracy: 89.62%\n",
      "    0-875 > Loss: 0.45648584, Accuracy: 89.58%\n",
      "    0-876 > Loss: 0.45665216, Accuracy: 89.40%\n",
      "    0-877 > Loss: 0.45689688, Accuracy: 89.45%\n",
      "    0-878 > Loss: 0.45586134, Accuracy: 89.55%\n",
      "    0-879 > Loss: 0.45612147, Accuracy: 89.43%\n",
      "    0-880 > Loss: 0.45619811, Accuracy: 89.47%\n",
      "    0-881 > Loss: 0.45973815, Accuracy: 89.26%\n",
      "    0-882 > Loss: 0.45676119, Accuracy: 89.27%\n",
      "    0-883 > Loss: 0.45702907, Accuracy: 89.16%\n",
      "    0-884 > Loss: 0.45611388, Accuracy: 89.21%\n",
      "    0-885 > Loss: 0.45609314, Accuracy: 89.23%\n",
      "    0-886 > Loss: 0.45359548, Accuracy: 89.51%\n",
      "    0-887 > Loss: 0.45494712, Accuracy: 89.39%\n",
      "    0-888 > Loss: 0.45285074, Accuracy: 89.37%\n",
      "    0-889 > Loss: 0.45376116, Accuracy: 89.45%\n",
      "    0-890 > Loss: 0.45246360, Accuracy: 89.34%\n",
      "    0-891 > Loss: 0.45304435, Accuracy: 89.37%\n",
      "    0-892 > Loss: 0.45291223, Accuracy: 89.32%\n",
      "    0-893 > Loss: 0.45131855, Accuracy: 89.55%\n",
      "    0-894 > Loss: 0.44781675, Accuracy: 89.71%\n",
      "    0-895 > Loss: 0.44570501, Accuracy: 89.77%\n",
      "    0-896 > Loss: 0.44550430, Accuracy: 89.68%\n",
      "    0-897 > Loss: 0.44585627, Accuracy: 89.70%\n",
      "    0-898 > Loss: 0.44558212, Accuracy: 89.68%\n",
      "    0-899 > Loss: 0.44594584, Accuracy: 89.67%\n",
      "    0-900 > Loss: 0.44525714, Accuracy: 89.80%\n",
      "    0-901 > Loss: 0.44300984, Accuracy: 89.82%\n",
      "    0-902 > Loss: 0.44257976, Accuracy: 89.69%\n",
      "    0-903 > Loss: 0.44184444, Accuracy: 89.73%\n",
      "    0-904 > Loss: 0.44229398, Accuracy: 89.56%\n",
      "    0-905 > Loss: 0.44187473, Accuracy: 89.53%\n",
      "    0-906 > Loss: 0.44036884, Accuracy: 89.54%\n",
      "    0-907 > Loss: 0.44131365, Accuracy: 89.62%\n",
      "    0-908 > Loss: 0.44094455, Accuracy: 89.48%\n",
      "    0-909 > Loss: 0.44202729, Accuracy: 89.59%\n",
      "    0-910 > Loss: 0.44176263, Accuracy: 89.70%\n",
      "    0-911 > Loss: 0.44317347, Accuracy: 89.81%\n",
      "    0-912 > Loss: 0.44402788, Accuracy: 89.79%\n",
      "    0-913 > Loss: 0.44469570, Accuracy: 89.74%\n",
      "    0-914 > Loss: 0.44422970, Accuracy: 89.69%\n",
      "    0-915 > Loss: 0.44564886, Accuracy: 89.65%\n",
      "    0-916 > Loss: 0.44538177, Accuracy: 89.61%\n",
      "    0-917 > Loss: 0.44554078, Accuracy: 89.68%\n",
      "    0-918 > Loss: 0.44546480, Accuracy: 89.64%\n",
      "    0-919 > Loss: 0.44604482, Accuracy: 89.67%\n",
      "    0-920 > Loss: 0.44483550, Accuracy: 89.81%\n",
      "    0-921 > Loss: 0.44570410, Accuracy: 89.90%\n",
      "    0-922 > Loss: 0.44780351, Accuracy: 89.73%\n",
      "    0-923 > Loss: 0.44614643, Accuracy: 89.77%\n",
      "    0-924 > Loss: 0.44514358, Accuracy: 89.83%\n",
      "    0-925 > Loss: 0.44315842, Accuracy: 89.89%\n",
      "    0-926 > Loss: 0.44128287, Accuracy: 89.85%\n",
      "    0-927 > Loss: 0.44128281, Accuracy: 89.78%\n",
      "    0-928 > Loss: 0.44092705, Accuracy: 89.69%\n",
      "    0-929 > Loss: 0.43976620, Accuracy: 89.76%\n",
      "    0-930 > Loss: 0.44287026, Accuracy: 89.79%\n",
      "    0-931 > Loss: 0.44252384, Accuracy: 89.73%\n",
      "    0-932 > Loss: 0.44411197, Accuracy: 89.46%\n",
      "    0-933 > Loss: 0.44579422, Accuracy: 89.40%\n",
      "    0-934 > Loss: 0.44589406, Accuracy: 89.38%\n",
      "    0-935 > Loss: 0.44679022, Accuracy: 89.41%\n",
      "    0-936 > Loss: 0.44582385, Accuracy: 89.46%\n",
      "    0-937 > Loss: 0.44600486, Accuracy: 89.42%\n",
      "    0-938 > Loss: 0.44520125, Accuracy: 89.47%\n",
      "    0-939 > Loss: 0.44547808, Accuracy: 89.30%\n",
      "    0-940 > Loss: 0.44480827, Accuracy: 89.22%\n",
      "    0-941 > Loss: 0.44766309, Accuracy: 89.20%\n",
      "    0-942 > Loss: 0.44961116, Accuracy: 89.04%\n",
      "    0-943 > Loss: 0.45038780, Accuracy: 88.97%\n",
      "    0-944 > Loss: 0.44860705, Accuracy: 89.11%\n",
      "    0-945 > Loss: 0.44900296, Accuracy: 89.22%\n",
      "    0-946 > Loss: 0.44824900, Accuracy: 89.14%\n",
      "    0-947 > Loss: 0.44908405, Accuracy: 89.46%\n",
      "    0-948 > Loss: 0.44758311, Accuracy: 89.32%\n",
      "    0-949 > Loss: 0.44595868, Accuracy: 89.35%\n",
      "    0-950 > Loss: 0.44809093, Accuracy: 89.51%\n",
      "    0-951 > Loss: 0.44830626, Accuracy: 89.32%\n",
      "    0-952 > Loss: 0.44354307, Accuracy: 89.48%\n",
      "    0-953 > Loss: 0.44375741, Accuracy: 89.29%\n",
      "    0-954 > Loss: 0.44227494, Accuracy: 89.59%\n",
      "    0-955 > Loss: 0.44016606, Accuracy: 89.80%\n",
      "    0-956 > Loss: 0.43961551, Accuracy: 89.77%\n",
      "    0-957 > Loss: 0.43955576, Accuracy: 89.79%\n",
      "    0-958 > Loss: 0.44308797, Accuracy: 89.68%\n",
      "    0-959 > Loss: 0.44253964, Accuracy: 89.63%\n",
      "    0-960 > Loss: 0.44112529, Accuracy: 89.65%\n",
      "    0-961 > Loss: 0.44433594, Accuracy: 89.48%\n",
      "    0-962 > Loss: 0.44629093, Accuracy: 89.53%\n",
      "    0-963 > Loss: 0.44803916, Accuracy: 89.50%\n",
      "    0-964 > Loss: 0.44740582, Accuracy: 89.45%\n",
      "    0-965 > Loss: 0.44579809, Accuracy: 89.78%\n",
      "    0-966 > Loss: 0.44660640, Accuracy: 89.72%\n",
      "    0-967 > Loss: 0.44509840, Accuracy: 89.81%\n",
      "    0-968 > Loss: 0.44347913, Accuracy: 89.68%\n",
      "    0-969 > Loss: 0.44107424, Accuracy: 89.96%\n",
      "    0-970 > Loss: 0.44094884, Accuracy: 89.83%\n",
      "    0-971 > Loss: 0.43965804, Accuracy: 89.98%\n",
      "    0-972 > Loss: 0.43881395, Accuracy: 89.86%\n",
      "    0-973 > Loss: 0.43816411, Accuracy: 89.86%\n",
      "    0-974 > Loss: 0.43768775, Accuracy: 89.86%\n",
      "    0-975 > Loss: 0.43669409, Accuracy: 89.81%\n",
      "    0-976 > Loss: 0.43532597, Accuracy: 89.91%\n",
      "    0-977 > Loss: 0.43492104, Accuracy: 89.98%\n",
      "    0-978 > Loss: 0.43444380, Accuracy: 89.99%\n",
      "    0-979 > Loss: 0.43732950, Accuracy: 89.97%\n",
      "    0-980 > Loss: 0.43579556, Accuracy: 90.25%\n",
      "    0-981 > Loss: 0.43566246, Accuracy: 90.12%\n",
      "    0-982 > Loss: 0.43500471, Accuracy: 90.03%\n",
      "    0-983 > Loss: 0.43444800, Accuracy: 90.04%\n",
      "    0-984 > Loss: 0.43442204, Accuracy: 90.09%\n",
      "    0-985 > Loss: 0.43445242, Accuracy: 90.08%\n",
      "    0-986 > Loss: 0.43384456, Accuracy: 89.95%\n",
      "    0-987 > Loss: 0.43395379, Accuracy: 89.93%\n",
      "    0-988 > Loss: 0.43247314, Accuracy: 90.05%\n",
      "    0-989 > Loss: 0.43342538, Accuracy: 90.13%\n",
      "    0-990 > Loss: 0.43380186, Accuracy: 90.09%\n",
      "    0-991 > Loss: 0.43314305, Accuracy: 89.96%\n",
      "    0-992 > Loss: 0.43499394, Accuracy: 89.98%\n",
      "    0-993 > Loss: 0.43475127, Accuracy: 89.97%\n",
      "    0-994 > Loss: 0.43376955, Accuracy: 90.06%\n",
      "    0-995 > Loss: 0.43497458, Accuracy: 90.02%\n",
      "    0-996 > Loss: 0.43498729, Accuracy: 89.90%\n",
      "    0-997 > Loss: 0.43842199, Accuracy: 89.76%\n",
      "    0-998 > Loss: 0.43811884, Accuracy: 89.71%\n",
      "    0-999 > Loss: 0.43845912, Accuracy: 89.86%\n",
      "    0-1000 > Loss: 0.43816448, Accuracy: 89.75%\n",
      "    0-1001 > Loss: 0.43531510, Accuracy: 89.84%\n",
      "    0-1002 > Loss: 0.43489312, Accuracy: 89.96%\n",
      "    0-1003 > Loss: 0.43690794, Accuracy: 89.88%\n",
      "    0-1004 > Loss: 0.43519930, Accuracy: 89.99%\n",
      "    0-1005 > Loss: 0.43356611, Accuracy: 90.23%\n",
      "    0-1006 > Loss: 0.43803351, Accuracy: 89.92%\n",
      "    0-1007 > Loss: 0.43725880, Accuracy: 89.89%\n",
      "    0-1008 > Loss: 0.43780247, Accuracy: 89.90%\n",
      "    0-1009 > Loss: 0.43626853, Accuracy: 89.83%\n",
      "    0-1010 > Loss: 0.43663880, Accuracy: 89.95%\n",
      "    0-1011 > Loss: 0.43327404, Accuracy: 89.97%\n",
      "    0-1012 > Loss: 0.43223044, Accuracy: 90.06%\n",
      "    0-1013 > Loss: 0.43392495, Accuracy: 90.00%\n",
      "    0-1014 > Loss: 0.43331913, Accuracy: 90.12%\n",
      "    0-1015 > Loss: 0.43304699, Accuracy: 90.06%\n",
      "    0-1016 > Loss: 0.43305735, Accuracy: 89.97%\n",
      "    0-1017 > Loss: 0.43322605, Accuracy: 89.93%\n",
      "    0-1018 > Loss: 0.43171931, Accuracy: 89.85%\n",
      "    0-1019 > Loss: 0.42926250, Accuracy: 90.01%\n",
      "    0-1020 > Loss: 0.42973798, Accuracy: 90.08%\n",
      "    0-1021 > Loss: 0.42764383, Accuracy: 90.11%\n",
      "    0-1022 > Loss: 0.42496594, Accuracy: 90.21%\n",
      "    0-1023 > Loss: 0.42411606, Accuracy: 90.40%\n",
      "    0-1024 > Loss: 0.42278907, Accuracy: 90.45%\n",
      "    0-1025 > Loss: 0.42360123, Accuracy: 90.38%\n",
      "    0-1026 > Loss: 0.42367412, Accuracy: 90.43%\n",
      "    0-1027 > Loss: 0.42393053, Accuracy: 90.33%\n",
      "    0-1028 > Loss: 0.42446871, Accuracy: 90.31%\n",
      "    0-1029 > Loss: 0.42341422, Accuracy: 90.28%\n",
      "    0-1030 > Loss: 0.42224232, Accuracy: 90.38%\n",
      "    0-1031 > Loss: 0.42192557, Accuracy: 90.32%\n",
      "    0-1032 > Loss: 0.42096971, Accuracy: 90.24%\n",
      "    0-1033 > Loss: 0.42049572, Accuracy: 90.29%\n",
      "    0-1034 > Loss: 0.41880922, Accuracy: 90.38%\n",
      "    0-1035 > Loss: 0.41948077, Accuracy: 90.34%\n",
      "    0-1036 > Loss: 0.42038366, Accuracy: 90.24%\n",
      "    0-1037 > Loss: 0.41897020, Accuracy: 90.32%\n",
      "    0-1038 > Loss: 0.42127069, Accuracy: 90.12%\n",
      "    0-1039 > Loss: 0.42332028, Accuracy: 89.90%\n",
      "    0-1040 > Loss: 0.42329471, Accuracy: 90.03%\n",
      "    0-1041 > Loss: 0.42375255, Accuracy: 89.90%\n",
      "    0-1042 > Loss: 0.42678255, Accuracy: 89.90%\n",
      "    0-1043 > Loss: 0.42576380, Accuracy: 89.72%\n",
      "    0-1044 > Loss: 0.42894362, Accuracy: 89.60%\n",
      "    0-1045 > Loss: 0.42766521, Accuracy: 89.84%\n",
      "    0-1046 > Loss: 0.42730507, Accuracy: 89.93%\n",
      "    0-1047 > Loss: 0.42549496, Accuracy: 89.88%\n",
      "    0-1048 > Loss: 0.42512896, Accuracy: 89.99%\n",
      "    0-1049 > Loss: 0.42620286, Accuracy: 89.93%\n",
      "    0-1050 > Loss: 0.42750012, Accuracy: 89.85%\n",
      "    0-1051 > Loss: 0.42742474, Accuracy: 89.97%\n",
      "    0-1052 > Loss: 0.42632179, Accuracy: 90.03%\n",
      "    0-1053 > Loss: 0.42531149, Accuracy: 89.95%\n",
      "    0-1054 > Loss: 0.42609698, Accuracy: 89.91%\n",
      "    0-1055 > Loss: 0.42794286, Accuracy: 89.91%\n",
      "    0-1056 > Loss: 0.42620559, Accuracy: 89.95%\n",
      "    0-1057 > Loss: 0.42521268, Accuracy: 89.88%\n",
      "    0-1058 > Loss: 0.42313801, Accuracy: 89.96%\n",
      "    0-1059 > Loss: 0.42236795, Accuracy: 90.10%\n",
      "    0-1060 > Loss: 0.42214945, Accuracy: 89.98%\n",
      "    0-1061 > Loss: 0.42167000, Accuracy: 89.96%\n",
      "    0-1062 > Loss: 0.42228925, Accuracy: 90.10%\n",
      "    0-1063 > Loss: 0.42307265, Accuracy: 90.18%\n",
      "    0-1064 > Loss: 0.42291211, Accuracy: 89.96%\n",
      "    0-1065 > Loss: 0.42321898, Accuracy: 89.89%\n",
      "    0-1066 > Loss: 0.42148110, Accuracy: 89.96%\n",
      "    0-1067 > Loss: 0.42019637, Accuracy: 89.99%\n",
      "    0-1068 > Loss: 0.41927586, Accuracy: 89.94%\n",
      "    0-1069 > Loss: 0.42008294, Accuracy: 89.90%\n",
      "    0-1070 > Loss: 0.41885748, Accuracy: 89.98%\n",
      "    0-1071 > Loss: 0.41787667, Accuracy: 90.00%\n",
      "    0-1072 > Loss: 0.41645273, Accuracy: 90.08%\n",
      "    0-1073 > Loss: 0.41674032, Accuracy: 89.98%\n",
      "    0-1074 > Loss: 0.41720380, Accuracy: 90.02%\n",
      "    0-1075 > Loss: 0.41657242, Accuracy: 90.15%\n",
      "    0-1076 > Loss: 0.41691734, Accuracy: 90.14%\n",
      "    0-1077 > Loss: 0.41580013, Accuracy: 90.14%\n",
      "    0-1078 > Loss: 0.41514834, Accuracy: 90.33%\n",
      "    0-1079 > Loss: 0.41705537, Accuracy: 90.04%\n",
      "    0-1080 > Loss: 0.41635910, Accuracy: 90.25%\n",
      "    0-1081 > Loss: 0.41658080, Accuracy: 90.19%\n",
      "    0-1082 > Loss: 0.41710218, Accuracy: 90.02%\n",
      "    0-1083 > Loss: 0.42153944, Accuracy: 89.91%\n",
      "    0-1084 > Loss: 0.41968767, Accuracy: 89.89%\n",
      "    0-1085 > Loss: 0.41908932, Accuracy: 90.01%\n",
      "    0-1086 > Loss: 0.41831702, Accuracy: 89.92%\n",
      "    0-1087 > Loss: 0.41782633, Accuracy: 89.92%\n",
      "    0-1088 > Loss: 0.41811932, Accuracy: 89.95%\n",
      "    0-1089 > Loss: 0.41729846, Accuracy: 89.96%\n",
      "    0-1090 > Loss: 0.41687272, Accuracy: 90.15%\n",
      "    0-1091 > Loss: 0.41765313, Accuracy: 90.21%\n",
      "    0-1092 > Loss: 0.41714145, Accuracy: 90.22%\n",
      "    0-1093 > Loss: 0.41525901, Accuracy: 90.28%\n",
      "    0-1094 > Loss: 0.41579849, Accuracy: 90.22%\n",
      "    0-1095 > Loss: 0.41615352, Accuracy: 90.22%\n",
      "    0-1096 > Loss: 0.41467638, Accuracy: 90.11%\n",
      "    0-1097 > Loss: 0.41435280, Accuracy: 90.20%\n",
      "    0-1098 > Loss: 0.41505388, Accuracy: 90.39%\n",
      "    0-1099 > Loss: 0.41655554, Accuracy: 90.37%\n",
      "    0-1100 > Loss: 0.41604317, Accuracy: 90.60%\n",
      "    0-1101 > Loss: 0.41583789, Accuracy: 90.54%\n",
      "    0-1102 > Loss: 0.41439663, Accuracy: 90.39%\n",
      "    0-1103 > Loss: 0.41344426, Accuracy: 90.53%\n",
      "    0-1104 > Loss: 0.41277700, Accuracy: 90.58%\n",
      "    0-1105 > Loss: 0.41304981, Accuracy: 90.39%\n",
      "    0-1106 > Loss: 0.41242611, Accuracy: 90.49%\n",
      "    0-1107 > Loss: 0.41229938, Accuracy: 90.44%\n",
      "    0-1108 > Loss: 0.41565137, Accuracy: 90.35%\n",
      "    0-1109 > Loss: 0.41509556, Accuracy: 90.18%\n",
      "    0-1110 > Loss: 0.41455024, Accuracy: 90.13%\n",
      "    0-1111 > Loss: 0.41715586, Accuracy: 90.02%\n",
      "    0-1112 > Loss: 0.41613041, Accuracy: 90.21%\n",
      "    0-1113 > Loss: 0.41839101, Accuracy: 89.89%\n",
      "    0-1114 > Loss: 0.41736054, Accuracy: 89.78%\n",
      "    0-1115 > Loss: 0.41652934, Accuracy: 89.79%\n",
      "    0-1116 > Loss: 0.41442131, Accuracy: 89.93%\n",
      "    0-1117 > Loss: 0.41160467, Accuracy: 90.15%\n",
      "    0-1118 > Loss: 0.41140665, Accuracy: 90.30%\n",
      "    0-1119 > Loss: 0.41147317, Accuracy: 90.30%\n",
      "    0-1120 > Loss: 0.41190884, Accuracy: 90.31%\n",
      "    0-1121 > Loss: 0.41063842, Accuracy: 90.21%\n",
      "    0-1122 > Loss: 0.41021713, Accuracy: 90.24%\n",
      "    0-1123 > Loss: 0.41078024, Accuracy: 90.31%\n",
      "    0-1124 > Loss: 0.41047952, Accuracy: 90.19%\n",
      "    0-1125 > Loss: 0.40993537, Accuracy: 90.32%\n",
      "    0-1126 > Loss: 0.41148613, Accuracy: 90.17%\n",
      "    0-1127 > Loss: 0.41117942, Accuracy: 90.13%\n",
      "    0-1128 > Loss: 0.41319937, Accuracy: 90.14%\n",
      "    0-1129 > Loss: 0.41276422, Accuracy: 90.04%\n",
      "    0-1130 > Loss: 0.41114636, Accuracy: 90.10%\n",
      "    0-1131 > Loss: 0.40965518, Accuracy: 90.20%\n",
      "    0-1132 > Loss: 0.41088632, Accuracy: 90.31%\n",
      "    0-1133 > Loss: 0.41123266, Accuracy: 90.26%\n",
      "    0-1134 > Loss: 0.41126034, Accuracy: 90.23%\n",
      "    0-1135 > Loss: 0.41070183, Accuracy: 90.28%\n",
      "    0-1136 > Loss: 0.41052232, Accuracy: 90.27%\n",
      "    0-1137 > Loss: 0.41146336, Accuracy: 90.08%\n",
      "    0-1138 > Loss: 0.41065657, Accuracy: 89.93%\n",
      "    0-1139 > Loss: 0.41065178, Accuracy: 90.08%\n",
      "    0-1140 > Loss: 0.41329804, Accuracy: 90.10%\n",
      "    0-1141 > Loss: 0.41426328, Accuracy: 90.00%\n",
      "    0-1142 > Loss: 0.41558723, Accuracy: 89.90%\n",
      "    0-1143 > Loss: 0.41292801, Accuracy: 89.90%\n",
      "    0-1144 > Loss: 0.41096745, Accuracy: 90.00%\n",
      "    0-1145 > Loss: 0.41069195, Accuracy: 90.04%\n",
      "    0-1146 > Loss: 0.41032761, Accuracy: 90.05%\n",
      "    0-1147 > Loss: 0.41100243, Accuracy: 89.94%\n",
      "    0-1148 > Loss: 0.40865794, Accuracy: 90.02%\n",
      "    0-1149 > Loss: 0.40881132, Accuracy: 90.07%\n",
      "    0-1150 > Loss: 0.40755914, Accuracy: 90.24%\n",
      "    0-1151 > Loss: 0.40593886, Accuracy: 90.27%\n",
      "    0-1152 > Loss: 0.40738574, Accuracy: 90.22%\n",
      "    0-1153 > Loss: 0.40715119, Accuracy: 90.32%\n",
      "    0-1154 > Loss: 0.40659393, Accuracy: 90.28%\n",
      "    0-1155 > Loss: 0.40859609, Accuracy: 90.25%\n",
      "    0-1156 > Loss: 0.40845845, Accuracy: 90.21%\n",
      "    0-1157 > Loss: 0.41005956, Accuracy: 90.19%\n",
      "    0-1158 > Loss: 0.41271861, Accuracy: 90.23%\n",
      "    0-1159 > Loss: 0.41005780, Accuracy: 90.20%\n",
      "    0-1160 > Loss: 0.40902504, Accuracy: 90.30%\n",
      "    0-1161 > Loss: 0.40900182, Accuracy: 90.35%\n",
      "    0-1162 > Loss: 0.40751895, Accuracy: 90.26%\n",
      "    0-1163 > Loss: 0.40789923, Accuracy: 90.32%\n",
      "    0-1164 > Loss: 0.40744569, Accuracy: 90.25%\n",
      "    0-1165 > Loss: 0.40445239, Accuracy: 90.30%\n",
      "    0-1166 > Loss: 0.40551472, Accuracy: 90.21%\n",
      "    0-1167 > Loss: 0.40647139, Accuracy: 90.19%\n",
      "    0-1168 > Loss: 0.40693889, Accuracy: 90.04%\n",
      "    0-1169 > Loss: 0.40583344, Accuracy: 90.24%\n",
      "    0-1170 > Loss: 0.40381700, Accuracy: 90.39%\n",
      "    0-1171 > Loss: 0.40278084, Accuracy: 90.42%\n",
      "    0-1172 > Loss: 0.40154308, Accuracy: 90.42%\n",
      "    0-1173 > Loss: 0.40312440, Accuracy: 90.34%\n",
      "    0-1174 > Loss: 0.40381492, Accuracy: 90.24%\n",
      "    0-1175 > Loss: 0.40406923, Accuracy: 90.21%\n",
      "    0-1176 > Loss: 0.40253783, Accuracy: 90.14%\n",
      "    0-1177 > Loss: 0.40061745, Accuracy: 90.32%\n",
      "    0-1178 > Loss: 0.40249382, Accuracy: 90.30%\n",
      "    0-1179 > Loss: 0.40081643, Accuracy: 90.40%\n",
      "    0-1180 > Loss: 0.39929892, Accuracy: 90.41%\n",
      "    0-1181 > Loss: 0.40009925, Accuracy: 90.36%\n",
      "    0-1182 > Loss: 0.40094154, Accuracy: 90.38%\n",
      "    0-1183 > Loss: 0.39951572, Accuracy: 90.28%\n",
      "    0-1184 > Loss: 0.39979958, Accuracy: 90.43%\n",
      "    0-1185 > Loss: 0.39996710, Accuracy: 90.42%\n",
      "    0-1186 > Loss: 0.40019885, Accuracy: 90.46%\n",
      "    0-1187 > Loss: 0.39940669, Accuracy: 90.44%\n",
      "    0-1188 > Loss: 0.39935857, Accuracy: 90.44%\n",
      "    0-1189 > Loss: 0.39860896, Accuracy: 90.46%\n",
      "    0-1190 > Loss: 0.39865844, Accuracy: 90.46%\n",
      "    0-1191 > Loss: 0.39756777, Accuracy: 90.43%\n",
      "    0-1192 > Loss: 0.39696733, Accuracy: 90.50%\n",
      "    0-1193 > Loss: 0.39611234, Accuracy: 90.59%\n",
      "    0-1194 > Loss: 0.39472968, Accuracy: 90.52%\n",
      "    0-1195 > Loss: 0.39549636, Accuracy: 90.49%\n",
      "    0-1196 > Loss: 0.39553863, Accuracy: 90.42%\n",
      "    0-1197 > Loss: 0.39525649, Accuracy: 90.37%\n",
      "    0-1198 > Loss: 0.39592902, Accuracy: 90.24%\n",
      "    0-1199 > Loss: 0.39627545, Accuracy: 90.30%\n",
      "    0-1200 > Loss: 0.39721683, Accuracy: 90.32%\n",
      "    0-1201 > Loss: 0.39566835, Accuracy: 90.38%\n",
      "    0-1202 > Loss: 0.39428820, Accuracy: 90.35%\n",
      "    0-1203 > Loss: 0.39675113, Accuracy: 90.31%\n",
      "    0-1204 > Loss: 0.39620726, Accuracy: 90.37%\n",
      "    0-1205 > Loss: 0.39635896, Accuracy: 90.33%\n",
      "    0-1206 > Loss: 0.39558666, Accuracy: 90.43%\n",
      "    0-1207 > Loss: 0.39540141, Accuracy: 90.55%\n",
      "    0-1208 > Loss: 0.39656232, Accuracy: 90.59%\n",
      "    0-1209 > Loss: 0.39697005, Accuracy: 90.65%\n",
      "    0-1210 > Loss: 0.39573124, Accuracy: 90.56%\n",
      "    0-1211 > Loss: 0.39555104, Accuracy: 90.74%\n",
      "    0-1212 > Loss: 0.39686837, Accuracy: 90.69%\n",
      "    0-1213 > Loss: 0.39744083, Accuracy: 90.58%\n",
      "    0-1214 > Loss: 0.39723696, Accuracy: 90.74%\n",
      "    0-1215 > Loss: 0.39690064, Accuracy: 90.59%\n",
      "    0-1216 > Loss: 0.39827574, Accuracy: 90.63%\n",
      "    0-1217 > Loss: 0.39725812, Accuracy: 90.68%\n",
      "    0-1218 > Loss: 0.39552378, Accuracy: 90.54%\n",
      "    0-1219 > Loss: 0.39659115, Accuracy: 90.49%\n",
      "    0-1220 > Loss: 0.39594067, Accuracy: 90.44%\n",
      "    0-1221 > Loss: 0.39471362, Accuracy: 90.41%\n",
      "    0-1222 > Loss: 0.39476436, Accuracy: 90.36%\n",
      "    0-1223 > Loss: 0.39402863, Accuracy: 90.48%\n",
      "    0-1224 > Loss: 0.39368848, Accuracy: 90.50%\n",
      "    0-1225 > Loss: 0.39275011, Accuracy: 90.48%\n",
      "    0-1226 > Loss: 0.39591342, Accuracy: 90.53%\n",
      "    0-1227 > Loss: 0.39807140, Accuracy: 90.54%\n",
      "    0-1228 > Loss: 0.39830468, Accuracy: 90.50%\n",
      "    0-1229 > Loss: 0.39989020, Accuracy: 90.33%\n",
      "    0-1230 > Loss: 0.40100849, Accuracy: 90.24%\n",
      "    0-1231 > Loss: 0.40040995, Accuracy: 90.42%\n",
      "    0-1232 > Loss: 0.40147425, Accuracy: 90.59%\n",
      "    0-1233 > Loss: 0.40457754, Accuracy: 90.30%\n",
      "    0-1234 > Loss: 0.40297300, Accuracy: 90.49%\n",
      "    0-1235 > Loss: 0.40152792, Accuracy: 90.41%\n",
      "    0-1236 > Loss: 0.40276771, Accuracy: 90.33%\n",
      "    0-1237 > Loss: 0.40065319, Accuracy: 90.44%\n",
      "    0-1238 > Loss: 0.40112381, Accuracy: 90.28%\n",
      "    0-1239 > Loss: 0.39972624, Accuracy: 90.05%\n",
      "    0-1240 > Loss: 0.40027895, Accuracy: 90.00%\n",
      "    0-1241 > Loss: 0.40010159, Accuracy: 90.04%\n",
      "    0-1242 > Loss: 0.39888514, Accuracy: 90.10%\n",
      "    0-1243 > Loss: 0.39844720, Accuracy: 90.14%\n",
      "    0-1244 > Loss: 0.39712815, Accuracy: 90.21%\n",
      "    0-1245 > Loss: 0.39779861, Accuracy: 90.34%\n",
      "    0-1246 > Loss: 0.39756870, Accuracy: 90.29%\n",
      "    0-1247 > Loss: 0.39784260, Accuracy: 90.43%\n",
      "    0-1248 > Loss: 0.39831278, Accuracy: 90.49%\n",
      "    0-1249 > Loss: 0.39949670, Accuracy: 90.63%\n",
      "    0-1250 > Loss: 0.39853408, Accuracy: 90.70%\n",
      "    0-1251 > Loss: 0.39692189, Accuracy: 90.63%\n",
      "    0-1252 > Loss: 0.39729568, Accuracy: 90.76%\n",
      "    0-1253 > Loss: 0.39706085, Accuracy: 90.76%\n",
      "    0-1254 > Loss: 0.39834466, Accuracy: 90.68%\n",
      "    0-1255 > Loss: 0.39785581, Accuracy: 90.48%\n",
      "    0-1256 > Loss: 0.39758366, Accuracy: 90.68%\n",
      "    0-1257 > Loss: 0.39805273, Accuracy: 90.62%\n",
      "    0-1258 > Loss: 0.39835594, Accuracy: 90.65%\n",
      "    0-1259 > Loss: 0.39842793, Accuracy: 90.61%\n",
      "    0-1260 > Loss: 0.39816886, Accuracy: 90.65%\n",
      "    0-1261 > Loss: 0.39807532, Accuracy: 90.60%\n",
      "    0-1262 > Loss: 0.39783651, Accuracy: 90.63%\n",
      "    0-1263 > Loss: 0.39724233, Accuracy: 90.48%\n",
      "    0-1264 > Loss: 0.39737214, Accuracy: 90.45%\n",
      "    0-1265 > Loss: 0.39696316, Accuracy: 90.33%\n",
      "    0-1266 > Loss: 0.39552641, Accuracy: 90.31%\n",
      "    0-1267 > Loss: 0.39417837, Accuracy: 90.44%\n",
      "    0-1268 > Loss: 0.39535001, Accuracy: 90.44%\n",
      "    0-1269 > Loss: 0.39477695, Accuracy: 90.51%\n",
      "    0-1270 > Loss: 0.39344696, Accuracy: 90.66%\n",
      "    0-1271 > Loss: 0.39338761, Accuracy: 90.62%\n",
      "    0-1272 > Loss: 0.39307298, Accuracy: 90.71%\n",
      "    0-1273 > Loss: 0.39209030, Accuracy: 90.76%\n",
      "    0-1274 > Loss: 0.39086908, Accuracy: 90.75%\n",
      "    0-1275 > Loss: 0.39037541, Accuracy: 90.78%\n",
      "    0-1276 > Loss: 0.39107079, Accuracy: 90.70%\n",
      "    0-1277 > Loss: 0.39016645, Accuracy: 90.71%\n",
      "    0-1278 > Loss: 0.38962913, Accuracy: 90.70%\n",
      "    0-1279 > Loss: 0.38978544, Accuracy: 90.62%\n",
      "    0-1280 > Loss: 0.38711105, Accuracy: 90.75%\n",
      "    0-1281 > Loss: 0.38779618, Accuracy: 90.65%\n",
      "    0-1282 > Loss: 0.38703655, Accuracy: 90.67%\n",
      "    0-1283 > Loss: 0.38741429, Accuracy: 90.66%\n",
      "    0-1284 > Loss: 0.38812299, Accuracy: 90.66%\n",
      "    0-1285 > Loss: 0.38856892, Accuracy: 90.56%\n",
      "    0-1286 > Loss: 0.38710582, Accuracy: 90.58%\n",
      "    0-1287 > Loss: 0.38636041, Accuracy: 90.59%\n",
      "    0-1288 > Loss: 0.38594698, Accuracy: 90.65%\n",
      "    0-1289 > Loss: 0.38626812, Accuracy: 90.53%\n",
      "    0-1290 > Loss: 0.38991113, Accuracy: 90.48%\n",
      "    0-1291 > Loss: 0.39388641, Accuracy: 90.39%\n",
      "    0-1292 > Loss: 0.39587502, Accuracy: 90.15%\n",
      "    0-1293 > Loss: 0.39785096, Accuracy: 90.17%\n",
      "    0-1294 > Loss: 0.39761072, Accuracy: 90.08%\n",
      "    0-1295 > Loss: 0.39664109, Accuracy: 90.19%\n",
      "    0-1296 > Loss: 0.39449717, Accuracy: 90.38%\n",
      "    0-1297 > Loss: 0.39277346, Accuracy: 90.36%\n",
      "    0-1298 > Loss: 0.39437502, Accuracy: 90.26%\n",
      "    0-1299 > Loss: 0.39763490, Accuracy: 90.17%\n",
      "    0-1300 > Loss: 0.39660214, Accuracy: 90.14%\n",
      "    0-1301 > Loss: 0.39587592, Accuracy: 90.18%\n",
      "    0-1302 > Loss: 0.39406198, Accuracy: 90.12%\n",
      "    0-1303 > Loss: 0.39367351, Accuracy: 90.25%\n",
      "    0-1304 > Loss: 0.39359515, Accuracy: 90.24%\n",
      "    0-1305 > Loss: 0.39509431, Accuracy: 90.21%\n",
      "    0-1306 > Loss: 0.39398469, Accuracy: 90.24%\n",
      "    0-1307 > Loss: 0.39184414, Accuracy: 90.22%\n",
      "    0-1308 > Loss: 0.39080306, Accuracy: 90.34%\n",
      "    0-1309 > Loss: 0.39238759, Accuracy: 90.31%\n",
      "    0-1310 > Loss: 0.39271039, Accuracy: 90.33%\n",
      "    0-1311 > Loss: 0.39479873, Accuracy: 90.18%\n",
      "    0-1312 > Loss: 0.39413615, Accuracy: 90.05%\n",
      "    0-1313 > Loss: 0.39436012, Accuracy: 90.10%\n",
      "    0-1314 > Loss: 0.39481413, Accuracy: 90.12%\n",
      "    0-1315 > Loss: 0.39496678, Accuracy: 90.07%\n",
      "    0-1316 > Loss: 0.39248194, Accuracy: 90.04%\n",
      "    0-1317 > Loss: 0.39163496, Accuracy: 90.04%\n",
      "    0-1318 > Loss: 0.39277449, Accuracy: 89.88%\n",
      "    0-1319 > Loss: 0.39325593, Accuracy: 89.89%\n",
      "    0-1320 > Loss: 0.39485981, Accuracy: 89.86%\n",
      "    0-1321 > Loss: 0.39467994, Accuracy: 89.81%\n",
      "    0-1322 > Loss: 0.39424794, Accuracy: 89.73%\n",
      "    0-1323 > Loss: 0.39454049, Accuracy: 89.74%\n",
      "    0-1324 > Loss: 0.39238950, Accuracy: 90.10%\n",
      "    0-1325 > Loss: 0.39089127, Accuracy: 90.18%\n",
      "    0-1326 > Loss: 0.39300363, Accuracy: 90.16%\n",
      "    0-1327 > Loss: 0.39119930, Accuracy: 90.36%\n",
      "    0-1328 > Loss: 0.39081233, Accuracy: 90.24%\n",
      "    0-1329 > Loss: 0.39119176, Accuracy: 90.29%\n",
      "    0-1330 > Loss: 0.39258727, Accuracy: 90.26%\n",
      "    0-1331 > Loss: 0.39164345, Accuracy: 90.31%\n",
      "    0-1332 > Loss: 0.39125624, Accuracy: 90.36%\n",
      "    0-1333 > Loss: 0.39147731, Accuracy: 90.49%\n",
      "    0-1334 > Loss: 0.38977134, Accuracy: 90.61%\n",
      "    0-1335 > Loss: 0.38938966, Accuracy: 90.65%\n",
      "    0-1336 > Loss: 0.38872518, Accuracy: 90.55%\n",
      "    0-1337 > Loss: 0.39079030, Accuracy: 90.29%\n",
      "    0-1338 > Loss: 0.39152843, Accuracy: 90.35%\n",
      "    0-1339 > Loss: 0.39124285, Accuracy: 90.25%\n",
      "    0-1340 > Loss: 0.39192889, Accuracy: 90.41%\n",
      "    0-1341 > Loss: 0.39274234, Accuracy: 90.46%\n",
      "    0-1342 > Loss: 0.39312880, Accuracy: 90.50%\n",
      "    0-1343 > Loss: 0.39253486, Accuracy: 90.60%\n",
      "    0-1344 > Loss: 0.39491765, Accuracy: 90.57%\n",
      "    0-1345 > Loss: 0.39497338, Accuracy: 90.49%\n",
      "    0-1346 > Loss: 0.39791400, Accuracy: 90.53%\n",
      "    0-1347 > Loss: 0.39876285, Accuracy: 90.48%\n",
      "    0-1348 > Loss: 0.39934338, Accuracy: 90.34%\n",
      "    0-1349 > Loss: 0.39836038, Accuracy: 90.39%\n",
      "    0-1350 > Loss: 0.39641481, Accuracy: 90.43%\n",
      "    0-1351 > Loss: 0.39580858, Accuracy: 90.59%\n",
      "    0-1352 > Loss: 0.39637644, Accuracy: 90.52%\n",
      "    0-1353 > Loss: 0.39614977, Accuracy: 90.69%\n",
      "    0-1354 > Loss: 0.39300093, Accuracy: 90.65%\n",
      "    0-1355 > Loss: 0.39290305, Accuracy: 90.72%\n",
      "    0-1356 > Loss: 0.39195454, Accuracy: 90.77%\n",
      "    0-1357 > Loss: 0.39141037, Accuracy: 90.65%\n",
      "    0-1358 > Loss: 0.39063145, Accuracy: 90.63%\n",
      "    0-1359 > Loss: 0.39004129, Accuracy: 90.57%\n",
      "    0-1360 > Loss: 0.39059716, Accuracy: 90.67%\n",
      "    0-1361 > Loss: 0.39030449, Accuracy: 90.74%\n",
      "    0-1362 > Loss: 0.38966318, Accuracy: 90.64%\n",
      "    0-1363 > Loss: 0.38797030, Accuracy: 90.75%\n",
      "    0-1364 > Loss: 0.38976644, Accuracy: 90.48%\n",
      "    0-1365 > Loss: 0.39076245, Accuracy: 90.51%\n",
      "    0-1366 > Loss: 0.39029505, Accuracy: 90.62%\n",
      "    0-1367 > Loss: 0.38700301, Accuracy: 90.58%\n",
      "    0-1368 > Loss: 0.38724037, Accuracy: 90.62%\n",
      "    0-1369 > Loss: 0.38829739, Accuracy: 90.58%\n",
      "    0-1370 > Loss: 0.38828157, Accuracy: 90.58%\n",
      "    0-1371 > Loss: 0.38947125, Accuracy: 90.46%\n",
      "    0-1372 > Loss: 0.38731961, Accuracy: 90.60%\n",
      "    0-1373 > Loss: 0.38496742, Accuracy: 90.71%\n",
      "    0-1374 > Loss: 0.38457291, Accuracy: 90.59%\n",
      "    0-1375 > Loss: 0.38527735, Accuracy: 90.61%\n",
      "    0-1376 > Loss: 0.38292061, Accuracy: 90.73%\n",
      "    0-1377 > Loss: 0.38242061, Accuracy: 90.81%\n",
      "    0-1378 > Loss: 0.38278232, Accuracy: 90.89%\n",
      "    0-1379 > Loss: 0.38245953, Accuracy: 90.78%\n",
      "    0-1380 > Loss: 0.38280454, Accuracy: 90.65%\n",
      "    0-1381 > Loss: 0.38618922, Accuracy: 90.58%\n",
      "    0-1382 > Loss: 0.38574440, Accuracy: 90.54%\n",
      "    0-1383 > Loss: 0.38402440, Accuracy: 90.60%\n",
      "    0-1384 > Loss: 0.38263813, Accuracy: 90.69%\n",
      "    0-1385 > Loss: 0.38486455, Accuracy: 90.61%\n",
      "    0-1386 > Loss: 0.38335414, Accuracy: 90.68%\n",
      "    0-1387 > Loss: 0.38284776, Accuracy: 90.56%\n",
      "    0-1388 > Loss: 0.38253034, Accuracy: 90.85%\n",
      "    0-1389 > Loss: 0.38397033, Accuracy: 90.70%\n",
      "    0-1390 > Loss: 0.38219490, Accuracy: 90.72%\n",
      "    0-1391 > Loss: 0.38217334, Accuracy: 90.76%\n",
      "    0-1392 > Loss: 0.38117839, Accuracy: 90.75%\n",
      "    0-1393 > Loss: 0.38085769, Accuracy: 90.66%\n",
      "    0-1394 > Loss: 0.38083818, Accuracy: 90.62%\n",
      "    0-1395 > Loss: 0.37899931, Accuracy: 90.74%\n",
      "    0-1396 > Loss: 0.37925866, Accuracy: 90.70%\n",
      "    0-1397 > Loss: 0.37825404, Accuracy: 90.74%\n",
      "    0-1398 > Loss: 0.37874611, Accuracy: 90.86%\n",
      "    0-1399 > Loss: 0.37788594, Accuracy: 90.84%\n",
      "    0-1400 > Loss: 0.37644049, Accuracy: 90.87%\n",
      "    0-1401 > Loss: 0.37715087, Accuracy: 90.73%\n",
      "    0-1402 > Loss: 0.37734510, Accuracy: 90.71%\n",
      "    0-1403 > Loss: 0.37683892, Accuracy: 90.82%\n",
      "    0-1404 > Loss: 0.37609725, Accuracy: 90.84%\n",
      "    0-1405 > Loss: 0.37644319, Accuracy: 90.80%\n",
      "    0-1406 > Loss: 0.37683348, Accuracy: 90.85%\n",
      "    0-1407 > Loss: 0.37726030, Accuracy: 90.72%\n",
      "    0-1408 > Loss: 0.37919043, Accuracy: 90.69%\n",
      "    0-1409 > Loss: 0.37939581, Accuracy: 90.63%\n",
      "    0-1410 > Loss: 0.38065832, Accuracy: 90.76%\n",
      "    0-1411 > Loss: 0.37945773, Accuracy: 90.73%\n",
      "    0-1412 > Loss: 0.38077824, Accuracy: 90.63%\n",
      "    0-1413 > Loss: 0.38003575, Accuracy: 90.57%\n",
      "    0-1414 > Loss: 0.38029423, Accuracy: 90.65%\n",
      "    0-1415 > Loss: 0.38047792, Accuracy: 90.62%\n",
      "    0-1416 > Loss: 0.38014415, Accuracy: 90.61%\n",
      "    0-1417 > Loss: 0.38000696, Accuracy: 90.65%\n",
      "    0-1418 > Loss: 0.37905610, Accuracy: 90.61%\n",
      "    0-1419 > Loss: 0.37705404, Accuracy: 90.64%\n",
      "    0-1420 > Loss: 0.37790514, Accuracy: 90.64%\n",
      "    0-1421 > Loss: 0.37633139, Accuracy: 90.79%\n",
      "    0-1422 > Loss: 0.38031124, Accuracy: 90.65%\n",
      "    0-1423 > Loss: 0.37914792, Accuracy: 90.66%\n",
      "    0-1424 > Loss: 0.37889655, Accuracy: 90.66%\n",
      "    0-1425 > Loss: 0.37954986, Accuracy: 90.56%\n",
      "    0-1426 > Loss: 0.38258516, Accuracy: 90.69%\n",
      "    0-1427 > Loss: 0.38413659, Accuracy: 90.68%\n",
      "    0-1428 > Loss: 0.38372414, Accuracy: 90.61%\n",
      "    0-1429 > Loss: 0.38383537, Accuracy: 90.55%\n",
      "    0-1430 > Loss: 0.38146094, Accuracy: 90.66%\n",
      "    0-1431 > Loss: 0.38053587, Accuracy: 90.57%\n",
      "    0-1432 > Loss: 0.38130087, Accuracy: 90.61%\n",
      "    0-1433 > Loss: 0.38315594, Accuracy: 90.75%\n",
      "    0-1434 > Loss: 0.38470012, Accuracy: 90.70%\n",
      "    0-1435 > Loss: 0.38597392, Accuracy: 90.78%\n",
      "    0-1436 > Loss: 0.38634226, Accuracy: 90.83%\n",
      "    0-1437 > Loss: 0.38547010, Accuracy: 90.77%\n",
      "    0-1438 > Loss: 0.38715094, Accuracy: 90.91%\n",
      "    0-1439 > Loss: 0.38557003, Accuracy: 90.86%\n",
      "    0-1440 > Loss: 0.38407802, Accuracy: 90.87%\n",
      "    0-1441 > Loss: 0.38547815, Accuracy: 90.78%\n",
      "    0-1442 > Loss: 0.38602824, Accuracy: 90.82%\n",
      "    0-1443 > Loss: 0.38295714, Accuracy: 90.95%\n",
      "    0-1444 > Loss: 0.38501947, Accuracy: 90.74%\n",
      "    0-1445 > Loss: 0.38824529, Accuracy: 90.53%\n",
      "    0-1446 > Loss: 0.39054077, Accuracy: 90.33%\n",
      "    0-1447 > Loss: 0.38716086, Accuracy: 90.56%\n",
      "    0-1448 > Loss: 0.38551975, Accuracy: 90.53%\n",
      "    0-1449 > Loss: 0.38626254, Accuracy: 90.48%\n",
      "    0-1450 > Loss: 0.38669285, Accuracy: 90.50%\n",
      "    0-1451 > Loss: 0.38686484, Accuracy: 90.37%\n",
      "    0-1452 > Loss: 0.38603861, Accuracy: 90.57%\n",
      "    0-1453 > Loss: 0.38492568, Accuracy: 90.71%\n",
      "    0-1454 > Loss: 0.38342974, Accuracy: 90.73%\n",
      "    0-1455 > Loss: 0.38380961, Accuracy: 90.77%\n",
      "    0-1456 > Loss: 0.38253831, Accuracy: 90.85%\n",
      "    0-1457 > Loss: 0.38398079, Accuracy: 90.49%\n",
      "    0-1458 > Loss: 0.38326676, Accuracy: 90.57%\n",
      "    0-1459 > Loss: 0.38191224, Accuracy: 90.67%\n",
      "    0-1460 > Loss: 0.38039555, Accuracy: 90.75%\n",
      "    0-1461 > Loss: 0.37883640, Accuracy: 90.71%\n",
      "    0-1462 > Loss: 0.37923944, Accuracy: 90.58%\n",
      "    0-1463 > Loss: 0.37975919, Accuracy: 90.53%\n",
      "    0-1464 > Loss: 0.38109507, Accuracy: 90.44%\n",
      "    0-1465 > Loss: 0.37979165, Accuracy: 90.37%\n",
      "    0-1466 > Loss: 0.38092399, Accuracy: 90.40%\n",
      "    0-1467 > Loss: 0.38154121, Accuracy: 90.29%\n",
      "    0-1468 > Loss: 0.37968892, Accuracy: 90.33%\n",
      "    0-1469 > Loss: 0.37720988, Accuracy: 90.49%\n",
      "    0-1470 > Loss: 0.37758402, Accuracy: 90.54%\n",
      "    0-1471 > Loss: 0.37693619, Accuracy: 90.55%\n",
      "    0-1472 > Loss: 0.37606346, Accuracy: 90.44%\n",
      "    0-1473 > Loss: 0.37520729, Accuracy: 90.52%\n",
      "    0-1474 > Loss: 0.37323882, Accuracy: 90.56%\n",
      "    0-1475 > Loss: 0.37625449, Accuracy: 90.55%\n",
      "    0-1476 > Loss: 0.37678820, Accuracy: 90.58%\n",
      "    0-1477 > Loss: 0.37681551, Accuracy: 90.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\7-Learning\\9-AI_ML_DL\\linear-regression-ml\\mini-batch-gradient-descent\\neural_network.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0-1478 > Loss: 0.37805902, Accuracy: 90.80%\n",
      "    0-1479 > Loss: 0.37691420, Accuracy: 90.76%\n",
      "    0-1480 > Loss: 0.37727980, Accuracy: 90.72%\n",
      "    0-1481 > Loss: 0.37683206, Accuracy: 90.75%\n",
      "    0-1482 > Loss: 0.37572699, Accuracy: 90.83%\n",
      "    0-1483 > Loss: 0.37710994, Accuracy: 90.80%\n",
      "    0-1484 > Loss: 0.37659126, Accuracy: 90.73%\n",
      "    0-1485 > Loss: 0.37630093, Accuracy: 90.75%\n",
      "    0-1486 > Loss: 0.37426638, Accuracy: 90.89%\n",
      "    0-1487 > Loss: 0.37751088, Accuracy: 90.69%\n",
      "    0-1488 > Loss: 0.37757014, Accuracy: 90.85%\n",
      "    0-1489 > Loss: 0.37725742, Accuracy: 90.85%\n",
      "    0-1490 > Loss: 0.37675907, Accuracy: 90.86%\n",
      "    0-1491 > Loss: 0.37643399, Accuracy: 90.96%\n",
      "    0-1492 > Loss: 0.37892720, Accuracy: 90.92%\n",
      "    0-1493 > Loss: 0.37781321, Accuracy: 90.79%\n",
      "    0-1494 > Loss: 0.37695138, Accuracy: 90.87%\n",
      "    0-1495 > Loss: 0.37570795, Accuracy: 90.90%\n",
      "    0-1496 > Loss: 0.37570810, Accuracy: 90.88%\n",
      "    0-1497 > Loss: 0.37680574, Accuracy: 90.94%\n",
      "    0-1498 > Loss: 0.37921251, Accuracy: 90.75%\n",
      "    0-1499 > Loss: 0.37971955, Accuracy: 90.80%\n",
      "    0-1500 > Loss: 0.37872780, Accuracy: 90.90%\n",
      "    0-1501 > Loss: 0.37797798, Accuracy: 90.85%\n",
      "    0-1502 > Loss: 0.37768752, Accuracy: 90.97%\n",
      "    0-1503 > Loss: 0.37852203, Accuracy: 90.94%\n",
      "    0-1504 > Loss: 0.37828380, Accuracy: 91.02%\n",
      "    0-1505 > Loss: 0.37609312, Accuracy: 90.96%\n",
      "    0-1506 > Loss: 0.37642828, Accuracy: 90.94%\n",
      "    0-1507 > Loss: 0.37511610, Accuracy: 90.92%\n",
      "    0-1508 > Loss: 0.37658820, Accuracy: 90.81%\n",
      "    0-1509 > Loss: 0.37679345, Accuracy: 90.90%\n",
      "    0-1510 > Loss: 0.37722159, Accuracy: 90.89%\n",
      "    0-1511 > Loss: 0.37865108, Accuracy: 90.87%\n",
      "    0-1512 > Loss: 0.37826240, Accuracy: 90.87%\n",
      "    0-1513 > Loss: 0.37839254, Accuracy: 90.80%\n",
      "    0-1514 > Loss: 0.37778974, Accuracy: 90.77%\n",
      "    0-1515 > Loss: 0.37817748, Accuracy: 90.70%\n",
      "    0-1516 > Loss: 0.37726720, Accuracy: 90.68%\n",
      "    0-1517 > Loss: 0.37610023, Accuracy: 90.76%\n",
      "    0-1518 > Loss: 0.37315781, Accuracy: 90.91%\n",
      "    0-1519 > Loss: 0.37173332, Accuracy: 90.90%\n",
      "    0-1520 > Loss: 0.37228044, Accuracy: 90.94%\n",
      "    0-1521 > Loss: 0.37181275, Accuracy: 90.97%\n",
      "    0-1522 > Loss: 0.37058206, Accuracy: 90.97%\n",
      "    0-1523 > Loss: 0.37170021, Accuracy: 91.01%\n",
      "    0-1524 > Loss: 0.37094466, Accuracy: 90.99%\n",
      "    0-1525 > Loss: 0.37081621, Accuracy: 90.91%\n",
      "    0-1526 > Loss: 0.37228535, Accuracy: 90.86%\n",
      "    0-1527 > Loss: 0.37251104, Accuracy: 90.83%\n",
      "    0-1528 > Loss: 0.37587433, Accuracy: 90.64%\n",
      "    0-1529 > Loss: 0.37978341, Accuracy: 90.42%\n",
      "    0-1530 > Loss: 0.38102746, Accuracy: 90.46%\n",
      "    0-1531 > Loss: 0.38265462, Accuracy: 90.36%\n",
      "    0-1532 > Loss: 0.38138134, Accuracy: 90.46%\n",
      "    0-1533 > Loss: 0.38172409, Accuracy: 90.45%\n",
      "    0-1534 > Loss: 0.38241690, Accuracy: 90.42%\n",
      "    0-1535 > Loss: 0.37963993, Accuracy: 90.59%\n",
      "    0-1536 > Loss: 0.38006861, Accuracy: 90.54%\n",
      "    0-1537 > Loss: 0.38293454, Accuracy: 90.24%\n",
      "    0-1538 > Loss: 0.38188470, Accuracy: 90.30%\n",
      "    0-1539 > Loss: 0.38072038, Accuracy: 90.36%\n",
      "    0-1540 > Loss: 0.38062296, Accuracy: 90.36%\n",
      "    0-1541 > Loss: 0.37903445, Accuracy: 90.45%\n",
      "    0-1542 > Loss: 0.37729498, Accuracy: 90.53%\n",
      "    0-1543 > Loss: 0.37753453, Accuracy: 90.60%\n",
      "    0-1544 > Loss: 0.37768377, Accuracy: 90.62%\n",
      "    0-1545 > Loss: 0.37619013, Accuracy: 90.59%\n",
      "    0-1546 > Loss: 0.37795198, Accuracy: 90.59%\n",
      "    0-1547 > Loss: 0.37819755, Accuracy: 90.78%\n",
      "    0-1548 > Loss: 0.38142300, Accuracy: 90.66%\n",
      "    0-1549 > Loss: 0.38101660, Accuracy: 90.65%\n",
      "    0-1550 > Loss: 0.38192660, Accuracy: 90.71%\n",
      "    0-1551 > Loss: 0.38039384, Accuracy: 90.76%\n",
      "    0-1552 > Loss: 0.38001588, Accuracy: 90.83%\n",
      "    0-1553 > Loss: 0.37876480, Accuracy: 90.76%\n",
      "    0-1554 > Loss: 0.37657590, Accuracy: 91.00%\n",
      "    0-1555 > Loss: 0.37608098, Accuracy: 90.94%\n",
      "    0-1556 > Loss: 0.37538149, Accuracy: 91.01%\n",
      "    0-1557 > Loss: 0.37454864, Accuracy: 91.00%\n",
      "    0-1558 > Loss: 0.37373635, Accuracy: 91.02%\n",
      "    0-1559 > Loss: 0.37841781, Accuracy: 91.02%\n",
      "    0-1560 > Loss: 0.37754875, Accuracy: 91.00%\n",
      "    0-1561 > Loss: 0.37697169, Accuracy: 90.93%\n",
      "    0-1562 > Loss: 0.37749503, Accuracy: 90.99%\n",
      "    0-1563 > Loss: 0.37656901, Accuracy: 90.96%\n",
      "    0-1564 > Loss: 0.37658896, Accuracy: 90.98%\n",
      "    0-1565 > Loss: 0.37544318, Accuracy: 90.97%\n",
      "    0-1566 > Loss: 0.37548169, Accuracy: 90.99%\n",
      "    0-1567 > Loss: 0.37743884, Accuracy: 90.93%\n",
      "    0-1568 > Loss: 0.37756652, Accuracy: 90.84%\n",
      "    0-1569 > Loss: 0.38044978, Accuracy: 90.81%\n",
      "    0-1570 > Loss: 0.37965374, Accuracy: 90.82%\n",
      "    0-1571 > Loss: 0.38094537, Accuracy: 90.85%\n",
      "    0-1572 > Loss: 0.38148660, Accuracy: 90.86%\n",
      "    0-1573 > Loss: 0.38409864, Accuracy: 90.84%\n",
      "    0-1574 > Loss: 0.38253357, Accuracy: 90.81%\n",
      "    0-1575 > Loss: 0.38460617, Accuracy: 90.93%\n",
      "    0-1576 > Loss: 0.38103316, Accuracy: 90.96%\n",
      "    0-1577 > Loss: 0.38008681, Accuracy: 90.94%\n",
      "    0-1578 > Loss: 0.37960364, Accuracy: 91.05%\n",
      "    0-1579 > Loss: 0.37851247, Accuracy: 90.89%\n",
      "    0-1580 > Loss: 0.37719149, Accuracy: 90.91%\n",
      "    0-1581 > Loss: 0.37789923, Accuracy: 91.00%\n",
      "    0-1582 > Loss: 0.37899826, Accuracy: 91.07%\n",
      "    0-1583 > Loss: 0.37915544, Accuracy: 90.97%\n",
      "    0-1584 > Loss: 0.38067187, Accuracy: 91.05%\n",
      "    0-1585 > Loss: 0.38068110, Accuracy: 91.02%\n",
      "    0-1586 > Loss: 0.38146005, Accuracy: 90.96%\n",
      "    0-1587 > Loss: 0.38173782, Accuracy: 91.03%\n",
      "    0-1588 > Loss: 0.38116878, Accuracy: 90.99%\n",
      "    0-1589 > Loss: 0.38123291, Accuracy: 91.01%\n",
      "    0-1590 > Loss: 0.38214917, Accuracy: 91.02%\n",
      "    0-1591 > Loss: 0.38052535, Accuracy: 91.03%\n",
      "    0-1592 > Loss: 0.37997057, Accuracy: 90.94%\n",
      "    0-1593 > Loss: 0.37996827, Accuracy: 90.88%\n",
      "    0-1594 > Loss: 0.37781631, Accuracy: 90.91%\n",
      "    0-1595 > Loss: 0.37767922, Accuracy: 90.99%\n",
      "    0-1596 > Loss: 0.38110084, Accuracy: 90.79%\n",
      "    0-1597 > Loss: 0.38051514, Accuracy: 90.85%\n",
      "    0-1598 > Loss: 0.38102173, Accuracy: 90.86%\n",
      "    0-1599 > Loss: 0.37864149, Accuracy: 90.96%\n",
      "    0-1600 > Loss: 0.37925250, Accuracy: 90.96%\n",
      "    0-1601 > Loss: 0.37795333, Accuracy: 91.01%\n",
      "    0-1602 > Loss: 0.37752441, Accuracy: 90.97%\n",
      "    0-1603 > Loss: 0.37893376, Accuracy: 90.83%\n",
      "    0-1604 > Loss: 0.38006553, Accuracy: 90.75%\n",
      "    0-1605 > Loss: 0.38354038, Accuracy: 90.74%\n",
      "    0-1606 > Loss: 0.38310426, Accuracy: 90.74%\n",
      "    0-1607 > Loss: 0.38136369, Accuracy: 90.89%\n",
      "    0-1608 > Loss: 0.38090572, Accuracy: 90.77%\n",
      "    0-1609 > Loss: 0.38041928, Accuracy: 90.81%\n",
      "    0-1610 > Loss: 0.37887958, Accuracy: 90.94%\n",
      "    0-1611 > Loss: 0.38007921, Accuracy: 90.74%\n",
      "    0-1612 > Loss: 0.37954376, Accuracy: 90.83%\n",
      "    0-1613 > Loss: 0.37972106, Accuracy: 90.82%\n",
      "    0-1614 > Loss: 0.38067439, Accuracy: 90.67%\n",
      "    0-1615 > Loss: 0.37875244, Accuracy: 90.78%\n",
      "    0-1616 > Loss: 0.37817463, Accuracy: 90.86%\n",
      "    0-1617 > Loss: 0.37931691, Accuracy: 90.81%\n",
      "    0-1618 > Loss: 0.37812435, Accuracy: 90.86%\n",
      "    0-1619 > Loss: 0.37727363, Accuracy: 90.82%\n",
      "    0-1620 > Loss: 0.37691086, Accuracy: 90.67%\n",
      "    0-1621 > Loss: 0.37430033, Accuracy: 90.81%\n",
      "    0-1622 > Loss: 0.37323681, Accuracy: 90.67%\n",
      "    0-1623 > Loss: 0.37031339, Accuracy: 90.90%\n",
      "    0-1624 > Loss: 0.37033229, Accuracy: 90.78%\n",
      "    0-1625 > Loss: 0.36913941, Accuracy: 90.73%\n",
      "    0-1626 > Loss: 0.36897608, Accuracy: 90.70%\n",
      "    0-1627 > Loss: 0.37103172, Accuracy: 90.80%\n",
      "    0-1628 > Loss: 0.37199806, Accuracy: 90.74%\n",
      "    0-1629 > Loss: 0.37186008, Accuracy: 90.64%\n",
      "    0-1630 > Loss: 0.37486080, Accuracy: 90.74%\n",
      "    0-1631 > Loss: 0.37048113, Accuracy: 90.97%\n",
      "    0-1632 > Loss: 0.37040157, Accuracy: 90.91%\n",
      "    0-1633 > Loss: 0.37068235, Accuracy: 90.80%\n",
      "    0-1634 > Loss: 0.36944073, Accuracy: 90.79%\n",
      "    0-1635 > Loss: 0.36874063, Accuracy: 90.84%\n",
      "    0-1636 > Loss: 0.36968397, Accuracy: 90.72%\n",
      "    0-1637 > Loss: 0.36863026, Accuracy: 90.83%\n",
      "    0-1638 > Loss: 0.36772768, Accuracy: 90.92%\n",
      "    0-1639 > Loss: 0.36990210, Accuracy: 90.93%\n",
      "    0-1640 > Loss: 0.36894514, Accuracy: 90.82%\n",
      "    0-1641 > Loss: 0.36752052, Accuracy: 90.83%\n",
      "    0-1642 > Loss: 0.36750213, Accuracy: 90.91%\n",
      "    0-1643 > Loss: 0.36836265, Accuracy: 90.91%\n",
      "    0-1644 > Loss: 0.36816253, Accuracy: 90.87%\n",
      "    0-1645 > Loss: 0.36886497, Accuracy: 90.80%\n",
      "    0-1646 > Loss: 0.37283957, Accuracy: 90.81%\n",
      "    0-1647 > Loss: 0.37474292, Accuracy: 90.69%\n",
      "    0-1648 > Loss: 0.37745726, Accuracy: 90.56%\n",
      "    0-1649 > Loss: 0.37571320, Accuracy: 90.68%\n",
      "    0-1650 > Loss: 0.37646215, Accuracy: 90.67%\n",
      "    0-1651 > Loss: 0.37606033, Accuracy: 90.69%\n",
      "    0-1652 > Loss: 0.37700436, Accuracy: 90.73%\n",
      "    0-1653 > Loss: 0.37693873, Accuracy: 90.83%\n",
      "    0-1654 > Loss: 0.37945229, Accuracy: 90.75%\n",
      "    0-1655 > Loss: 0.37812853, Accuracy: 90.77%\n",
      "    0-1656 > Loss: 0.37863428, Accuracy: 90.74%\n",
      "    0-1657 > Loss: 0.37751548, Accuracy: 90.91%\n",
      "    0-1658 > Loss: 0.37687157, Accuracy: 90.93%\n",
      "    0-1659 > Loss: 0.37719901, Accuracy: 90.86%\n",
      "    0-1660 > Loss: 0.37449889, Accuracy: 90.93%\n",
      "    0-1661 > Loss: 0.37455558, Accuracy: 90.87%\n",
      "    0-1662 > Loss: 0.37434049, Accuracy: 90.95%\n",
      "    0-1663 > Loss: 0.37502143, Accuracy: 90.81%\n",
      "    0-1664 > Loss: 0.37349158, Accuracy: 90.88%\n",
      "    0-1665 > Loss: 0.37087734, Accuracy: 90.93%\n",
      "    0-1666 > Loss: 0.36978865, Accuracy: 91.07%\n",
      "    0-1667 > Loss: 0.36883001, Accuracy: 91.07%\n",
      "    0-1668 > Loss: 0.36889608, Accuracy: 91.07%\n",
      "    0-1669 > Loss: 0.36789190, Accuracy: 91.07%\n",
      "    0-1670 > Loss: 0.36873772, Accuracy: 91.10%\n",
      "    0-1671 > Loss: 0.36971714, Accuracy: 90.95%\n",
      "    0-1672 > Loss: 0.37106368, Accuracy: 90.89%\n",
      "    0-1673 > Loss: 0.37350538, Accuracy: 90.71%\n",
      "    0-1674 > Loss: 0.37515154, Accuracy: 90.75%\n",
      "    0-1675 > Loss: 0.37534765, Accuracy: 90.75%\n",
      "    0-1676 > Loss: 0.37850792, Accuracy: 90.69%\n",
      "    0-1677 > Loss: 0.37619270, Accuracy: 90.68%\n",
      "    0-1678 > Loss: 0.37501484, Accuracy: 90.81%\n",
      "    0-1679 > Loss: 0.37471290, Accuracy: 90.75%\n",
      "    0-1680 > Loss: 0.37500414, Accuracy: 90.76%\n",
      "    0-1681 > Loss: 0.37411910, Accuracy: 90.68%\n",
      "    0-1682 > Loss: 0.37193496, Accuracy: 90.84%\n",
      "    0-1683 > Loss: 0.37649916, Accuracy: 90.59%\n",
      "    0-1684 > Loss: 0.37678430, Accuracy: 90.55%\n",
      "    0-1685 > Loss: 0.37469541, Accuracy: 90.58%\n",
      "    0-1686 > Loss: 0.37432201, Accuracy: 90.54%\n",
      "    0-1687 > Loss: 0.37421384, Accuracy: 90.48%\n",
      "    0-1688 > Loss: 0.37277870, Accuracy: 90.58%\n",
      "    0-1689 > Loss: 0.36988266, Accuracy: 90.79%\n",
      "    0-1690 > Loss: 0.36589900, Accuracy: 91.15%\n",
      "    0-1691 > Loss: 0.36582541, Accuracy: 91.18%\n",
      "    0-1692 > Loss: 0.36486463, Accuracy: 91.22%\n",
      "    0-1693 > Loss: 0.36524172, Accuracy: 91.05%\n",
      "    0-1694 > Loss: 0.36506513, Accuracy: 91.05%\n",
      "    0-1695 > Loss: 0.36462498, Accuracy: 91.13%\n",
      "    0-1696 > Loss: 0.36339271, Accuracy: 91.16%\n",
      "    0-1697 > Loss: 0.36161715, Accuracy: 91.19%\n",
      "    0-1698 > Loss: 0.36306919, Accuracy: 91.24%\n",
      "    0-1699 > Loss: 0.36406949, Accuracy: 91.16%\n",
      "    0-1700 > Loss: 0.36405479, Accuracy: 91.05%\n",
      "    0-1701 > Loss: 0.36389681, Accuracy: 91.23%\n",
      "    0-1702 > Loss: 0.36299470, Accuracy: 91.22%\n",
      "    0-1703 > Loss: 0.36256930, Accuracy: 91.21%\n",
      "    0-1704 > Loss: 0.36107906, Accuracy: 91.05%\n",
      "    0-1705 > Loss: 0.36201619, Accuracy: 91.23%\n",
      "    0-1706 > Loss: 0.36067752, Accuracy: 91.18%\n",
      "    0-1707 > Loss: 0.36097153, Accuracy: 91.22%\n",
      "    0-1708 > Loss: 0.36153226, Accuracy: 91.10%\n",
      "    0-1709 > Loss: 0.35943376, Accuracy: 91.15%\n",
      "    0-1710 > Loss: 0.35852215, Accuracy: 91.17%\n",
      "    0-1711 > Loss: 0.35778877, Accuracy: 91.21%\n",
      "    0-1712 > Loss: 0.35996179, Accuracy: 90.97%\n",
      "    0-1713 > Loss: 0.36004964, Accuracy: 90.98%\n",
      "    0-1714 > Loss: 0.36080218, Accuracy: 90.90%\n",
      "    0-1715 > Loss: 0.36098084, Accuracy: 90.98%\n",
      "    0-1716 > Loss: 0.35982421, Accuracy: 91.25%\n",
      "    0-1717 > Loss: 0.35985797, Accuracy: 91.24%\n",
      "    0-1718 > Loss: 0.35929554, Accuracy: 91.31%\n",
      "    0-1719 > Loss: 0.35930706, Accuracy: 91.26%\n",
      "    0-1720 > Loss: 0.35923758, Accuracy: 91.33%\n",
      "    0-1721 > Loss: 0.35806111, Accuracy: 91.39%\n",
      "    0-1722 > Loss: 0.35816791, Accuracy: 91.36%\n",
      "    0-1723 > Loss: 0.35850018, Accuracy: 91.25%\n",
      "    0-1724 > Loss: 0.36043567, Accuracy: 91.14%\n",
      "    0-1725 > Loss: 0.35934078, Accuracy: 91.23%\n",
      "    0-1726 > Loss: 0.35884091, Accuracy: 91.28%\n",
      "    0-1727 > Loss: 0.35986513, Accuracy: 91.27%\n",
      "    0-1728 > Loss: 0.36201979, Accuracy: 91.23%\n",
      "    0-1729 > Loss: 0.36422649, Accuracy: 91.15%\n",
      "    0-1730 > Loss: 0.36270232, Accuracy: 91.13%\n",
      "    0-1731 > Loss: 0.36315792, Accuracy: 91.12%\n",
      "    0-1732 > Loss: 0.36454281, Accuracy: 91.11%\n",
      "    0-1733 > Loss: 0.36284337, Accuracy: 91.24%\n",
      "    0-1734 > Loss: 0.36335488, Accuracy: 91.18%\n",
      "    0-1735 > Loss: 0.36251551, Accuracy: 91.27%\n",
      "    0-1736 > Loss: 0.36088069, Accuracy: 91.28%\n",
      "    0-1737 > Loss: 0.36003536, Accuracy: 91.34%\n",
      "    0-1738 > Loss: 0.35989183, Accuracy: 91.20%\n",
      "    0-1739 > Loss: 0.35847963, Accuracy: 91.25%\n",
      "    0-1740 > Loss: 0.35814260, Accuracy: 91.21%\n",
      "    0-1741 > Loss: 0.35989965, Accuracy: 91.15%\n",
      "    0-1742 > Loss: 0.36040698, Accuracy: 91.25%\n",
      "    0-1743 > Loss: 0.36013391, Accuracy: 91.13%\n",
      "    0-1744 > Loss: 0.35909926, Accuracy: 91.24%\n",
      "    0-1745 > Loss: 0.35894209, Accuracy: 91.19%\n",
      "    0-1746 > Loss: 0.35990458, Accuracy: 91.26%\n",
      "    0-1747 > Loss: 0.35850270, Accuracy: 91.34%\n",
      "    0-1748 > Loss: 0.35792808, Accuracy: 91.36%\n",
      "    0-1749 > Loss: 0.35711651, Accuracy: 91.37%\n",
      "    0-1750 > Loss: 0.35725383, Accuracy: 91.22%\n",
      "    0-1751 > Loss: 0.35747789, Accuracy: 91.27%\n",
      "    0-1752 > Loss: 0.35619885, Accuracy: 91.32%\n",
      "    0-1753 > Loss: 0.35649683, Accuracy: 91.25%\n",
      "    0-1754 > Loss: 0.35694720, Accuracy: 91.29%\n",
      "    0-1755 > Loss: 0.35682393, Accuracy: 91.27%\n",
      "    0-1756 > Loss: 0.35659655, Accuracy: 91.14%\n",
      "    0-1757 > Loss: 0.35624160, Accuracy: 91.30%\n",
      "    0-1758 > Loss: 0.35615995, Accuracy: 91.13%\n",
      "    0-1759 > Loss: 0.35492078, Accuracy: 91.04%\n",
      "    0-1760 > Loss: 0.35439775, Accuracy: 91.09%\n",
      "    0-1761 > Loss: 0.35426623, Accuracy: 91.11%\n",
      "    0-1762 > Loss: 0.35214873, Accuracy: 91.01%\n",
      "    0-1763 > Loss: 0.35197197, Accuracy: 90.97%\n",
      "    0-1764 > Loss: 0.35357708, Accuracy: 90.99%\n",
      "    0-1765 > Loss: 0.35501038, Accuracy: 90.86%\n",
      "    0-1766 > Loss: 0.35396554, Accuracy: 90.97%\n",
      "    0-1767 > Loss: 0.35339253, Accuracy: 91.06%\n",
      "    0-1768 > Loss: 0.35312168, Accuracy: 91.04%\n",
      "    0-1769 > Loss: 0.35247238, Accuracy: 91.02%\n",
      "    0-1770 > Loss: 0.35375916, Accuracy: 91.08%\n",
      "    0-1771 > Loss: 0.35250741, Accuracy: 91.13%\n",
      "    0-1772 > Loss: 0.35173639, Accuracy: 91.21%\n",
      "    0-1773 > Loss: 0.35065530, Accuracy: 91.22%\n",
      "    0-1774 > Loss: 0.34967647, Accuracy: 91.18%\n",
      "    0-1775 > Loss: 0.35049032, Accuracy: 91.15%\n",
      "    0-1776 > Loss: 0.35122829, Accuracy: 91.10%\n",
      "    0-1777 > Loss: 0.35312586, Accuracy: 91.02%\n",
      "    0-1778 > Loss: 0.35331078, Accuracy: 91.04%\n",
      "    0-1779 > Loss: 0.35268332, Accuracy: 91.12%\n",
      "    0-1780 > Loss: 0.35163792, Accuracy: 91.12%\n",
      "    0-1781 > Loss: 0.35066890, Accuracy: 91.04%\n",
      "    0-1782 > Loss: 0.35005588, Accuracy: 91.02%\n",
      "    0-1783 > Loss: 0.34961617, Accuracy: 90.94%\n",
      "    0-1784 > Loss: 0.34892781, Accuracy: 91.03%\n",
      "    0-1785 > Loss: 0.34835601, Accuracy: 91.15%\n",
      "    0-1786 > Loss: 0.34710607, Accuracy: 91.24%\n",
      "    0-1787 > Loss: 0.34566772, Accuracy: 91.22%\n",
      "    0-1788 > Loss: 0.34612946, Accuracy: 91.22%\n",
      "    0-1789 > Loss: 0.34731062, Accuracy: 91.22%\n",
      "    0-1790 > Loss: 0.34793894, Accuracy: 91.16%\n",
      "    0-1791 > Loss: 0.34884498, Accuracy: 91.27%\n",
      "    0-1792 > Loss: 0.34906509, Accuracy: 91.35%\n",
      "    0-1793 > Loss: 0.35068235, Accuracy: 91.23%\n",
      "    0-1794 > Loss: 0.34929186, Accuracy: 91.22%\n",
      "    0-1795 > Loss: 0.34745632, Accuracy: 91.18%\n",
      "    0-1796 > Loss: 0.34723864, Accuracy: 91.09%\n",
      "    0-1797 > Loss: 0.34718486, Accuracy: 91.10%\n",
      "    0-1798 > Loss: 0.34673337, Accuracy: 91.27%\n",
      "    0-1799 > Loss: 0.34647226, Accuracy: 91.20%\n",
      "    0-1800 > Loss: 0.34628162, Accuracy: 91.09%\n",
      "    0-1801 > Loss: 0.35088430, Accuracy: 91.00%\n",
      "    0-1802 > Loss: 0.34863002, Accuracy: 91.04%\n",
      "    0-1803 > Loss: 0.34985573, Accuracy: 91.01%\n",
      "    0-1804 > Loss: 0.34931451, Accuracy: 90.98%\n",
      "    0-1805 > Loss: 0.34971135, Accuracy: 90.98%\n",
      "    0-1806 > Loss: 0.34829225, Accuracy: 91.24%\n",
      "    0-1807 > Loss: 0.34838023, Accuracy: 91.17%\n",
      "    0-1808 > Loss: 0.34743741, Accuracy: 91.24%\n",
      "    0-1809 > Loss: 0.34895605, Accuracy: 91.21%\n",
      "    0-1810 > Loss: 0.34957845, Accuracy: 91.24%\n",
      "    0-1811 > Loss: 0.35029447, Accuracy: 91.12%\n",
      "    0-1812 > Loss: 0.34899555, Accuracy: 91.12%\n",
      "    0-1813 > Loss: 0.34796974, Accuracy: 91.21%\n",
      "    0-1814 > Loss: 0.34645053, Accuracy: 91.24%\n",
      "    0-1815 > Loss: 0.34538225, Accuracy: 91.42%\n",
      "    0-1816 > Loss: 0.34429240, Accuracy: 91.44%\n",
      "    0-1817 > Loss: 0.34312596, Accuracy: 91.37%\n",
      "    0-1818 > Loss: 0.34254076, Accuracy: 91.38%\n",
      "    0-1819 > Loss: 0.34209979, Accuracy: 91.27%\n",
      "    0-1820 > Loss: 0.34148789, Accuracy: 91.33%\n",
      "    0-1821 > Loss: 0.34188705, Accuracy: 91.34%\n",
      "    0-1822 > Loss: 0.34219717, Accuracy: 91.31%\n",
      "    0-1823 > Loss: 0.34238161, Accuracy: 91.29%\n",
      "    0-1824 > Loss: 0.34228896, Accuracy: 91.25%\n",
      "    0-1825 > Loss: 0.34196931, Accuracy: 91.26%\n",
      "    0-1826 > Loss: 0.34235781, Accuracy: 91.22%\n",
      "    0-1827 > Loss: 0.34456955, Accuracy: 91.05%\n",
      "    0-1828 > Loss: 0.34457804, Accuracy: 91.10%\n",
      "    0-1829 > Loss: 0.34226569, Accuracy: 91.24%\n",
      "    0-1830 > Loss: 0.34227146, Accuracy: 91.23%\n",
      "    0-1831 > Loss: 0.34275865, Accuracy: 91.14%\n",
      "    0-1832 > Loss: 0.34259567, Accuracy: 91.07%\n",
      "    0-1833 > Loss: 0.34133994, Accuracy: 91.18%\n",
      "    0-1834 > Loss: 0.34077728, Accuracy: 91.28%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mneural_network\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m w1, w2 = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m               \u001B[49m\u001B[43mn_hidden_nodes\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\7-Learning\\9-AI_ML_DL\\linear-regression-ml\\mini-batch-gradient-descent\\neural_network.py:100\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(X_train, Y_train, X_test, Y_test, n_hidden_nodes, epochs, batch_size, lr)\u001B[39m\n\u001B[32m     98\u001B[39m         w1 = w1 - (w1_gradient * lr)\n\u001B[32m     99\u001B[39m         w2 = w2 - (w2_gradient * lr)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m         \u001B[43mreport\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (w1, w2)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\7-Learning\\9-AI_ML_DL\\linear-regression-ml\\mini-batch-gradient-descent\\neural_network.py:77\u001B[39m, in \u001B[36mreport\u001B[39m\u001B[34m(epoch, batch, X_train, Y_train, X_test, Y_test, w1, w2)\u001B[39m\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mreport\u001B[39m(epoch, batch, X_train, Y_train, X_test, Y_test, w1, w2):\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m     y_hat, _ = \u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     78\u001B[39m     training_loss = loss(Y_train, y_hat)\n\u001B[32m     79\u001B[39m     classifications = classify(X_test, w1, w2)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\7-Learning\\9-AI_ML_DL\\linear-regression-ml\\mini-batch-gradient-descent\\neural_network.py:34\u001B[39m, in \u001B[36mforward\u001B[39m\u001B[34m(X, w1, w2)\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(X, w1, w2):\n\u001B[32m     33\u001B[39m     h = sigmoid(np.matmul(prepend_bias(X), w1))\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m     y_hat = softmax(np.matmul(\u001B[43mprepend_bias\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m)\u001B[49m, w2))\n\u001B[32m     35\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m y_hat, h\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\7-Learning\\9-AI_ML_DL\\linear-regression-ml\\mini-batch-gradient-descent\\neural_network.py:26\u001B[39m, in \u001B[36mprepend_bias\u001B[39m\u001B[34m(X)\u001B[39m\n\u001B[32m     22\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m -np.sum(Y * np.log(y_hat)) / Y.shape[\u001B[32m0\u001B[39m]\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# Adding bias\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprepend_bias\u001B[39m(X):\n\u001B[32m     27\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m np.insert(X, \u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, axis=\u001B[32m1\u001B[39m)\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# Basically doing prediction but named forward as its\u001B[39;00m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# performing Forward-Propagation\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
