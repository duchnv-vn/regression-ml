{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:09:05.284866Z",
     "start_time": "2025-05-27T16:07:55.088545Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# computing the predictions\n",
    "def predict(X, w):\n",
    "    return np.matmul(X, w)\n",
    "\n",
    "\n",
    "# calculating the loss\n",
    "def loss(X, Y, w):\n",
    "    return np.average((predict(X, w) - Y) ** 2)\n",
    "\n",
    "\n",
    "def gradient(X, Y, w):\n",
    "    return 2 * np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration %4d => Loss: %.20f\" % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w\n",
    "\n",
    "\n",
    "# Import the dataset\n",
    "data = np.loadtxt(\"pizza_3_vars.txt\", skiprows=1)\n",
    "X = data[:, :-1]\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "Y = data[:, -1].reshape(-1, 1)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "w = train(X, Y, iterations=50000, lr=0.001)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 4)\n",
      "(30, 1)\n",
      "Iteration    0 => Loss: 1333.56666666666660603369\n",
      "Iteration    1 => Loss: 152.37148173674077611395\n",
      "Iteration    2 => Loss: 65.17252143398702912691\n",
      "Iteration    3 => Loss: 57.29348107043313831355\n",
      "Iteration    4 => Loss: 55.24408375010616367717\n",
      "Iteration    5 => Loss: 53.69548034496939692417\n",
      "Iteration    6 => Loss: 52.25567333361473743025\n",
      "Iteration    7 => Loss: 50.89073275996361189755\n",
      "Iteration    8 => Loss: 49.59315053477826751305\n",
      "Iteration    9 => Loss: 48.35777747932070980141\n",
      "Iteration   10 => Loss: 47.18003972981341576087\n",
      "Iteration   11 => Loss: 46.05577241746296834890\n",
      "Iteration   12 => Loss: 44.98117392026742322741\n",
      "Iteration   13 => Loss: 43.95277197431995830357\n",
      "Iteration   14 => Loss: 42.96739348065429453527\n",
      "Iteration   15 => Loss: 42.02213715248831249482\n",
      "Iteration   16 => Loss: 41.11434870159362731101\n",
      "Iteration   17 => Loss: 40.24159832491247357211\n",
      "Iteration   18 => Loss: 39.40166027726748865234\n",
      "Iteration   19 => Loss: 38.59249433612597357524\n",
      "Iteration   20 => Loss: 37.81222898245525243510\n",
      "Iteration   21 => Loss: 37.05914613808704416442\n",
      "Iteration   22 => Loss: 36.33166731486301870291\n",
      "Iteration   23 => Loss: 35.62834104430513804118\n",
      "Iteration   24 => Loss: 34.94783146877178836576\n",
      "Iteration   25 => Loss: 34.28890798614057899840\n",
      "Iteration   26 => Loss: 33.65043585010659654699\n",
      "Iteration   27 => Loss: 33.03136763729814617818\n",
      "Iteration   28 => Loss: 32.43073550067536103825\n",
      "Iteration   29 => Loss: 31.84764413617338263407\n",
      "Iteration   30 => Loss: 31.28126439634725031169\n",
      "Iteration   31 => Loss: 30.73082749094158927505\n",
      "Iteration   32 => Loss: 30.19561971989698179186\n",
      "Iteration   33 => Loss: 29.67497768937651159149\n",
      "Iteration   34 => Loss: 29.16828396599244399567\n",
      "Iteration   35 => Loss: 28.67496312858387952360\n",
      "Iteration   36 => Loss: 28.19447818067768452011\n",
      "Iteration   37 => Loss: 27.72632729019449726593\n",
      "Iteration   38 => Loss: 27.27004082607204082933\n",
      "Iteration   39 => Loss: 26.82517866429934372263\n",
      "Iteration   40 => Loss: 26.39132773841262391556\n",
      "Iteration   41 => Loss: 25.96809981182499527108\n",
      "Iteration   42 => Loss: 25.55512945146533709817\n",
      "Iteration   43 => Loss: 25.15207218411041623085\n",
      "Iteration   44 => Loss: 24.75860281852498800959\n",
      "Iteration   45 => Loss: 24.37441391809391433299\n",
      "Iteration   46 => Loss: 23.99921441005419708858\n",
      "Iteration   47 => Loss: 23.63272831872566115408\n",
      "Iteration   48 => Loss: 23.27469361130964031759\n",
      "Iteration   49 => Loss: 22.92486114588738388420\n",
      "Iteration   50 => Loss: 22.58299371221250595454\n",
      "Iteration   51 => Loss: 22.24886515676564613386\n",
      "Iteration   52 => Loss: 21.92225958433097332545\n",
      "Iteration   53 => Loss: 21.60297062907391563158\n",
      "Iteration   54 => Loss: 21.29080078874931913901\n",
      "Iteration   55 => Loss: 20.98556081626192337808\n",
      "Iteration   56 => Loss: 20.68706916333597334301\n",
      "Iteration   57 => Loss: 20.39515147153707630423\n",
      "Iteration   58 => Loss: 20.10964010633085763402\n",
      "Iteration   59 => Loss: 19.83037373026216343419\n",
      "Iteration   60 => Loss: 19.55719691170171259387\n",
      "Iteration   61 => Loss: 19.28995976593592587278\n",
      "Iteration   62 => Loss: 19.02851762567396320947\n",
      "Iteration   63 => Loss: 18.77273073831700855862\n",
      "Iteration   64 => Loss: 18.52246398757958090187\n",
      "Iteration   65 => Loss: 18.27758663727617260975\n",
      "Iteration   66 => Loss: 18.03797209528795164601\n",
      "Iteration   67 => Loss: 17.80349769590739938963\n",
      "Iteration   68 => Loss: 17.57404449892518627507\n",
      "Iteration   69 => Loss: 17.34949710397455646671\n",
      "Iteration   70 => Loss: 17.12974347878415315449\n",
      "Iteration   71 => Loss: 16.91467480011579738175\n",
      "Iteration   72 => Loss: 16.70418530627479469786\n",
      "Iteration   73 => Loss: 16.49817216018341170525\n",
      "Iteration   74 => Loss: 16.29653532210031841032\n",
      "Iteration   75 => Loss: 16.09917743115273935928\n",
      "Iteration   76 => Loss: 15.90600369492455001819\n",
      "Iteration   77 => Loss: 15.71692178641252013449\n",
      "Iteration   78 => Loss: 15.53184174772565206979\n",
      "Iteration   79 => Loss: 15.35067589995937531455\n",
      "Iteration   80 => Loss: 15.17333875872839854537\n",
      "Iteration   81 => Loss: 14.99974695488835862989\n",
      "Iteration   82 => Loss: 14.82981916001932454208\n",
      "Iteration   83 => Loss: 14.66347601628250352235\n",
      "Iteration   84 => Loss: 14.50064007029704882257\n",
      "Iteration   85 => Loss: 14.34123571071471836547\n",
      "Iteration   86 => Loss: 14.18518910920014874932\n",
      "Iteration   87 => Loss: 14.03242816454965513628\n",
      "Iteration   88 => Loss: 13.88288244970590490368\n",
      "Iteration   89 => Loss: 13.73648316144712566711\n",
      "Iteration   90 => Loss: 13.59316307254914235614\n",
      "Iteration   91 => Loss: 13.45285648623623409037\n",
      "Iteration   92 => Loss: 13.31549919275329330048\n",
      "Iteration   93 => Loss: 13.18102842790592710287\n",
      "Iteration   94 => Loss: 13.04938283342887750393\n",
      "Iteration   95 => Loss: 12.92050241905510965523\n",
      "Iteration   96 => Loss: 12.79432852616885085695\n",
      "Iteration   97 => Loss: 12.67080379293586922529\n",
      "Iteration   98 => Loss: 12.54987212081349667869\n",
      "Iteration   99 => Loss: 12.43147864235101351937\n",
      "Iteration  100 => Loss: 12.31556969019837310952\n",
      "Iteration  101 => Loss: 12.20209276724857616614\n",
      "Iteration  102 => Loss: 12.09099651784453399728\n",
      "Iteration  103 => Loss: 11.98223069998744527709\n",
      "Iteration  104 => Loss: 11.87574615848847159327\n",
      "Iteration  105 => Loss: 11.77149479901047257613\n",
      "Iteration  106 => Loss: 11.66942956295058664296\n",
      "Iteration  107 => Loss: 11.56950440311833183671\n",
      "Iteration  108 => Loss: 11.47167426016751612394\n",
      "Iteration  109 => Loss: 11.37589503974357363347\n",
      "Iteration  110 => Loss: 11.28212359031036982060\n",
      "Iteration  111 => Loss: 11.19031768162393447597\n",
      "Iteration  112 => Loss: 11.10043598382222818088\n",
      "Iteration  113 => Loss: 11.01243804710299301064\n",
      "Iteration  114 => Loss: 10.92628428196308654208\n",
      "Iteration  115 => Loss: 10.84193593997517801597\n",
      "Iteration  116 => Loss: 10.75935509507888454550\n",
      "Iteration  117 => Loss: 10.67850462536518918455\n",
      "Iteration  118 => Loss: 10.59934819533444816386\n",
      "Iteration  119 => Loss: 10.52185023860941370799\n",
      "Iteration  120 => Loss: 10.44597594108608973329\n",
      "Iteration  121 => Loss: 10.37169122450598557350\n",
      "Iteration  122 => Loss: 10.29896273043494048238\n",
      "Iteration  123 => Loss: 10.22775780463370587370\n",
      "Iteration  124 => Loss: 10.15804448180733920992\n",
      "Iteration  125 => Loss: 10.08979147072032311883\n",
      "Iteration  126 => Loss: 10.02296813966565380838\n",
      "Iteration  127 => Loss: 9.95754450227640219850\n",
      "Iteration  128 => Loss: 9.89349120366926548797\n",
      "Iteration  129 => Loss: 9.83077950690958601854\n",
      "Iteration  130 => Loss: 9.76938127978855597178\n",
      "Iteration  131 => Loss: 9.70926898190307241521\n",
      "Iteration  132 => Loss: 9.65041565202981210803\n",
      "Iteration  133 => Loss: 9.59279489578493560487\n",
      "Iteration  134 => Loss: 9.53638087356158692387\n",
      "Iteration  135 => Loss: 9.48114828873755222105\n",
      "Iteration  136 => Loss: 9.42707237614581750051\n",
      "Iteration  137 => Loss: 9.37412889080096789485\n",
      "Iteration  138 => Loss: 9.32229409687483290270\n",
      "Iteration  139 => Loss: 9.27154475691484769584\n",
      "Iteration  140 => Loss: 9.22185812129902160450\n",
      "Iteration  141 => Loss: 9.17321191792151147126\n",
      "Iteration  142 => Loss: 9.12558434210311197887\n",
      "Iteration  143 => Loss: 9.07895404672107808608\n",
      "Iteration  144 => Loss: 9.03330013255295760644\n",
      "Iteration  145 => Loss: 8.98860213882929848239\n",
      "Iteration  146 => Loss: 8.94484003399026583736\n",
      "Iteration  147 => Loss: 8.90199420664119678293\n",
      "Iteration  148 => Loss: 8.86004545670268051083\n",
      "Iteration  149 => Loss: 8.81897498675036040083\n",
      "Iteration  150 => Loss: 8.77876439354023574424\n",
      "Iteration  151 => Loss: 8.73939565971519982668\n",
      "Iteration  152 => Loss: 8.70085114568854578465\n",
      "Iteration  153 => Loss: 8.66311358170059264694\n",
      "Iteration  154 => Loss: 8.62616606004442587619\n",
      "Iteration  155 => Loss: 8.58999202745695278338\n",
      "Iteration  156 => Loss: 8.55457527767164904731\n",
      "Iteration  157 => Loss: 8.51989994412929618761\n",
      "Iteration  158 => Loss: 8.48595049284331182093\n",
      "Iteration  159 => Loss: 8.45271171541623012047\n",
      "Iteration  160 => Loss: 8.42016872220395207194\n",
      "Iteration  161 => Loss: 8.38830693562460005808\n",
      "Iteration  162 => Loss: 8.35711208360884150181\n",
      "Iteration  163 => Loss: 8.32657019318846103317\n",
      "Iteration  164 => Loss: 8.29666758422038697063\n",
      "Iteration  165 => Loss: 8.26739086324298533270\n",
      "Iteration  166 => Loss: 8.23872691746209184771\n",
      "Iteration  167 => Loss: 8.21066290886361649370\n",
      "Iteration  168 => Loss: 8.18318626845027630168\n",
      "Iteration  169 => Loss: 8.15628469059972793787\n",
      "Iteration  170 => Loss: 8.12994612754133960664\n",
      "Iteration  171 => Loss: 8.10415878394931077366\n",
      "Iteration  172 => Loss: 8.07891111164949471402\n",
      "Iteration  173 => Loss: 8.05419180443751514531\n",
      "Iteration  174 => Loss: 8.02998979300581972041\n",
      "Iteration  175 => Loss: 8.00629423997743394636\n",
      "Iteration  176 => Loss: 7.98309453504396415724\n",
      "Iteration  177 => Loss: 7.96038029020586623830\n",
      "Iteration  178 => Loss: 7.93814133511256514453\n",
      "Iteration  179 => Loss: 7.91636771250052806437\n",
      "Iteration  180 => Loss: 7.89504967372707433526\n",
      "Iteration  181 => Loss: 7.87417767439797522400\n",
      "Iteration  182 => Loss: 7.85374237008679454419\n",
      "Iteration  183 => Loss: 7.83373461214410937714\n",
      "Iteration  184 => Loss: 7.81414544359462936995\n",
      "Iteration  185 => Loss: 7.79496609512043470147\n",
      "Iteration  186 => Loss: 7.77618798112845954762\n",
      "Iteration  187 => Loss: 7.75780269590048465744\n",
      "Iteration  188 => Loss: 7.73980200982388755193\n",
      "Iteration  189 => Loss: 7.72217786570140063418\n",
      "Iteration  190 => Loss: 7.70492237513834599127\n",
      "Iteration  191 => Loss: 7.68802781500553411576\n",
      "Iteration  192 => Loss: 7.67148662397638148036\n",
      "Iteration  193 => Loss: 7.65529139913667755479\n",
      "Iteration  194 => Loss: 7.63943489266530217918\n",
      "Iteration  195 => Loss: 7.62391000858464895629\n",
      "Iteration  196 => Loss: 7.60870979957913284863\n",
      "Iteration  197 => Loss: 7.59382746388037599417\n",
      "Iteration  198 => Loss: 7.57925634221769595200\n",
      "Iteration  199 => Loss: 7.56498991483251703727\n",
      "Iteration  200 => Loss: 7.55102179855533428565\n",
      "Iteration  201 => Loss: 7.53734574394399370334\n",
      "Iteration  202 => Loss: 7.52395563248189258587\n",
      "Iteration  203 => Loss: 7.51084547383492306949\n",
      "Iteration  204 => Loss: 7.49800940316591901791\n",
      "Iteration  205 => Loss: 7.48544167850535568931\n",
      "Iteration  206 => Loss: 7.47313667817715376884\n",
      "Iteration  207 => Loss: 7.46108889827843491105\n",
      "Iteration  208 => Loss: 7.44929295021211324013\n",
      "Iteration  209 => Loss: 7.43774355827111399719\n",
      "Iteration  210 => Loss: 7.42643555727331250438\n",
      "Iteration  211 => Loss: 7.41536389024594466690\n",
      "Iteration  212 => Loss: 7.40452360615859639381\n",
      "Iteration  213 => Loss: 7.39390985770367592522\n",
      "Iteration  214 => Loss: 7.38351789912336275989\n",
      "Iteration  215 => Loss: 7.37334308408216010378\n",
      "Iteration  216 => Loss: 7.36338086358390775388\n",
      "Iteration  217 => Loss: 7.35362678393257684917\n",
      "Iteration  218 => Loss: 7.34407648473567942204\n",
      "Iteration  219 => Loss: 7.33472569694957154951\n",
      "Iteration  220 => Loss: 7.32557024096567577232\n",
      "Iteration  221 => Loss: 7.31660602473684118507\n",
      "Iteration  222 => Loss: 7.30782904194288462918\n",
      "Iteration  223 => Loss: 7.29923537019468771092\n",
      "Iteration  224 => Loss: 7.29082116927577317256\n",
      "Iteration  225 => Loss: 7.28258267942085524282\n",
      "Iteration  226 => Loss: 7.27451621963036654250\n",
      "Iteration  227 => Loss: 7.26661818602032028025\n",
      "Iteration  228 => Loss: 7.25888505020677499147\n",
      "Iteration  229 => Loss: 7.25131335772414153951\n",
      "Iteration  230 => Loss: 7.24389972647654367677\n",
      "Iteration  231 => Loss: 7.23664084522175077296\n",
      "Iteration  232 => Loss: 7.22953347208677055136\n",
      "Iteration  233 => Loss: 7.22257443311460001212\n",
      "Iteration  234 => Loss: 7.21576062084140179564\n",
      "Iteration  235 => Loss: 7.20908899290351534717\n",
      "Iteration  236 => Loss: 7.20255657067355414824\n",
      "Iteration  237 => Loss: 7.19616043792521864475\n",
      "Iteration  238 => Loss: 7.18989773952598909545\n",
      "Iteration  239 => Loss: 7.18376568015724270566\n",
      "Iteration  240 => Loss: 7.17776152306112180668\n",
      "Iteration  241 => Loss: 7.17188258881378803977\n",
      "Iteration  242 => Loss: 7.16612625412419923521\n",
      "Iteration  243 => Loss: 7.16048995065825000239\n",
      "Iteration  244 => Loss: 7.15497116388733989112\n",
      "Iteration  245 => Loss: 7.14956743196124566708\n",
      "Iteration  246 => Loss: 7.14427634460447169573\n",
      "Iteration  247 => Loss: 7.13909554203582441545\n",
      "Iteration  248 => Loss: 7.13402271391055187166\n",
      "Iteration  249 => Loss: 7.12905559828466817152\n",
      "Iteration  250 => Loss: 7.12419198060096814373\n",
      "Iteration  251 => Loss: 7.11942969269631653617\n",
      "Iteration  252 => Loss: 7.11476661182967973218\n",
      "Iteration  253 => Loss: 7.11020065973059978148\n",
      "Iteration  254 => Loss: 7.10572980166747480979\n",
      "Iteration  255 => Loss: 7.10135204553550281048\n",
      "Iteration  256 => Loss: 7.09706544096366087615\n",
      "Iteration  257 => Loss: 7.09286807844035305237\n",
      "Iteration  258 => Loss: 7.08875808845746835374\n",
      "Iteration  259 => Loss: 7.08473364067230804153\n",
      "Iteration  260 => Loss: 7.08079294308707130057\n",
      "Iteration  261 => Loss: 7.07693424124556624832\n",
      "Iteration  262 => Loss: 7.07315581744673771425\n",
      "Iteration  263 => Loss: 7.06945598997461122082\n",
      "Iteration  264 => Loss: 7.06583311234446753701\n",
      "Iteration  265 => Loss: 7.06228557256476108250\n",
      "Iteration  266 => Loss: 7.05881179241447309636\n",
      "Iteration  267 => Loss: 7.05541022673564555134\n",
      "Iteration  268 => Loss: 7.05207936274073077243\n",
      "Iteration  269 => Loss: 7.04881771933446010792\n",
      "Iteration  270 => Loss: 7.04562384644984174287\n",
      "Iteration  271 => Loss: 7.04249632439821393604\n",
      "Iteration  272 => Loss: 7.03943376323276304163\n",
      "Iteration  273 => Loss: 7.03643480212549476960\n",
      "Iteration  274 => Loss: 7.03349810875714975822\n",
      "Iteration  275 => Loss: 7.03062237872001549732\n",
      "Iteration  276 => Loss: 7.02780633493317541394\n",
      "Iteration  277 => Loss: 7.02504872706998639842\n",
      "Iteration  278 => Loss: 7.02234833099764443887\n",
      "Iteration  279 => Loss: 7.01970394822837739923\n",
      "Iteration  280 => Loss: 7.01711440538224362484\n",
      "Iteration  281 => Loss: 7.01457855366110027973\n",
      "Iteration  282 => Loss: 7.01209526833364993337\n",
      "Iteration  283 => Loss: 7.00966344823121367824\n",
      "Iteration  284 => Loss: 7.00728201525413751938\n",
      "Iteration  285 => Loss: 7.00494991388849630454\n",
      "Iteration  286 => Loss: 7.00266611073292732925\n",
      "Iteration  287 => Loss: 7.00042959403537512486\n",
      "Iteration  288 => Loss: 6.99823937323957867562\n",
      "Iteration  289 => Loss: 6.99609447854099375519\n",
      "Iteration  290 => Loss: 6.99399396045205534733\n",
      "Iteration  291 => Loss: 6.99193688937656432358\n",
      "Iteration  292 => Loss: 6.98992235519297455681\n",
      "Iteration  293 => Loss: 6.98794946684637885426\n",
      "Iteration  294 => Loss: 6.98601735194908801674\n",
      "Iteration  295 => Loss: 6.98412515638956854502\n",
      "Iteration  296 => Loss: 6.98227204394955158762\n",
      "Iteration  297 => Loss: 6.98045719592918167962\n",
      "Iteration  298 => Loss: 6.97867981078002230788\n",
      "Iteration  299 => Loss: 6.97693910374570691602\n",
      "Iteration  300 => Loss: 6.97523430651019893389\n",
      "Iteration  301 => Loss: 6.97356466685333487021\n",
      "Iteration  302 => Loss: 6.97192944831365579716\n",
      "Iteration  303 => Loss: 6.97032792985823679288\n",
      "Iteration  304 => Loss: 6.96875940555949746624\n",
      "Iteration  305 => Loss: 6.96722318427870757063\n",
      "Iteration  306 => Loss: 6.96571858935622412190\n",
      "Iteration  307 => Loss: 6.96424495830811007835\n",
      "Iteration  308 => Loss: 6.96280164252917987966\n",
      "Iteration  309 => Loss: 6.96138800700226667573\n",
      "Iteration  310 => Loss: 6.96000343001350429972\n",
      "Iteration  311 => Loss: 6.95864730287369415151\n",
      "Iteration  312 => Loss: 6.95731902964538573286\n",
      "Iteration  313 => Loss: 6.95601802687576853401\n",
      "Iteration  314 => Loss: 6.95474372333512125266\n",
      "Iteration  315 => Loss: 6.95349555976073308017\n",
      "Iteration  316 => Loss: 6.95227298860617981546\n",
      "Iteration  317 => Loss: 6.95107547379593437853\n",
      "Iteration  318 => Loss: 6.94990249048498753837\n",
      "Iteration  319 => Loss: 6.94875352482362007578\n",
      "Iteration  320 => Loss: 6.94762807372701729491\n",
      "Iteration  321 => Loss: 6.94652564464980049053\n",
      "Iteration  322 => Loss: 6.94544575536518227210\n",
      "Iteration  323 => Loss: 6.94438793374882035181\n",
      "Iteration  324 => Loss: 6.94335171756715663349\n",
      "Iteration  325 => Loss: 6.94233665427023627359\n",
      "Iteration  326 => Loss: 6.94134230078883707193\n",
      "Iteration  327 => Loss: 6.94036822333580083466\n",
      "Iteration  328 => Loss: 6.93941399721167151426\n",
      "Iteration  329 => Loss: 6.93847920661422001842\n",
      "Iteration  330 => Loss: 6.93756344445210171301\n",
      "Iteration  331 => Loss: 6.93666631216235529678\n",
      "Iteration  332 => Loss: 6.93578741953174393586\n",
      "Iteration  333 => Loss: 6.93492638452180010233\n",
      "Iteration  334 => Loss: 6.93408283309762918378\n",
      "Iteration  335 => Loss: 6.93325639906016455427\n",
      "Iteration  336 => Loss: 6.93244672388207483493\n",
      "Iteration  337 => Loss: 6.93165345654700004729\n",
      "Iteration  338 => Loss: 6.93087625339222412890\n",
      "Iteration  339 => Loss: 6.93011477795459907014\n",
      "Iteration  340 => Loss: 6.92936870081974465307\n",
      "Iteration  341 => Loss: 6.92863769947435859109\n",
      "Iteration  342 => Loss: 6.92792145816162641125\n",
      "Iteration  343 => Loss: 6.92721966773973374387\n",
      "Iteration  344 => Loss: 6.92653202554322433571\n",
      "Iteration  345 => Loss: 6.92585823524734589540\n",
      "Iteration  346 => Loss: 6.92519800673522922807\n",
      "Iteration  347 => Loss: 6.92455105596780118304\n",
      "Iteration  348 => Loss: 6.92391710485650069273\n",
      "Iteration  349 => Loss: 6.92329588113858473974\n",
      "Iteration  350 => Loss: 6.92268711825508731295\n",
      "Iteration  351 => Loss: 6.92209055523133542920\n",
      "Iteration  352 => Loss: 6.92150593655995827191\n",
      "Iteration  353 => Loss: 6.92093301208634326116\n",
      "Iteration  354 => Loss: 6.92037153689650086363\n",
      "Iteration  355 => Loss: 6.91982127120728929270\n",
      "Iteration  356 => Loss: 6.91928198025889695799\n",
      "Iteration  357 => Loss: 6.91875343420962085617\n",
      "Iteration  358 => Loss: 6.91823540803285919054\n",
      "Iteration  359 => Loss: 6.91772768141619032178\n",
      "Iteration  360 => Loss: 6.91723003866264019024\n",
      "Iteration  361 => Loss: 6.91674226859399166045\n",
      "Iteration  362 => Loss: 6.91626416445610647799\n",
      "Iteration  363 => Loss: 6.91579552382625539764\n",
      "Iteration  364 => Loss: 6.91533614852236766524\n",
      "Iteration  365 => Loss: 6.91488584451418564214\n",
      "Iteration  366 => Loss: 6.91444442183630769705\n",
      "Iteration  367 => Loss: 6.91401169450301988917\n",
      "Iteration  368 => Loss: 6.91358748042489867913\n",
      "Iteration  369 => Loss: 6.91317160132723262933\n",
      "Iteration  370 => Loss: 6.91276388267006947075\n",
      "Iteration  371 => Loss: 6.91236415356997913051\n",
      "Iteration  372 => Loss: 6.91197224672346877128\n",
      "Iteration  373 => Loss: 6.91158799833197345919\n",
      "Iteration  374 => Loss: 6.91121124802843489476\n",
      "Iteration  375 => Loss: 6.91084183880538560629\n",
      "Iteration  376 => Loss: 6.91047961694460433080\n",
      "Iteration  377 => Loss: 6.91012443194819159231\n",
      "Iteration  378 => Loss: 6.90977613647110811002\n",
      "Iteration  379 => Loss: 6.90943458625511208737\n",
      "Iteration  380 => Loss: 6.90909964006411492221\n",
      "Iteration  381 => Loss: 6.90877115962083898637\n",
      "Iteration  382 => Loss: 6.90844900954485829914\n",
      "Iteration  383 => Loss: 6.90813305729189686133\n",
      "Iteration  384 => Loss: 6.90782317309442728259\n",
      "Iteration  385 => Loss: 6.90751922990346223230\n",
      "Iteration  386 => Loss: 6.90722110333164796003\n",
      "Iteration  387 => Loss: 6.90692867159747780903\n",
      "Iteration  388 => Loss: 6.90664181547070565870\n",
      "Iteration  389 => Loss: 6.90636041821890334091\n",
      "Iteration  390 => Loss: 6.90608436555511762123\n",
      "Iteration  391 => Loss: 6.90581354558666671295\n",
      "Iteration  392 => Loss: 6.90554784876495197921\n",
      "Iteration  393 => Loss: 6.90528716783634877174\n",
      "Iteration  394 => Loss: 6.90503139779416663657\n",
      "Iteration  395 => Loss: 6.90478043583154832419\n",
      "Iteration  396 => Loss: 6.90453418129536711234\n",
      "Iteration  397 => Loss: 6.90429253564114198127\n",
      "Iteration  398 => Loss: 6.90405540238885429005\n",
      "Iteration  399 => Loss: 6.90382268707966151311\n",
      "Iteration  400 => Loss: 6.90359429723361994746\n",
      "Iteration  401 => Loss: 6.90337014230818013516\n",
      "Iteration  402 => Loss: 6.90315013365759266861\n",
      "Iteration  403 => Loss: 6.90293418449321727337\n",
      "Iteration  404 => Loss: 6.90272220984457174353\n",
      "Iteration  405 => Loss: 6.90251412652126905556\n",
      "Iteration  406 => Loss: 6.90230985307569788745\n",
      "Iteration  407 => Loss: 6.90210930976656733549\n",
      "Iteration  408 => Loss: 6.90191241852308667859\n",
      "Iteration  409 => Loss: 6.90171910291001733384\n",
      "Iteration  410 => Loss: 6.90152928809339716310\n",
      "Iteration  411 => Loss: 6.90134290080700374403\n",
      "Iteration  412 => Loss: 6.90115986931948643957\n",
      "Iteration  413 => Loss: 6.90098012340224098438\n",
      "Iteration  414 => Loss: 6.90080359429789602643\n",
      "Iteration  415 => Loss: 6.90063021468952175752\n",
      "Iteration  416 => Loss: 6.90045991867043895240\n",
      "Iteration  417 => Loss: 6.90029264171466483191\n",
      "Iteration  418 => Loss: 6.90012832064801617804\n",
      "Iteration  419 => Loss: 6.89996689361975423793\n",
      "Iteration  420 => Loss: 6.89980830007489487343\n",
      "Iteration  421 => Loss: 6.89965248072702852511\n",
      "Iteration  422 => Loss: 6.89949937753176900657\n",
      "Iteration  423 => Loss: 6.89934893366073875853\n",
      "Iteration  424 => Loss: 6.89920109347605592376\n",
      "Iteration  425 => Loss: 6.89905580250543515319\n",
      "Iteration  426 => Loss: 6.89891300741775292948\n",
      "Iteration  427 => Loss: 6.89877265599913069849\n",
      "Iteration  428 => Loss: 6.89863469712954380242\n",
      "Iteration  429 => Loss: 6.89849908075991624656\n",
      "Iteration  430 => Loss: 6.89836575788963290989\n",
      "Iteration  431 => Loss: 6.89823468054464861154\n",
      "Iteration  432 => Loss: 6.89810580175592136243\n",
      "Iteration  433 => Loss: 6.89797907553839895201\n",
      "Iteration  434 => Loss: 6.89785445687036258278\n",
      "Iteration  435 => Loss: 6.89773190167328298372\n",
      "Iteration  436 => Loss: 6.89761136679206110500\n",
      "Iteration  437 => Loss: 6.89749280997565072937\n",
      "Iteration  438 => Loss: 6.89737618985813760730\n",
      "Iteration  439 => Loss: 6.89726146594020583791\n",
      "Iteration  440 => Loss: 6.89714859857097284390\n",
      "Iteration  441 => Loss: 6.89703754893019826966\n",
      "Iteration  442 => Loss: 6.89692827901092808673\n",
      "Iteration  443 => Loss: 6.89682075160240604106\n",
      "Iteration  444 => Loss: 6.89671493027341586668\n",
      "Iteration  445 => Loss: 6.89661077935596456001\n",
      "Iteration  446 => Loss: 6.89650826392926941111\n",
      "Iteration  447 => Loss: 6.89640734980412517530\n",
      "Iteration  448 => Loss: 6.89630800350753769834\n",
      "Iteration  449 => Loss: 6.89621019226777232092\n",
      "Iteration  450 => Loss: 6.89611388399964475582\n",
      "Iteration  451 => Loss: 6.89601904729010239947\n",
      "Iteration  452 => Loss: 6.89592565138419377746\n",
      "Iteration  453 => Loss: 6.89583366617127335729\n",
      "Iteration  454 => Loss: 6.89574306217146570930\n",
      "Iteration  455 => Loss: 6.89565381052248582705\n",
      "Iteration  456 => Loss: 6.89556588296668770965\n",
      "Iteration  457 => Loss: 6.89547925183840959562\n",
      "Iteration  458 => Loss: 6.89539389005156255763\n",
      "Iteration  459 => Loss: 6.89530977108748288629\n",
      "Iteration  460 => Loss: 6.89522686898305270375\n",
      "Iteration  461 => Loss: 6.89514515831907637278\n",
      "Iteration  462 => Loss: 6.89506461420885052860\n",
      "Iteration  463 => Loss: 6.89498521228703342700\n",
      "Iteration  464 => Loss: 6.89490692869872034976\n",
      "Iteration  465 => Loss: 6.89482974008874816008\n",
      "Iteration  466 => Loss: 6.89475362359121746181\n",
      "Iteration  467 => Loss: 6.89467855681926256040\n",
      "Iteration  468 => Loss: 6.89460451785498218413\n",
      "Iteration  469 => Loss: 6.89453148523965975158\n",
      "Iteration  470 => Loss: 6.89445943796410354310\n",
      "Iteration  471 => Loss: 6.89438835545925332582\n",
      "Iteration  472 => Loss: 6.89431821758695484448\n",
      "Iteration  473 => Loss: 6.89424900463092349412\n",
      "Iteration  474 => Loss: 6.89418069728790428030\n",
      "Iteration  475 => Loss: 6.89411327665905027118\n",
      "Iteration  476 => Loss: 6.89404672424140763098\n",
      "Iteration  477 => Loss: 6.89398102191965733709\n",
      "Iteration  478 => Loss: 6.89391615195796081395\n",
      "Iteration  479 => Loss: 6.89385209699203649336\n",
      "Iteration  480 => Loss: 6.89378884002138025977\n",
      "Iteration  481 => Loss: 6.89372636440161468130\n",
      "Iteration  482 => Loss: 6.89366465383704696279\n",
      "Iteration  483 => Loss: 6.89360369237337344828\n",
      "Iteration  484 => Loss: 6.89354346439049425754\n",
      "Iteration  485 => Loss: 6.89348395459556417819\n",
      "Iteration  486 => Loss: 6.89342514801608174935\n",
      "Iteration  487 => Loss: 6.89336702999322525898\n",
      "Iteration  488 => Loss: 6.89330958617523670284\n",
      "Iteration  489 => Loss: 6.89325280251102068263\n",
      "Iteration  490 => Loss: 6.89319666524384455641\n",
      "Iteration  491 => Loss: 6.89314116090515138779\n",
      "Iteration  492 => Loss: 6.89308627630854786617\n",
      "Iteration  493 => Loss: 6.89303199854386328127\n",
      "Iteration  494 => Loss: 6.89297831497139679158\n",
      "Iteration  495 => Loss: 6.89292521321621887154\n",
      "Iteration  496 => Loss: 6.89287268116265749995\n",
      "Iteration  497 => Loss: 6.89282070694883053363\n",
      "Iteration  498 => Loss: 6.89276927896135571672\n",
      "Iteration  499 => Loss: 6.89271838583015217239\n",
      "Iteration  500 => Loss: 6.89266801642329429711\n",
      "Iteration  501 => Loss: 6.89261815984206727137\n",
      "Iteration  502 => Loss: 6.89256880541610694735\n",
      "Iteration  503 => Loss: 6.89251994269852819031\n",
      "Iteration  504 => Loss: 6.89247156146130457444\n",
      "Iteration  505 => Loss: 6.89242365169069781672\n",
      "Iteration  506 => Loss: 6.89237620358271918519\n",
      "Iteration  507 => Loss: 6.89232920753877920106\n",
      "Iteration  508 => Loss: 6.89228265416134799892\n",
      "Iteration  509 => Loss: 6.89223653424975601922\n",
      "Iteration  510 => Loss: 6.89219083879609772936\n",
      "Iteration  511 => Loss: 6.89214555898115399657\n",
      "Iteration  512 => Loss: 6.89210068617045390482\n",
      "Iteration  513 => Loss: 6.89205621191039519147\n",
      "Iteration  514 => Loss: 6.89201212792450679245\n",
      "Iteration  515 => Loss: 6.89196842610965543230\n",
      "Iteration  516 => Loss: 6.89192509853244850149\n",
      "Iteration  517 => Loss: 6.89188213742572575171\n",
      "Iteration  518 => Loss: 6.89183953518501191127\n",
      "Iteration  519 => Loss: 6.89179728436512650802\n",
      "Iteration  520 => Loss: 6.89175537767686652302\n",
      "Iteration  521 => Loss: 6.89171380798371480125\n",
      "Iteration  522 => Loss: 6.89167256829865859658\n",
      "Iteration  523 => Loss: 6.89163165178104453190\n",
      "Iteration  524 => Loss: 6.89159105173352592999\n",
      "Iteration  525 => Loss: 6.89155076159904389499\n",
      "Iteration  526 => Loss: 6.89151077495790786998\n",
      "Iteration  527 => Loss: 6.89147108552488951716\n",
      "Iteration  528 => Loss: 6.89143168714644183126\n",
      "Iteration  529 => Loss: 6.89139257379792447011\n",
      "Iteration  530 => Loss: 6.89135373958087260604\n",
      "Iteration  531 => Loss: 6.89131517872041499118\n",
      "Iteration  532 => Loss: 6.89127688556262807396\n",
      "Iteration  533 => Loss: 6.89123885457204732319\n",
      "Iteration  534 => Loss: 6.89120108032914746587\n",
      "Iteration  535 => Loss: 6.89116355752793552369\n",
      "Iteration  536 => Loss: 6.89112628097356694212\n",
      "Iteration  537 => Loss: 6.89108924558000968119\n",
      "Iteration  538 => Loss: 6.89105244636778380141\n",
      "Iteration  539 => Loss: 6.89101587846172503049\n",
      "Iteration  540 => Loss: 6.89097953708876609369\n",
      "Iteration  541 => Loss: 6.89094341757583883634\n",
      "Iteration  542 => Loss: 6.89090751534776835285\n",
      "Iteration  543 => Loss: 6.89087182592520886004\n",
      "Iteration  544 => Loss: 6.89083634492265240112\n",
      "Iteration  545 => Loss: 6.89080106804644554330\n",
      "Iteration  546 => Loss: 6.89076599109289311684\n",
      "Iteration  547 => Loss: 6.89073110994632997972\n",
      "Iteration  548 => Loss: 6.89069642057733044993\n",
      "Iteration  549 => Loss: 6.89066191904085556530\n",
      "Iteration  550 => Loss: 6.89062760147451047743\n",
      "Iteration  551 => Loss: 6.89059346409680184564\n",
      "Iteration  552 => Loss: 6.89055950320545207433\n",
      "Iteration  553 => Loss: 6.89052571517573486659\n",
      "Iteration  554 => Loss: 6.89049209645884275233\n",
      "Iteration  555 => Loss: 6.89045864358030879515\n",
      "Iteration  556 => Loss: 6.89042535313845139200\n",
      "Iteration  557 => Loss: 6.89039222180284305352\n",
      "Iteration  558 => Loss: 6.89035924631280316532\n",
      "Iteration  559 => Loss: 6.89032642347596002708\n",
      "Iteration  560 => Loss: 6.89029375016681733257\n",
      "Iteration  561 => Loss: 6.89026122332533841330\n",
      "Iteration  562 => Loss: 6.89022883995558643733\n",
      "Iteration  563 => Loss: 6.89019659712437526622\n",
      "Iteration  564 => Loss: 6.89016449195996738553\n",
      "Iteration  565 => Loss: 6.89013252165076117706\n",
      "Iteration  566 => Loss: 6.89010068344406434449\n",
      "Iteration  567 => Loss: 6.89006897464483003546\n",
      "Iteration  568 => Loss: 6.89003739261447289977\n",
      "Iteration  569 => Loss: 6.89000593476965583761\n",
      "Iteration  570 => Loss: 6.88997459858117444753\n",
      "Iteration  571 => Loss: 6.88994338157278285451\n",
      "Iteration  572 => Loss: 6.88991228132012434315\n",
      "Iteration  573 => Loss: 6.88988129544959626571\n",
      "Iteration  574 => Loss: 6.88985042163731709053\n",
      "Iteration  575 => Loss: 6.88981965760809167421\n",
      "Iteration  576 => Loss: 6.88978900113437564556\n",
      "Iteration  577 => Loss: 6.88975845003527176402\n",
      "Iteration  578 => Loss: 6.88972800217557335145\n",
      "Iteration  579 => Loss: 6.88969765546479706586\n",
      "Iteration  580 => Loss: 6.88966740785622810961\n",
      "Iteration  581 => Loss: 6.88963725734603471551\n",
      "Iteration  582 => Loss: 6.88960720197234710582\n",
      "Iteration  583 => Loss: 6.88957723981439151828\n",
      "Iteration  584 => Loss: 6.88954736899162067942\n",
      "Iteration  585 => Loss: 6.88951758766288335778\n",
      "Iteration  586 => Loss: 6.88948789402556727168\n",
      "Iteration  587 => Loss: 6.88945828631484058491\n",
      "Iteration  588 => Loss: 6.88942876280281613077\n",
      "Iteration  589 => Loss: 6.88939932179780800681\n",
      "Iteration  590 => Loss: 6.88936996164354109595\n",
      "Iteration  591 => Loss: 6.88934068071846272829\n",
      "Iteration  592 => Loss: 6.88931147743494687319\n",
      "Iteration  593 => Loss: 6.88928235023863511088\n",
      "Iteration  594 => Loss: 6.88925329760771543164\n",
      "Iteration  595 => Loss: 6.88922431805225876644\n",
      "Iteration  596 => Loss: 6.88919541011352709603\n",
      "Iteration  597 => Loss: 6.88916657236334017966\n",
      "Iteration  598 => Loss: 6.88913780340341919128\n",
      "Iteration  599 => Loss: 6.88910910186478631090\n",
      "Iteration  600 => Loss: 6.88908046640710747255\n",
      "Iteration  601 => Loss: 6.88905189571814968730\n",
      "Iteration  602 => Loss: 6.88902338851313711388\n",
      "Iteration  603 => Loss: 6.88899494353420394077\n",
      "Iteration  604 => Loss: 6.88896655954983216930\n",
      "Iteration  605 => Loss: 6.88893823535428850846\n",
      "Iteration  606 => Loss: 6.88890996976709946154\n",
      "Iteration  607 => Loss: 6.88888176163250420814\n",
      "Iteration  608 => Loss: 6.88885360981893413168\n",
      "Iteration  609 => Loss: 6.88882551321853853210\n",
      "Iteration  610 => Loss: 6.88879747074666148876\n",
      "Iteration  611 => Loss: 6.88876948134133559876\n",
      "Iteration  612 => Loss: 6.88874154396286542124\n",
      "Iteration  613 => Loss: 6.88871365759331055756\n",
      "Iteration  614 => Loss: 6.88868582123605310841\n",
      "Iteration  615 => Loss: 6.88865803391532871558\n",
      "Iteration  616 => Loss: 6.88863029467581799992\n",
      "Iteration  617 => Loss: 6.88860260258220780116\n",
      "Iteration  618 => Loss: 6.88857495671876129961\n",
      "Iteration  619 => Loss: 6.88854735618892188853\n",
      "Iteration  620 => Loss: 6.88851980011490816480\n",
      "Iteration  621 => Loss: 6.88849228763730092595\n",
      "Iteration  622 => Loss: 6.88846481791470033329\n",
      "Iteration  623 => Loss: 6.88843739012330669169\n",
      "Iteration  624 => Loss: 6.88841000345658205362\n",
      "Iteration  625 => Loss: 6.88838265712486119696\n",
      "Iteration  626 => Loss: 6.88835535035501589363\n",
      "Iteration  627 => Loss: 6.88832808239011562534\n",
      "Iteration  628 => Loss: 6.88830085248906431872\n",
      "Iteration  629 => Loss: 6.88827365992628681823\n",
      "Iteration  630 => Loss: 6.88824650399140470114\n",
      "Iteration  631 => Loss: 6.88821938398890942779\n",
      "Iteration  632 => Loss: 6.88819229923783904468\n",
      "Iteration  633 => Loss: 6.88816524907150817825\n",
      "Iteration  634 => Loss: 6.88813823283717763246\n",
      "Iteration  635 => Loss: 6.88811124989577283628\n",
      "Iteration  636 => Loss: 6.88808429962160673199\n",
      "Iteration  637 => Loss: 6.88805738140207512998\n",
      "Iteration  638 => Loss: 6.88803049463741867697\n",
      "Iteration  639 => Loss: 6.88800363874041199352\n",
      "Iteration  640 => Loss: 6.88797681313613008314\n",
      "Iteration  641 => Loss: 6.88795001726169164868\n",
      "Iteration  642 => Loss: 6.88792325056598908617\n",
      "Iteration  643 => Loss: 6.88789651250944245930\n",
      "Iteration  644 => Loss: 6.88786980256378988940\n",
      "Iteration  645 => Loss: 6.88784312021180689101\n",
      "Iteration  646 => Loss: 6.88781646494709232087\n",
      "Iteration  647 => Loss: 6.88778983627385965605\n",
      "Iteration  648 => Loss: 6.88776323370669718571\n",
      "Iteration  649 => Loss: 6.88773665677034241384\n",
      "Iteration  650 => Loss: 6.88771010499948221906\n",
      "Iteration  651 => Loss: 6.88768357793854146820\n",
      "Iteration  652 => Loss: 6.88765707514149294610\n",
      "Iteration  653 => Loss: 6.88763059617162021198\n",
      "Iteration  654 => Loss: 6.88760414060137637904\n",
      "Iteration  655 => Loss: 6.88757770801213276002\n",
      "Iteration  656 => Loss: 6.88755129799403409407\n",
      "Iteration  657 => Loss: 6.88752491014580758844\n",
      "Iteration  658 => Loss: 6.88749854407455863736\n",
      "Iteration  659 => Loss: 6.88747219939564025992\n",
      "Iteration  660 => Loss: 6.88744587573243460810\n",
      "Iteration  661 => Loss: 6.88741957271620375280\n",
      "Iteration  662 => Loss: 6.88739328998593691722\n",
      "Iteration  663 => Loss: 6.88736702718816751201\n",
      "Iteration  664 => Loss: 6.88734078397683013861\n",
      "Iteration  665 => Loss: 6.88731456001309272352\n",
      "Iteration  666 => Loss: 6.88728835496521352155\n",
      "Iteration  667 => Loss: 6.88726216850840078365\n",
      "Iteration  668 => Loss: 6.88723600032464755571\n",
      "Iteration  669 => Loss: 6.88720985010260644543\n",
      "Iteration  670 => Loss: 6.88718371753742619745\n",
      "Iteration  671 => Loss: 6.88715760233065577012\n",
      "Iteration  672 => Loss: 6.88713150419006669978\n",
      "Iteration  673 => Loss: 6.88710542282956428295\n",
      "Iteration  674 => Loss: 6.88707935796901082881\n",
      "Iteration  675 => Loss: 6.88705330933415904582\n",
      "Iteration  676 => Loss: 6.88702727665646641242\n",
      "Iteration  677 => Loss: 6.88700125967303034002\n",
      "Iteration  678 => Loss: 6.88697525812643007725\n",
      "Iteration  679 => Loss: 6.88694927176463700391\n",
      "Iteration  680 => Loss: 6.88692330034089650326\n",
      "Iteration  681 => Loss: 6.88689734361360716974\n",
      "Iteration  682 => Loss: 6.88687140134620978671\n",
      "Iteration  683 => Loss: 6.88684547330711094304\n",
      "Iteration  684 => Loss: 6.88681955926955602365\n",
      "Iteration  685 => Loss: 6.88679365901150752904\n",
      "Iteration  686 => Loss: 6.88676777231560333092\n",
      "Iteration  687 => Loss: 6.88674189896900745822\n",
      "Iteration  688 => Loss: 6.88671603876333104921\n",
      "Iteration  689 => Loss: 6.88669019149455063911\n",
      "Iteration  690 => Loss: 6.88666435696289980228\n",
      "Iteration  691 => Loss: 6.88663853497279454530\n",
      "Iteration  692 => Loss: 6.88661272533272939000\n",
      "Iteration  693 => Loss: 6.88658692785520543111\n",
      "Iteration  694 => Loss: 6.88656114235664951195\n",
      "Iteration  695 => Loss: 6.88653536865731652483\n",
      "Iteration  696 => Loss: 6.88650960658121302771\n",
      "Iteration  697 => Loss: 6.88648385595604040077\n",
      "Iteration  698 => Loss: 6.88645811661307583051\n",
      "Iteration  699 => Loss: 6.88643238838713411809\n",
      "Iteration  700 => Loss: 6.88640667111649484866\n",
      "Iteration  701 => Loss: 6.88638096464279758635\n",
      "Iteration  702 => Loss: 6.88635526881099035990\n",
      "Iteration  703 => Loss: 6.88632958346926482562\n",
      "Iteration  704 => Loss: 6.88630390846898521318\n",
      "Iteration  705 => Loss: 6.88627824366462348848\n",
      "Iteration  706 => Loss: 6.88625258891367142411\n",
      "Iteration  707 => Loss: 6.88622694407661217753\n",
      "Iteration  708 => Loss: 6.88620130901685545410\n",
      "Iteration  709 => Loss: 6.88617568360063803112\n",
      "Iteration  710 => Loss: 6.88615006769698378974\n",
      "Iteration  711 => Loss: 6.88612446117770016230\n",
      "Iteration  712 => Loss: 6.88609886391720760201\n",
      "Iteration  713 => Loss: 6.88607327579258665651\n",
      "Iteration  714 => Loss: 6.88604769668347671541\n",
      "Iteration  715 => Loss: 6.88602212647202005513\n",
      "Iteration  716 => Loss: 6.88599656504282897629\n",
      "Iteration  717 => Loss: 6.88597101228291208486\n",
      "Iteration  718 => Loss: 6.88594546808165564045\n",
      "Iteration  719 => Loss: 6.88591993233072496849\n",
      "Iteration  720 => Loss: 6.88589440492407067751\n",
      "Iteration  721 => Loss: 6.88586888575783984123\n",
      "Iteration  722 => Loss: 6.88584337473035557053\n",
      "Iteration  723 => Loss: 6.88581787174205750546\n",
      "Iteration  724 => Loss: 6.88579237669545740630\n",
      "Iteration  725 => Loss: 6.88576688949510806736\n",
      "Iteration  726 => Loss: 6.88574141004754647355\n",
      "Iteration  727 => Loss: 6.88571593826125827320\n",
      "Iteration  728 => Loss: 6.88569047404664047463\n",
      "Iteration  729 => Loss: 6.88566501731595170810\n",
      "Iteration  730 => Loss: 6.88563956798328646869\n",
      "Iteration  731 => Loss: 6.88561412596452004919\n",
      "Iteration  732 => Loss: 6.88558869117729344111\n",
      "Iteration  733 => Loss: 6.88556326354095027398\n",
      "Iteration  734 => Loss: 6.88553784297653148627\n",
      "Iteration  735 => Loss: 6.88551242940669450121\n",
      "Iteration  736 => Loss: 6.88548702275573454301\n",
      "Iteration  737 => Loss: 6.88546162294950914173\n",
      "Iteration  738 => Loss: 6.88543622991543280421\n",
      "Iteration  739 => Loss: 6.88541084358241128882\n",
      "Iteration  740 => Loss: 6.88538546388084160554\n",
      "Iteration  741 => Loss: 6.88536009074257115969\n",
      "Iteration  742 => Loss: 6.88533472410086933024\n",
      "Iteration  743 => Loss: 6.88530936389037506729\n",
      "Iteration  744 => Loss: 6.88528401004711021471\n",
      "Iteration  745 => Loss: 6.88525866250840845595\n",
      "Iteration  746 => Loss: 6.88523332121291975483\n",
      "Iteration  747 => Loss: 6.88520798610056949940\n",
      "Iteration  748 => Loss: 6.88518265711251853389\n",
      "Iteration  749 => Loss: 6.88515733419117115233\n",
      "Iteration  750 => Loss: 6.88513201728009871516\n",
      "Iteration  751 => Loss: 6.88510670632408494640\n",
      "Iteration  752 => Loss: 6.88508140126903001033\n",
      "Iteration  753 => Loss: 6.88505610206196472234\n",
      "Iteration  754 => Loss: 6.88503080865102923269\n",
      "Iteration  755 => Loss: 6.88500552098543217028\n",
      "Iteration  756 => Loss: 6.88498023901544620173\n",
      "Iteration  757 => Loss: 6.88495496269236362252\n",
      "Iteration  758 => Loss: 6.88492969196849191604\n",
      "Iteration  759 => Loss: 6.88490442679712622009\n",
      "Iteration  760 => Loss: 6.88487916713254133327\n",
      "Iteration  761 => Loss: 6.88485391292994375334\n",
      "Iteration  762 => Loss: 6.88482866414548499989\n",
      "Iteration  763 => Loss: 6.88480342073621809362\n",
      "Iteration  764 => Loss: 6.88477818266008867454\n",
      "Iteration  765 => Loss: 6.88475294987590835660\n",
      "Iteration  766 => Loss: 6.88472772234334762231\n",
      "Iteration  767 => Loss: 6.88470250002291184188\n",
      "Iteration  768 => Loss: 6.88467728287592173331\n",
      "Iteration  769 => Loss: 6.88465207086450003970\n",
      "Iteration  770 => Loss: 6.88462686395155465391\n",
      "Iteration  771 => Loss: 6.88460166210075552584\n",
      "Iteration  772 => Loss: 6.88457646527653643886\n",
      "Iteration  773 => Loss: 6.88455127344405681811\n",
      "Iteration  774 => Loss: 6.88452608656919196051\n",
      "Iteration  775 => Loss: 6.88450090461853836388\n",
      "Iteration  776 => Loss: 6.88447572755936487710\n",
      "Iteration  777 => Loss: 6.88445055535963224003\n",
      "Iteration  778 => Loss: 6.88442538798796022093\n",
      "Iteration  779 => Loss: 6.88440022541360896469\n",
      "Iteration  780 => Loss: 6.88437506760646211745\n",
      "Iteration  781 => Loss: 6.88434991453705968922\n",
      "Iteration  782 => Loss: 6.88432476617651900597\n",
      "Iteration  783 => Loss: 6.88429962249656224316\n",
      "Iteration  784 => Loss: 6.88427448346949777402\n",
      "Iteration  785 => Loss: 6.88424934906820418234\n",
      "Iteration  786 => Loss: 6.88422421926611693976\n",
      "Iteration  787 => Loss: 6.88419909403721330676\n",
      "Iteration  788 => Loss: 6.88417397335601055630\n",
      "Iteration  789 => Loss: 6.88414885719755265114\n",
      "Iteration  790 => Loss: 6.88412374553739692118\n",
      "Iteration  791 => Loss: 6.88409863835160251710\n",
      "Iteration  792 => Loss: 6.88407353561672241682\n",
      "Iteration  793 => Loss: 6.88404843730978477367\n",
      "Iteration  794 => Loss: 6.88402334340828403469\n",
      "Iteration  795 => Loss: 6.88399825389019515143\n",
      "Iteration  796 => Loss: 6.88397316873393538827\n",
      "Iteration  797 => Loss: 6.88394808791837586881\n",
      "Iteration  798 => Loss: 6.88392301142278917325\n",
      "Iteration  799 => Loss: 6.88389793922690440553\n",
      "Iteration  800 => Loss: 6.88387287131085390257\n",
      "Iteration  801 => Loss: 6.88384780765518566881\n",
      "Iteration  802 => Loss: 6.88382274824082340814\n",
      "Iteration  803 => Loss: 6.88379769304910382743\n",
      "Iteration  804 => Loss: 6.88377264206174199757\n",
      "Iteration  805 => Loss: 6.88374759526081536620\n",
      "Iteration  806 => Loss: 6.88372255262876819870\n",
      "Iteration  807 => Loss: 6.88369751414841957171\n",
      "Iteration  808 => Loss: 6.88367247980292251697\n",
      "Iteration  809 => Loss: 6.88364744957578000850\n",
      "Iteration  810 => Loss: 6.88362242345082719908\n",
      "Iteration  811 => Loss: 6.88359740141223674925\n",
      "Iteration  812 => Loss: 6.88357238344448774114\n",
      "Iteration  813 => Loss: 6.88354736953239854103\n",
      "Iteration  814 => Loss: 6.88352235966108061405\n",
      "Iteration  815 => Loss: 6.88349735381594562966\n",
      "Iteration  816 => Loss: 6.88347235198270102075\n",
      "Iteration  817 => Loss: 6.88344735414736419443\n",
      "Iteration  818 => Loss: 6.88342236029620568871\n",
      "Iteration  819 => Loss: 6.88339737041580157495\n",
      "Iteration  820 => Loss: 6.88337238449298016718\n",
      "Iteration  821 => Loss: 6.88334740251484156204\n",
      "Iteration  822 => Loss: 6.88332242446876030328\n",
      "Iteration  823 => Loss: 6.88329745034234363743\n",
      "Iteration  824 => Loss: 6.88327248012346792905\n",
      "Iteration  825 => Loss: 6.88324751380023780456\n",
      "Iteration  826 => Loss: 6.88322255136101457396\n",
      "Iteration  827 => Loss: 6.88319759279437892729\n",
      "Iteration  828 => Loss: 6.88317263808915225098\n",
      "Iteration  829 => Loss: 6.88314768723437886422\n",
      "Iteration  830 => Loss: 6.88312274021931091994\n",
      "Iteration  831 => Loss: 6.88309779703343149748\n",
      "Iteration  832 => Loss: 6.88307285766642618086\n",
      "Iteration  833 => Loss: 6.88304792210820259868\n",
      "Iteration  834 => Loss: 6.88302299034883890982\n",
      "Iteration  835 => Loss: 6.88299806237864242320\n",
      "Iteration  836 => Loss: 6.88297313818810163610\n",
      "Iteration  837 => Loss: 6.88294821776789778056\n",
      "Iteration  838 => Loss: 6.88292330110888350703\n",
      "Iteration  839 => Loss: 6.88289838820212107606\n",
      "Iteration  840 => Loss: 6.88287347903882196221\n",
      "Iteration  841 => Loss: 6.88284857361040103285\n",
      "Iteration  842 => Loss: 6.88282367190841615212\n",
      "Iteration  843 => Loss: 6.88279877392461081342\n",
      "Iteration  844 => Loss: 6.88277387965088394139\n",
      "Iteration  845 => Loss: 6.88274898907929344460\n",
      "Iteration  846 => Loss: 6.88272410220206332099\n",
      "Iteration  847 => Loss: 6.88269921901156411792\n",
      "Iteration  848 => Loss: 6.88267433950032270218\n",
      "Iteration  849 => Loss: 6.88264946366099827912\n",
      "Iteration  850 => Loss: 6.88262459148640726170\n",
      "Iteration  851 => Loss: 6.88259972296951172410\n",
      "Iteration  852 => Loss: 6.88257485810339897370\n",
      "Iteration  853 => Loss: 6.88254999688129487367\n",
      "Iteration  854 => Loss: 6.88252513929657272485\n",
      "Iteration  855 => Loss: 6.88250028534270885672\n",
      "Iteration  856 => Loss: 6.88247543501332792459\n",
      "Iteration  857 => Loss: 6.88245058830216827062\n",
      "Iteration  858 => Loss: 6.88242574520309702280\n",
      "Iteration  859 => Loss: 6.88240090571009943687\n",
      "Iteration  860 => Loss: 6.88237606981726646183\n",
      "Iteration  861 => Loss: 6.88235123751881605614\n",
      "Iteration  862 => Loss: 6.88232640880908252967\n",
      "Iteration  863 => Loss: 6.88230158368248101652\n",
      "Iteration  864 => Loss: 6.88227676213357586477\n",
      "Iteration  865 => Loss: 6.88225194415699714767\n",
      "Iteration  866 => Loss: 6.88222712974751082982\n",
      "Iteration  867 => Loss: 6.88220231889994238372\n",
      "Iteration  868 => Loss: 6.88217751160926827225\n",
      "Iteration  869 => Loss: 6.88215270787051913715\n",
      "Iteration  870 => Loss: 6.88212790767883486609\n",
      "Iteration  871 => Loss: 6.88210311102945304640\n",
      "Iteration  872 => Loss: 6.88207831791768942509\n",
      "Iteration  873 => Loss: 6.88205352833897254783\n",
      "Iteration  874 => Loss: 6.88202874228877536922\n",
      "Iteration  875 => Loss: 6.88200395976270140608\n",
      "Iteration  876 => Loss: 6.88197918075640568958\n",
      "Iteration  877 => Loss: 6.88195440526564894412\n",
      "Iteration  878 => Loss: 6.88192963328623896757\n",
      "Iteration  879 => Loss: 6.88190486481409546826\n",
      "Iteration  880 => Loss: 6.88188009984520387974\n",
      "Iteration  881 => Loss: 6.88185533837560470261\n",
      "Iteration  882 => Loss: 6.88183058040144057799\n",
      "Iteration  883 => Loss: 6.88180582591890832589\n",
      "Iteration  884 => Loss: 6.88178107492429003145\n",
      "Iteration  885 => Loss: 6.88175632741391041236\n",
      "Iteration  886 => Loss: 6.88173158338418655688\n",
      "Iteration  887 => Loss: 6.88170684283159150851\n",
      "Iteration  888 => Loss: 6.88168210575266048323\n",
      "Iteration  889 => Loss: 6.88165737214400152766\n",
      "Iteration  890 => Loss: 6.88163264200226887368\n",
      "Iteration  891 => Loss: 6.88160791532419402472\n",
      "Iteration  892 => Loss: 6.88158319210656177489\n",
      "Iteration  893 => Loss: 6.88155847234620487995\n",
      "Iteration  894 => Loss: 6.88153375604002714994\n",
      "Iteration  895 => Loss: 6.88150904318497946832\n",
      "Iteration  896 => Loss: 6.88148433377808466105\n",
      "Iteration  897 => Loss: 6.88145962781638065309\n",
      "Iteration  898 => Loss: 6.88143492529699152271\n",
      "Iteration  899 => Loss: 6.88141022621708842166\n",
      "Iteration  900 => Loss: 6.88138553057387980516\n",
      "Iteration  901 => Loss: 6.88136083836462919550\n",
      "Iteration  902 => Loss: 6.88133614958665251748\n",
      "Iteration  903 => Loss: 6.88131146423730388761\n",
      "Iteration  904 => Loss: 6.88128678231399604215\n",
      "Iteration  905 => Loss: 6.88126210381417013906\n",
      "Iteration  906 => Loss: 6.88123742873532595610\n",
      "Iteration  907 => Loss: 6.88121275707500057450\n",
      "Iteration  908 => Loss: 6.88118808883076571448\n",
      "Iteration  909 => Loss: 6.88116342400025526871\n",
      "Iteration  910 => Loss: 6.88113876258112622253\n",
      "Iteration  911 => Loss: 6.88111410457106753569\n",
      "Iteration  912 => Loss: 6.88108944996783566950\n",
      "Iteration  913 => Loss: 6.88106479876919951977\n",
      "Iteration  914 => Loss: 6.88104015097297860848\n",
      "Iteration  915 => Loss: 6.88101550657702087932\n",
      "Iteration  916 => Loss: 6.88099086557921690854\n",
      "Iteration  917 => Loss: 6.88096622797749368772\n",
      "Iteration  918 => Loss: 6.88094159376979686016\n",
      "Iteration  919 => Loss: 6.88091696295411914264\n",
      "Iteration  920 => Loss: 6.88089233552848789088\n",
      "Iteration  921 => Loss: 6.88086771149095355327\n",
      "Iteration  922 => Loss: 6.88084309083961009890\n",
      "Iteration  923 => Loss: 6.88081847357257014863\n",
      "Iteration  924 => Loss: 6.88079385968797385686\n",
      "Iteration  925 => Loss: 6.88076924918400489872\n",
      "Iteration  926 => Loss: 6.88074464205887270651\n",
      "Iteration  927 => Loss: 6.88072003831080092340\n",
      "Iteration  928 => Loss: 6.88069543793806293053\n",
      "Iteration  929 => Loss: 6.88067084093894276720\n",
      "Iteration  930 => Loss: 6.88064624731175555894\n",
      "Iteration  931 => Loss: 6.88062165705483419487\n",
      "Iteration  932 => Loss: 6.88059707016655774936\n",
      "Iteration  933 => Loss: 6.88057248664531773130\n",
      "Iteration  934 => Loss: 6.88054790648951453136\n",
      "Iteration  935 => Loss: 6.88052332969761426540\n",
      "Iteration  936 => Loss: 6.88049875626805818030\n",
      "Iteration  937 => Loss: 6.88047418619934081363\n",
      "Iteration  938 => Loss: 6.88044961948997180201\n",
      "Iteration  939 => Loss: 6.88042505613847676926\n",
      "Iteration  940 => Loss: 6.88040049614341420181\n",
      "Iteration  941 => Loss: 6.88037593950335502058\n",
      "Iteration  942 => Loss: 6.88035138621689146277\n",
      "Iteration  943 => Loss: 6.88032683628263708187\n",
      "Iteration  944 => Loss: 6.88030228969923030036\n",
      "Iteration  945 => Loss: 6.88027774646531664615\n",
      "Iteration  946 => Loss: 6.88025320657958161519\n",
      "Iteration  947 => Loss: 6.88022867004069649255\n",
      "Iteration  948 => Loss: 6.88020413684738763038\n",
      "Iteration  949 => Loss: 6.88017960699836894634\n",
      "Iteration  950 => Loss: 6.88015508049239432609\n",
      "Iteration  951 => Loss: 6.88013055732822031985\n",
      "Iteration  952 => Loss: 6.88010603750461946504\n",
      "Iteration  953 => Loss: 6.88008152102039538534\n",
      "Iteration  954 => Loss: 6.88005700787434815169\n",
      "Iteration  955 => Loss: 6.88003249806531336219\n",
      "Iteration  956 => Loss: 6.88000799159212661493\n",
      "Iteration  957 => Loss: 6.87998348845364304793\n",
      "Iteration  958 => Loss: 6.87995898864873645095\n",
      "Iteration  959 => Loss: 6.87993449217628949555\n",
      "Iteration  960 => Loss: 6.87990999903520528136\n",
      "Iteration  961 => Loss: 6.87988550922439223712\n",
      "Iteration  962 => Loss: 6.87986102274277300239\n",
      "Iteration  963 => Loss: 6.87983653958929686212\n",
      "Iteration  964 => Loss: 6.87981205976291221305\n",
      "Iteration  965 => Loss: 6.87978758326259143274\n",
      "Iteration  966 => Loss: 6.87976311008729979335\n",
      "Iteration  967 => Loss: 6.87973864023603098872\n",
      "Iteration  968 => Loss: 6.87971417370779292355\n",
      "Iteration  969 => Loss: 6.87968971050159439073\n",
      "Iteration  970 => Loss: 6.87966525061646017036\n",
      "Iteration  971 => Loss: 6.87964079405143102974\n",
      "Iteration  972 => Loss: 6.87961634080555128890\n",
      "Iteration  973 => Loss: 6.87959189087786437966\n",
      "Iteration  974 => Loss: 6.87956744426746791277\n",
      "Iteration  975 => Loss: 6.87954300097341331366\n",
      "Iteration  976 => Loss: 6.87951856099481240392\n",
      "Iteration  977 => Loss: 6.87949412433074947160\n",
      "Iteration  978 => Loss: 6.87946969098032923284\n",
      "Iteration  979 => Loss: 6.87944526094267683192\n",
      "Iteration  980 => Loss: 6.87942083421692185397\n",
      "Iteration  981 => Loss: 6.87939641080219832503\n",
      "Iteration  982 => Loss: 6.87937199069763760662\n",
      "Iteration  983 => Loss: 6.87934757390241546915\n",
      "Iteration  984 => Loss: 6.87932316041567482046\n",
      "Iteration  985 => Loss: 6.87929875023660120092\n",
      "Iteration  986 => Loss: 6.87927434336435528195\n",
      "Iteration  987 => Loss: 6.87924993979814036749\n",
      "Iteration  988 => Loss: 6.87922553953714643882\n",
      "Iteration  989 => Loss: 6.87920114258057058265\n",
      "Iteration  990 => Loss: 6.87917674892762498473\n",
      "Iteration  991 => Loss: 6.87915235857751650173\n",
      "Iteration  992 => Loss: 6.87912797152949373469\n",
      "Iteration  993 => Loss: 6.87910358778276620484\n",
      "Iteration  994 => Loss: 6.87907920733657629597\n",
      "Iteration  995 => Loss: 6.87905483019017349733\n",
      "Iteration  996 => Loss: 6.87903045634281173903\n",
      "Iteration  997 => Loss: 6.87900608579373695761\n",
      "Iteration  998 => Loss: 6.87898171854221729404\n",
      "Iteration  999 => Loss: 6.87895735458752888292\n",
      "Iteration 1000 => Loss: 6.87893299392894164157\n",
      "Iteration 1001 => Loss: 6.87890863656573969820\n",
      "Iteration 1002 => Loss: 6.87888428249721517460\n",
      "Iteration 1003 => Loss: 6.87885993172265752804\n",
      "Iteration 1004 => Loss: 6.87883558424136865028\n",
      "Iteration 1005 => Loss: 6.87881124005264510402\n",
      "Iteration 1006 => Loss: 6.87878689915580920911\n",
      "Iteration 1007 => Loss: 6.87876256155016729821\n",
      "Iteration 1008 => Loss: 6.87873822723504790844\n",
      "Iteration 1009 => Loss: 6.87871389620977602419\n",
      "Iteration 1010 => Loss: 6.87868956847367130081\n",
      "Iteration 1011 => Loss: 6.87866524402608092714\n",
      "Iteration 1012 => Loss: 6.87864092286634321027\n",
      "Iteration 1013 => Loss: 6.87861660499379556910\n",
      "Iteration 1014 => Loss: 6.87859229040779407427\n",
      "Iteration 1015 => Loss: 6.87856797910768769100\n",
      "Iteration 1016 => Loss: 6.87854367109284225990\n",
      "Iteration 1017 => Loss: 6.87851936636261118707\n",
      "Iteration 1018 => Loss: 6.87849506491636297767\n",
      "Iteration 1019 => Loss: 6.87847076675347501862\n",
      "Iteration 1020 => Loss: 6.87844647187332913774\n",
      "Iteration 1021 => Loss: 6.87842218027527430024\n",
      "Iteration 1022 => Loss: 6.87839789195871365024\n",
      "Iteration 1023 => Loss: 6.87837360692303967369\n",
      "Iteration 1024 => Loss: 6.87834932516761998755\n",
      "Iteration 1025 => Loss: 6.87832504669187017043\n",
      "Iteration 1026 => Loss: 6.87830077149516672108\n",
      "Iteration 1027 => Loss: 6.87827649957692788263\n",
      "Iteration 1028 => Loss: 6.87825223093655235829\n",
      "Iteration 1029 => Loss: 6.87822796557344418034\n",
      "Iteration 1030 => Loss: 6.87820370348700738106\n",
      "Iteration 1031 => Loss: 6.87817944467667441444\n",
      "Iteration 1032 => Loss: 6.87815518914183776644\n",
      "Iteration 1033 => Loss: 6.87813093688194143738\n",
      "Iteration 1034 => Loss: 6.87810668789639212406\n",
      "Iteration 1035 => Loss: 6.87808244218462050412\n",
      "Iteration 1036 => Loss: 6.87805819974605103795\n",
      "Iteration 1037 => Loss: 6.87803396058012417313\n",
      "Iteration 1038 => Loss: 6.87800972468626437006\n",
      "Iteration 1039 => Loss: 6.87798549206391207633\n",
      "Iteration 1040 => Loss: 6.87796126271250152229\n",
      "Iteration 1041 => Loss: 6.87793703663148559002\n",
      "Iteration 1042 => Loss: 6.87791281382030561531\n",
      "Iteration 1043 => Loss: 6.87788859427840115757\n",
      "Iteration 1044 => Loss: 6.87786437800522598707\n",
      "Iteration 1045 => Loss: 6.87784016500023298590\n",
      "Iteration 1046 => Loss: 6.87781595526288125342\n",
      "Iteration 1047 => Loss: 6.87779174879260768449\n",
      "Iteration 1048 => Loss: 6.87776754558889535929\n",
      "Iteration 1049 => Loss: 6.87774334565119183083\n",
      "Iteration 1050 => Loss: 6.87771914897895708663\n",
      "Iteration 1051 => Loss: 6.87769495557167154232\n",
      "Iteration 1052 => Loss: 6.87767076542878630363\n",
      "Iteration 1053 => Loss: 6.87764657854977290441\n",
      "Iteration 1054 => Loss: 6.87762239493411708935\n",
      "Iteration 1055 => Loss: 6.87759821458126818783\n",
      "Iteration 1056 => Loss: 6.87757403749072082633\n",
      "Iteration 1057 => Loss: 6.87754986366193943326\n",
      "Iteration 1058 => Loss: 6.87752569309441863510\n",
      "Iteration 1059 => Loss: 6.87750152578762197209\n",
      "Iteration 1060 => Loss: 6.87747736174103874163\n",
      "Iteration 1061 => Loss: 6.87745320095415202388\n",
      "Iteration 1062 => Loss: 6.87742904342644667537\n",
      "Iteration 1063 => Loss: 6.87740488915740311171\n",
      "Iteration 1064 => Loss: 6.87738073814653017024\n",
      "Iteration 1065 => Loss: 6.87735659039330204934\n",
      "Iteration 1066 => Loss: 6.87733244589721071094\n",
      "Iteration 1067 => Loss: 6.87730830465775966331\n",
      "Iteration 1068 => Loss: 6.87728416667443109844\n",
      "Iteration 1069 => Loss: 6.87726003194673474184\n",
      "Iteration 1070 => Loss: 6.87723590047415278548\n",
      "Iteration 1071 => Loss: 6.87721177225620028395\n",
      "Iteration 1072 => Loss: 6.87718764729236031741\n",
      "Iteration 1073 => Loss: 6.87716352558216037494\n",
      "Iteration 1074 => Loss: 6.87713940712507643127\n",
      "Iteration 1075 => Loss: 6.87711529192062620552\n",
      "Iteration 1076 => Loss: 6.87709117996831764685\n",
      "Iteration 1077 => Loss: 6.87706707126766225713\n",
      "Iteration 1078 => Loss: 6.87704296581814755740\n",
      "Iteration 1079 => Loss: 6.87701886361930281311\n",
      "Iteration 1080 => Loss: 6.87699476467063242069\n",
      "Iteration 1081 => Loss: 6.87697066897164077659\n",
      "Iteration 1082 => Loss: 6.87694657652185536989\n",
      "Iteration 1083 => Loss: 6.87692248732077882067\n",
      "Iteration 1084 => Loss: 6.87689840136792973624\n",
      "Iteration 1085 => Loss: 6.87687431866282228299\n",
      "Iteration 1086 => Loss: 6.87685023920497595640\n",
      "Iteration 1087 => Loss: 6.87682616299391025194\n",
      "Iteration 1088 => Loss: 6.87680209002914288874\n",
      "Iteration 1089 => Loss: 6.87677802031018892137\n",
      "Iteration 1090 => Loss: 6.87675395383657850346\n",
      "Iteration 1091 => Loss: 6.87672989060782491322\n",
      "Iteration 1092 => Loss: 6.87670583062346274517\n",
      "Iteration 1093 => Loss: 6.87668177388300261299\n",
      "Iteration 1094 => Loss: 6.87665772038596845306\n",
      "Iteration 1095 => Loss: 6.87663367013190729438\n",
      "Iteration 1096 => Loss: 6.87660962312032086885\n",
      "Iteration 1097 => Loss: 6.87658557935074554734\n",
      "Iteration 1098 => Loss: 6.87656153882271770073\n",
      "Iteration 1099 => Loss: 6.87653750153574794268\n",
      "Iteration 1100 => Loss: 6.87651346748938063769\n",
      "Iteration 1101 => Loss: 6.87648943668314682753\n",
      "Iteration 1102 => Loss: 6.87646540911656511952\n",
      "Iteration 1103 => Loss: 6.87644138478918076629\n",
      "Iteration 1104 => Loss: 6.87641736370052036875\n",
      "Iteration 1105 => Loss: 6.87639334585011496870\n",
      "Iteration 1106 => Loss: 6.87636933123749827246\n",
      "Iteration 1107 => Loss: 6.87634531986221197997\n",
      "Iteration 1108 => Loss: 6.87632131172378446848\n",
      "Iteration 1109 => Loss: 6.87629730682176099066\n",
      "Iteration 1110 => Loss: 6.87627330515566548286\n",
      "Iteration 1111 => Loss: 6.87624930672504586227\n",
      "Iteration 1112 => Loss: 6.87622531152942872978\n",
      "Iteration 1113 => Loss: 6.87620131956836821985\n",
      "Iteration 1114 => Loss: 6.87617733084139448607\n",
      "Iteration 1115 => Loss: 6.87615334534804212296\n",
      "Iteration 1116 => Loss: 6.87612936308785815953\n",
      "Iteration 1117 => Loss: 6.87610538406038163117\n",
      "Iteration 1118 => Loss: 6.87608140826516223143\n",
      "Iteration 1119 => Loss: 6.87605743570172656121\n",
      "Iteration 1120 => Loss: 6.87603346636962964311\n",
      "Iteration 1121 => Loss: 6.87600950026840607165\n",
      "Iteration 1122 => Loss: 6.87598553739760465220\n",
      "Iteration 1123 => Loss: 6.87596157775677241375\n",
      "Iteration 1124 => Loss: 6.87593762134544039810\n",
      "Iteration 1125 => Loss: 6.87591366816316806876\n",
      "Iteration 1126 => Loss: 6.87588971820949446112\n",
      "Iteration 1127 => Loss: 6.87586577148396393966\n",
      "Iteration 1128 => Loss: 6.87584182798612797427\n",
      "Iteration 1129 => Loss: 6.87581788771553359396\n",
      "Iteration 1130 => Loss: 6.87579395067171805778\n",
      "Iteration 1131 => Loss: 6.87577001685424438193\n",
      "Iteration 1132 => Loss: 6.87574608626264360822\n",
      "Iteration 1133 => Loss: 6.87572215889648319376\n",
      "Iteration 1134 => Loss: 6.87569823475529684487\n",
      "Iteration 1135 => Loss: 6.87567431383863869598\n",
      "Iteration 1136 => Loss: 6.87565039614606643426\n",
      "Iteration 1137 => Loss: 6.87562648167711820690\n",
      "Iteration 1138 => Loss: 6.87560257043135170107\n",
      "Iteration 1139 => Loss: 6.87557866240831572213\n",
      "Iteration 1140 => Loss: 6.87555475760755907544\n",
      "Iteration 1141 => Loss: 6.87553085602863500725\n",
      "Iteration 1142 => Loss: 6.87550695767110120471\n",
      "Iteration 1143 => Loss: 6.87548306253450114411\n",
      "Iteration 1144 => Loss: 6.87545917061838718354\n",
      "Iteration 1145 => Loss: 6.87543528192232766827\n",
      "Iteration 1146 => Loss: 6.87541139644586074553\n",
      "Iteration 1147 => Loss: 6.87538751418854143793\n",
      "Iteration 1148 => Loss: 6.87536363514993276169\n",
      "Iteration 1149 => Loss: 6.87533975932957730492\n",
      "Iteration 1150 => Loss: 6.87531588672704696563\n",
      "Iteration 1151 => Loss: 6.87529201734187278561\n",
      "Iteration 1152 => Loss: 6.87526815117363110375\n",
      "Iteration 1153 => Loss: 6.87524428822186628452\n",
      "Iteration 1154 => Loss: 6.87522042848613423871\n",
      "Iteration 1155 => Loss: 6.87519657196600153526\n",
      "Iteration 1156 => Loss: 6.87517271866100987410\n",
      "Iteration 1157 => Loss: 6.87514886857073115323\n",
      "Iteration 1158 => Loss: 6.87512502169470174351\n",
      "Iteration 1159 => Loss: 6.87510117803250420110\n",
      "Iteration 1160 => Loss: 6.87507733758367667320\n",
      "Iteration 1161 => Loss: 6.87505350034778039969\n",
      "Iteration 1162 => Loss: 6.87502966632438372585\n",
      "Iteration 1163 => Loss: 6.87500583551303368068\n",
      "Iteration 1164 => Loss: 6.87498200791329772130\n",
      "Iteration 1165 => Loss: 6.87495818352472465307\n",
      "Iteration 1166 => Loss: 6.87493436234687926856\n",
      "Iteration 1167 => Loss: 6.87491054437932103127\n",
      "Iteration 1168 => Loss: 6.87488672962160851654\n",
      "Iteration 1169 => Loss: 6.87486291807329852332\n",
      "Iteration 1170 => Loss: 6.87483910973395317967\n",
      "Iteration 1171 => Loss: 6.87481530460313550179\n",
      "Iteration 1172 => Loss: 6.87479150268040051230\n",
      "Iteration 1173 => Loss: 6.87476770396531655649\n",
      "Iteration 1174 => Loss: 6.87474390845743776879\n",
      "Iteration 1175 => Loss: 6.87472011615632450088\n",
      "Iteration 1176 => Loss: 6.87469632706153888080\n",
      "Iteration 1177 => Loss: 6.87467254117264747748\n",
      "Iteration 1178 => Loss: 6.87464875848920975443\n",
      "Iteration 1179 => Loss: 6.87462497901078339879\n",
      "Iteration 1180 => Loss: 6.87460120273692876225\n",
      "Iteration 1181 => Loss: 6.87457742966721507827\n",
      "Iteration 1182 => Loss: 6.87455365980120181035\n",
      "Iteration 1183 => Loss: 6.87452989313844753383\n",
      "Iteration 1184 => Loss: 6.87450612967851792945\n",
      "Iteration 1185 => Loss: 6.87448236942098223068\n",
      "Iteration 1186 => Loss: 6.87445861236539812467\n",
      "Iteration 1187 => Loss: 6.87443485851132418674\n",
      "Iteration 1188 => Loss: 6.87441110785832609764\n",
      "Iteration 1189 => Loss: 6.87438736040597131449\n",
      "Iteration 1190 => Loss: 6.87436361615382285351\n",
      "Iteration 1191 => Loss: 6.87433987510144017818\n",
      "Iteration 1192 => Loss: 6.87431613724838630475\n",
      "Iteration 1193 => Loss: 6.87429240259423846027\n",
      "Iteration 1194 => Loss: 6.87426867113855166735\n",
      "Iteration 1195 => Loss: 6.87424494288088894223\n",
      "Iteration 1196 => Loss: 6.87422121782081507746\n",
      "Iteration 1197 => Loss: 6.87419749595789841834\n",
      "Iteration 1198 => Loss: 6.87417377729170553380\n",
      "Iteration 1199 => Loss: 6.87415006182178878191\n",
      "Iteration 1200 => Loss: 6.87412634954772538975\n",
      "Iteration 1201 => Loss: 6.87410264046907926172\n",
      "Iteration 1202 => Loss: 6.87407893458541430221\n",
      "Iteration 1203 => Loss: 6.87405523189629352743\n",
      "Iteration 1204 => Loss: 6.87403153240129327628\n",
      "Iteration 1205 => Loss: 6.87400783609996857138\n",
      "Iteration 1206 => Loss: 6.87398414299188953436\n",
      "Iteration 1207 => Loss: 6.87396045307661829327\n",
      "Iteration 1208 => Loss: 6.87393676635373207517\n",
      "Iteration 1209 => Loss: 6.87391308282277790909\n",
      "Iteration 1210 => Loss: 6.87388940248334101568\n",
      "Iteration 1211 => Loss: 6.87386572533498529936\n",
      "Iteration 1212 => Loss: 6.87384205137727199997\n",
      "Iteration 1213 => Loss: 6.87381838060976235738\n",
      "Iteration 1214 => Loss: 6.87379471303204603316\n",
      "Iteration 1215 => Loss: 6.87377104864366650361\n",
      "Iteration 1216 => Loss: 6.87374738744419566672\n",
      "Iteration 1217 => Loss: 6.87372372943321074956\n",
      "Iteration 1218 => Loss: 6.87370007461027565654\n",
      "Iteration 1219 => Loss: 6.87367642297495340387\n",
      "Iteration 1220 => Loss: 6.87365277452681322501\n",
      "Iteration 1221 => Loss: 6.87362912926543234704\n",
      "Iteration 1222 => Loss: 6.87360548719035957532\n",
      "Iteration 1223 => Loss: 6.87358184830118368325\n",
      "Iteration 1224 => Loss: 6.87355821259745525253\n",
      "Iteration 1225 => Loss: 6.87353458007876039204\n",
      "Iteration 1226 => Loss: 6.87351095074465145984\n",
      "Iteration 1227 => Loss: 6.87348732459471012390\n",
      "Iteration 1228 => Loss: 6.87346370162849762409\n",
      "Iteration 1229 => Loss: 6.87344008184558230568\n",
      "Iteration 1230 => Loss: 6.87341646524553251396\n",
      "Iteration 1231 => Loss: 6.87339285182792192330\n",
      "Iteration 1232 => Loss: 6.87336924159231799081\n",
      "Iteration 1233 => Loss: 6.87334563453828817359\n",
      "Iteration 1234 => Loss: 6.87332203066540259329\n",
      "Iteration 1235 => Loss: 6.87329842997323670062\n",
      "Iteration 1236 => Loss: 6.87327483246134640638\n",
      "Iteration 1237 => Loss: 6.87325123812931160217\n",
      "Iteration 1238 => Loss: 6.87322764697669619238\n",
      "Iteration 1239 => Loss: 6.87320405900307562774\n",
      "Iteration 1240 => Loss: 6.87318047420802180625\n",
      "Iteration 1241 => Loss: 6.87315689259109241505\n",
      "Iteration 1242 => Loss: 6.87313331415187356299\n",
      "Iteration 1243 => Loss: 6.87310973888992293723\n",
      "Iteration 1244 => Loss: 6.87308616680481510031\n",
      "Iteration 1245 => Loss: 6.87306259789611839750\n",
      "Iteration 1246 => Loss: 6.87303903216340916771\n",
      "Iteration 1247 => Loss: 6.87301546960625664440\n",
      "Iteration 1248 => Loss: 6.87299191022422739650\n",
      "Iteration 1249 => Loss: 6.87296835401688976930\n",
      "Iteration 1250 => Loss: 6.87294480098382276623\n",
      "Iteration 1251 => Loss: 6.87292125112459384439\n",
      "Iteration 1252 => Loss: 6.87289770443876513184\n",
      "Iteration 1253 => Loss: 6.87287416092591918471\n",
      "Iteration 1254 => Loss: 6.87285062058563589460\n",
      "Iteration 1255 => Loss: 6.87282708341745873781\n",
      "Iteration 1256 => Loss: 6.87280354942098004045\n",
      "Iteration 1257 => Loss: 6.87278001859577081234\n",
      "Iteration 1258 => Loss: 6.87275649094138962880\n",
      "Iteration 1259 => Loss: 6.87273296645742082234\n",
      "Iteration 1260 => Loss: 6.87270944514343096188\n",
      "Iteration 1261 => Loss: 6.87268592699898661635\n",
      "Iteration 1262 => Loss: 6.87266241202366945373\n",
      "Iteration 1263 => Loss: 6.87263890021704604294\n",
      "Iteration 1264 => Loss: 6.87261539157869005834\n",
      "Iteration 1265 => Loss: 6.87259188610817250975\n",
      "Iteration 1266 => Loss: 6.87256838380506351882\n",
      "Iteration 1267 => Loss: 6.87254488466893853627\n",
      "Iteration 1268 => Loss: 6.87252138869937478916\n",
      "Iteration 1269 => Loss: 6.87249789589592907646\n",
      "Iteration 1270 => Loss: 6.87247440625818928339\n",
      "Iteration 1271 => Loss: 6.87245091978571931435\n",
      "Iteration 1272 => Loss: 6.87242743647809550822\n",
      "Iteration 1273 => Loss: 6.87240395633488976301\n",
      "Iteration 1274 => Loss: 6.87238047935567397673\n",
      "Iteration 1275 => Loss: 6.87235700554002804097\n",
      "Iteration 1276 => Loss: 6.87233353488751141924\n",
      "Iteration 1277 => Loss: 6.87231006739770133862\n",
      "Iteration 1278 => Loss: 6.87228660307018213160\n",
      "Iteration 1279 => Loss: 6.87226314190451148534\n",
      "Iteration 1280 => Loss: 6.87223968390027373232\n",
      "Iteration 1281 => Loss: 6.87221622905703366513\n",
      "Iteration 1282 => Loss: 6.87219277737437472808\n",
      "Iteration 1283 => Loss: 6.87216932885186171376\n",
      "Iteration 1284 => Loss: 6.87214588348907096105\n",
      "Iteration 1285 => Loss: 6.87212244128556992706\n",
      "Iteration 1286 => Loss: 6.87209900224095360244\n",
      "Iteration 1287 => Loss: 6.87207556635476546347\n",
      "Iteration 1288 => Loss: 6.87205213362659694809\n",
      "Iteration 1289 => Loss: 6.87202870405602439519\n",
      "Iteration 1290 => Loss: 6.87200527764261170915\n",
      "Iteration 1291 => Loss: 6.87198185438593878160\n",
      "Iteration 1292 => Loss: 6.87195843428557751054\n",
      "Iteration 1293 => Loss: 6.87193501734111134027\n",
      "Iteration 1294 => Loss: 6.87191160355209706978\n",
      "Iteration 1295 => Loss: 6.87188819291812080792\n",
      "Iteration 1296 => Loss: 6.87186478543875001179\n",
      "Iteration 1297 => Loss: 6.87184138111357345480\n",
      "Iteration 1298 => Loss: 6.87181797994214882408\n",
      "Iteration 1299 => Loss: 6.87179458192405334671\n",
      "Iteration 1300 => Loss: 6.87177118705886957883\n",
      "Iteration 1301 => Loss: 6.87174779534616408938\n",
      "Iteration 1302 => Loss: 6.87172440678552121085\n",
      "Iteration 1303 => Loss: 6.87170102137650928853\n",
      "Iteration 1304 => Loss: 6.87167763911870643767\n",
      "Iteration 1305 => Loss: 6.87165426001167567449\n",
      "Iteration 1306 => Loss: 6.87163088405501465417\n",
      "Iteration 1307 => Loss: 6.87160751124827218206\n",
      "Iteration 1308 => Loss: 6.87158414159103969610\n",
      "Iteration 1309 => Loss: 6.87156077508289264699\n",
      "Iteration 1310 => Loss: 6.87153741172339938004\n",
      "Iteration 1311 => Loss: 6.87151405151214778044\n",
      "Iteration 1312 => Loss: 6.87149069444869464718\n",
      "Iteration 1313 => Loss: 6.87146734053263052999\n",
      "Iteration 1314 => Loss: 6.87144398976351844510\n",
      "Iteration 1315 => Loss: 6.87142064214093917229\n",
      "Iteration 1316 => Loss: 6.87139729766448414949\n",
      "Iteration 1317 => Loss: 6.87137395633370307024\n",
      "Iteration 1318 => Loss: 6.87135061814819092518\n",
      "Iteration 1319 => Loss: 6.87132728310750895417\n",
      "Iteration 1320 => Loss: 6.87130395121124415425\n",
      "Iteration 1321 => Loss: 6.87128062245897197613\n",
      "Iteration 1322 => Loss: 6.87125729685026076510\n",
      "Iteration 1323 => Loss: 6.87123397438469396548\n",
      "Iteration 1324 => Loss: 6.87121065506184169891\n",
      "Iteration 1325 => Loss: 6.87118733888128208065\n",
      "Iteration 1326 => Loss: 6.87116402584259500230\n",
      "Iteration 1327 => Loss: 6.87114071594536302001\n",
      "Iteration 1328 => Loss: 6.87111740918914204457\n",
      "Iteration 1329 => Loss: 6.87109410557352351390\n",
      "Iteration 1330 => Loss: 6.87107080509808287871\n",
      "Iteration 1331 => Loss: 6.87104750776239647791\n",
      "Iteration 1332 => Loss: 6.87102421356603887403\n",
      "Iteration 1333 => Loss: 6.87100092250858640597\n",
      "Iteration 1334 => Loss: 6.87097763458961541261\n",
      "Iteration 1335 => Loss: 6.87095434980870933828\n",
      "Iteration 1336 => Loss: 6.87093106816543564008\n",
      "Iteration 1337 => Loss: 6.87090778965937509781\n",
      "Iteration 1338 => Loss: 6.87088451429010582672\n",
      "Iteration 1339 => Loss: 6.87086124205719883662\n",
      "Iteration 1340 => Loss: 6.87083797296024734180\n",
      "Iteration 1341 => Loss: 6.87081470699881702302\n",
      "Iteration 1342 => Loss: 6.87079144417247444920\n",
      "Iteration 1343 => Loss: 6.87076818448081994006\n",
      "Iteration 1344 => Loss: 6.87074492792341651182\n",
      "Iteration 1345 => Loss: 6.87072167449984405607\n",
      "Iteration 1346 => Loss: 6.87069842420968246444\n",
      "Iteration 1347 => Loss: 6.87067517705250629945\n",
      "Iteration 1348 => Loss: 6.87065193302789190000\n",
      "Iteration 1349 => Loss: 6.87062869213542537494\n",
      "Iteration 1350 => Loss: 6.87060545437467151686\n",
      "Iteration 1351 => Loss: 6.87058221974522176367\n",
      "Iteration 1352 => Loss: 6.87055898824665245428\n",
      "Iteration 1353 => Loss: 6.87053575987852749307\n",
      "Iteration 1354 => Loss: 6.87051253464043920616\n",
      "Iteration 1355 => Loss: 6.87048931253196037972\n",
      "Iteration 1356 => Loss: 6.87046609355266379993\n",
      "Iteration 1357 => Loss: 6.87044287770213202293\n",
      "Iteration 1358 => Loss: 6.87041966497995737484\n",
      "Iteration 1359 => Loss: 6.87039645538569843097\n",
      "Iteration 1360 => Loss: 6.87037324891893952383\n",
      "Iteration 1361 => Loss: 6.87035004557925432778\n",
      "Iteration 1362 => Loss: 6.87032684536623516891\n",
      "Iteration 1363 => Loss: 6.87030364827945216888\n",
      "Iteration 1364 => Loss: 6.87028045431848521929\n",
      "Iteration 1365 => Loss: 6.87025726348290621814\n",
      "Iteration 1366 => Loss: 6.87023407577230127430\n",
      "Iteration 1367 => Loss: 6.87021089118625205572\n",
      "Iteration 1368 => Loss: 6.87018770972432868405\n",
      "Iteration 1369 => Loss: 6.87016453138611726814\n",
      "Iteration 1370 => Loss: 6.87014135617119503507\n",
      "Iteration 1371 => Loss: 6.87011818407913832374\n",
      "Iteration 1372 => Loss: 6.87009501510952613756\n",
      "Iteration 1373 => Loss: 6.87007184926194280905\n",
      "Iteration 1374 => Loss: 6.87004868653595845984\n",
      "Iteration 1375 => Loss: 6.87002552693116097515\n",
      "Iteration 1376 => Loss: 6.87000237044712935841\n",
      "Iteration 1377 => Loss: 6.86997921708343639580\n",
      "Iteration 1378 => Loss: 6.86995606683966553163\n",
      "Iteration 1379 => Loss: 6.86993291971539221663\n",
      "Iteration 1380 => Loss: 6.86990977571021410597\n",
      "Iteration 1381 => Loss: 6.86988663482367734048\n",
      "Iteration 1382 => Loss: 6.86986349705539556254\n",
      "Iteration 1383 => Loss: 6.86984036240492734748\n",
      "Iteration 1384 => Loss: 6.86981723087186058052\n",
      "Iteration 1385 => Loss: 6.86979410245577248872\n",
      "Iteration 1386 => Loss: 6.86977097715624562824\n",
      "Iteration 1387 => Loss: 6.86974785497285989067\n",
      "Iteration 1388 => Loss: 6.86972473590518450948\n",
      "Iteration 1389 => Loss: 6.86970161995281713985\n",
      "Iteration 1390 => Loss: 6.86967850711533056796\n",
      "Iteration 1391 => Loss: 6.86965539739230290905\n",
      "Iteration 1392 => Loss: 6.86963229078331139021\n",
      "Iteration 1393 => Loss: 6.86960918728794656118\n",
      "Iteration 1394 => Loss: 6.86958608690577587907\n",
      "Iteration 1395 => Loss: 6.86956298963639522270\n",
      "Iteration 1396 => Loss: 6.86953989547936405557\n",
      "Iteration 1397 => Loss: 6.86951680443428713829\n",
      "Iteration 1398 => Loss: 6.86949371650072393436\n",
      "Iteration 1399 => Loss: 6.86947063167826943442\n",
      "Iteration 1400 => Loss: 6.86944754996649820100\n",
      "Iteration 1401 => Loss: 6.86942447136499545479\n",
      "Iteration 1402 => Loss: 6.86940139587332954108\n",
      "Iteration 1403 => Loss: 6.86937832349110255592\n",
      "Iteration 1404 => Loss: 6.86935525421787840372\n",
      "Iteration 1405 => Loss: 6.86933218805323786427\n",
      "Iteration 1406 => Loss: 6.86930912499677148730\n",
      "Iteration 1407 => Loss: 6.86928606504805738808\n",
      "Iteration 1408 => Loss: 6.86926300820667634639\n",
      "Iteration 1409 => Loss: 6.86923995447220292476\n",
      "Iteration 1410 => Loss: 6.86921690384422500841\n",
      "Iteration 1411 => Loss: 6.86919385632233403527\n",
      "Iteration 1412 => Loss: 6.86917081190608680430\n",
      "Iteration 1413 => Loss: 6.86914777059508896428\n",
      "Iteration 1414 => Loss: 6.86912473238890797234\n",
      "Iteration 1415 => Loss: 6.86910169728712372006\n",
      "Iteration 1416 => Loss: 6.86907866528932764538\n",
      "Iteration 1417 => Loss: 6.86905563639509519902\n",
      "Iteration 1418 => Loss: 6.86903261060400982529\n",
      "Iteration 1419 => Loss: 6.86900958791565763306\n",
      "Iteration 1420 => Loss: 6.86898656832961318486\n",
      "Iteration 1421 => Loss: 6.86896355184546791861\n",
      "Iteration 1422 => Loss: 6.86894053846278929143\n",
      "Iteration 1423 => Loss: 6.86891752818116962942\n",
      "Iteration 1424 => Loss: 6.86889452100018882419\n",
      "Iteration 1425 => Loss: 6.86887151691943120824\n",
      "Iteration 1426 => Loss: 6.86884851593847578499\n",
      "Iteration 1427 => Loss: 6.86882551805689800517\n",
      "Iteration 1428 => Loss: 6.86880252327430085302\n",
      "Iteration 1429 => Loss: 6.86877953159024823293\n",
      "Iteration 1430 => Loss: 6.86875654300432714194\n",
      "Iteration 1431 => Loss: 6.86873355751612102438\n",
      "Iteration 1432 => Loss: 6.86871057512521421273\n",
      "Iteration 1433 => Loss: 6.86868759583118748679\n",
      "Iteration 1434 => Loss: 6.86866461963362517906\n",
      "Iteration 1435 => Loss: 6.86864164653210629297\n",
      "Iteration 1436 => Loss: 6.86861867652621604918\n",
      "Iteration 1437 => Loss: 6.86859570961553433932\n",
      "Iteration 1438 => Loss: 6.86857274579965615402\n",
      "Iteration 1439 => Loss: 6.86854978507814717403\n",
      "Iteration 1440 => Loss: 6.86852682745059972547\n",
      "Iteration 1441 => Loss: 6.86850387291659369993\n",
      "Iteration 1442 => Loss: 6.86848092147571964716\n",
      "Iteration 1443 => Loss: 6.86845797312755124153\n",
      "Iteration 1444 => Loss: 6.86843502787167903278\n",
      "Iteration 1445 => Loss: 6.86841208570768024799\n",
      "Iteration 1446 => Loss: 6.86838914663514277237\n",
      "Iteration 1447 => Loss: 6.86836621065364472116\n",
      "Iteration 1448 => Loss: 6.86834327776277842048\n",
      "Iteration 1449 => Loss: 6.86832034796211754468\n",
      "Iteration 1450 => Loss: 6.86829742125124731444\n",
      "Iteration 1451 => Loss: 6.86827449762976094405\n",
      "Iteration 1452 => Loss: 6.86825157709723210786\n",
      "Iteration 1453 => Loss: 6.86822865965324869109\n",
      "Iteration 1454 => Loss: 6.86820574529738969716\n",
      "Iteration 1455 => Loss: 6.86818283402924123493\n",
      "Iteration 1456 => Loss: 6.86815992584839829505\n",
      "Iteration 1457 => Loss: 6.86813702075442567008\n",
      "Iteration 1458 => Loss: 6.86811411874692101520\n",
      "Iteration 1459 => Loss: 6.86809121982547399199\n",
      "Iteration 1460 => Loss: 6.86806832398964495212\n",
      "Iteration 1461 => Loss: 6.86804543123903421531\n",
      "Iteration 1462 => Loss: 6.86802254157323055495\n",
      "Iteration 1463 => Loss: 6.86799965499180853357\n",
      "Iteration 1464 => Loss: 6.86797677149435781274\n",
      "Iteration 1465 => Loss: 6.86795389108045828408\n",
      "Iteration 1466 => Loss: 6.86793101374969783279\n",
      "Iteration 1467 => Loss: 6.86790813950165368595\n",
      "Iteration 1468 => Loss: 6.86788526833593149235\n",
      "Iteration 1469 => Loss: 6.86786240025208716276\n",
      "Iteration 1470 => Loss: 6.86783953524972723415\n",
      "Iteration 1471 => Loss: 6.86781667332842538087\n",
      "Iteration 1472 => Loss: 6.86779381448777836994\n",
      "Iteration 1473 => Loss: 6.86777095872734832938\n",
      "Iteration 1474 => Loss: 6.86774810604673824344\n",
      "Iteration 1475 => Loss: 6.86772525644553777369\n",
      "Iteration 1476 => Loss: 6.86770240992331526542\n",
      "Iteration 1477 => Loss: 6.86767956647967192652\n",
      "Iteration 1478 => Loss: 6.86765672611417965499\n",
      "Iteration 1479 => Loss: 6.86763388882642278332\n",
      "Iteration 1480 => Loss: 6.86761105461600429578\n",
      "Iteration 1481 => Loss: 6.86758822348248809675\n",
      "Iteration 1482 => Loss: 6.86756539542547717048\n",
      "Iteration 1483 => Loss: 6.86754257044454519132\n",
      "Iteration 1484 => Loss: 6.86751974853928537357\n",
      "Iteration 1485 => Loss: 6.86749692970927494429\n",
      "Iteration 1486 => Loss: 6.86747411395410356505\n",
      "Iteration 1487 => Loss: 6.86745130127336000925\n",
      "Iteration 1488 => Loss: 6.86742849166662594484\n",
      "Iteration 1489 => Loss: 6.86740568513349014523\n",
      "Iteration 1490 => Loss: 6.86738288167353605473\n",
      "Iteration 1491 => Loss: 6.86736008128635511127\n",
      "Iteration 1492 => Loss: 6.86733728397152010103\n",
      "Iteration 1493 => Loss: 6.86731448972863400826\n",
      "Iteration 1494 => Loss: 6.86729169855726517824\n",
      "Iteration 1495 => Loss: 6.86726891045701393068\n",
      "Iteration 1496 => Loss: 6.86724612542746015720\n",
      "Iteration 1497 => Loss: 6.86722334346819351936\n",
      "Iteration 1498 => Loss: 6.86720056457879302059\n",
      "Iteration 1499 => Loss: 6.86717778875885631606\n",
      "Iteration 1500 => Loss: 6.86715501600795796833\n",
      "Iteration 1501 => Loss: 6.86713224632568941530\n",
      "Iteration 1502 => Loss: 6.86710947971164387127\n",
      "Iteration 1503 => Loss: 6.86708671616539945148\n",
      "Iteration 1504 => Loss: 6.86706395568654315298\n",
      "Iteration 1505 => Loss: 6.86704119827466552550\n",
      "Iteration 1506 => Loss: 6.86701844392934468431\n",
      "Iteration 1507 => Loss: 6.86699569265018272546\n",
      "Iteration 1508 => Loss: 6.86697294443675421149\n",
      "Iteration 1509 => Loss: 6.86695019928864880399\n",
      "Iteration 1510 => Loss: 6.86692745720545438814\n",
      "Iteration 1511 => Loss: 6.86690471818676240190\n",
      "Iteration 1512 => Loss: 6.86688198223214651961\n",
      "Iteration 1513 => Loss: 6.86685924934120706098\n",
      "Iteration 1514 => Loss: 6.86683651951352391762\n",
      "Iteration 1515 => Loss: 6.86681379274869652107\n",
      "Iteration 1516 => Loss: 6.86679106904629410479\n",
      "Iteration 1517 => Loss: 6.86676834840591254761\n",
      "Iteration 1518 => Loss: 6.86674563082714239926\n",
      "Iteration 1519 => Loss: 6.86672291630956710407\n",
      "Iteration 1520 => Loss: 6.86670020485277454725\n",
      "Iteration 1521 => Loss: 6.86667749645635172584\n",
      "Iteration 1522 => Loss: 6.86665479111988741323\n",
      "Iteration 1523 => Loss: 6.86663208884297215917\n",
      "Iteration 1524 => Loss: 6.86660938962518763162\n",
      "Iteration 1525 => Loss: 6.86658669346612349216\n",
      "Iteration 1526 => Loss: 6.86656400036537473142\n",
      "Iteration 1527 => Loss: 6.86654131032251502376\n",
      "Iteration 1528 => Loss: 6.86651862333714912978\n",
      "Iteration 1529 => Loss: 6.86649593940885516474\n",
      "Iteration 1530 => Loss: 6.86647325853721390843\n",
      "Iteration 1531 => Loss: 6.86645058072183100961\n",
      "Iteration 1532 => Loss: 6.86642790596228547173\n",
      "Iteration 1533 => Loss: 6.86640523425816251546\n",
      "Iteration 1534 => Loss: 6.86638256560904824966\n",
      "Iteration 1535 => Loss: 6.86635990001454210585\n",
      "Iteration 1536 => Loss: 6.86633723747422752837\n",
      "Iteration 1537 => Loss: 6.86631457798769329059\n",
      "Iteration 1538 => Loss: 6.86629192155452283686\n",
      "Iteration 1539 => Loss: 6.86626926817431204597\n",
      "Iteration 1540 => Loss: 6.86624661784664258590\n",
      "Iteration 1541 => Loss: 6.86622397057111211183\n",
      "Iteration 1542 => Loss: 6.86620132634729873899\n",
      "Iteration 1543 => Loss: 6.86617868517479745805\n",
      "Iteration 1544 => Loss: 6.86615604705319793055\n",
      "Iteration 1545 => Loss: 6.86613341198208004812\n",
      "Iteration 1546 => Loss: 6.86611077996105301224\n",
      "Iteration 1547 => Loss: 6.86608815098967983914\n",
      "Iteration 1548 => Loss: 6.86606552506757061849\n",
      "Iteration 1549 => Loss: 6.86604290219430257736\n",
      "Iteration 1550 => Loss: 6.86602028236947248274\n",
      "Iteration 1551 => Loss: 6.86599766559266377897\n",
      "Iteration 1552 => Loss: 6.86597505186346346306\n",
      "Iteration 1553 => Loss: 6.86595244118147096657\n",
      "Iteration 1554 => Loss: 6.86592983354626529291\n",
      "Iteration 1555 => Loss: 6.86590722895744853815\n",
      "Iteration 1556 => Loss: 6.86588462741459260030\n",
      "Iteration 1557 => Loss: 6.86586202891730312814\n",
      "Iteration 1558 => Loss: 6.86583943346515468420\n",
      "Iteration 1559 => Loss: 6.86581684105775025273\n",
      "Iteration 1560 => Loss: 6.86579425169467594259\n",
      "Iteration 1561 => Loss: 6.86577166537551697445\n",
      "Iteration 1562 => Loss: 6.86574908209986922714\n",
      "Iteration 1563 => Loss: 6.86572650186731525679\n",
      "Iteration 1564 => Loss: 6.86570392467745715948\n",
      "Iteration 1565 => Loss: 6.86568135052986949773\n",
      "Iteration 1566 => Loss: 6.86565877942415259128\n",
      "Iteration 1567 => Loss: 6.86563621135989876620\n",
      "Iteration 1568 => Loss: 6.86561364633668702595\n",
      "Iteration 1569 => Loss: 6.86559108435411946658\n",
      "Iteration 1570 => Loss: 6.86556852541177331517\n",
      "Iteration 1571 => Loss: 6.86554596950924977961\n",
      "Iteration 1572 => Loss: 6.86552341664613852146\n",
      "Iteration 1573 => Loss: 6.86550086682202831412\n",
      "Iteration 1574 => Loss: 6.86547832003650349009\n",
      "Iteration 1575 => Loss: 6.86545577628916703361\n",
      "Iteration 1576 => Loss: 6.86543323557959617176\n",
      "Iteration 1577 => Loss: 6.86541069790738767153\n",
      "Iteration 1578 => Loss: 6.86538816327213918811\n",
      "Iteration 1579 => Loss: 6.86536563167343327763\n",
      "Iteration 1580 => Loss: 6.86534310311085338441\n",
      "Iteration 1581 => Loss: 6.86532057758400515723\n",
      "Iteration 1582 => Loss: 6.86529805509247559314\n",
      "Iteration 1583 => Loss: 6.86527553563584902463\n",
      "Iteration 1584 => Loss: 6.86525301921372221869\n",
      "Iteration 1585 => Loss: 6.86523050582568483691\n",
      "Iteration 1586 => Loss: 6.86520799547132831719\n",
      "Iteration 1587 => Loss: 6.86518548815024320930\n",
      "Iteration 1588 => Loss: 6.86516298386202361570\n",
      "Iteration 1589 => Loss: 6.86514048260626097431\n",
      "Iteration 1590 => Loss: 6.86511798438253784127\n",
      "Iteration 1591 => Loss: 6.86509548919045631266\n",
      "Iteration 1592 => Loss: 6.86507299702959983279\n",
      "Iteration 1593 => Loss: 6.86505050789957227408\n",
      "Iteration 1594 => Loss: 6.86502802179994819909\n",
      "Iteration 1595 => Loss: 6.86500553873033325658\n",
      "Iteration 1596 => Loss: 6.86498305869031266724\n",
      "Iteration 1597 => Loss: 6.86496058167947875717\n",
      "Iteration 1598 => Loss: 6.86493810769742118794\n",
      "Iteration 1599 => Loss: 6.86491563674373495019\n",
      "Iteration 1600 => Loss: 6.86489316881801947545\n",
      "Iteration 1601 => Loss: 6.86487070391984577356\n",
      "Iteration 1602 => Loss: 6.86484824204883015142\n",
      "Iteration 1603 => Loss: 6.86482578320454095433\n",
      "Iteration 1604 => Loss: 6.86480332738659448921\n",
      "Iteration 1605 => Loss: 6.86478087459456798314\n",
      "Iteration 1606 => Loss: 6.86475842482805287403\n",
      "Iteration 1607 => Loss: 6.86473597808665747522\n",
      "Iteration 1608 => Loss: 6.86471353436995102015\n",
      "Iteration 1609 => Loss: 6.86469109367753915762\n",
      "Iteration 1610 => Loss: 6.86466865600901865463\n",
      "Iteration 1611 => Loss: 6.86464622136396585006\n",
      "Iteration 1612 => Loss: 6.86462378974198816906\n",
      "Iteration 1613 => Loss: 6.86460136114267704954\n",
      "Iteration 1614 => Loss: 6.86457893556560971859\n",
      "Iteration 1615 => Loss: 6.86455651301040425949\n",
      "Iteration 1616 => Loss: 6.86453409347663434659\n",
      "Iteration 1617 => Loss: 6.86451167696390118778\n",
      "Iteration 1618 => Loss: 6.86448926347178378649\n",
      "Iteration 1619 => Loss: 6.86446685299990289053\n",
      "Iteration 1620 => Loss: 6.86444444554782151613\n",
      "Iteration 1621 => Loss: 6.86442204111514975295\n",
      "Iteration 1622 => Loss: 6.86439963970147903893\n",
      "Iteration 1623 => Loss: 6.86437724130640170017\n",
      "Iteration 1624 => Loss: 6.86435484592950562188\n",
      "Iteration 1625 => Loss: 6.86433245357039201195\n",
      "Iteration 1626 => Loss: 6.86431006422864609107\n",
      "Iteration 1627 => Loss: 6.86428767790387350800\n",
      "Iteration 1628 => Loss: 6.86426529459565415436\n",
      "Iteration 1629 => Loss: 6.86424291430359012622\n",
      "Iteration 1630 => Loss: 6.86422053702726575608\n",
      "Iteration 1631 => Loss: 6.86419816276629024543\n",
      "Iteration 1632 => Loss: 6.86417579152023993316\n",
      "Iteration 1633 => Loss: 6.86415342328872313260\n",
      "Iteration 1634 => Loss: 6.86413105807132595260\n",
      "Iteration 1635 => Loss: 6.86410869586764249561\n",
      "Iteration 1636 => Loss: 6.86408633667726242322\n",
      "Iteration 1637 => Loss: 6.86406398049979493692\n",
      "Iteration 1638 => Loss: 6.86404162733481992831\n",
      "Iteration 1639 => Loss: 6.86401927718193771710\n",
      "Iteration 1640 => Loss: 6.86399693004073796487\n",
      "Iteration 1641 => Loss: 6.86397458591081033319\n",
      "Iteration 1642 => Loss: 6.86395224479176668808\n",
      "Iteration 1643 => Loss: 6.86392990668318514480\n",
      "Iteration 1644 => Loss: 6.86390757158467224031\n",
      "Iteration 1645 => Loss: 6.86388523949580608985\n",
      "Iteration 1646 => Loss: 6.86386291041619767128\n",
      "Iteration 1647 => Loss: 6.86384058434542954075\n",
      "Iteration 1648 => Loss: 6.86381826128310112978\n",
      "Iteration 1649 => Loss: 6.86379594122881186991\n",
      "Iteration 1650 => Loss: 6.86377362418215231088\n",
      "Iteration 1651 => Loss: 6.86375131014270944974\n",
      "Iteration 1652 => Loss: 6.86372899911009337615\n",
      "Iteration 1653 => Loss: 6.86370669108388664625\n",
      "Iteration 1654 => Loss: 6.86368438606369046795\n",
      "Iteration 1655 => Loss: 6.86366208404909272645\n",
      "Iteration 1656 => Loss: 6.86363978503969551781\n",
      "Iteration 1657 => Loss: 6.86361748903509116815\n",
      "Iteration 1658 => Loss: 6.86359519603487733264\n",
      "Iteration 1659 => Loss: 6.86357290603864456102\n",
      "Iteration 1660 => Loss: 6.86355061904599139666\n",
      "Iteration 1661 => Loss: 6.86352833505650572476\n",
      "Iteration 1662 => Loss: 6.86350605406980474044\n",
      "Iteration 1663 => Loss: 6.86348377608545323625\n",
      "Iteration 1664 => Loss: 6.86346150110307196002\n",
      "Iteration 1665 => Loss: 6.86343922912224169153\n",
      "Iteration 1666 => Loss: 6.86341696014256630320\n",
      "Iteration 1667 => Loss: 6.86339469416363101573\n",
      "Iteration 1668 => Loss: 6.86337243118503881334\n",
      "Iteration 1669 => Loss: 6.86335017120639001575\n",
      "Iteration 1670 => Loss: 6.86332791422728050179\n",
      "Iteration 1671 => Loss: 6.86330566024729016306\n",
      "Iteration 1672 => Loss: 6.86328340926602553651\n",
      "Iteration 1673 => Loss: 6.86326116128308871822\n",
      "Iteration 1674 => Loss: 6.86323891629806759340\n",
      "Iteration 1675 => Loss: 6.86321667431055271180\n",
      "Iteration 1676 => Loss: 6.86319443532015416309\n",
      "Iteration 1677 => Loss: 6.86317219932646072067\n",
      "Iteration 1678 => Loss: 6.86314996632906915153\n",
      "Iteration 1679 => Loss: 6.86312773632757622266\n",
      "Iteration 1680 => Loss: 6.86310550932157426018\n",
      "Iteration 1681 => Loss: 6.86308328531066447198\n",
      "Iteration 1682 => Loss: 6.86306106429444540140\n",
      "Iteration 1683 => Loss: 6.86303884627250315731\n",
      "Iteration 1684 => Loss: 6.86301663124445138209\n",
      "Iteration 1685 => Loss: 6.86299441920986996735\n",
      "Iteration 1686 => Loss: 6.86297221016836367369\n",
      "Iteration 1687 => Loss: 6.86295000411952393904\n",
      "Iteration 1688 => Loss: 6.86292780106295108311\n",
      "Iteration 1689 => Loss: 6.86290560099825075469\n",
      "Iteration 1690 => Loss: 6.86288340392500195719\n",
      "Iteration 1691 => Loss: 6.86286120984281122759\n",
      "Iteration 1692 => Loss: 6.86283901875127799741\n",
      "Iteration 1693 => Loss: 6.86281683064999192823\n",
      "Iteration 1694 => Loss: 6.86279464553855689246\n",
      "Iteration 1695 => Loss: 6.86277246341656521622\n",
      "Iteration 1696 => Loss: 6.86275028428361899557\n",
      "Iteration 1697 => Loss: 6.86272810813931055662\n",
      "Iteration 1698 => Loss: 6.86270593498324465997\n",
      "Iteration 1699 => Loss: 6.86268376481501274355\n",
      "Iteration 1700 => Loss: 6.86266159763420890982\n",
      "Iteration 1701 => Loss: 6.86263943344043081396\n",
      "Iteration 1702 => Loss: 6.86261727223328854564\n",
      "Iteration 1703 => Loss: 6.86259511401236466099\n",
      "Iteration 1704 => Loss: 6.86257295877725770339\n",
      "Iteration 1705 => Loss: 6.86255080652758309157\n",
      "Iteration 1706 => Loss: 6.86252865726291361170\n",
      "Iteration 1707 => Loss: 6.86250651098286379437\n",
      "Iteration 1708 => Loss: 6.86248436768703218291\n",
      "Iteration 1709 => Loss: 6.86246222737500488620\n",
      "Iteration 1710 => Loss: 6.86244009004639465843\n",
      "Iteration 1711 => Loss: 6.86241795570078227939\n",
      "Iteration 1712 => Loss: 6.86239582433777339787\n",
      "Iteration 1713 => Loss: 6.86237369595697455082\n",
      "Iteration 1714 => Loss: 6.86235157055797362347\n",
      "Iteration 1715 => Loss: 6.86232944814036915915\n",
      "Iteration 1716 => Loss: 6.86230732870376591848\n",
      "Iteration 1717 => Loss: 6.86228521224776333298\n",
      "Iteration 1718 => Loss: 6.86226309877194307063\n",
      "Iteration 1719 => Loss: 6.86224098827591966199\n",
      "Iteration 1720 => Loss: 6.86221888075929609130\n",
      "Iteration 1721 => Loss: 6.86219677622165313835\n",
      "Iteration 1722 => Loss: 6.86217467466260266917\n",
      "Iteration 1723 => Loss: 6.86215257608173700987\n",
      "Iteration 1724 => Loss: 6.86213048047865470380\n",
      "Iteration 1725 => Loss: 6.86210838785296051157\n",
      "Iteration 1726 => Loss: 6.86208629820424587109\n",
      "Iteration 1727 => Loss: 6.86206421153211465480\n",
      "Iteration 1728 => Loss: 6.86204212783616718241\n",
      "Iteration 1729 => Loss: 6.86202004711600022091\n",
      "Iteration 1730 => Loss: 6.86199796937120076734\n",
      "Iteration 1731 => Loss: 6.86197589460139045769\n",
      "Iteration 1732 => Loss: 6.86195382280615095993\n",
      "Iteration 1733 => Loss: 6.86193175398509591645\n",
      "Iteration 1734 => Loss: 6.86190968813780877156\n",
      "Iteration 1735 => Loss: 6.86188762526389428587\n",
      "Iteration 1736 => Loss: 6.86186556536296077269\n",
      "Iteration 1737 => Loss: 6.86184350843459700542\n",
      "Iteration 1738 => Loss: 6.86182145447840863284\n",
      "Iteration 1739 => Loss: 6.86179940349399153376\n",
      "Iteration 1740 => Loss: 6.86177735548094247520\n",
      "Iteration 1741 => Loss: 6.86175531043886799409\n",
      "Iteration 1742 => Loss: 6.86173326836736041656\n",
      "Iteration 1743 => Loss: 6.86171122926603338499\n",
      "Iteration 1744 => Loss: 6.86168919313447034369\n",
      "Iteration 1745 => Loss: 6.86166715997227516510\n",
      "Iteration 1746 => Loss: 6.86164512977905349800\n",
      "Iteration 1747 => Loss: 6.86162310255440210938\n",
      "Iteration 1748 => Loss: 6.86160107829792487166\n",
      "Iteration 1749 => Loss: 6.86157905700920878189\n",
      "Iteration 1750 => Loss: 6.86155703868787281152\n",
      "Iteration 1751 => Loss: 6.86153502333350129305\n",
      "Iteration 1752 => Loss: 6.86151301094570520434\n",
      "Iteration 1753 => Loss: 6.86149100152407864783\n",
      "Iteration 1754 => Loss: 6.86146899506821750236\n",
      "Iteration 1755 => Loss: 6.86144699157773274578\n",
      "Iteration 1756 => Loss: 6.86142499105222292144\n",
      "Iteration 1757 => Loss: 6.86140299349128301998\n",
      "Iteration 1758 => Loss: 6.86138099889451424929\n",
      "Iteration 1759 => Loss: 6.86135900726152136997\n",
      "Iteration 1760 => Loss: 6.86133701859189937267\n",
      "Iteration 1761 => Loss: 6.86131503288526012341\n",
      "Iteration 1762 => Loss: 6.86129305014118795469\n",
      "Iteration 1763 => Loss: 6.86127107035929473255\n",
      "Iteration 1764 => Loss: 6.86124909353917722399\n",
      "Iteration 1765 => Loss: 6.86122711968044196595\n",
      "Iteration 1766 => Loss: 6.86120514878268572545\n",
      "Iteration 1767 => Loss: 6.86118318084550704583\n",
      "Iteration 1768 => Loss: 6.86116121586851068770\n",
      "Iteration 1769 => Loss: 6.86113925385129519441\n",
      "Iteration 1770 => Loss: 6.86111729479346710292\n",
      "Iteration 1771 => Loss: 6.86109533869462318023\n",
      "Iteration 1772 => Loss: 6.86107338555436019334\n",
      "Iteration 1773 => Loss: 6.86105143537228912010\n",
      "Iteration 1774 => Loss: 6.86102948814800761568\n",
      "Iteration 1775 => Loss: 6.86100754388110889437\n",
      "Iteration 1776 => Loss: 6.86098560257120837491\n",
      "Iteration 1777 => Loss: 6.86096366421789927159\n",
      "Iteration 1778 => Loss: 6.86094172882078900955\n",
      "Iteration 1779 => Loss: 6.86091979637947435577\n",
      "Iteration 1780 => Loss: 6.86089786689355385363\n",
      "Iteration 1781 => Loss: 6.86087594036263492825\n",
      "Iteration 1782 => Loss: 6.86085401678631789935\n",
      "Iteration 1783 => Loss: 6.86083209616420575117\n",
      "Iteration 1784 => Loss: 6.86081017849589613888\n",
      "Iteration 1785 => Loss: 6.86078826378099293493\n",
      "Iteration 1786 => Loss: 6.86076635201910445261\n",
      "Iteration 1787 => Loss: 6.86074444320982212986\n",
      "Iteration 1788 => Loss: 6.86072253735275516817\n",
      "Iteration 1789 => Loss: 6.86070063444750921633\n",
      "Iteration 1790 => Loss: 6.86067873449367127137\n",
      "Iteration 1791 => Loss: 6.86065683749086741017\n",
      "Iteration 1792 => Loss: 6.86063494343867752434\n",
      "Iteration 1793 => Loss: 6.86061305233671259174\n",
      "Iteration 1794 => Loss: 6.86059116418458003750\n",
      "Iteration 1795 => Loss: 6.86056927898187396409\n",
      "Iteration 1796 => Loss: 6.86054739672820357299\n",
      "Iteration 1797 => Loss: 6.86052551742316829575\n",
      "Iteration 1798 => Loss: 6.86050364106636312300\n",
      "Iteration 1799 => Loss: 6.86048176765740613803\n",
      "Iteration 1800 => Loss: 6.86045989719589410782\n",
      "Iteration 1801 => Loss: 6.86043802968142824028\n",
      "Iteration 1802 => Loss: 6.86041616511361507236\n",
      "Iteration 1803 => Loss: 6.86039430349204337745\n",
      "Iteration 1804 => Loss: 6.86037244481633123883\n",
      "Iteration 1805 => Loss: 6.86035058908607986439\n",
      "Iteration 1806 => Loss: 6.86032873630088602113\n",
      "Iteration 1807 => Loss: 6.86030688646036157508\n",
      "Iteration 1808 => Loss: 6.86028503956409885234\n",
      "Iteration 1809 => Loss: 6.86026319561171060712\n",
      "Iteration 1810 => Loss: 6.86024135460280160004\n",
      "Iteration 1811 => Loss: 6.86021951653695971629\n",
      "Iteration 1812 => Loss: 6.86019768141380570370\n",
      "Iteration 1813 => Loss: 6.86017584923293100019\n",
      "Iteration 1814 => Loss: 6.86015401999395280086\n",
      "Iteration 1815 => Loss: 6.86013219369646254364\n",
      "Iteration 1816 => Loss: 6.86011037034006498914\n",
      "Iteration 1817 => Loss: 6.86008854992436667430\n",
      "Iteration 1818 => Loss: 6.86006673244896969521\n",
      "Iteration 1819 => Loss: 6.86004491791348769425\n",
      "Iteration 1820 => Loss: 6.86002310631751033299\n",
      "Iteration 1821 => Loss: 6.86000129766064770109\n",
      "Iteration 1822 => Loss: 6.85997949194249923011\n",
      "Iteration 1823 => Loss: 6.85995768916267856241\n",
      "Iteration 1824 => Loss: 6.85993588932078335318\n",
      "Iteration 1825 => Loss: 6.85991409241641481032\n",
      "Iteration 1826 => Loss: 6.85989229844918746437\n",
      "Iteration 1827 => Loss: 6.85987050741869364145\n",
      "Iteration 1828 => Loss: 6.85984871932454165488\n",
      "Iteration 1829 => Loss: 6.85982693416634070616\n",
      "Iteration 1830 => Loss: 6.85980515194369022680\n",
      "Iteration 1831 => Loss: 6.85978337265619941832\n",
      "Iteration 1832 => Loss: 6.85976159630345971863\n",
      "Iteration 1833 => Loss: 6.85973982288509276373\n",
      "Iteration 1834 => Loss: 6.85971805240069265608\n",
      "Iteration 1835 => Loss: 6.85969628484986948536\n",
      "Iteration 1836 => Loss: 6.85967452023223067670\n",
      "Iteration 1837 => Loss: 6.85965275854736322714\n",
      "Iteration 1838 => Loss: 6.85963099979489232538\n",
      "Iteration 1839 => Loss: 6.85960924397441651479\n",
      "Iteration 1840 => Loss: 6.85958749108553345053\n",
      "Iteration 1841 => Loss: 6.85956574112786210407\n",
      "Iteration 1842 => Loss: 6.85954399410098769607\n",
      "Iteration 1843 => Loss: 6.85952225000453452708\n",
      "Iteration 1844 => Loss: 6.85950050883809936408\n",
      "Iteration 1845 => Loss: 6.85947877060128607951\n",
      "Iteration 1846 => Loss: 6.85945703529370565121\n",
      "Iteration 1847 => Loss: 6.85943530291495839890\n",
      "Iteration 1848 => Loss: 6.85941357346464908318\n",
      "Iteration 1849 => Loss: 6.85939184694238424100\n",
      "Iteration 1850 => Loss: 6.85937012334777396205\n",
      "Iteration 1851 => Loss: 6.85934840268041590150\n",
      "Iteration 1852 => Loss: 6.85932668493991926084\n",
      "Iteration 1853 => Loss: 6.85930497012589412975\n",
      "Iteration 1854 => Loss: 6.85928325823793816340\n",
      "Iteration 1855 => Loss: 6.85926154927565967512\n",
      "Iteration 1856 => Loss: 6.85923984323866964274\n",
      "Iteration 1857 => Loss: 6.85921814012656749782\n",
      "Iteration 1858 => Loss: 6.85919643993895977729\n",
      "Iteration 1859 => Loss: 6.85917474267545745903\n",
      "Iteration 1860 => Loss: 6.85915304833565908638\n",
      "Iteration 1861 => Loss: 6.85913135691918007808\n",
      "Iteration 1862 => Loss: 6.85910966842561720114\n",
      "Iteration 1863 => Loss: 6.85908798285458143340\n",
      "Iteration 1864 => Loss: 6.85906630020567487094\n",
      "Iteration 1865 => Loss: 6.85904462047851026796\n",
      "Iteration 1866 => Loss: 6.85902294367269238506\n",
      "Iteration 1867 => Loss: 6.85900126978782509468\n",
      "Iteration 1868 => Loss: 6.85897959882351315741\n",
      "Iteration 1869 => Loss: 6.85895793077936932747\n",
      "Iteration 1870 => Loss: 6.85893626565499214820\n",
      "Iteration 1871 => Loss: 6.85891460344999348564\n",
      "Iteration 1872 => Loss: 6.85889294416397188314\n",
      "Iteration 1873 => Loss: 6.85887128779655164124\n",
      "Iteration 1874 => Loss: 6.85884963434732597420\n",
      "Iteration 1875 => Loss: 6.85882798381590319536\n",
      "Iteration 1876 => Loss: 6.85880633620189428257\n",
      "Iteration 1877 => Loss: 6.85878469150489333828\n",
      "Iteration 1878 => Loss: 6.85876304972452466302\n",
      "Iteration 1879 => Loss: 6.85874141086038502380\n",
      "Iteration 1880 => Loss: 6.85871977491208539846\n",
      "Iteration 1881 => Loss: 6.85869814187923321214\n",
      "Iteration 1882 => Loss: 6.85867651176142967273\n",
      "Iteration 1883 => Loss: 6.85865488455828931080\n",
      "Iteration 1884 => Loss: 6.85863326026941688696\n",
      "Iteration 1885 => Loss: 6.85861163889441627362\n",
      "Iteration 1886 => Loss: 6.85859002043289667228\n",
      "Iteration 1887 => Loss: 6.85856840488447083715\n",
      "Iteration 1888 => Loss: 6.85854679224873642340\n",
      "Iteration 1889 => Loss: 6.85852518252530973797\n",
      "Iteration 1890 => Loss: 6.85850357571379376509\n",
      "Iteration 1891 => Loss: 6.85848197181380303533\n",
      "Iteration 1892 => Loss: 6.85846037082492632209\n",
      "Iteration 1893 => Loss: 6.85843877274679591949\n",
      "Iteration 1894 => Loss: 6.85841717757900859453\n",
      "Iteration 1895 => Loss: 6.85839558532116821965\n",
      "Iteration 1896 => Loss: 6.85837399597288221997\n",
      "Iteration 1897 => Loss: 6.85835240953377134332\n",
      "Iteration 1898 => Loss: 6.85833082600342791579\n",
      "Iteration 1899 => Loss: 6.85830924538147446157\n",
      "Iteration 1900 => Loss: 6.85828766766750241857\n",
      "Iteration 1901 => Loss: 6.85826609286113697550\n",
      "Iteration 1902 => Loss: 6.85824452096197223483\n",
      "Iteration 1903 => Loss: 6.85822295196962539165\n",
      "Iteration 1904 => Loss: 6.85820138588369943022\n",
      "Iteration 1905 => Loss: 6.85817982270380710474\n",
      "Iteration 1906 => Loss: 6.85815826242955317582\n",
      "Iteration 1907 => Loss: 6.85813670506054950948\n",
      "Iteration 1908 => Loss: 6.85811515059640353087\n",
      "Iteration 1909 => Loss: 6.85809359903671911241\n",
      "Iteration 1910 => Loss: 6.85807205038111611373\n",
      "Iteration 1911 => Loss: 6.85805050462919396637\n",
      "Iteration 1912 => Loss: 6.85802896178056364818\n",
      "Iteration 1913 => Loss: 6.85800742183482991976\n",
      "Iteration 1914 => Loss: 6.85798588479160731168\n",
      "Iteration 1915 => Loss: 6.85796435065050502544\n",
      "Iteration 1916 => Loss: 6.85794281941112693346\n",
      "Iteration 1917 => Loss: 6.85792129107309111902\n",
      "Iteration 1918 => Loss: 6.85789976563599523729\n",
      "Iteration 1919 => Loss: 6.85787824309945470702\n",
      "Iteration 1920 => Loss: 6.85785672346307784153\n",
      "Iteration 1921 => Loss: 6.85783520672647561867\n",
      "Iteration 1922 => Loss: 6.85781369288925457539\n",
      "Iteration 1923 => Loss: 6.85779218195101858413\n",
      "Iteration 1924 => Loss: 6.85777067391138750452\n",
      "Iteration 1925 => Loss: 6.85774916876996876169\n",
      "Iteration 1926 => Loss: 6.85772766652637066898\n",
      "Iteration 1927 => Loss: 6.85770616718019976332\n",
      "Iteration 1928 => Loss: 6.85768467073106524623\n",
      "Iteration 1929 => Loss: 6.85766317717857898373\n",
      "Iteration 1930 => Loss: 6.85764168652234840096\n",
      "Iteration 1931 => Loss: 6.85762019876199246937\n",
      "Iteration 1932 => Loss: 6.85759871389710795597\n",
      "Iteration 1933 => Loss: 6.85757723192731649675\n",
      "Iteration 1934 => Loss: 6.85755575285221308235\n",
      "Iteration 1935 => Loss: 6.85753427667142645419\n",
      "Iteration 1936 => Loss: 6.85751280338455160290\n",
      "Iteration 1937 => Loss: 6.85749133299120394724\n",
      "Iteration 1938 => Loss: 6.85746986549099624142\n",
      "Iteration 1939 => Loss: 6.85744840088352791696\n",
      "Iteration 1940 => Loss: 6.85742693916843037982\n",
      "Iteration 1941 => Loss: 6.85740548034528885069\n",
      "Iteration 1942 => Loss: 6.85738402441373207097\n",
      "Iteration 1943 => Loss: 6.85736257137336036038\n",
      "Iteration 1944 => Loss: 6.85734112122378913767\n",
      "Iteration 1945 => Loss: 6.85731967396463115705\n",
      "Iteration 1946 => Loss: 6.85729822959548673822\n",
      "Iteration 1947 => Loss: 6.85727678811597574082\n",
      "Iteration 1948 => Loss: 6.85725534952570381364\n",
      "Iteration 1949 => Loss: 6.85723391382428548724\n",
      "Iteration 1950 => Loss: 6.85721248101132818675\n",
      "Iteration 1951 => Loss: 6.85719105108643933733\n",
      "Iteration 1952 => Loss: 6.85716962404924235130\n",
      "Iteration 1953 => Loss: 6.85714819989933932476\n",
      "Iteration 1954 => Loss: 6.85712677863634212372\n",
      "Iteration 1955 => Loss: 6.85710536025985639697\n",
      "Iteration 1956 => Loss: 6.85708394476950289231\n",
      "Iteration 1957 => Loss: 6.85706253216488814672\n",
      "Iteration 1958 => Loss: 6.85704112244562224987\n",
      "Iteration 1959 => Loss: 6.85701971561132150867\n",
      "Iteration 1960 => Loss: 6.85699831166158535467\n",
      "Iteration 1961 => Loss: 6.85697691059604252928\n",
      "Iteration 1962 => Loss: 6.85695551241429246403\n",
      "Iteration 1963 => Loss: 6.85693411711594524860\n",
      "Iteration 1964 => Loss: 6.85691272470061630173\n",
      "Iteration 1965 => Loss: 6.85689133516792015399\n",
      "Iteration 1966 => Loss: 6.85686994851746600688\n",
      "Iteration 1967 => Loss: 6.85684856474886395006\n",
      "Iteration 1968 => Loss: 6.85682718386172140868\n",
      "Iteration 1969 => Loss: 6.85680580585565913054\n",
      "Iteration 1970 => Loss: 6.85678443073028454080\n",
      "Iteration 1971 => Loss: 6.85676305848521217001\n",
      "Iteration 1972 => Loss: 6.85674168912004766696\n",
      "Iteration 1973 => Loss: 6.85672032263440822675\n",
      "Iteration 1974 => Loss: 6.85669895902790749176\n",
      "Iteration 1975 => Loss: 6.85667759830015199896\n",
      "Iteration 1976 => Loss: 6.85665624045075272619\n",
      "Iteration 1977 => Loss: 6.85663488547932509221\n",
      "Iteration 1978 => Loss: 6.85661353338548540393\n",
      "Iteration 1979 => Loss: 6.85659218416884552738\n",
      "Iteration 1980 => Loss: 6.85657083782900933500\n",
      "Iteration 1981 => Loss: 6.85654949436559491005\n",
      "Iteration 1982 => Loss: 6.85652815377820967768\n",
      "Iteration 1983 => Loss: 6.85650681606647616206\n",
      "Iteration 1984 => Loss: 6.85648548122999823562\n",
      "Iteration 1985 => Loss: 6.85646414926839309345\n",
      "Iteration 1986 => Loss: 6.85644282018126816070\n",
      "Iteration 1987 => Loss: 6.85642149396824418517\n",
      "Iteration 1988 => Loss: 6.85640017062892592747\n",
      "Iteration 1989 => Loss: 6.85637885016293147089\n",
      "Iteration 1990 => Loss: 6.85635753256986379967\n",
      "Iteration 1991 => Loss: 6.85633621784935609611\n",
      "Iteration 1992 => Loss: 6.85631490600099535726\n",
      "Iteration 1993 => Loss: 6.85629359702442009450\n",
      "Iteration 1994 => Loss: 6.85627229091922529847\n",
      "Iteration 1995 => Loss: 6.85625098768502727609\n",
      "Iteration 1996 => Loss: 6.85622968732144233428\n",
      "Iteration 1997 => Loss: 6.85620838982808589179\n",
      "Iteration 1998 => Loss: 6.85618709520456981465\n",
      "Iteration 1999 => Loss: 6.85616580345050330436\n",
      "Iteration 2000 => Loss: 6.85614451456550533237\n",
      "Iteration 2001 => Loss: 6.85612322854917710657\n",
      "Iteration 2002 => Loss: 6.85610194540115180928\n",
      "Iteration 2003 => Loss: 6.85608066512102709567\n",
      "Iteration 2004 => Loss: 6.85605938770842371355\n",
      "Iteration 2005 => Loss: 6.85603811316294997624\n",
      "Iteration 2006 => Loss: 6.85601684148422929610\n",
      "Iteration 2007 => Loss: 6.85599557267186199283\n",
      "Iteration 2008 => Loss: 6.85597430672547591968\n",
      "Iteration 2009 => Loss: 6.85595304364467139635\n",
      "Iteration 2010 => Loss: 6.85593178342907272338\n",
      "Iteration 2011 => Loss: 6.85591052607828732590\n",
      "Iteration 2012 => Loss: 6.85588927159193417538\n",
      "Iteration 2013 => Loss: 6.85586801996962513783\n",
      "Iteration 2014 => Loss: 6.85584677121096941477\n",
      "Iteration 2015 => Loss: 6.85582552531558953035\n",
      "Iteration 2016 => Loss: 6.85580428228309646244\n",
      "Iteration 2017 => Loss: 6.85578304211310030070\n",
      "Iteration 2018 => Loss: 6.85576180480522090477\n",
      "Iteration 2019 => Loss: 6.85574057035906481161\n",
      "Iteration 2020 => Loss: 6.85571933877425632176\n",
      "Iteration 2021 => Loss: 6.85569811005040374852\n",
      "Iteration 2022 => Loss: 6.85567688418712872789\n",
      "Iteration 2023 => Loss: 6.85565566118404046136\n",
      "Iteration 2024 => Loss: 6.85563444104074815044\n",
      "Iteration 2025 => Loss: 6.85561322375687698383\n",
      "Iteration 2026 => Loss: 6.85559200933202728123\n",
      "Iteration 2027 => Loss: 6.85557079776583044861\n",
      "Iteration 2028 => Loss: 6.85554958905788858203\n",
      "Iteration 2029 => Loss: 6.85552838320783397563\n",
      "Iteration 2030 => Loss: 6.85550718021525806734\n",
      "Iteration 2031 => Loss: 6.85548598007978871038\n",
      "Iteration 2032 => Loss: 6.85546478280104221170\n",
      "Iteration 2033 => Loss: 6.85544358837862599643\n",
      "Iteration 2034 => Loss: 6.85542239681216081237\n",
      "Iteration 2035 => Loss: 6.85540120810125941375\n",
      "Iteration 2036 => Loss: 6.85538002224554965380\n",
      "Iteration 2037 => Loss: 6.85535883924462474681\n",
      "Iteration 2038 => Loss: 6.85533765909811521055\n",
      "Iteration 2039 => Loss: 6.85531648180563024653\n",
      "Iteration 2040 => Loss: 6.85529530736678527347\n",
      "Iteration 2041 => Loss: 6.85527413578119748649\n",
      "Iteration 2042 => Loss: 6.85525296704849118612\n",
      "Iteration 2043 => Loss: 6.85523180116826935659\n",
      "Iteration 2044 => Loss: 6.85521063814015185756\n",
      "Iteration 2045 => Loss: 6.85518947796374877868\n",
      "Iteration 2046 => Loss: 6.85516832063868353231\n",
      "Iteration 2047 => Loss: 6.85514716616456443177\n",
      "Iteration 2048 => Loss: 6.85512601454102465937\n",
      "Iteration 2049 => Loss: 6.85510486576765654121\n",
      "Iteration 2050 => Loss: 6.85508371984409325961\n",
      "Iteration 2051 => Loss: 6.85506257676993868699\n",
      "Iteration 2052 => Loss: 6.85504143654482334114\n",
      "Iteration 2053 => Loss: 6.85502029916835020629\n",
      "Iteration 2054 => Loss: 6.85499916464014180661\n",
      "Iteration 2055 => Loss: 6.85497803295981267269\n",
      "Iteration 2056 => Loss: 6.85495690412697911142\n",
      "Iteration 2057 => Loss: 6.85493577814125298886\n",
      "Iteration 2058 => Loss: 6.85491465500226127006\n",
      "Iteration 2059 => Loss: 6.85489353470960605108\n",
      "Iteration 2060 => Loss: 6.85487241726292317878\n",
      "Iteration 2061 => Loss: 6.85485130266180764380\n",
      "Iteration 2062 => Loss: 6.85483019090589618116\n",
      "Iteration 2063 => Loss: 6.85480908199478644605\n",
      "Iteration 2064 => Loss: 6.85478797592810984440\n",
      "Iteration 2065 => Loss: 6.85476687270547824227\n",
      "Iteration 2066 => Loss: 6.85474577232650528202\n",
      "Iteration 2067 => Loss: 6.85472467479080727060\n",
      "Iteration 2068 => Loss: 6.85470358009800584398\n",
      "Iteration 2069 => Loss: 6.85468248824771642091\n",
      "Iteration 2070 => Loss: 6.85466139923955442015\n",
      "Iteration 2071 => Loss: 6.85464031307313881314\n",
      "Iteration 2072 => Loss: 6.85461922974808590681\n",
      "Iteration 2073 => Loss: 6.85459814926401733715\n",
      "Iteration 2074 => Loss: 6.85457707162053697658\n",
      "Iteration 2075 => Loss: 6.85455599681727356653\n",
      "Iteration 2076 => Loss: 6.85453492485384252575\n",
      "Iteration 2077 => Loss: 6.85451385572985838479\n",
      "Iteration 2078 => Loss: 6.85449278944494455601\n",
      "Iteration 2079 => Loss: 6.85447172599871024090\n",
      "Iteration 2080 => Loss: 6.85445066539077973999\n",
      "Iteration 2081 => Loss: 6.85442960762076314296\n",
      "Iteration 2082 => Loss: 6.85440855268829007940\n",
      "Iteration 2083 => Loss: 6.85438750059296353356\n",
      "Iteration 2084 => Loss: 6.85436645133441491140\n",
      "Iteration 2085 => Loss: 6.85434540491225341441\n",
      "Iteration 2086 => Loss: 6.85432436132609801405\n",
      "Iteration 2087 => Loss: 6.85430332057556856995\n",
      "Iteration 2088 => Loss: 6.85428228266028227722\n",
      "Iteration 2089 => Loss: 6.85426124757985633096\n",
      "Iteration 2090 => Loss: 6.85424021533390348537\n",
      "Iteration 2091 => Loss: 6.85421918592205070553\n",
      "Iteration 2092 => Loss: 6.85419815934391518653\n",
      "Iteration 2093 => Loss: 6.85417713559911145893\n",
      "Iteration 2094 => Loss: 6.85415611468725494149\n",
      "Iteration 2095 => Loss: 6.85413509660797526379\n",
      "Iteration 2096 => Loss: 6.85411408136088162735\n",
      "Iteration 2097 => Loss: 6.85409306894559389178\n",
      "Iteration 2098 => Loss: 6.85407205936172658767\n",
      "Iteration 2099 => Loss: 6.85405105260890668006\n",
      "Iteration 2100 => Loss: 6.85403004868674781136\n",
      "Iteration 2101 => Loss: 6.85400904759486806483\n",
      "Iteration 2102 => Loss: 6.85398804933288641195\n",
      "Iteration 2103 => Loss: 6.85396705390042537687\n",
      "Iteration 2104 => Loss: 6.85394606129709682563\n",
      "Iteration 2105 => Loss: 6.85392507152252594693\n",
      "Iteration 2106 => Loss: 6.85390408457632638317\n",
      "Iteration 2107 => Loss: 6.85388310045812065852\n",
      "Iteration 2108 => Loss: 6.85386211916752863260\n",
      "Iteration 2109 => Loss: 6.85384114070416572417\n",
      "Iteration 2110 => Loss: 6.85382016506764646380\n",
      "Iteration 2111 => Loss: 6.85379919225760492196\n",
      "Iteration 2112 => Loss: 6.85377822227364674745\n",
      "Iteration 2113 => Loss: 6.85375725511539357626\n",
      "Iteration 2114 => Loss: 6.85373629078247414981\n",
      "Iteration 2115 => Loss: 6.85371532927448967598\n",
      "Iteration 2116 => Loss: 6.85369437059108133070\n",
      "Iteration 2117 => Loss: 6.85367341473184854550\n",
      "Iteration 2118 => Loss: 6.85365246169642450269\n",
      "Iteration 2119 => Loss: 6.85363151148441751559\n",
      "Iteration 2120 => Loss: 6.85361056409545810197\n",
      "Iteration 2121 => Loss: 6.85358961952916079241\n",
      "Iteration 2122 => Loss: 6.85356867778514633471\n",
      "Iteration 2123 => Loss: 6.85354773886302748309\n",
      "Iteration 2124 => Loss: 6.85352680276244008439\n",
      "Iteration 2125 => Loss: 6.85350586948298801104\n",
      "Iteration 2126 => Loss: 6.85348493902429556357\n",
      "Iteration 2127 => Loss: 6.85346401138598437797\n",
      "Iteration 2128 => Loss: 6.85344308656767431387\n",
      "Iteration 2129 => Loss: 6.85342216456899144816\n",
      "Iteration 2130 => Loss: 6.85340124538954675870\n",
      "Iteration 2131 => Loss: 6.85338032902895744058\n",
      "Iteration 2132 => Loss: 6.85335941548685045888\n",
      "Iteration 2133 => Loss: 6.85333850476284833775\n",
      "Iteration 2134 => Loss: 6.85331759685656560777\n",
      "Iteration 2135 => Loss: 6.85329669176762390492\n",
      "Iteration 2136 => Loss: 6.85327578949564664157\n",
      "Iteration 2137 => Loss: 6.85325489004025278916\n",
      "Iteration 2138 => Loss: 6.85323399340106309552\n",
      "Iteration 2139 => Loss: 6.85321309957769120302\n",
      "Iteration 2140 => Loss: 6.85319220856976940581\n",
      "Iteration 2141 => Loss: 6.85317132037691401081\n",
      "Iteration 2142 => Loss: 6.85315043499874310129\n",
      "Iteration 2143 => Loss: 6.85312955243488364232\n",
      "Iteration 2144 => Loss: 6.85310867268493328908\n",
      "Iteration 2145 => Loss: 6.85308779574853943473\n",
      "Iteration 2146 => Loss: 6.85306692162531927437\n",
      "Iteration 2147 => Loss: 6.85304605031488733857\n",
      "Iteration 2148 => Loss: 6.85302518181686703969\n",
      "Iteration 2149 => Loss: 6.85300431613087823735\n",
      "Iteration 2150 => Loss: 6.85298345325654079119\n",
      "Iteration 2151 => Loss: 6.85296259319347278449\n",
      "Iteration 2152 => Loss: 6.85294173594130473504\n",
      "Iteration 2153 => Loss: 6.85292088149965561428\n",
      "Iteration 2154 => Loss: 6.85290002986814172914\n",
      "Iteration 2155 => Loss: 6.85287918104638560379\n",
      "Iteration 2156 => Loss: 6.85285833503400798605\n",
      "Iteration 2157 => Loss: 6.85283749183064117005\n",
      "Iteration 2158 => Loss: 6.85281665143588547551\n",
      "Iteration 2159 => Loss: 6.85279581384938296651\n",
      "Iteration 2160 => Loss: 6.85277497907074550909\n",
      "Iteration 2161 => Loss: 6.85275414709959296289\n",
      "Iteration 2162 => Loss: 6.85273331793555318114\n",
      "Iteration 2163 => Loss: 6.85271249157823980624\n",
      "Iteration 2164 => Loss: 6.85269166802728602050\n",
      "Iteration 2165 => Loss: 6.85267084728230368995\n",
      "Iteration 2166 => Loss: 6.85265002934292333236\n",
      "Iteration 2167 => Loss: 6.85262921420875681378\n",
      "Iteration 2168 => Loss: 6.85260840187943287560\n",
      "Iteration 2169 => Loss: 6.85258759235456960113\n",
      "Iteration 2170 => Loss: 6.85256678563379484359\n",
      "Iteration 2171 => Loss: 6.85254598171672490992\n",
      "Iteration 2172 => Loss: 6.85252518060298321245\n",
      "Iteration 2173 => Loss: 6.85250438229220115716\n",
      "Iteration 2174 => Loss: 6.85248358678398616917\n",
      "Iteration 2175 => Loss: 6.85246279407796610172\n",
      "Iteration 2176 => Loss: 6.85244200417376969625\n",
      "Iteration 2177 => Loss: 6.85242121707101148331\n",
      "Iteration 2178 => Loss: 6.85240043276931842797\n",
      "Iteration 2179 => Loss: 6.85237965126831038987\n",
      "Iteration 2180 => Loss: 6.85235887256760989317\n",
      "Iteration 2181 => Loss: 6.85233809666684479112\n",
      "Iteration 2182 => Loss: 6.85231732356563494335\n",
      "Iteration 2183 => Loss: 6.85229655326360020950\n",
      "Iteration 2184 => Loss: 6.85227578576035867286\n",
      "Iteration 2185 => Loss: 6.85225502105554706844\n",
      "Iteration 2186 => Loss: 6.85223425914878436771\n",
      "Iteration 2187 => Loss: 6.85221350003968332487\n",
      "Iteration 2188 => Loss: 6.85219274372787090499\n",
      "Iteration 2189 => Loss: 6.85217199021298117856\n",
      "Iteration 2190 => Loss: 6.85215123949462601161\n",
      "Iteration 2191 => Loss: 6.85213049157243236920\n",
      "Iteration 2192 => Loss: 6.85210974644602099914\n",
      "Iteration 2193 => Loss: 6.85208900411502153105\n",
      "Iteration 2194 => Loss: 6.85206826457905382455\n",
      "Iteration 2195 => Loss: 6.85204752783773418656\n",
      "Iteration 2196 => Loss: 6.85202679389068869398\n",
      "Iteration 2197 => Loss: 6.85200606273754786457\n",
      "Iteration 2198 => Loss: 6.85198533437793422252\n",
      "Iteration 2199 => Loss: 6.85196460881147118016\n",
      "Iteration 2200 => Loss: 6.85194388603777682079\n",
      "Iteration 2201 => Loss: 6.85192316605647899763\n",
      "Iteration 2202 => Loss: 6.85190244886719401762\n",
      "Iteration 2203 => Loss: 6.85188173446955861579\n",
      "Iteration 2204 => Loss: 6.85186102286318554633\n",
      "Iteration 2205 => Loss: 6.85184031404770799156\n",
      "Iteration 2206 => Loss: 6.85181960802273959388\n",
      "Iteration 2207 => Loss: 6.85179890478791442376\n",
      "Iteration 2208 => Loss: 6.85177820434284967632\n",
      "Iteration 2209 => Loss: 6.85175750668716787573\n",
      "Iteration 2210 => Loss: 6.85173681182050309246\n",
      "Iteration 2211 => Loss: 6.85171611974246808074\n",
      "Iteration 2212 => Loss: 6.85169543045269602288\n",
      "Iteration 2213 => Loss: 6.85167474395080500216\n",
      "Iteration 2214 => Loss: 6.85165406023642109545\n",
      "Iteration 2215 => Loss: 6.85163337930916949148\n",
      "Iteration 2216 => Loss: 6.85161270116867893165\n",
      "Iteration 2217 => Loss: 6.85159202581456572290\n",
      "Iteration 2218 => Loss: 6.85157135324646482388\n",
      "Iteration 2219 => Loss: 6.85155068346398277157\n",
      "Iteration 2220 => Loss: 6.85153001646675807734\n",
      "Iteration 2221 => Loss: 6.85150935225441415355\n",
      "Iteration 2222 => Loss: 6.85148869082657707708\n",
      "Iteration 2223 => Loss: 6.85146803218286937209\n",
      "Iteration 2224 => Loss: 6.85144737632291267460\n",
      "Iteration 2225 => Loss: 6.85142672324633750236\n",
      "Iteration 2226 => Loss: 6.85140607295276993227\n",
      "Iteration 2227 => Loss: 6.85138542544181827765\n",
      "Iteration 2228 => Loss: 6.85136478071313259619\n",
      "Iteration 2229 => Loss: 6.85134413876631676033\n",
      "Iteration 2230 => Loss: 6.85132349960100750508\n",
      "Iteration 2231 => Loss: 6.85130286321683001916\n",
      "Iteration 2232 => Loss: 6.85128222961339972130\n",
      "Iteration 2233 => Loss: 6.85126159879036222833\n",
      "Iteration 2234 => Loss: 6.85124097074731697177\n",
      "Iteration 2235 => Loss: 6.85122034548391134479\n",
      "Iteration 2236 => Loss: 6.85119972299976165431\n",
      "Iteration 2237 => Loss: 6.85117910329448154272\n",
      "Iteration 2238 => Loss: 6.85115848636772373226\n",
      "Iteration 2239 => Loss: 6.85113787221908587810\n",
      "Iteration 2240 => Loss: 6.85111726084821714977\n",
      "Iteration 2241 => Loss: 6.85109665225472230787\n",
      "Iteration 2242 => Loss: 6.85107604643823808743\n",
      "Iteration 2243 => Loss: 6.85105544339839145351\n",
      "Iteration 2244 => Loss: 6.85103484313481292389\n",
      "Iteration 2245 => Loss: 6.85101424564711170007\n",
      "Iteration 2246 => Loss: 6.85099365093493073431\n",
      "Iteration 2247 => Loss: 6.85097305899788633354\n",
      "Iteration 2248 => Loss: 6.85095246983560191012\n",
      "Iteration 2249 => Loss: 6.85093188344771686360\n",
      "Iteration 2250 => Loss: 6.85091129983384217184\n",
      "Iteration 2251 => Loss: 6.85089071899361545803\n",
      "Iteration 2252 => Loss: 6.85087014092665302911\n",
      "Iteration 2253 => Loss: 6.85084956563259694917\n",
      "Iteration 2254 => Loss: 6.85082899311105553153\n",
      "Iteration 2255 => Loss: 6.85080842336166906392\n",
      "Iteration 2256 => Loss: 6.85078785638405030056\n",
      "Iteration 2257 => Loss: 6.85076729217783331194\n",
      "Iteration 2258 => Loss: 6.85074673074265216854\n",
      "Iteration 2259 => Loss: 6.85072617207811962459\n",
      "Iteration 2260 => Loss: 6.85070561618387419145\n",
      "Iteration 2261 => Loss: 6.85068506305953039970\n",
      "Iteration 2262 => Loss: 6.85066451270472498436\n",
      "Iteration 2263 => Loss: 6.85064396511908491050\n",
      "Iteration 2264 => Loss: 6.85062342030222914957\n",
      "Iteration 2265 => Loss: 6.85060287825378999571\n",
      "Iteration 2266 => Loss: 6.85058233897339263763\n",
      "Iteration 2267 => Loss: 6.85056180246066581674\n",
      "Iteration 2268 => Loss: 6.85054126871524093900\n",
      "Iteration 2269 => Loss: 6.85052073773673075863\n",
      "Iteration 2270 => Loss: 6.85050020952477289882\n",
      "Iteration 2271 => Loss: 6.85047968407899610099\n",
      "Iteration 2272 => Loss: 6.85045916139902910658\n",
      "Iteration 2273 => Loss: 6.85043864148448555795\n",
      "Iteration 2274 => Loss: 6.85041812433500663104\n",
      "Iteration 2275 => Loss: 6.85039760995022284362\n",
      "Iteration 2276 => Loss: 6.85037709832974694990\n",
      "Iteration 2277 => Loss: 6.85035658947320946766\n",
      "Iteration 2278 => Loss: 6.85033608338024446738\n",
      "Iteration 2279 => Loss: 6.85031558005048069049\n",
      "Iteration 2280 => Loss: 6.85029507948354243752\n",
      "Iteration 2281 => Loss: 6.85027458167905134445\n",
      "Iteration 2282 => Loss: 6.85025408663664947539\n",
      "Iteration 2283 => Loss: 6.85023359435594780820\n",
      "Iteration 2284 => Loss: 6.85021310483658840695\n",
      "Iteration 2285 => Loss: 6.85019261807818580223\n",
      "Iteration 2286 => Loss: 6.85017213408038294631\n",
      "Iteration 2287 => Loss: 6.85015165284280058700\n",
      "Iteration 2288 => Loss: 6.85013117436506302482\n",
      "Iteration 2289 => Loss: 6.85011069864679988939\n",
      "Iteration 2290 => Loss: 6.85009022568764258665\n",
      "Iteration 2291 => Loss: 6.85006975548722163438\n",
      "Iteration 2292 => Loss: 6.85004928804516222129\n",
      "Iteration 2293 => Loss: 6.85002882336108509520\n",
      "Iteration 2294 => Loss: 6.85000836143463143202\n",
      "Iteration 2295 => Loss: 6.84998790226542997317\n",
      "Iteration 2296 => Loss: 6.84996744585309258468\n",
      "Iteration 2297 => Loss: 6.84994699219725955430\n",
      "Iteration 2298 => Loss: 6.84992654129756228798\n",
      "Iteration 2299 => Loss: 6.84990609315362508625\n",
      "Iteration 2300 => Loss: 6.84988564776507846688\n",
      "Iteration 2301 => Loss: 6.84986520513154584222\n",
      "Iteration 2302 => Loss: 6.84984476525266039459\n",
      "Iteration 2303 => Loss: 6.84982432812804997724\n",
      "Iteration 2304 => Loss: 6.84980389375734155522\n",
      "Iteration 2305 => Loss: 6.84978346214016919902\n",
      "Iteration 2306 => Loss: 6.84976303327615987371\n",
      "Iteration 2307 => Loss: 6.84974260716494054435\n",
      "Iteration 2308 => Loss: 6.84972218380614528144\n",
      "Iteration 2309 => Loss: 6.84970176319938950371\n",
      "Iteration 2310 => Loss: 6.84968134534431705163\n",
      "Iteration 2311 => Loss: 6.84966093024055400207\n",
      "Iteration 2312 => Loss: 6.84964051788772376739\n",
      "Iteration 2313 => Loss: 6.84962010828546308261\n",
      "Iteration 2314 => Loss: 6.84959970143340157733\n",
      "Iteration 2315 => Loss: 6.84957929733115289395\n",
      "Iteration 2316 => Loss: 6.84955889597837241922\n",
      "Iteration 2317 => Loss: 6.84953849737466402559\n",
      "Iteration 2318 => Loss: 6.84951810151967421803\n",
      "Iteration 2319 => Loss: 6.84949770841303084978\n",
      "Iteration 2320 => Loss: 6.84947731805435999775\n",
      "Iteration 2321 => Loss: 6.84945693044328507426\n",
      "Iteration 2322 => Loss: 6.84943654557944814343\n",
      "Iteration 2323 => Loss: 6.84941616346246817670\n",
      "Iteration 2324 => Loss: 6.84939578409198279729\n",
      "Iteration 2325 => Loss: 6.84937540746762252297\n",
      "Iteration 2326 => Loss: 6.84935503358901076609\n",
      "Iteration 2327 => Loss: 6.84933466245577893261\n",
      "Iteration 2328 => Loss: 6.84931429406755931666\n",
      "Iteration 2329 => Loss: 6.84929392842398510055\n",
      "Iteration 2330 => Loss: 6.84927356552467703210\n",
      "Iteration 2331 => Loss: 6.84925320536928072812\n",
      "Iteration 2332 => Loss: 6.84923284795741249553\n",
      "Iteration 2333 => Loss: 6.84921249328870107576\n",
      "Iteration 2334 => Loss: 6.84919214136278764471\n",
      "Iteration 2335 => Loss: 6.84917179217929916746\n",
      "Iteration 2336 => Loss: 6.84915144573785461546\n",
      "Iteration 2337 => Loss: 6.84913110203810848731\n",
      "Iteration 2338 => Loss: 6.84911076107967797810\n",
      "Iteration 2339 => Loss: 6.84909042286218472384\n",
      "Iteration 2340 => Loss: 6.84907008738526901226\n",
      "Iteration 2341 => Loss: 6.84904975464856669021\n",
      "Iteration 2342 => Loss: 6.84902942465169406461\n",
      "Iteration 2343 => Loss: 6.84900909739429586409\n",
      "Iteration 2344 => Loss: 6.84898877287599283648\n",
      "Iteration 2345 => Loss: 6.84896845109642349314\n",
      "Iteration 2346 => Loss: 6.84894813205521391097\n",
      "Iteration 2347 => Loss: 6.84892781575199727229\n",
      "Iteration 2348 => Loss: 6.84890750218640587121\n",
      "Iteration 2349 => Loss: 6.84888719135806578464\n",
      "Iteration 2350 => Loss: 6.84886688326661552395\n",
      "Iteration 2351 => Loss: 6.84884657791168027785\n",
      "Iteration 2352 => Loss: 6.84882627529288878776\n",
      "Iteration 2353 => Loss: 6.84880597540988222960\n",
      "Iteration 2354 => Loss: 6.84878567826228046300\n",
      "Iteration 2355 => Loss: 6.84876538384972999296\n",
      "Iteration 2356 => Loss: 6.84874509217184890275\n",
      "Iteration 2357 => Loss: 6.84872480322826771015\n",
      "Iteration 2358 => Loss: 6.84870451701862403837\n",
      "Iteration 2359 => Loss: 6.84868423354255551061\n",
      "Iteration 2360 => Loss: 6.84866395279968820375\n",
      "Iteration 2361 => Loss: 6.84864367478965085922\n",
      "Iteration 2362 => Loss: 6.84862339951207044209\n",
      "Iteration 2363 => Loss: 6.84860312696659434550\n",
      "Iteration 2364 => Loss: 6.84858285715283532369\n",
      "Iteration 2365 => Loss: 6.84856259007044343434\n",
      "Iteration 2366 => Loss: 6.84854232571903498439\n",
      "Iteration 2367 => Loss: 6.84852206409825381428\n",
      "Iteration 2368 => Loss: 6.84850180520773132997\n",
      "Iteration 2369 => Loss: 6.84848154904708916746\n",
      "Iteration 2370 => Loss: 6.84846129561597116719\n",
      "Iteration 2371 => Loss: 6.84844104491400518242\n",
      "Iteration 2372 => Loss: 6.84842079694081640184\n",
      "Iteration 2373 => Loss: 6.84840055169604600138\n",
      "Iteration 2374 => Loss: 6.84838030917933338060\n",
      "Iteration 2375 => Loss: 6.84836006939028774099\n",
      "Iteration 2376 => Loss: 6.84833983232856002843\n",
      "Iteration 2377 => Loss: 6.84831959799378076070\n",
      "Iteration 2378 => Loss: 6.84829936638558312012\n",
      "Iteration 2379 => Loss: 6.84827913750359051903\n",
      "Iteration 2380 => Loss: 6.84825891134743880428\n",
      "Iteration 2381 => Loss: 6.84823868791677181633\n",
      "Iteration 2382 => Loss: 6.84821846721120497392\n",
      "Iteration 2383 => Loss: 6.84819824923038211750\n",
      "Iteration 2384 => Loss: 6.84817803397393465303\n",
      "Iteration 2385 => Loss: 6.84815782144149132193\n",
      "Iteration 2386 => Loss: 6.84813761163269685284\n",
      "Iteration 2387 => Loss: 6.84811740454716488813\n",
      "Iteration 2388 => Loss: 6.84809720018454459733\n",
      "Iteration 2389 => Loss: 6.84807699854446916277\n",
      "Iteration 2390 => Loss: 6.84805679962655844406\n",
      "Iteration 2391 => Loss: 6.84803660343045628167\n",
      "Iteration 2392 => Loss: 6.84801640995579319338\n",
      "Iteration 2393 => Loss: 6.84799621920220236149\n",
      "Iteration 2394 => Loss: 6.84797603116931430378\n",
      "Iteration 2395 => Loss: 6.84795584585676575529\n",
      "Iteration 2396 => Loss: 6.84793566326419345103\n",
      "Iteration 2397 => Loss: 6.84791548339122257971\n",
      "Iteration 2398 => Loss: 6.84789530623750497540\n",
      "Iteration 2399 => Loss: 6.84787513180264362234\n",
      "Iteration 2400 => Loss: 6.84785496008629834819\n",
      "Iteration 2401 => Loss: 6.84783479108808368352\n",
      "Iteration 2402 => Loss: 6.84781462480765235057\n",
      "Iteration 2403 => Loss: 6.84779446124463220258\n",
      "Iteration 2404 => Loss: 6.84777430039864221101\n",
      "Iteration 2405 => Loss: 6.84775414226933420991\n",
      "Iteration 2406 => Loss: 6.84773398685633516436\n",
      "Iteration 2407 => Loss: 6.84771383415927559213\n",
      "Iteration 2408 => Loss: 6.84769368417779400460\n",
      "Iteration 2409 => Loss: 6.84767353691153424222\n",
      "Iteration 2410 => Loss: 6.84765339236010994739\n",
      "Iteration 2411 => Loss: 6.84763325052316496055\n",
      "Iteration 2412 => Loss: 6.84761311140034134581\n",
      "Iteration 2413 => Loss: 6.84759297499126073916\n",
      "Iteration 2414 => Loss: 6.84757284129556698105\n",
      "Iteration 2415 => Loss: 6.84755271031288437200\n",
      "Iteration 2416 => Loss: 6.84753258204286030519\n",
      "Iteration 2417 => Loss: 6.84751245648511641662\n",
      "Iteration 2418 => Loss: 6.84749233363929032947\n",
      "Iteration 2419 => Loss: 6.84747221350502499604\n",
      "Iteration 2420 => Loss: 6.84745209608195004591\n",
      "Iteration 2421 => Loss: 6.84743198136969510870\n",
      "Iteration 2422 => Loss: 6.84741186936791113027\n",
      "Iteration 2423 => Loss: 6.84739176007620553577\n",
      "Iteration 2424 => Loss: 6.84737165349423371197\n",
      "Iteration 2425 => Loss: 6.84735154962163150572\n",
      "Iteration 2426 => Loss: 6.84733144845801167122\n",
      "Iteration 2427 => Loss: 6.84731135000304114158\n",
      "Iteration 2428 => Loss: 6.84729125425633444735\n",
      "Iteration 2429 => Loss: 6.84727116121752832356\n",
      "Iteration 2430 => Loss: 6.84725107088625950524\n",
      "Iteration 2431 => Loss: 6.84723098326217893828\n",
      "Iteration 2432 => Loss: 6.84721089834489404780\n",
      "Iteration 2433 => Loss: 6.84719081613405400333\n",
      "Iteration 2434 => Loss: 6.84717073662929376354\n",
      "Iteration 2435 => Loss: 6.84715065983024828711\n",
      "Iteration 2436 => Loss: 6.84713058573655963812\n",
      "Iteration 2437 => Loss: 6.84711051434784767622\n",
      "Iteration 2438 => Loss: 6.84709044566376423546\n",
      "Iteration 2439 => Loss: 6.84707037968392384641\n",
      "Iteration 2440 => Loss: 6.84705031640799166581\n",
      "Iteration 2441 => Loss: 6.84703025583557778333\n",
      "Iteration 2442 => Loss: 6.84701019796633225667\n",
      "Iteration 2443 => Loss: 6.84699014279988826814\n",
      "Iteration 2444 => Loss: 6.84697009033587722371\n",
      "Iteration 2445 => Loss: 6.84695004057393497021\n",
      "Iteration 2446 => Loss: 6.84692999351370534811\n",
      "Iteration 2447 => Loss: 6.84690994915480910521\n",
      "Iteration 2448 => Loss: 6.84688990749690162829\n",
      "Iteration 2449 => Loss: 6.84686986853960810606\n",
      "Iteration 2450 => Loss: 6.84684983228256172083\n",
      "Iteration 2451 => Loss: 6.84682979872539831945\n",
      "Iteration 2452 => Loss: 6.84680976786775996601\n",
      "Iteration 2453 => Loss: 6.84678973970929050097\n",
      "Iteration 2454 => Loss: 6.84676971424961156032\n",
      "Iteration 2455 => Loss: 6.84674969148836076727\n",
      "Iteration 2456 => Loss: 6.84672967142518285044\n",
      "Iteration 2457 => Loss: 6.84670965405971010398\n",
      "Iteration 2458 => Loss: 6.84668963939157837473\n",
      "Iteration 2459 => Loss: 6.84666962742042706225\n",
      "Iteration 2460 => Loss: 6.84664961814588579614\n",
      "Iteration 2461 => Loss: 6.84662961156759486414\n",
      "Iteration 2462 => Loss: 6.84660960768519544217\n",
      "Iteration 2463 => Loss: 6.84658960649832515344\n",
      "Iteration 2464 => Loss: 6.84656960800660918665\n",
      "Iteration 2465 => Loss: 6.84654961220969671132\n",
      "Iteration 2466 => Loss: 6.84652961910721469252\n",
      "Iteration 2467 => Loss: 6.84650962869880341799\n",
      "Iteration 2468 => Loss: 6.84648964098410495183\n",
      "Iteration 2469 => Loss: 6.84646965596275069998\n",
      "Iteration 2470 => Loss: 6.84644967363438095020\n",
      "Iteration 2471 => Loss: 6.84642969399863332569\n",
      "Iteration 2472 => Loss: 6.84640971705513923240\n",
      "Iteration 2473 => Loss: 6.84638974280354251079\n",
      "Iteration 2474 => Loss: 6.84636977124348167223\n",
      "Iteration 2475 => Loss: 6.84634980237458456998\n",
      "Iteration 2476 => Loss: 6.84632983619649504448\n",
      "Iteration 2477 => Loss: 6.84630987270884983076\n",
      "Iteration 2478 => Loss: 6.84628991191129010474\n",
      "Iteration 2479 => Loss: 6.84626995380344283149\n",
      "Iteration 2480 => Loss: 6.84624999838495362781\n",
      "Iteration 2481 => Loss: 6.84623004565546100508\n",
      "Iteration 2482 => Loss: 6.84621009561459548109\n",
      "Iteration 2483 => Loss: 6.84619014826200267265\n",
      "Iteration 2484 => Loss: 6.84617020359731931478\n",
      "Iteration 2485 => Loss: 6.84615026162017592526\n",
      "Iteration 2486 => Loss: 6.84613032233021634454\n",
      "Iteration 2487 => Loss: 6.84611038572708796579\n",
      "Iteration 2488 => Loss: 6.84609045181040887229\n",
      "Iteration 2489 => Loss: 6.84607052057982290449\n",
      "Iteration 2490 => Loss: 6.84605059203498633735\n",
      "Iteration 2491 => Loss: 6.84603066617551458961\n",
      "Iteration 2492 => Loss: 6.84601074300104706083\n",
      "Iteration 2493 => Loss: 6.84599082251123736143\n",
      "Iteration 2494 => Loss: 6.84597090470570623921\n",
      "Iteration 2495 => Loss: 6.84595098958411441004\n",
      "Iteration 2496 => Loss: 6.84593107714607462810\n",
      "Iteration 2497 => Loss: 6.84591116739124672108\n",
      "Iteration 2498 => Loss: 6.84589126031925676585\n",
      "Iteration 2499 => Loss: 6.84587135592974505016\n",
      "Iteration 2500 => Loss: 6.84585145422235097357\n",
      "Iteration 2501 => Loss: 6.84583155519671482381\n",
      "Iteration 2502 => Loss: 6.84581165885246978320\n",
      "Iteration 2503 => Loss: 6.84579176518925969219\n",
      "Iteration 2504 => Loss: 6.84577187420672750306\n",
      "Iteration 2505 => Loss: 6.84575198590450817449\n",
      "Iteration 2506 => Loss: 6.84573210028223133605\n",
      "Iteration 2507 => Loss: 6.84571221733954882183\n",
      "Iteration 2508 => Loss: 6.84569233707609114958\n",
      "Iteration 2509 => Loss: 6.84567245949150482431\n",
      "Iteration 2510 => Loss: 6.84565258458542214015\n",
      "Iteration 2511 => Loss: 6.84563271235748160848\n",
      "Iteration 2512 => Loss: 6.84561284280732529339\n",
      "Iteration 2513 => Loss: 6.84559297593459969988\n",
      "Iteration 2514 => Loss: 6.84557311173893268119\n",
      "Iteration 2515 => Loss: 6.84555325021996274870\n",
      "Iteration 2516 => Loss: 6.84553339137734084829\n",
      "Iteration 2517 => Loss: 6.84551353521070016228\n",
      "Iteration 2518 => Loss: 6.84549368171967564933\n",
      "Iteration 2519 => Loss: 6.84547383090390848537\n",
      "Iteration 2520 => Loss: 6.84545398276304162266\n",
      "Iteration 2521 => Loss: 6.84543413729671890167\n",
      "Iteration 2522 => Loss: 6.84541429450456728745\n",
      "Iteration 2523 => Loss: 6.84539445438623950224\n",
      "Iteration 2524 => Loss: 6.84537461694136784018\n",
      "Iteration 2525 => Loss: 6.84535478216958814812\n",
      "Iteration 2526 => Loss: 6.84533495007054959558\n",
      "Iteration 2527 => Loss: 6.84531512064388625305\n",
      "Iteration 2528 => Loss: 6.84529529388924462552\n",
      "Iteration 2529 => Loss: 6.84527546980625523076\n",
      "Iteration 2530 => Loss: 6.84525564839456190924\n",
      "Iteration 2531 => Loss: 6.84523582965380317233\n",
      "Iteration 2532 => Loss: 6.84521601358362641321\n",
      "Iteration 2533 => Loss: 6.84519620018366659053\n",
      "Iteration 2534 => Loss: 6.84517638945355955116\n",
      "Iteration 2535 => Loss: 6.84515658139295446460\n",
      "Iteration 2536 => Loss: 6.84513677600148362501\n",
      "Iteration 2537 => Loss: 6.84511697327879797825\n",
      "Iteration 2538 => Loss: 6.84509717322452715393\n",
      "Iteration 2539 => Loss: 6.84507737583830522254\n",
      "Iteration 2540 => Loss: 6.84505758111979467628\n",
      "Iteration 2541 => Loss: 6.84503778906862070386\n",
      "Iteration 2542 => Loss: 6.84501799968443247479\n",
      "Iteration 2543 => Loss: 6.84499821296685428962\n",
      "Iteration 2544 => Loss: 6.84497842891554508782\n",
      "Iteration 2545 => Loss: 6.84495864753013183446\n",
      "Iteration 2546 => Loss: 6.84493886881027258084\n",
      "Iteration 2547 => Loss: 6.84491909275558541026\n",
      "Iteration 2548 => Loss: 6.84489931936573459126\n",
      "Iteration 2549 => Loss: 6.84487954864034175984\n",
      "Iteration 2550 => Loss: 6.84485978057905786187\n",
      "Iteration 2551 => Loss: 6.84484001518152407328\n",
      "Iteration 2552 => Loss: 6.84482025244738068182\n",
      "Iteration 2553 => Loss: 6.84480049237626086978\n",
      "Iteration 2554 => Loss: 6.84478073496780847762\n",
      "Iteration 2555 => Loss: 6.84476098022167800394\n",
      "Iteration 2556 => Loss: 6.84474122813749552563\n",
      "Iteration 2557 => Loss: 6.84472147871491021220\n",
      "Iteration 2558 => Loss: 6.84470173195356235141\n",
      "Iteration 2559 => Loss: 6.84468198785309134280\n",
      "Iteration 2560 => Loss: 6.84466224641314457955\n",
      "Iteration 2561 => Loss: 6.84464250763334636218\n",
      "Iteration 2562 => Loss: 6.84462277151335829473\n",
      "Iteration 2563 => Loss: 6.84460303805281622402\n",
      "Iteration 2564 => Loss: 6.84458330725135244421\n",
      "Iteration 2565 => Loss: 6.84456357910861967753\n",
      "Iteration 2566 => Loss: 6.84454385362425110628\n",
      "Iteration 2567 => Loss: 6.84452413079789856454\n",
      "Iteration 2568 => Loss: 6.84450441062919434643\n",
      "Iteration 2569 => Loss: 6.84448469311779206237\n",
      "Iteration 2570 => Loss: 6.84446497826331778924\n",
      "Iteration 2571 => Loss: 6.84444526606542513747\n",
      "Iteration 2572 => Loss: 6.84442555652375528297\n",
      "Iteration 2573 => Loss: 6.84440584963794318440\n",
      "Iteration 2574 => Loss: 6.84438614540763357041\n",
      "Iteration 2575 => Loss: 6.84436644383247383416\n",
      "Iteration 2576 => Loss: 6.84434674491210071068\n",
      "Iteration 2577 => Loss: 6.84432704864616603402\n",
      "Iteration 2578 => Loss: 6.84430735503429765743\n",
      "Iteration 2579 => Loss: 6.84428766407614563860\n",
      "Iteration 2580 => Loss: 6.84426797577135470618\n",
      "Iteration 2581 => Loss: 6.84424829011956514790\n",
      "Iteration 2582 => Loss: 6.84422860712041103426\n",
      "Iteration 2583 => Loss: 6.84420892677355219291\n",
      "Iteration 2584 => Loss: 6.84418924907861470075\n",
      "Iteration 2585 => Loss: 6.84416957403524861547\n",
      "Iteration 2586 => Loss: 6.84414990164310399479\n",
      "Iteration 2587 => Loss: 6.84413023190180602739\n",
      "Iteration 2588 => Loss: 6.84411056481101187643\n",
      "Iteration 2589 => Loss: 6.84409090037036182963\n",
      "Iteration 2590 => Loss: 6.84407123857949173384\n",
      "Iteration 2591 => Loss: 6.84405157943805431131\n",
      "Iteration 2592 => Loss: 6.84403192294568096798\n",
      "Iteration 2593 => Loss: 6.84401226910202886700\n",
      "Iteration 2594 => Loss: 6.84399261790673119066\n",
      "Iteration 2595 => Loss: 6.84397296935943000307\n",
      "Iteration 2596 => Loss: 6.84395332345977713828\n",
      "Iteration 2597 => Loss: 6.84393368020740400226\n",
      "Iteration 2598 => Loss: 6.84391403960196953449\n",
      "Iteration 2599 => Loss: 6.84389440164310158821\n",
      "Iteration 2600 => Loss: 6.84387476633045466201\n",
      "Iteration 2601 => Loss: 6.84385513366366282639\n",
      "Iteration 2602 => Loss: 6.84383550364237791541\n",
      "Iteration 2603 => Loss: 6.84381587626623755227\n",
      "Iteration 2604 => Loss: 6.84379625153489268286\n",
      "Iteration 2605 => Loss: 6.84377662944797737765\n",
      "Iteration 2606 => Loss: 6.84375701000513902983\n",
      "Iteration 2607 => Loss: 6.84373739320602236802\n",
      "Iteration 2608 => Loss: 6.84371777905027833810\n",
      "Iteration 2609 => Loss: 6.84369816753753568150\n",
      "Iteration 2610 => Loss: 6.84367855866744978499\n",
      "Iteration 2611 => Loss: 6.84365895243965649541\n",
      "Iteration 2612 => Loss: 6.84363934885380853501\n",
      "Iteration 2613 => Loss: 6.84361974790953819792\n",
      "Iteration 2614 => Loss: 6.84360014960650175908\n",
      "Iteration 2615 => Loss: 6.84358055394433861807\n",
      "Iteration 2616 => Loss: 6.84356096092269172715\n",
      "Iteration 2617 => Loss: 6.84354137054120492678\n",
      "Iteration 2618 => Loss: 6.84352178279952205742\n",
      "Iteration 2619 => Loss: 6.84350219769729228858\n",
      "Iteration 2620 => Loss: 6.84348261523415501983\n",
      "Iteration 2621 => Loss: 6.84346303540975942070\n",
      "Iteration 2622 => Loss: 6.84344345822373956167\n",
      "Iteration 2623 => Loss: 6.84342388367575793495\n",
      "Iteration 2624 => Loss: 6.84340431176543972924\n",
      "Iteration 2625 => Loss: 6.84338474249244033132\n",
      "Iteration 2626 => Loss: 6.84336517585640091710\n",
      "Iteration 2627 => Loss: 6.84334561185697243246\n",
      "Iteration 2628 => Loss: 6.84332605049377828976\n",
      "Iteration 2629 => Loss: 6.84330649176649696841\n",
      "Iteration 2630 => Loss: 6.84328693567475010440\n",
      "Iteration 2631 => Loss: 6.84326738221818775543\n",
      "Iteration 2632 => Loss: 6.84324783139645465013\n",
      "Iteration 2633 => Loss: 6.84322828320919818168\n",
      "Iteration 2634 => Loss: 6.84320873765605774963\n",
      "Iteration 2635 => Loss: 6.84318919473668607623\n",
      "Iteration 2636 => Loss: 6.84316965445072167284\n",
      "Iteration 2637 => Loss: 6.84315011679781193266\n",
      "Iteration 2638 => Loss: 6.84313058177760868972\n",
      "Iteration 2639 => Loss: 6.84311104938974867906\n",
      "Iteration 2640 => Loss: 6.84309151963387041206\n",
      "Iteration 2641 => Loss: 6.84307199250963726911\n",
      "Iteration 2642 => Loss: 6.84305246801667976797\n",
      "Iteration 2643 => Loss: 6.84303294615465862449\n",
      "Iteration 2644 => Loss: 6.84301342692319813921\n",
      "Iteration 2645 => Loss: 6.84299391032196258067\n",
      "Iteration 2646 => Loss: 6.84297439635058690754\n",
      "Iteration 2647 => Loss: 6.84295488500873183568\n",
      "Iteration 2648 => Loss: 6.84293537629601633654\n",
      "Iteration 2649 => Loss: 6.84291587021210734321\n",
      "Iteration 2650 => Loss: 6.84289636675664603160\n",
      "Iteration 2651 => Loss: 6.84287686592927357765\n",
      "Iteration 2652 => Loss: 6.84285736772964359176\n",
      "Iteration 2653 => Loss: 6.84283787215739369714\n",
      "Iteration 2654 => Loss: 6.84281837921217572784\n",
      "Iteration 2655 => Loss: 6.84279888889363707705\n",
      "Iteration 2656 => Loss: 6.84277940120141181524\n",
      "Iteration 2657 => Loss: 6.84275991613516243461\n",
      "Iteration 2658 => Loss: 6.84274043369452478203\n",
      "Iteration 2659 => Loss: 6.84272095387914536246\n",
      "Iteration 2660 => Loss: 6.84270147668867512181\n",
      "Iteration 2661 => Loss: 6.84268200212275079508\n",
      "Iteration 2662 => Loss: 6.84266253018103398631\n",
      "Iteration 2663 => Loss: 6.84264306086316764777\n",
      "Iteration 2664 => Loss: 6.84262359416878407359\n",
      "Iteration 2665 => Loss: 6.84260413009753687419\n",
      "Iteration 2666 => Loss: 6.84258466864907965999\n",
      "Iteration 2667 => Loss: 6.84256520982305538325\n",
      "Iteration 2668 => Loss: 6.84254575361910521991\n",
      "Iteration 2669 => Loss: 6.84252630003688100402\n",
      "Iteration 2670 => Loss: 6.84250684907603368146\n",
      "Iteration 2671 => Loss: 6.84248740073620353996\n",
      "Iteration 2672 => Loss: 6.84246795501702997910\n",
      "Iteration 2673 => Loss: 6.84244851191817726743\n",
      "Iteration 2674 => Loss: 6.84242907143928391633\n",
      "Iteration 2675 => Loss: 6.84240963357999465444\n",
      "Iteration 2676 => Loss: 6.84239019833995598674\n",
      "Iteration 2677 => Loss: 6.84237076571882241183\n",
      "Iteration 2678 => Loss: 6.84235133571623066473\n",
      "Iteration 2679 => Loss: 6.84233190833183702040\n",
      "Iteration 2680 => Loss: 6.84231248356528531929\n",
      "Iteration 2681 => Loss: 6.84229306141622206638\n",
      "Iteration 2682 => Loss: 6.84227364188429021397\n",
      "Iteration 2683 => Loss: 6.84225422496914514880\n",
      "Iteration 2684 => Loss: 6.84223481067043071135\n",
      "Iteration 2685 => Loss: 6.84221539898779163025\n",
      "Iteration 2686 => Loss: 6.84219598992088418044\n",
      "Iteration 2687 => Loss: 6.84217658346934332059\n",
      "Iteration 2688 => Loss: 6.84215717963283420744\n",
      "Iteration 2689 => Loss: 6.84213777841098202970\n",
      "Iteration 2690 => Loss: 6.84211837980345283228\n",
      "Iteration 2691 => Loss: 6.84209898380988956745\n",
      "Iteration 2692 => Loss: 6.84207959042992985843\n",
      "Iteration 2693 => Loss: 6.84206019966323797377\n",
      "Iteration 2694 => Loss: 6.84204081150944265488\n",
      "Iteration 2695 => Loss: 6.84202142596821083487\n",
      "Iteration 2696 => Loss: 6.84200204303917747239\n",
      "Iteration 2697 => Loss: 6.84198266272200061877\n",
      "Iteration 2698 => Loss: 6.84196328501632056174\n",
      "Iteration 2699 => Loss: 6.84194390992178735900\n",
      "Iteration 2700 => Loss: 6.84192453743805195643\n",
      "Iteration 2701 => Loss: 6.84190516756475908267\n",
      "Iteration 2702 => Loss: 6.84188580030155613088\n",
      "Iteration 2703 => Loss: 6.84186643564809582330\n",
      "Iteration 2704 => Loss: 6.84184707360402111220\n",
      "Iteration 2705 => Loss: 6.84182771416898471983\n",
      "Iteration 2706 => Loss: 6.84180835734263137482\n",
      "Iteration 2707 => Loss: 6.84178900312461646394\n",
      "Iteration 2708 => Loss: 6.84176965151457849856\n",
      "Iteration 2709 => Loss: 6.84175030251217730637\n",
      "Iteration 2710 => Loss: 6.84173095611705761598\n",
      "Iteration 2711 => Loss: 6.84171161232886060333\n",
      "Iteration 2712 => Loss: 6.84169227114724165517\n",
      "Iteration 2713 => Loss: 6.84167293257184994104\n",
      "Iteration 2714 => Loss: 6.84165359660232663686\n",
      "Iteration 2715 => Loss: 6.84163426323833601117\n",
      "Iteration 2716 => Loss: 6.84161493247951213448\n",
      "Iteration 2717 => Loss: 6.84159560432551483444\n",
      "Iteration 2718 => Loss: 6.84157627877598528698\n",
      "Iteration 2719 => Loss: 6.84155695583056644438\n",
      "Iteration 2720 => Loss: 6.84153763548892435153\n",
      "Iteration 2721 => Loss: 6.84151831775069840802\n",
      "Iteration 2722 => Loss: 6.84149900261553600700\n",
      "Iteration 2723 => Loss: 6.84147969008309608796\n",
      "Iteration 2724 => Loss: 6.84146038015301716229\n",
      "Iteration 2725 => Loss: 6.84144107282496083400\n",
      "Iteration 2726 => Loss: 6.84142176809855673270\n",
      "Iteration 2727 => Loss: 6.84140246597346912694\n",
      "Iteration 2728 => Loss: 6.84138316644935073896\n",
      "Iteration 2729 => Loss: 6.84136386952583386289\n",
      "Iteration 2730 => Loss: 6.84134457520258720820\n",
      "Iteration 2731 => Loss: 6.84132528347924928624\n",
      "Iteration 2732 => Loss: 6.84130599435547548381\n",
      "Iteration 2733 => Loss: 6.84128670783090964136\n",
      "Iteration 2734 => Loss: 6.84126742390520448112\n",
      "Iteration 2735 => Loss: 6.84124814257801272532\n",
      "Iteration 2736 => Loss: 6.84122886384898087897\n",
      "Iteration 2737 => Loss: 6.84120958771775811158\n",
      "Iteration 2738 => Loss: 6.84119031418399181632\n",
      "Iteration 2739 => Loss: 6.84117104324733737997\n",
      "Iteration 2740 => Loss: 6.84115177490744930111\n",
      "Iteration 2741 => Loss: 6.84113250916396431478\n",
      "Iteration 2742 => Loss: 6.84111324601654136046\n",
      "Iteration 2743 => Loss: 6.84109398546483316039\n",
      "Iteration 2744 => Loss: 6.84107472750847822596\n",
      "Iteration 2745 => Loss: 6.84105547214714526660\n",
      "Iteration 2746 => Loss: 6.84103621938046391193\n",
      "Iteration 2747 => Loss: 6.84101696920809843050\n",
      "Iteration 2748 => Loss: 6.84099772162969443912\n",
      "Iteration 2749 => Loss: 6.84097847664490377184\n",
      "Iteration 2750 => Loss: 6.84095923425337737456\n",
      "Iteration 2751 => Loss: 6.84093999445476175225\n",
      "Iteration 2752 => Loss: 6.84092075724870962716\n",
      "Iteration 2753 => Loss: 6.84090152263487016882\n",
      "Iteration 2754 => Loss: 6.84088229061290853394\n",
      "Iteration 2755 => Loss: 6.84086306118245168761\n",
      "Iteration 2756 => Loss: 6.84084383434317100381\n",
      "Iteration 2757 => Loss: 6.84082461009470410573\n",
      "Iteration 2758 => Loss: 6.84080538843670460381\n",
      "Iteration 2759 => Loss: 6.84078616936882610844\n",
      "Iteration 2760 => Loss: 6.84076695289071690098\n",
      "Iteration 2761 => Loss: 6.84074773900203059185\n",
      "Iteration 2762 => Loss: 6.84072852770242079146\n",
      "Iteration 2763 => Loss: 6.84070931899153489297\n",
      "Iteration 2764 => Loss: 6.84069011286901584867\n",
      "Iteration 2765 => Loss: 6.84067090933453059165\n",
      "Iteration 2766 => Loss: 6.84065170838772562689\n",
      "Iteration 2767 => Loss: 6.84063251002824479485\n",
      "Iteration 2768 => Loss: 6.84061331425574525866\n",
      "Iteration 2769 => Loss: 6.84059412106987885238\n",
      "Iteration 2770 => Loss: 6.84057493047029474553\n",
      "Iteration 2771 => Loss: 6.84055574245664921307\n",
      "Iteration 2772 => Loss: 6.84053655702858254273\n",
      "Iteration 2773 => Loss: 6.84051737418575633853\n",
      "Iteration 2774 => Loss: 6.84049819392782243455\n",
      "Iteration 2775 => Loss: 6.84047901625442467122\n",
      "Iteration 2776 => Loss: 6.84045984116522731711\n",
      "Iteration 2777 => Loss: 6.84044066865986533088\n",
      "Iteration 2778 => Loss: 6.84042149873800742199\n",
      "Iteration 2779 => Loss: 6.84040233139929387818\n",
      "Iteration 2780 => Loss: 6.84038316664338097439\n",
      "Iteration 2781 => Loss: 6.84036400446992320923\n",
      "Iteration 2782 => Loss: 6.84034484487856442314\n",
      "Iteration 2783 => Loss: 6.84032568786897332558\n",
      "Iteration 2784 => Loss: 6.84030653344079020428\n",
      "Iteration 2785 => Loss: 6.84028738159365534699\n",
      "Iteration 2786 => Loss: 6.84026823232723923951\n",
      "Iteration 2787 => Loss: 6.84024908564118661047\n",
      "Iteration 2788 => Loss: 6.84022994153515284665\n",
      "Iteration 2789 => Loss: 6.84021080000879244665\n",
      "Iteration 2790 => Loss: 6.84019166106175280362\n",
      "Iteration 2791 => Loss: 6.84017252469368486345\n",
      "Iteration 2792 => Loss: 6.84015339090424667745\n",
      "Iteration 2793 => Loss: 6.84013425969308741514\n",
      "Iteration 2794 => Loss: 6.84011513105985624605\n",
      "Iteration 2795 => Loss: 6.84009600500421477420\n",
      "Iteration 2796 => Loss: 6.84007688152580950458\n",
      "Iteration 2797 => Loss: 6.84005776062429404760\n",
      "Iteration 2798 => Loss: 6.84003864229932201368\n",
      "Iteration 2799 => Loss: 6.84001952655054612507\n",
      "Iteration 2800 => Loss: 6.84000041337761199856\n",
      "Iteration 2801 => Loss: 6.83998130278018923178\n",
      "Iteration 2802 => Loss: 6.83996219475791811249\n",
      "Iteration 2803 => Loss: 6.83994308931045580380\n",
      "Iteration 2804 => Loss: 6.83992398643745147524\n",
      "Iteration 2805 => Loss: 6.83990488613855696087\n",
      "Iteration 2806 => Loss: 6.83988578841343652925\n",
      "Iteration 2807 => Loss: 6.83986669326173402084\n",
      "Iteration 2808 => Loss: 6.83984760068310393422\n",
      "Iteration 2809 => Loss: 6.83982851067719810345\n",
      "Iteration 2810 => Loss: 6.83980942324367369167\n",
      "Iteration 2811 => Loss: 6.83979033838218342112\n",
      "Iteration 2812 => Loss: 6.83977125609237734949\n",
      "Iteration 2813 => Loss: 6.83975217637391885717\n",
      "Iteration 2814 => Loss: 6.83973309922644379100\n",
      "Iteration 2815 => Loss: 6.83971402464962441314\n",
      "Iteration 2816 => Loss: 6.83969495264309568228\n",
      "Iteration 2817 => Loss: 6.83967588320653252509\n",
      "Iteration 2818 => Loss: 6.83965681633956990026\n",
      "Iteration 2819 => Loss: 6.83963775204186852363\n",
      "Iteration 2820 => Loss: 6.83961869031308644651\n",
      "Iteration 2821 => Loss: 6.83959963115287461477\n",
      "Iteration 2822 => Loss: 6.83958057456088752701\n",
      "Iteration 2823 => Loss: 6.83956152053677612912\n",
      "Iteration 2824 => Loss: 6.83954246908019047879\n",
      "Iteration 2825 => Loss: 6.83952342019079306823\n",
      "Iteration 2826 => Loss: 6.83950437386823217878\n",
      "Iteration 2827 => Loss: 6.83948533011217385535\n",
      "Iteration 2828 => Loss: 6.83946628892225483298\n",
      "Iteration 2829 => Loss: 6.83944725029814204476\n",
      "Iteration 2830 => Loss: 6.83942821423948377202\n",
      "Iteration 2831 => Loss: 6.83940918074593451337\n",
      "Iteration 2832 => Loss: 6.83939014981715232011\n",
      "Iteration 2833 => Loss: 6.83937112145278369724\n",
      "Iteration 2834 => Loss: 6.83935209565249824237\n",
      "Iteration 2835 => Loss: 6.83933307241593890780\n",
      "Iteration 2836 => Loss: 6.83931405174275752756\n",
      "Iteration 2837 => Loss: 6.83929503363261481752\n",
      "Iteration 2838 => Loss: 6.83927601808516438808\n",
      "Iteration 2839 => Loss: 6.83925700510006162602\n",
      "Iteration 2840 => Loss: 6.83923799467695747722\n",
      "Iteration 2841 => Loss: 6.83921898681551088117\n",
      "Iteration 2842 => Loss: 6.83919998151536834285\n",
      "Iteration 2843 => Loss: 6.83918097877620123626\n",
      "Iteration 2844 => Loss: 6.83916197859765251366\n",
      "Iteration 2845 => Loss: 6.83914298097937933818\n",
      "Iteration 2846 => Loss: 6.83912398592103798478\n",
      "Iteration 2847 => Loss: 6.83910499342228028752\n",
      "Iteration 2848 => Loss: 6.83908600348275896863\n",
      "Iteration 2849 => Loss: 6.83906701610213829667\n",
      "Iteration 2850 => Loss: 6.83904803128007099389\n",
      "Iteration 2851 => Loss: 6.83902904901620178890\n",
      "Iteration 2852 => Loss: 6.83901006931020116753\n",
      "Iteration 2853 => Loss: 6.83899109216171563475\n",
      "Iteration 2854 => Loss: 6.83897211757040235369\n",
      "Iteration 2855 => Loss: 6.83895314553591582296\n",
      "Iteration 2856 => Loss: 6.83893417605791320568\n",
      "Iteration 2857 => Loss: 6.83891520913604811227\n",
      "Iteration 2858 => Loss: 6.83889624476997948221\n",
      "Iteration 2859 => Loss: 6.83887728295935737322\n",
      "Iteration 2860 => Loss: 6.83885832370384161294\n",
      "Iteration 2861 => Loss: 6.83883936700308936452\n",
      "Iteration 2862 => Loss: 6.83882041285675157383\n",
      "Iteration 2863 => Loss: 6.83880146126449339761\n",
      "Iteration 2864 => Loss: 6.83878251222595157088\n",
      "Iteration 2865 => Loss: 6.83876356574080279671\n",
      "Iteration 2866 => Loss: 6.83874462180869269190\n",
      "Iteration 2867 => Loss: 6.83872568042927753140\n",
      "Iteration 2868 => Loss: 6.83870674160221980742\n",
      "Iteration 2869 => Loss: 6.83868780532716336040\n",
      "Iteration 2870 => Loss: 6.83866887160376979438\n",
      "Iteration 2871 => Loss: 6.83864994043170337790\n",
      "Iteration 2872 => Loss: 6.83863101181061416867\n",
      "Iteration 2873 => Loss: 6.83861208574015666528\n",
      "Iteration 2874 => Loss: 6.83859316221999247176\n",
      "Iteration 2875 => Loss: 6.83857424124976365221\n",
      "Iteration 2876 => Loss: 6.83855532282913713971\n",
      "Iteration 2877 => Loss: 6.83853640695778253189\n",
      "Iteration 2878 => Loss: 6.83851749363533478743\n",
      "Iteration 2879 => Loss: 6.83849858286145639852\n",
      "Iteration 2880 => Loss: 6.83847967463581518643\n",
      "Iteration 2881 => Loss: 6.83846076895804522167\n",
      "Iteration 2882 => Loss: 6.83844186582782320727\n",
      "Iteration 2883 => Loss: 6.83842296524480541819\n",
      "Iteration 2884 => Loss: 6.83840406720864013579\n",
      "Iteration 2885 => Loss: 6.83838517171898363500\n",
      "Iteration 2886 => Loss: 6.83836627877549485532\n",
      "Iteration 2887 => Loss: 6.83834738837783628895\n",
      "Iteration 2888 => Loss: 6.83832850052566065813\n",
      "Iteration 2889 => Loss: 6.83830961521861802055\n",
      "Iteration 2890 => Loss: 6.83829073245637619749\n",
      "Iteration 2891 => Loss: 6.83827185223858879937\n",
      "Iteration 2892 => Loss: 6.83825297456490677206\n",
      "Iteration 2893 => Loss: 6.83823409943500148955\n",
      "Iteration 2894 => Loss: 6.83821522684851501594\n",
      "Iteration 2895 => Loss: 6.83819635680511428433\n",
      "Iteration 2896 => Loss: 6.83817748930444846422\n",
      "Iteration 2897 => Loss: 6.83815862434618448873\n",
      "Iteration 2898 => Loss: 6.83813976192997774461\n",
      "Iteration 2899 => Loss: 6.83812090205547828958\n",
      "Iteration 2900 => Loss: 6.83810204472234595130\n",
      "Iteration 2901 => Loss: 6.83808318993024588650\n",
      "Iteration 2902 => Loss: 6.83806433767882904107\n",
      "Iteration 2903 => Loss: 6.83804548796775524266\n",
      "Iteration 2904 => Loss: 6.83802664079668076624\n",
      "Iteration 2905 => Loss: 6.83800779616526099858\n",
      "Iteration 2906 => Loss: 6.83798895407315843187\n",
      "Iteration 2907 => Loss: 6.83797011452002845289\n",
      "Iteration 2908 => Loss: 6.83795127750553000112\n",
      "Iteration 2909 => Loss: 6.83793244302933000967\n",
      "Iteration 2910 => Loss: 6.83791361109106876626\n",
      "Iteration 2911 => Loss: 6.83789478169040965128\n",
      "Iteration 2912 => Loss: 6.83787595482702403871\n",
      "Iteration 2913 => Loss: 6.83785713050055043993\n",
      "Iteration 2914 => Loss: 6.83783830871066111712\n",
      "Iteration 2915 => Loss: 6.83781948945700435161\n",
      "Iteration 2916 => Loss: 6.83780067273924796467\n",
      "Iteration 2917 => Loss: 6.83778185855703934948\n",
      "Iteration 2918 => Loss: 6.83776304691005254455\n",
      "Iteration 2919 => Loss: 6.83774423779793494305\n",
      "Iteration 2920 => Loss: 6.83772543122033926721\n",
      "Iteration 2921 => Loss: 6.83770662717693600285\n",
      "Iteration 2922 => Loss: 6.83768782566737876039\n",
      "Iteration 2923 => Loss: 6.83766902669132736747\n",
      "Iteration 2924 => Loss: 6.83765023024844431632\n",
      "Iteration 2925 => Loss: 6.83763143633837167101\n",
      "Iteration 2926 => Loss: 6.83761264496079590458\n",
      "Iteration 2927 => Loss: 6.83759385611534220573\n",
      "Iteration 2928 => Loss: 6.83757506980169882382\n",
      "Iteration 2929 => Loss: 6.83755628601950693479\n",
      "Iteration 2930 => Loss: 6.83753750476843258355\n",
      "Iteration 2931 => Loss: 6.83751872604813115686\n",
      "Iteration 2932 => Loss: 6.83749994985826958782\n",
      "Iteration 2933 => Loss: 6.83748117619849793414\n",
      "Iteration 2934 => Loss: 6.83746240506847779983\n",
      "Iteration 2935 => Loss: 6.83744363646787611799\n",
      "Iteration 2936 => Loss: 6.83742487039633140000\n",
      "Iteration 2937 => Loss: 6.83740610685352034892\n",
      "Iteration 2938 => Loss: 6.83738734583910279241\n",
      "Iteration 2939 => Loss: 6.83736858735272878818\n",
      "Iteration 2940 => Loss: 6.83734983139406704566\n",
      "Iteration 2941 => Loss: 6.83733107796276851076\n",
      "Iteration 2942 => Loss: 6.83731232705849834019\n",
      "Iteration 2943 => Loss: 6.83729357868091192074\n",
      "Iteration 2944 => Loss: 6.83727483282967174461\n",
      "Iteration 2945 => Loss: 6.83725608950444208034\n",
      "Iteration 2946 => Loss: 6.83723734870486676840\n",
      "Iteration 2947 => Loss: 6.83721861043062340002\n",
      "Iteration 2948 => Loss: 6.83719987468136114472\n",
      "Iteration 2949 => Loss: 6.83718114145673805382\n",
      "Iteration 2950 => Loss: 6.83716241075643083036\n",
      "Iteration 2951 => Loss: 6.83714368258007709755\n",
      "Iteration 2952 => Loss: 6.83712495692734911756\n",
      "Iteration 2953 => Loss: 6.83710623379789961263\n",
      "Iteration 2954 => Loss: 6.83708751319140262126\n",
      "Iteration 2955 => Loss: 6.83706879510750198392\n",
      "Iteration 2956 => Loss: 6.83705007954586907459\n",
      "Iteration 2957 => Loss: 6.83703136650615395098\n",
      "Iteration 2958 => Loss: 6.83701265598802088164\n",
      "Iteration 2959 => Loss: 6.83699394799114212873\n",
      "Iteration 2960 => Loss: 6.83697524251516064453\n",
      "Iteration 2961 => Loss: 6.83695653955974069760\n",
      "Iteration 2962 => Loss: 6.83693783912454922103\n",
      "Iteration 2963 => Loss: 6.83691914120924781884\n",
      "Iteration 2964 => Loss: 6.83690044581348566055\n",
      "Iteration 2965 => Loss: 6.83688175293693234380\n",
      "Iteration 2966 => Loss: 6.83686306257923970264\n",
      "Iteration 2967 => Loss: 6.83684437474008177560\n",
      "Iteration 2968 => Loss: 6.83682568941911039673\n",
      "Iteration 2969 => Loss: 6.83680700661597562373\n",
      "Iteration 2970 => Loss: 6.83678832633036126509\n",
      "Iteration 2971 => Loss: 6.83676964856192181941\n",
      "Iteration 2972 => Loss: 6.83675097331030201531\n",
      "Iteration 2973 => Loss: 6.83673230057518210856\n",
      "Iteration 2974 => Loss: 6.83671363035621038051\n",
      "Iteration 2975 => Loss: 6.83669496265305376426\n",
      "Iteration 2976 => Loss: 6.83667629746536764657\n",
      "Iteration 2977 => Loss: 6.83665763479282340143\n",
      "Iteration 2978 => Loss: 6.83663897463506753382\n",
      "Iteration 2979 => Loss: 6.83662031699177763500\n",
      "Iteration 2980 => Loss: 6.83660166186259932175\n",
      "Iteration 2981 => Loss: 6.83658300924720663261\n",
      "Iteration 2982 => Loss: 6.83656435914525761888\n",
      "Iteration 2983 => Loss: 6.83654571155640322644\n",
      "Iteration 2984 => Loss: 6.83652706648031927017\n",
      "Iteration 2985 => Loss: 6.83650842391665758413\n",
      "Iteration 2986 => Loss: 6.83648978386508421323\n",
      "Iteration 2987 => Loss: 6.83647114632526164968\n",
      "Iteration 2988 => Loss: 6.83645251129683995117\n",
      "Iteration 2989 => Loss: 6.83643387877950114984\n",
      "Iteration 2990 => Loss: 6.83641524877289352702\n",
      "Iteration 2991 => Loss: 6.83639662127667335767\n",
      "Iteration 2992 => Loss: 6.83637799629051379213\n",
      "Iteration 2993 => Loss: 6.83635937381407021718\n",
      "Iteration 2994 => Loss: 6.83634075384700867772\n",
      "Iteration 2995 => Loss: 6.83632213638898988961\n",
      "Iteration 2996 => Loss: 6.83630352143967900957\n",
      "Iteration 2997 => Loss: 6.83628490899872609532\n",
      "Iteration 2998 => Loss: 6.83626629906580429719\n",
      "Iteration 2999 => Loss: 6.83624769164057344284\n",
      "Iteration 3000 => Loss: 6.83622908672269069541\n",
      "Iteration 3001 => Loss: 6.83621048431183098160\n",
      "Iteration 3002 => Loss: 6.83619188440763991821\n",
      "Iteration 3003 => Loss: 6.83617328700979154377\n",
      "Iteration 3004 => Loss: 6.83615469211794035687\n",
      "Iteration 3005 => Loss: 6.83613609973175684331\n",
      "Iteration 3006 => Loss: 6.83611750985089461352\n",
      "Iteration 3007 => Loss: 6.83609892247501971241\n",
      "Iteration 3008 => Loss: 6.83608033760379729671\n",
      "Iteration 3009 => Loss: 6.83606175523688808227\n",
      "Iteration 3010 => Loss: 6.83604317537395633764\n",
      "Iteration 3011 => Loss: 6.83602459801465922595\n",
      "Iteration 3012 => Loss: 6.83600602315866279213\n",
      "Iteration 3013 => Loss: 6.83598745080563041654\n",
      "Iteration 3014 => Loss: 6.83596888095522459139\n",
      "Iteration 3015 => Loss: 6.83595031360710336799\n",
      "Iteration 3016 => Loss: 6.83593174876093900849\n",
      "Iteration 3017 => Loss: 6.83591318641638956422\n",
      "Iteration 3018 => Loss: 6.83589462657311575100\n",
      "Iteration 3019 => Loss: 6.83587606923077828469\n",
      "Iteration 3020 => Loss: 6.83585751438904853927\n",
      "Iteration 3021 => Loss: 6.83583896204758456605\n",
      "Iteration 3022 => Loss: 6.83582041220604974541\n",
      "Iteration 3023 => Loss: 6.83580186486410745772\n",
      "Iteration 3024 => Loss: 6.83578332002142374790\n",
      "Iteration 3025 => Loss: 6.83576477767765400273\n",
      "Iteration 3026 => Loss: 6.83574623783247137254\n",
      "Iteration 3027 => Loss: 6.83572770048553390865\n",
      "Iteration 3028 => Loss: 6.83570916563650055053\n",
      "Iteration 3029 => Loss: 6.83569063328504356036\n",
      "Iteration 3030 => Loss: 6.83567210343081832491\n",
      "Iteration 3031 => Loss: 6.83565357607349355362\n",
      "Iteration 3032 => Loss: 6.83563505121273795595\n",
      "Iteration 3033 => Loss: 6.83561652884820336595\n",
      "Iteration 3034 => Loss: 6.83559800897956204579\n",
      "Iteration 3035 => Loss: 6.83557949160647115860\n",
      "Iteration 3036 => Loss: 6.83556097672859852565\n",
      "Iteration 3037 => Loss: 6.83554246434560841550\n",
      "Iteration 3038 => Loss: 6.83552395445716065581\n",
      "Iteration 3039 => Loss: 6.83550544706292040331\n",
      "Iteration 3040 => Loss: 6.83548694216255547929\n",
      "Iteration 3041 => Loss: 6.83546843975572926411\n",
      "Iteration 3042 => Loss: 6.83544993984210336180\n",
      "Iteration 3043 => Loss: 6.83543144242134292909\n",
      "Iteration 3044 => Loss: 6.83541294749310157641\n",
      "Iteration 3045 => Loss: 6.83539445505706932948\n",
      "Iteration 3046 => Loss: 6.83537596511288203516\n",
      "Iteration 3047 => Loss: 6.83535747766022350191\n",
      "Iteration 3048 => Loss: 6.83533899269874911653\n",
      "Iteration 3049 => Loss: 6.83532051022811870666\n",
      "Iteration 3050 => Loss: 6.83530203024800631084\n",
      "Iteration 3051 => Loss: 6.83528355275807353308\n",
      "Iteration 3052 => Loss: 6.83526507775798730648\n",
      "Iteration 3053 => Loss: 6.83524660524740479417\n",
      "Iteration 3054 => Loss: 6.83522813522599026470\n",
      "Iteration 3055 => Loss: 6.83520966769342308567\n",
      "Iteration 3056 => Loss: 6.83519120264934887388\n",
      "Iteration 3057 => Loss: 6.83517274009344610874\n",
      "Iteration 3058 => Loss: 6.83515428002536840069\n",
      "Iteration 3059 => Loss: 6.83513582244479245276\n",
      "Iteration 3060 => Loss: 6.83511736735137187537\n",
      "Iteration 3061 => Loss: 6.83509891474477715434\n",
      "Iteration 3062 => Loss: 6.83508046462467522275\n",
      "Iteration 3063 => Loss: 6.83506201699073390188\n",
      "Iteration 3064 => Loss: 6.83504357184259969671\n",
      "Iteration 3065 => Loss: 6.83502512917996263297\n",
      "Iteration 3066 => Loss: 6.83500668900246743931\n",
      "Iteration 3067 => Loss: 6.83498825130978548970\n",
      "Iteration 3068 => Loss: 6.83496981610159348719\n",
      "Iteration 3069 => Loss: 6.83495138337754415403\n",
      "Iteration 3070 => Loss: 6.83493295313730175877\n",
      "Iteration 3071 => Loss: 6.83491452538053678722\n",
      "Iteration 3072 => Loss: 6.83489610010692061337\n",
      "Iteration 3073 => Loss: 6.83487767731610507127\n",
      "Iteration 3074 => Loss: 6.83485925700776064673\n",
      "Iteration 3075 => Loss: 6.83484083918156137827\n",
      "Iteration 3076 => Loss: 6.83482242383715821177\n",
      "Iteration 3077 => Loss: 6.83480401097422252121\n",
      "Iteration 3078 => Loss: 6.83478560059243012148\n",
      "Iteration 3079 => Loss: 6.83476719269143107027\n",
      "Iteration 3080 => Loss: 6.83474878727090029429\n",
      "Iteration 3081 => Loss: 6.83473038433050206208\n",
      "Iteration 3082 => Loss: 6.83471198386990330675\n",
      "Iteration 3083 => Loss: 6.83469358588876563232\n",
      "Iteration 3084 => Loss: 6.83467519038675241916\n",
      "Iteration 3085 => Loss: 6.83465679736354658758\n",
      "Iteration 3086 => Loss: 6.83463840681879286620\n",
      "Iteration 3087 => Loss: 6.83462001875217062263\n",
      "Iteration 3088 => Loss: 6.83460163316333701999\n",
      "Iteration 3089 => Loss: 6.83458325005196787316\n",
      "Iteration 3090 => Loss: 6.83456486941772656252\n",
      "Iteration 3091 => Loss: 6.83454649126026847483\n",
      "Iteration 3092 => Loss: 6.83452811557927120134\n",
      "Iteration 3093 => Loss: 6.83450974237439634607\n",
      "Iteration 3094 => Loss: 6.83449137164532327660\n",
      "Iteration 3095 => Loss: 6.83447300339169849792\n",
      "Iteration 3096 => Loss: 6.83445463761319960128\n",
      "Iteration 3097 => Loss: 6.83443627430949440793\n",
      "Iteration 3098 => Loss: 6.83441791348023830466\n",
      "Iteration 3099 => Loss: 6.83439955512510532998\n",
      "Iteration 3100 => Loss: 6.83438119924377485148\n",
      "Iteration 3101 => Loss: 6.83436284583588804509\n",
      "Iteration 3102 => Loss: 6.83434449490112605474\n",
      "Iteration 3103 => Loss: 6.83432614643915847807\n",
      "Iteration 3104 => Loss: 6.83430780044964869546\n",
      "Iteration 3105 => Loss: 6.83428945693225031732\n",
      "Iteration 3106 => Loss: 6.83427111588665336939\n",
      "Iteration 3107 => Loss: 6.83425277731250879754\n",
      "Iteration 3108 => Loss: 6.83423444120949774572\n",
      "Iteration 3109 => Loss: 6.83421610757726583074\n",
      "Iteration 3110 => Loss: 6.83419777641549952563\n",
      "Iteration 3111 => Loss: 6.83417944772385954622\n",
      "Iteration 3112 => Loss: 6.83416112150201016107\n",
      "Iteration 3113 => Loss: 6.83414279774961475056\n",
      "Iteration 3114 => Loss: 6.83412447646635445864\n",
      "Iteration 3115 => Loss: 6.83410615765189088933\n",
      "Iteration 3116 => Loss: 6.83408784130587587669\n",
      "Iteration 3117 => Loss: 6.83406952742799944645\n",
      "Iteration 3118 => Loss: 6.83405121601791787356\n",
      "Iteration 3119 => Loss: 6.83403290707530164383\n",
      "Iteration 3120 => Loss: 6.83401460059981946671\n",
      "Iteration 3121 => Loss: 6.83399629659112672897\n",
      "Iteration 3122 => Loss: 6.83397799504890102185\n",
      "Iteration 3123 => Loss: 6.83395969597281993657\n",
      "Iteration 3124 => Loss: 6.83394139936253530720\n",
      "Iteration 3125 => Loss: 6.83392310521772117227\n",
      "Iteration 3126 => Loss: 6.83390481353804357667\n",
      "Iteration 3127 => Loss: 6.83388652432317122987\n",
      "Iteration 3128 => Loss: 6.83386823757277106495\n",
      "Iteration 3129 => Loss: 6.83384995328651800861\n",
      "Iteration 3130 => Loss: 6.83383167146406922399\n",
      "Iteration 3131 => Loss: 6.83381339210509430870\n",
      "Iteration 3132 => Loss: 6.83379511520926374857\n",
      "Iteration 3133 => Loss: 6.83377684077624891756\n",
      "Iteration 3134 => Loss: 6.83375856880571941332\n",
      "Iteration 3135 => Loss: 6.83374029929733417532\n",
      "Iteration 3136 => Loss: 6.83372203225076990663\n",
      "Iteration 3137 => Loss: 6.83370376766569620486\n",
      "Iteration 3138 => Loss: 6.83368550554176490408\n",
      "Iteration 3139 => Loss: 6.83366724587866869456\n",
      "Iteration 3140 => Loss: 6.83364898867605408128\n",
      "Iteration 3141 => Loss: 6.83363073393360309638\n",
      "Iteration 3142 => Loss: 6.83361248165098178475\n",
      "Iteration 3143 => Loss: 6.83359423182784730955\n",
      "Iteration 3144 => Loss: 6.83357598446388792013\n",
      "Iteration 3145 => Loss: 6.83355773955875989145\n",
      "Iteration 3146 => Loss: 6.83353949711213104479\n",
      "Iteration 3147 => Loss: 6.83352125712367985955\n",
      "Iteration 3148 => Loss: 6.83350301959306261068\n",
      "Iteration 3149 => Loss: 6.83348478451995333671\n",
      "Iteration 3150 => Loss: 6.83346655190402429980\n",
      "Iteration 3151 => Loss: 6.83344832174494420940\n",
      "Iteration 3152 => Loss: 6.83343009404237555771\n",
      "Iteration 3153 => Loss: 6.83341186879599327142\n",
      "Iteration 3154 => Loss: 6.83339364600546250728\n",
      "Iteration 3155 => Loss: 6.83337542567045641562\n",
      "Iteration 3156 => Loss: 6.83335720779064192953\n",
      "Iteration 3157 => Loss: 6.83333899236568775848\n",
      "Iteration 3158 => Loss: 6.83332077939525994736\n",
      "Iteration 3159 => Loss: 6.83330256887903697560\n",
      "Iteration 3160 => Loss: 6.83328436081667689450\n",
      "Iteration 3161 => Loss: 6.83326615520785995983\n",
      "Iteration 3162 => Loss: 6.83324795205224688743\n",
      "Iteration 3163 => Loss: 6.83322975134951082765\n",
      "Iteration 3164 => Loss: 6.83321155309931604904\n",
      "Iteration 3165 => Loss: 6.83319335730135168916\n",
      "Iteration 3166 => Loss: 6.83317516395526070028\n",
      "Iteration 3167 => Loss: 6.83315697306072689088\n",
      "Iteration 3168 => Loss: 6.83313878461741719406\n",
      "Iteration 3169 => Loss: 6.83312059862500387197\n",
      "Iteration 3170 => Loss: 6.83310241508315030501\n",
      "Iteration 3171 => Loss: 6.83308423399153408440\n",
      "Iteration 3172 => Loss: 6.83306605534981859051\n",
      "Iteration 3173 => Loss: 6.83304787915767697370\n",
      "Iteration 3174 => Loss: 6.83302970541477971977\n",
      "Iteration 3175 => Loss: 6.83301153412079820271\n",
      "Iteration 3176 => Loss: 6.83299336527539402653\n",
      "Iteration 3177 => Loss: 6.83297519887824744700\n",
      "Iteration 3178 => Loss: 6.83295703492901740361\n",
      "Iteration 3179 => Loss: 6.83293887342739303392\n",
      "Iteration 3180 => Loss: 6.83292071437302350745\n",
      "Iteration 3181 => Loss: 6.83290255776559174450\n",
      "Iteration 3182 => Loss: 6.83288440360476378999\n",
      "Iteration 3183 => Loss: 6.83286625189021101789\n",
      "Iteration 3184 => Loss: 6.83284810262160124950\n",
      "Iteration 3185 => Loss: 6.83282995579861118784\n",
      "Iteration 3186 => Loss: 6.83281181142090154879\n",
      "Iteration 3187 => Loss: 6.83279366948814903537\n",
      "Iteration 3188 => Loss: 6.83277553000002146888\n",
      "Iteration 3189 => Loss: 6.83275739295619821689\n",
      "Iteration 3190 => Loss: 6.83273925835633821890\n",
      "Iteration 3191 => Loss: 6.83272112620011373707\n",
      "Iteration 3192 => Loss: 6.83270299648720502717\n",
      "Iteration 3193 => Loss: 6.83268486921727813410\n",
      "Iteration 3194 => Loss: 6.83266674438999821461\n",
      "Iteration 3195 => Loss: 6.83264862200503930723\n",
      "Iteration 3196 => Loss: 6.83263050206207100956\n",
      "Iteration 3197 => Loss: 6.83261238456077180103\n",
      "Iteration 3198 => Loss: 6.83259426950080506202\n",
      "Iteration 3199 => Loss: 6.83257615688184749558\n",
      "Iteration 3200 => Loss: 6.83255804670356248209\n",
      "Iteration 3201 => Loss: 6.83253993896562228372\n",
      "Iteration 3202 => Loss: 6.83252183366771248529\n",
      "Iteration 3203 => Loss: 6.83250373080948225635\n",
      "Iteration 3204 => Loss: 6.83248563039061984625\n",
      "Iteration 3205 => Loss: 6.83246753241078419450\n",
      "Iteration 3206 => Loss: 6.83244943686965022778\n",
      "Iteration 3207 => Loss: 6.83243134376690264276\n",
      "Iteration 3208 => Loss: 6.83241325310220304345\n",
      "Iteration 3209 => Loss: 6.83239516487521036936\n",
      "Iteration 3210 => Loss: 6.83237707908561642256\n",
      "Iteration 3211 => Loss: 6.83235899573308280708\n",
      "Iteration 3212 => Loss: 6.83234091481727734418\n",
      "Iteration 3213 => Loss: 6.83232283633788028965\n",
      "Iteration 3214 => Loss: 6.83230476029455768838\n",
      "Iteration 3215 => Loss: 6.83228668668698535527\n",
      "Iteration 3216 => Loss: 6.83226861551483111157\n",
      "Iteration 3217 => Loss: 6.83225054677777432488\n",
      "Iteration 3218 => Loss: 6.83223248047547926376\n",
      "Iteration 3219 => Loss: 6.83221441660761463766\n",
      "Iteration 3220 => Loss: 6.83219635517385981416\n",
      "Iteration 3221 => Loss: 6.83217829617388971997\n",
      "Iteration 3222 => Loss: 6.83216023960736418275\n",
      "Iteration 3223 => Loss: 6.83214218547396612280\n",
      "Iteration 3224 => Loss: 6.83212413377336247322\n",
      "Iteration 3225 => Loss: 6.83210608450522727253\n",
      "Iteration 3226 => Loss: 6.83208803766923189471\n",
      "Iteration 3227 => Loss: 6.83206999326505304282\n",
      "Iteration 3228 => Loss: 6.83205195129235409723\n",
      "Iteration 3229 => Loss: 6.83203391175080998465\n",
      "Iteration 3230 => Loss: 6.83201587464010362538\n",
      "Iteration 3231 => Loss: 6.83199783995989662344\n",
      "Iteration 3232 => Loss: 6.83197980770986035282\n",
      "Iteration 3233 => Loss: 6.83196177788967240474\n",
      "Iteration 3234 => Loss: 6.83194375049900948227\n",
      "Iteration 3235 => Loss: 6.83192572553753674214\n",
      "Iteration 3236 => Loss: 6.83190770300492467015\n",
      "Iteration 3237 => Loss: 6.83188968290086062751\n",
      "Iteration 3238 => Loss: 6.83187166522499111920\n",
      "Iteration 3239 => Loss: 6.83185364997701505274\n",
      "Iteration 3240 => Loss: 6.83183563715659136761\n",
      "Iteration 3241 => Loss: 6.83181762676339765505\n",
      "Iteration 3242 => Loss: 6.83179961879710262451\n",
      "Iteration 3243 => Loss: 6.83178161325739186083\n",
      "Iteration 3244 => Loss: 6.83176361014392341531\n",
      "Iteration 3245 => Loss: 6.83174560945637665554\n",
      "Iteration 3246 => Loss: 6.83172761119442117916\n",
      "Iteration 3247 => Loss: 6.83170961535773546558\n",
      "Iteration 3248 => Loss: 6.83169162194599266513\n",
      "Iteration 3249 => Loss: 6.83167363095885704638\n",
      "Iteration 3250 => Loss: 6.83165564239601241781\n",
      "Iteration 3251 => Loss: 6.83163765625712660068\n",
      "Iteration 3252 => Loss: 6.83161967254187718623\n",
      "Iteration 3253 => Loss: 6.83160169124993021939\n",
      "Iteration 3254 => Loss: 6.83158371238096506772\n",
      "Iteration 3255 => Loss: 6.83156573593465754612\n",
      "Iteration 3256 => Loss: 6.83154776191067547586\n",
      "Iteration 3257 => Loss: 6.83152979030869111909\n",
      "Iteration 3258 => Loss: 6.83151182112838650795\n",
      "Iteration 3259 => Loss: 6.83149385436942857552\n",
      "Iteration 3260 => Loss: 6.83147589003148958398\n",
      "Iteration 3261 => Loss: 6.83145792811425067725\n",
      "Iteration 3262 => Loss: 6.83143996861738056481\n",
      "Iteration 3263 => Loss: 6.83142201154055772605\n",
      "Iteration 3264 => Loss: 6.83140405688344376500\n",
      "Iteration 3265 => Loss: 6.83138610464572870740\n",
      "Iteration 3266 => Loss: 6.83136815482707859815\n",
      "Iteration 3267 => Loss: 6.83135020742716214670\n",
      "Iteration 3268 => Loss: 6.83133226244566227336\n",
      "Iteration 3269 => Loss: 6.83131431988224768759\n",
      "Iteration 3270 => Loss: 6.83129637973660308603\n",
      "Iteration 3271 => Loss: 6.83127844200839273725\n",
      "Iteration 3272 => Loss: 6.83126050669728623888\n",
      "Iteration 3273 => Loss: 6.83124257380296207032\n",
      "Iteration 3274 => Loss: 6.83122464332510936913\n",
      "Iteration 3275 => Loss: 6.83120671526338085755\n",
      "Iteration 3276 => Loss: 6.83118878961746212042\n",
      "Iteration 3277 => Loss: 6.83117086638702453172\n",
      "Iteration 3278 => Loss: 6.83115294557174479451\n",
      "Iteration 3279 => Loss: 6.83113502717129694730\n",
      "Iteration 3280 => Loss: 6.83111711118534969955\n",
      "Iteration 3281 => Loss: 6.83109919761359307699\n",
      "Iteration 3282 => Loss: 6.83108128645568246640\n",
      "Iteration 3283 => Loss: 6.83106337771129989989\n",
      "Iteration 3284 => Loss: 6.83104547138013540319\n",
      "Iteration 3285 => Loss: 6.83102756746183903402\n",
      "Iteration 3286 => Loss: 6.83100966595610259446\n",
      "Iteration 3287 => Loss: 6.83099176686258768854\n",
      "Iteration 3288 => Loss: 6.83097387018098078926\n",
      "Iteration 3289 => Loss: 6.83095597591095859968\n",
      "Iteration 3290 => Loss: 6.83093808405219071744\n",
      "Iteration 3291 => Loss: 6.83092019460434674016\n",
      "Iteration 3292 => Loss: 6.83090230756710958815\n",
      "Iteration 3293 => Loss: 6.83088442294015507628\n",
      "Iteration 3294 => Loss: 6.83086654072315102582\n",
      "Iteration 3295 => Loss: 6.83084866091577591618\n",
      "Iteration 3296 => Loss: 6.83083078351770556225\n",
      "Iteration 3297 => Loss: 6.83081290852861933161\n",
      "Iteration 3298 => Loss: 6.83079503594818948642\n",
      "Iteration 3299 => Loss: 6.83077716577609184156\n",
      "Iteration 3300 => Loss: 6.83075929801199954738\n",
      "Iteration 3301 => Loss: 6.83074143265558664240\n",
      "Iteration 3302 => Loss: 6.83072356970653693509\n",
      "Iteration 3303 => Loss: 6.83070570916452002308\n",
      "Iteration 3304 => Loss: 6.83068785102921172125\n",
      "Iteration 3305 => Loss: 6.83066999530029317356\n",
      "Iteration 3306 => Loss: 6.83065214197743220126\n",
      "Iteration 3307 => Loss: 6.83063429106030373106\n",
      "Iteration 3308 => Loss: 6.83061644254859068326\n",
      "Iteration 3309 => Loss: 6.83059859644196709638\n",
      "Iteration 3310 => Loss: 6.83058075274010789713\n",
      "Iteration 3311 => Loss: 6.83056291144268712401\n",
      "Iteration 3312 => Loss: 6.83054507254937881555\n",
      "Iteration 3313 => Loss: 6.83052723605986855659\n",
      "Iteration 3314 => Loss: 6.83050940197382860930\n",
      "Iteration 3315 => Loss: 6.83049157029092501858\n",
      "Iteration 3316 => Loss: 6.83047374101085313924\n",
      "Iteration 3317 => Loss: 6.83045591413327279895\n",
      "Iteration 3318 => Loss: 6.83043808965785892440\n",
      "Iteration 3319 => Loss: 6.83042026758429976496\n",
      "Iteration 3320 => Loss: 6.83040244791226847099\n",
      "Iteration 3321 => Loss: 6.83038463064143730463\n",
      "Iteration 3322 => Loss: 6.83036681577148385713\n",
      "Iteration 3323 => Loss: 6.83034900330208927244\n",
      "Iteration 3324 => Loss: 6.83033119323292492453\n",
      "Iteration 3325 => Loss: 6.83031338556366396375\n",
      "Iteration 3326 => Loss: 6.83029558029399819219\n",
      "Iteration 3327 => Loss: 6.83027777742358388480\n",
      "Iteration 3328 => Loss: 6.83025997695211284366\n",
      "Iteration 3329 => Loss: 6.83024217887925644277\n",
      "Iteration 3330 => Loss: 6.83022438320469049700\n",
      "Iteration 3331 => Loss: 6.83020658992808993304\n",
      "Iteration 3332 => Loss: 6.83018879904914122392\n",
      "Iteration 3333 => Loss: 6.83017101056751485544\n",
      "Iteration 3334 => Loss: 6.83015322448287509616\n",
      "Iteration 3335 => Loss: 6.83013544079492440630\n",
      "Iteration 3336 => Loss: 6.83011765950331994901\n",
      "Iteration 3337 => Loss: 6.83009988060775263818\n",
      "Iteration 3338 => Loss: 6.83008210410788763056\n",
      "Iteration 3339 => Loss: 6.83006433000341228734\n",
      "Iteration 3340 => Loss: 6.83004655829399798250\n",
      "Iteration 3341 => Loss: 6.83002878897932319546\n",
      "Iteration 3342 => Loss: 6.83001102205905841203\n",
      "Iteration 3343 => Loss: 6.82999325753289543428\n",
      "Iteration 3344 => Loss: 6.82997549540049586625\n",
      "Iteration 3345 => Loss: 6.82995773566155417456\n",
      "Iteration 3346 => Loss: 6.82993997831573107504\n",
      "Iteration 3347 => Loss: 6.82992222336271659344\n",
      "Iteration 3348 => Loss: 6.82990447080217677467\n",
      "Iteration 3349 => Loss: 6.82988672063380253263\n",
      "Iteration 3350 => Loss: 6.82986897285726168860\n",
      "Iteration 3351 => Loss: 6.82985122747223893924\n",
      "Iteration 3352 => Loss: 6.82983348447840477036\n",
      "Iteration 3353 => Loss: 6.82981574387544299043\n",
      "Iteration 3354 => Loss: 6.82979800566302497344\n",
      "Iteration 3355 => Loss: 6.82978026984083719242\n",
      "Iteration 3356 => Loss: 6.82976253640855013316\n",
      "Iteration 3357 => Loss: 6.82974480536584316326\n",
      "Iteration 3358 => Loss: 6.82972707671240097937\n",
      "Iteration 3359 => Loss: 6.82970935044788962642\n",
      "Iteration 3360 => Loss: 6.82969162657199913014\n",
      "Iteration 3361 => Loss: 6.82967390508439908814\n",
      "Iteration 3362 => Loss: 6.82965618598476975620\n",
      "Iteration 3363 => Loss: 6.82963846927279494281\n",
      "Iteration 3364 => Loss: 6.82962075494814335741\n",
      "Iteration 3365 => Loss: 6.82960304301049792031\n",
      "Iteration 3366 => Loss: 6.82958533345953977545\n",
      "Iteration 3367 => Loss: 6.82956762629494740224\n",
      "Iteration 3368 => Loss: 6.82954992151639661557\n",
      "Iteration 3369 => Loss: 6.82953221912356678303\n",
      "Iteration 3370 => Loss: 6.82951451911613016676\n",
      "Iteration 3371 => Loss: 6.82949682149377768070\n",
      "Iteration 3372 => Loss: 6.82947912625617714610\n",
      "Iteration 3373 => Loss: 6.82946143340300881874\n",
      "Iteration 3374 => Loss: 6.82944374293395828346\n",
      "Iteration 3375 => Loss: 6.82942605484869780241\n",
      "Iteration 3376 => Loss: 6.82940836914690763138\n",
      "Iteration 3377 => Loss: 6.82939068582827069065\n",
      "Iteration 3378 => Loss: 6.82937300489246013058\n",
      "Iteration 3379 => Loss: 6.82935532633915443057\n",
      "Iteration 3380 => Loss: 6.82933765016803828729\n",
      "Iteration 3381 => Loss: 6.82931997637878485108\n",
      "Iteration 3382 => Loss: 6.82930230497107704224\n",
      "Iteration 3383 => Loss: 6.82928463594459156383\n",
      "Iteration 3384 => Loss: 6.82926696929901400068\n",
      "Iteration 3385 => Loss: 6.82924930503401217408\n",
      "Iteration 3386 => Loss: 6.82923164314928232699\n",
      "Iteration 3387 => Loss: 6.82921398364447895801\n",
      "Iteration 3388 => Loss: 6.82919632651930452738\n",
      "Iteration 3389 => Loss: 6.82917867177342596818\n",
      "Iteration 3390 => Loss: 6.82916101940652442437\n",
      "Iteration 3391 => Loss: 6.82914336941828459260\n",
      "Iteration 3392 => Loss: 6.82912572180837695868\n",
      "Iteration 3393 => Loss: 6.82910807657648710745\n",
      "Iteration 3394 => Loss: 6.82909043372229085378\n",
      "Iteration 3395 => Loss: 6.82907279324547911159\n",
      "Iteration 3396 => Loss: 6.82905515514571437308\n",
      "Iteration 3397 => Loss: 6.82903751942269199304\n",
      "Iteration 3398 => Loss: 6.82901988607608334547\n",
      "Iteration 3399 => Loss: 6.82900225510556868613\n",
      "Iteration 3400 => Loss: 6.82898462651082827080\n",
      "Iteration 3401 => Loss: 6.82896700029154146705\n",
      "Iteration 3402 => Loss: 6.82894937644738764249\n",
      "Iteration 3403 => Loss: 6.82893175497805593466\n",
      "Iteration 3404 => Loss: 6.82891413588320972394\n",
      "Iteration 3405 => Loss: 6.82889651916254347697\n",
      "Iteration 3406 => Loss: 6.82887890481572767953\n",
      "Iteration 3407 => Loss: 6.82886129284244702831\n",
      "Iteration 3408 => Loss: 6.82884368324238355541\n",
      "Iteration 3409 => Loss: 6.82882607601521574026\n",
      "Iteration 3410 => Loss: 6.82880847116062206226\n",
      "Iteration 3411 => Loss: 6.82879086867827833629\n",
      "Iteration 3412 => Loss: 6.82877326856788169351\n",
      "Iteration 3413 => Loss: 6.82875567082909373795\n",
      "Iteration 3414 => Loss: 6.82873807546160449533\n",
      "Iteration 3415 => Loss: 6.82872048246508711600\n",
      "Iteration 3416 => Loss: 6.82870289183922896115\n",
      "Iteration 3417 => Loss: 6.82868530358371650379\n",
      "Iteration 3418 => Loss: 6.82866771769821401250\n",
      "Iteration 3419 => Loss: 6.82865013418241506571\n",
      "Iteration 3420 => Loss: 6.82863255303599814283\n",
      "Iteration 3421 => Loss: 6.82861497425864172328\n",
      "Iteration 3422 => Loss: 6.82859739785002695101\n",
      "Iteration 3423 => Loss: 6.82857982380983319359\n",
      "Iteration 3424 => Loss: 6.82856225213773981864\n",
      "Iteration 3425 => Loss: 6.82854468283343329915\n",
      "Iteration 3426 => Loss: 6.82852711589659389091\n",
      "Iteration 3427 => Loss: 6.82850955132690184968\n",
      "Iteration 3428 => Loss: 6.82849198912403210215\n",
      "Iteration 3429 => Loss: 6.82847442928766934500\n",
      "Iteration 3430 => Loss: 6.82845687181749738670\n",
      "Iteration 3431 => Loss: 6.82843931671320003574\n",
      "Iteration 3432 => Loss: 6.82842176397445221880\n",
      "Iteration 3433 => Loss: 6.82840421360093685621\n",
      "Iteration 3434 => Loss: 6.82838666559233864461\n",
      "Iteration 3435 => Loss: 6.82836911994832629347\n",
      "Iteration 3436 => Loss: 6.82835157666860137482\n",
      "Iteration 3437 => Loss: 6.82833403575283082176\n",
      "Iteration 3438 => Loss: 6.82831649720070110732\n",
      "Iteration 3439 => Loss: 6.82829896101189071089\n",
      "Iteration 3440 => Loss: 6.82828142718608344097\n",
      "Iteration 3441 => Loss: 6.82826389572296399422\n",
      "Iteration 3442 => Loss: 6.82824636662220552097\n",
      "Iteration 3443 => Loss: 6.82822883988349715878\n",
      "Iteration 3444 => Loss: 6.82821131550651916342\n",
      "Iteration 3445 => Loss: 6.82819379349094823795\n",
      "Iteration 3446 => Loss: 6.82817627383647796080\n",
      "Iteration 3447 => Loss: 6.82815875654277437690\n",
      "Iteration 3448 => Loss: 6.82814124160953372922\n",
      "Iteration 3449 => Loss: 6.82812372903642916810\n",
      "Iteration 3450 => Loss: 6.82810621882313917297\n",
      "Iteration 3451 => Loss: 6.82808871096935998679\n",
      "Iteration 3452 => Loss: 6.82807120547476653627\n",
      "Iteration 3453 => Loss: 6.82805370233903907717\n",
      "Iteration 3454 => Loss: 6.82803620156185431256\n",
      "Iteration 3455 => Loss: 6.82801870314290404451\n",
      "Iteration 3456 => Loss: 6.82800120708186675245\n",
      "Iteration 3457 => Loss: 6.82798371337841913942\n",
      "Iteration 3458 => Loss: 6.82796622203226188930\n",
      "Iteration 3459 => Loss: 6.82794873304306104700\n",
      "Iteration 3460 => Loss: 6.82793124641049775647\n",
      "Iteration 3461 => Loss: 6.82791376213426737252\n",
      "Iteration 3462 => Loss: 6.82789628021403771641\n",
      "Iteration 3463 => Loss: 6.82787880064949970205\n",
      "Iteration 3464 => Loss: 6.82786132344033358521\n",
      "Iteration 3465 => Loss: 6.82784384858622317438\n",
      "Iteration 3466 => Loss: 6.82782637608685227804\n",
      "Iteration 3467 => Loss: 6.82780890594190115195\n",
      "Iteration 3468 => Loss: 6.82779143815105182824\n",
      "Iteration 3469 => Loss: 6.82777397271399610901\n",
      "Iteration 3470 => Loss: 6.82775650963039915098\n",
      "Iteration 3471 => Loss: 6.82773904889995630896\n",
      "Iteration 3472 => Loss: 6.82772159052234961507\n",
      "Iteration 3473 => Loss: 6.82770413449726376598\n",
      "Iteration 3474 => Loss: 6.82768668082437724109\n",
      "Iteration 3475 => Loss: 6.82766922950337029619\n",
      "Iteration 3476 => Loss: 6.82765178053393295698\n",
      "Iteration 3477 => Loss: 6.82763433391575258469\n",
      "Iteration 3478 => Loss: 6.82761688964849611239\n",
      "Iteration 3479 => Loss: 6.82759944773186067124\n",
      "Iteration 3480 => Loss: 6.82758200816551763523\n",
      "Iteration 3481 => Loss: 6.82756457094916946460\n",
      "Iteration 3482 => Loss: 6.82754713608248042789\n",
      "Iteration 3483 => Loss: 6.82752970356513877448\n",
      "Iteration 3484 => Loss: 6.82751227339683541828\n",
      "Iteration 3485 => Loss: 6.82749484557724795053\n",
      "Iteration 3486 => Loss: 6.82747742010605751517\n",
      "Iteration 3487 => Loss: 6.82745999698295147340\n",
      "Iteration 3488 => Loss: 6.82744257620761363370\n",
      "Iteration 3489 => Loss: 6.82742515777972514002\n",
      "Iteration 3490 => Loss: 6.82740774169897157719\n",
      "Iteration 3491 => Loss: 6.82739032796503675371\n",
      "Iteration 3492 => Loss: 6.82737291657760625441\n",
      "Iteration 3493 => Loss: 6.82735550753635944687\n",
      "Iteration 3494 => Loss: 6.82733810084098013959\n",
      "Iteration 3495 => Loss: 6.82732069649116013466\n",
      "Iteration 3496 => Loss: 6.82730329448657347058\n",
      "Iteration 3497 => Loss: 6.82728589482690839674\n",
      "Iteration 3498 => Loss: 6.82726849751184960979\n",
      "Iteration 3499 => Loss: 6.82725110254108003005\n",
      "Iteration 3500 => Loss: 6.82723370991428524235\n",
      "Iteration 3501 => Loss: 6.82721631963115083153\n",
      "Iteration 3502 => Loss: 6.82719893169135172428\n",
      "Iteration 3503 => Loss: 6.82718154609457883453\n",
      "Iteration 3504 => Loss: 6.82716416284052307617\n",
      "Iteration 3505 => Loss: 6.82714678192885582320\n",
      "Iteration 3506 => Loss: 6.82712940335927154223\n",
      "Iteration 3507 => Loss: 6.82711202713144871268\n",
      "Iteration 3508 => Loss: 6.82709465324507469575\n",
      "Iteration 3509 => Loss: 6.82707728169983862898\n",
      "Iteration 3510 => Loss: 6.82705991249540922183\n",
      "Iteration 3511 => Loss: 6.82704254563148715818\n",
      "Iteration 3512 => Loss: 6.82702518110775624649\n",
      "Iteration 3513 => Loss: 6.82700781892388874894\n",
      "Iteration 3514 => Loss: 6.82699045907958002033\n",
      "Iteration 3515 => Loss: 6.82697310157450854007\n",
      "Iteration 3516 => Loss: 6.82695574640836788660\n",
      "Iteration 3517 => Loss: 6.82693839358083032209\n",
      "Iteration 3518 => Loss: 6.82692104309159031317\n",
      "Iteration 3519 => Loss: 6.82690369494033078013\n",
      "Iteration 3520 => Loss: 6.82688634912674086053\n",
      "Iteration 3521 => Loss: 6.82686900565049459289\n",
      "Iteration 3522 => Loss: 6.82685166451128999654\n",
      "Iteration 3523 => Loss: 6.82683432570879134005\n",
      "Iteration 3524 => Loss: 6.82681698924271262996\n",
      "Iteration 3525 => Loss: 6.82679965511271813483\n",
      "Iteration 3526 => Loss: 6.82678232331849255132\n",
      "Iteration 3527 => Loss: 6.82676499385973567513\n",
      "Iteration 3528 => Loss: 6.82674766673612509749\n",
      "Iteration 3529 => Loss: 6.82673034194734018598\n",
      "Iteration 3530 => Loss: 6.82671301949307629542\n",
      "Iteration 3531 => Loss: 6.82669569937301279339\n",
      "Iteration 3532 => Loss: 6.82667838158683526473\n",
      "Iteration 3533 => Loss: 6.82666106613423462335\n",
      "Iteration 3534 => Loss: 6.82664375301489023684\n",
      "Iteration 3535 => Loss: 6.82662644222848857822\n",
      "Iteration 3536 => Loss: 6.82660913377471612051\n",
      "Iteration 3537 => Loss: 6.82659182765326466580\n",
      "Iteration 3538 => Loss: 6.82657452386381091713\n",
      "Iteration 3539 => Loss: 6.82655722240604934115\n",
      "Iteration 3540 => Loss: 6.82653992327965219999\n",
      "Iteration 3541 => Loss: 6.82652262648432373027\n",
      "Iteration 3542 => Loss: 6.82650533201972287145\n",
      "Iteration 3543 => Loss: 6.82648803988556984734\n",
      "Iteration 3544 => Loss: 6.82647075008152448561\n",
      "Iteration 3545 => Loss: 6.82645346260728480559\n",
      "Iteration 3546 => Loss: 6.82643617746253017486\n",
      "Iteration 3547 => Loss: 6.82641889464694884282\n",
      "Iteration 3548 => Loss: 6.82640161416023882879\n",
      "Iteration 3549 => Loss: 6.82638433600206440133\n",
      "Iteration 3550 => Loss: 6.82636706017212357978\n",
      "Iteration 3551 => Loss: 6.82634978667011083076\n",
      "Iteration 3552 => Loss: 6.82633251549570108097\n",
      "Iteration 3553 => Loss: 6.82631524664857991525\n",
      "Iteration 3554 => Loss: 6.82629798012843647115\n",
      "Iteration 3555 => Loss: 6.82628071593496077440\n",
      "Iteration 3556 => Loss: 6.82626345406783574532\n",
      "Iteration 3557 => Loss: 6.82624619452675140963\n",
      "Iteration 3558 => Loss: 6.82622893731138447038\n",
      "Iteration 3559 => Loss: 6.82621168242144005234\n",
      "Iteration 3560 => Loss: 6.82619442985658420042\n",
      "Iteration 3561 => Loss: 6.82617717961651759850\n",
      "Iteration 3562 => Loss: 6.82615993170091606146\n",
      "Iteration 3563 => Loss: 6.82614268610947760862\n",
      "Iteration 3564 => Loss: 6.82612544284188427213\n",
      "Iteration 3565 => Loss: 6.82610820189781630773\n",
      "Iteration 3566 => Loss: 6.82609096327696907025\n",
      "Iteration 3567 => Loss: 6.82607372697903169723\n",
      "Iteration 3568 => Loss: 6.82605649300368089172\n",
      "Iteration 3569 => Loss: 6.82603926135061644942\n",
      "Iteration 3570 => Loss: 6.82602203201951951428\n",
      "Iteration 3571 => Loss: 6.82600480501007034206\n",
      "Iteration 3572 => Loss: 6.82598758032196517576\n",
      "Iteration 3573 => Loss: 6.82597035795488604748\n",
      "Iteration 3574 => Loss: 6.82595313790852653568\n",
      "Iteration 3575 => Loss: 6.82593592018256245524\n",
      "Iteration 3576 => Loss: 6.82591870477669182549\n",
      "Iteration 3577 => Loss: 6.82590149169059667855\n",
      "Iteration 3578 => Loss: 6.82588428092397148106\n",
      "Iteration 3579 => Loss: 6.82586707247649382424\n",
      "Iteration 3580 => Loss: 6.82584986634785995108\n",
      "Iteration 3581 => Loss: 6.82583266253774922916\n",
      "Iteration 3582 => Loss: 6.82581546104585168422\n",
      "Iteration 3583 => Loss: 6.82579826187186355924\n",
      "Iteration 3584 => Loss: 6.82578106501546244544\n",
      "Iteration 3585 => Loss: 6.82576387047633659222\n",
      "Iteration 3586 => Loss: 6.82574667825417691347\n",
      "Iteration 3587 => Loss: 6.82572948834867254675\n",
      "Iteration 3588 => Loss: 6.82571230075950552418\n",
      "Iteration 3589 => Loss: 6.82569511548637120057\n",
      "Iteration 3590 => Loss: 6.82567793252895604894\n",
      "Iteration 3591 => Loss: 6.82566075188693943687\n",
      "Iteration 3592 => Loss: 6.82564357356001849553\n",
      "Iteration 3593 => Loss: 6.82562639754787792157\n",
      "Iteration 3594 => Loss: 6.82560922385020774072\n",
      "Iteration 3595 => Loss: 6.82559205246669531419\n",
      "Iteration 3596 => Loss: 6.82557488339702622682\n",
      "Iteration 3597 => Loss: 6.82555771664088695161\n",
      "Iteration 3598 => Loss: 6.82554055219797728427\n",
      "Iteration 3599 => Loss: 6.82552339006797037513\n",
      "Iteration 3600 => Loss: 6.82550623025056779625\n",
      "Iteration 3601 => Loss: 6.82548907274544713886\n",
      "Iteration 3602 => Loss: 6.82547191755230286958\n",
      "Iteration 3603 => Loss: 6.82545476467082057326\n",
      "Iteration 3604 => Loss: 6.82543761410069294016\n",
      "Iteration 3605 => Loss: 6.82542046584160821965\n",
      "Iteration 3606 => Loss: 6.82540331989325288475\n",
      "Iteration 3607 => Loss: 6.82538617625530896760\n",
      "Iteration 3608 => Loss: 6.82536903492747892841\n",
      "Iteration 3609 => Loss: 6.82535189590944035842\n",
      "Iteration 3610 => Loss: 6.82533475920088594791\n",
      "Iteration 3611 => Loss: 6.82531762480150572259\n",
      "Iteration 3612 => Loss: 6.82530049271098970820\n",
      "Iteration 3613 => Loss: 6.82528336292901727234\n",
      "Iteration 3614 => Loss: 6.82526623545528732251\n",
      "Iteration 3615 => Loss: 6.82524911028948721992\n",
      "Iteration 3616 => Loss: 6.82523198743130521393\n",
      "Iteration 3617 => Loss: 6.82521486688042511304\n",
      "Iteration 3618 => Loss: 6.82519774863654227204\n",
      "Iteration 3619 => Loss: 6.82518063269934582848\n",
      "Iteration 3620 => Loss: 6.82516351906852847264\n",
      "Iteration 3621 => Loss: 6.82514640774376957211\n",
      "Iteration 3622 => Loss: 6.82512929872476181714\n",
      "Iteration 3623 => Loss: 6.82511219201119168076\n",
      "Iteration 3624 => Loss: 6.82509508760275807049\n",
      "Iteration 3625 => Loss: 6.82507798549914035391\n",
      "Iteration 3626 => Loss: 6.82506088570003477400\n",
      "Iteration 3627 => Loss: 6.82504378820513135651\n",
      "Iteration 3628 => Loss: 6.82502669301411302172\n",
      "Iteration 3629 => Loss: 6.82500960012667690080\n",
      "Iteration 3630 => Loss: 6.82499250954250502588\n",
      "Iteration 3631 => Loss: 6.82497542126129186357\n",
      "Iteration 3632 => Loss: 6.82495833528272388691\n",
      "Iteration 3633 => Loss: 6.82494125160649911521\n",
      "Iteration 3634 => Loss: 6.82492417023229336337\n",
      "Iteration 3635 => Loss: 6.82490709115981086796\n",
      "Iteration 3636 => Loss: 6.82489001438872922023\n",
      "Iteration 3637 => Loss: 6.82487293991874555132\n",
      "Iteration 3638 => Loss: 6.82485586774955077516\n",
      "Iteration 3639 => Loss: 6.82483879788083136475\n",
      "Iteration 3640 => Loss: 6.82482173031227823401\n",
      "Iteration 3641 => Loss: 6.82480466504358229685\n",
      "Iteration 3642 => Loss: 6.82478760207442292085\n",
      "Iteration 3643 => Loss: 6.82477054140450700714\n",
      "Iteration 3644 => Loss: 6.82475348303351925239\n",
      "Iteration 3645 => Loss: 6.82473642696114879413\n",
      "Iteration 3646 => Loss: 6.82471937318708388176\n",
      "Iteration 3647 => Loss: 6.82470232171101454099\n",
      "Iteration 3648 => Loss: 6.82468527253263435028\n",
      "Iteration 3649 => Loss: 6.82466822565163333536\n",
      "Iteration 3650 => Loss: 6.82465118106769708106\n",
      "Iteration 3651 => Loss: 6.82463413878052449490\n",
      "Iteration 3652 => Loss: 6.82461709878980293809\n",
      "Iteration 3653 => Loss: 6.82460006109521444273\n",
      "Iteration 3654 => Loss: 6.82458302569645969271\n",
      "Iteration 3655 => Loss: 6.82456599259322516104\n",
      "Iteration 3656 => Loss: 6.82454896178520709071\n",
      "Iteration 3657 => Loss: 6.82453193327209195473\n",
      "Iteration 3658 => Loss: 6.82451490705356977884\n",
      "Iteration 3659 => Loss: 6.82449788312932703604\n",
      "Iteration 3660 => Loss: 6.82448086149906085751\n",
      "Iteration 3661 => Loss: 6.82446384216245682808\n",
      "Iteration 3662 => Loss: 6.82444682511921563162\n",
      "Iteration 3663 => Loss: 6.82442981036902551750\n",
      "Iteration 3664 => Loss: 6.82441279791156940604\n",
      "Iteration 3665 => Loss: 6.82439578774654354021\n",
      "Iteration 3666 => Loss: 6.82437877987363705756\n",
      "Iteration 3667 => Loss: 6.82436177429254620108\n",
      "Iteration 3668 => Loss: 6.82434477100296188468\n",
      "Iteration 3669 => Loss: 6.82432777000457058136\n",
      "Iteration 3670 => Loss: 6.82431077129705965234\n",
      "Iteration 3671 => Loss: 6.82429377488012978148\n",
      "Iteration 3672 => Loss: 6.82427678075346744180\n",
      "Iteration 3673 => Loss: 6.82425978891675999449\n",
      "Iteration 3674 => Loss: 6.82424279936970723526\n",
      "Iteration 3675 => Loss: 6.82422581211199830165\n",
      "Iteration 3676 => Loss: 6.82420882714332233121\n",
      "Iteration 3677 => Loss: 6.82419184446337734329\n",
      "Iteration 3678 => Loss: 6.82417486407184537001\n",
      "Iteration 3679 => Loss: 6.82415788596842887159\n",
      "Iteration 3680 => Loss: 6.82414091015280277475\n",
      "Iteration 3681 => Loss: 6.82412393662467220423\n",
      "Iteration 3682 => Loss: 6.82410696538373073849\n",
      "Iteration 3683 => Loss: 6.82408999642966396237\n",
      "Iteration 3684 => Loss: 6.82407302976215657253\n",
      "Iteration 3685 => Loss: 6.82405606538091813462\n",
      "Iteration 3686 => Loss: 6.82403910328563156895\n",
      "Iteration 3687 => Loss: 6.82402214347598778943\n",
      "Iteration 3688 => Loss: 6.82400518595167682179\n",
      "Iteration 3689 => Loss: 6.82398823071239402083\n",
      "Iteration 3690 => Loss: 6.82397127775783296499\n",
      "Iteration 3691 => Loss: 6.82395432708767835095\n",
      "Iteration 3692 => Loss: 6.82393737870163441528\n",
      "Iteration 3693 => Loss: 6.82392043259938319011\n",
      "Iteration 3694 => Loss: 6.82390348878062535931\n",
      "Iteration 3695 => Loss: 6.82388654724504739590\n",
      "Iteration 3696 => Loss: 6.82386960799233666108\n",
      "Iteration 3697 => Loss: 6.82385267102219206237\n",
      "Iteration 3698 => Loss: 6.82383573633431250727\n",
      "Iteration 3699 => Loss: 6.82381880392837913973\n",
      "Iteration 3700 => Loss: 6.82380187380409175546\n",
      "Iteration 3701 => Loss: 6.82378494596113416293\n",
      "Iteration 3702 => Loss: 6.82376802039921148690\n",
      "Iteration 3703 => Loss: 6.82375109711800487133\n",
      "Iteration 3704 => Loss: 6.82373417611721766463\n",
      "Iteration 3705 => Loss: 6.82371725739652568166\n",
      "Iteration 3706 => Loss: 6.82370034095563848808\n",
      "Iteration 3707 => Loss: 6.82368342679424344510\n",
      "Iteration 3708 => Loss: 6.82366651491203590751\n",
      "Iteration 3709 => Loss: 6.82364960530870678923\n",
      "Iteration 3710 => Loss: 6.82363269798394167509\n",
      "Iteration 3711 => Loss: 6.82361579293743769625\n",
      "Iteration 3712 => Loss: 6.82359889016889997748\n",
      "Iteration 3713 => Loss: 6.82358198967800344548\n",
      "Iteration 3714 => Loss: 6.82356509146445411318\n",
      "Iteration 3715 => Loss: 6.82354819552793401272\n",
      "Iteration 3716 => Loss: 6.82353130186814915703\n",
      "Iteration 3717 => Loss: 6.82351441048478513096\n",
      "Iteration 3718 => Loss: 6.82349752137753551295\n",
      "Iteration 3719 => Loss: 6.82348063454609210510\n",
      "Iteration 3720 => Loss: 6.82346374999015736762\n",
      "Iteration 3721 => Loss: 6.82344686770940711540\n",
      "Iteration 3722 => Loss: 6.82342998770355890770\n",
      "Iteration 3723 => Loss: 6.82341310997227612489\n",
      "Iteration 3724 => Loss: 6.82339623451528343168\n",
      "Iteration 3725 => Loss: 6.82337936133225753110\n",
      "Iteration 3726 => Loss: 6.82336249042289022526\n",
      "Iteration 3727 => Loss: 6.82334562178688219802\n",
      "Iteration 3728 => Loss: 6.82332875542392169876\n",
      "Iteration 3729 => Loss: 6.82331189133370763500\n",
      "Iteration 3730 => Loss: 6.82329502951592914428\n",
      "Iteration 3731 => Loss: 6.82327816997028246959\n",
      "Iteration 3732 => Loss: 6.82326131269646030120\n",
      "Iteration 3733 => Loss: 6.82324445769416154661\n",
      "Iteration 3734 => Loss: 6.82322760496306912614\n",
      "Iteration 3735 => Loss: 6.82321075450288905273\n",
      "Iteration 3736 => Loss: 6.82319390631330158214\n",
      "Iteration 3737 => Loss: 6.82317706039401183915\n",
      "Iteration 3738 => Loss: 6.82316021674471961944\n",
      "Iteration 3739 => Loss: 6.82314337536509984972\n",
      "Iteration 3740 => Loss: 6.82312653625486120745\n",
      "Iteration 3741 => Loss: 6.82310969941369638292\n",
      "Iteration 3742 => Loss: 6.82309286484129007277\n",
      "Iteration 3743 => Loss: 6.82307603253735184268\n",
      "Iteration 3744 => Loss: 6.82305920250156727747\n",
      "Iteration 3745 => Loss: 6.82304237473362640287\n",
      "Iteration 3746 => Loss: 6.82302554923323345548\n",
      "Iteration 3747 => Loss: 6.82300872600007757285\n",
      "Iteration 3748 => Loss: 6.82299190503385055706\n",
      "Iteration 3749 => Loss: 6.82297508633425398017\n",
      "Iteration 3750 => Loss: 6.82295826990096809794\n",
      "Iteration 3751 => Loss: 6.82294145573371135782\n",
      "Iteration 3752 => Loss: 6.82292464383216312740\n",
      "Iteration 3753 => Loss: 6.82290783419601165605\n",
      "Iteration 3754 => Loss: 6.82289102682495851582\n",
      "Iteration 3755 => Loss: 6.82287422171871149601\n",
      "Iteration 3756 => Loss: 6.82285741887694641150\n",
      "Iteration 3757 => Loss: 6.82284061829937016341\n",
      "Iteration 3758 => Loss: 6.82282381998566656023\n",
      "Iteration 3759 => Loss: 6.82280702393554161489\n",
      "Iteration 3760 => Loss: 6.82279023014868535313\n",
      "Iteration 3761 => Loss: 6.82277343862479135339\n",
      "Iteration 3762 => Loss: 6.82275664936355941137\n",
      "Iteration 3763 => Loss: 6.82273986236467955280\n",
      "Iteration 3764 => Loss: 6.82272307762785334972\n",
      "Iteration 3765 => Loss: 6.82270629515276461063\n",
      "Iteration 3766 => Loss: 6.82268951493911401940\n",
      "Iteration 3767 => Loss: 6.82267273698660758896\n",
      "Iteration 3768 => Loss: 6.82265596129492823962\n",
      "Iteration 3769 => Loss: 6.82263918786377487891\n",
      "Iteration 3770 => Loss: 6.82262241669284286161\n",
      "Iteration 3771 => Loss: 6.82260564778182487800\n",
      "Iteration 3772 => Loss: 6.82258888113042072376\n",
      "Iteration 3773 => Loss: 6.82257211673832575372\n",
      "Iteration 3774 => Loss: 6.82255535460522910540\n",
      "Iteration 3775 => Loss: 6.82253859473083501541\n",
      "Iteration 3776 => Loss: 6.82252183711483528583\n",
      "Iteration 3777 => Loss: 6.82250508175692349511\n",
      "Iteration 3778 => Loss: 6.82248832865679766257\n",
      "Iteration 3779 => Loss: 6.82247157781415491939\n",
      "Iteration 3780 => Loss: 6.82245482922868706765\n",
      "Iteration 3781 => Loss: 6.82243808290009656758\n",
      "Iteration 3782 => Loss: 6.82242133882807433309\n",
      "Iteration 3783 => Loss: 6.82240459701231127809\n",
      "Iteration 3784 => Loss: 6.82238785745251608006\n",
      "Iteration 3785 => Loss: 6.82237112014837876472\n",
      "Iteration 3786 => Loss: 6.82235438509958846964\n",
      "Iteration 3787 => Loss: 6.82233765230584765504\n",
      "Iteration 3788 => Loss: 6.82232092176686055751\n",
      "Iteration 3789 => Loss: 6.82230419348230743282\n",
      "Iteration 3790 => Loss: 6.82228746745189784662\n",
      "Iteration 3791 => Loss: 6.82227074367531027832\n",
      "Iteration 3792 => Loss: 6.82225402215226228719\n",
      "Iteration 3793 => Loss: 6.82223730288244301079\n",
      "Iteration 3794 => Loss: 6.82222058586554247483\n",
      "Iteration 3795 => Loss: 6.82220387110126136321\n",
      "Iteration 3796 => Loss: 6.82218715858929947160\n",
      "Iteration 3797 => Loss: 6.82217044832934238485\n",
      "Iteration 3798 => Loss: 6.82215374032110144498\n",
      "Iteration 3799 => Loss: 6.82213703456425779592\n",
      "Iteration 3800 => Loss: 6.82212033105852011516\n",
      "Iteration 3801 => Loss: 6.82210362980358464569\n",
      "Iteration 3802 => Loss: 6.82208693079914141322\n",
      "Iteration 3803 => Loss: 6.82207023404489287799\n",
      "Iteration 3804 => Loss: 6.82205353954052906573\n",
      "Iteration 3805 => Loss: 6.82203684728575066032\n",
      "Iteration 3806 => Loss: 6.82202015728026101016\n",
      "Iteration 3807 => Loss: 6.82200346952374570009\n",
      "Iteration 3808 => Loss: 6.82198678401590896669\n",
      "Iteration 3809 => Loss: 6.82197010075644971749\n",
      "Iteration 3810 => Loss: 6.82195341974505353733\n",
      "Iteration 3811 => Loss: 6.82193674098142732731\n",
      "Iteration 3812 => Loss: 6.82192006446526466590\n",
      "Iteration 3813 => Loss: 6.82190339019626623696\n",
      "Iteration 3814 => Loss: 6.82188671817412917164\n",
      "Iteration 3815 => Loss: 6.82187004839853727844\n",
      "Iteration 3816 => Loss: 6.82185338086921255751\n",
      "Iteration 3817 => Loss: 6.82183671558583348826\n",
      "Iteration 3818 => Loss: 6.82182005254809986639\n",
      "Iteration 3819 => Loss: 6.82180339175571859300\n",
      "Iteration 3820 => Loss: 6.82178673320837347660\n",
      "Iteration 3821 => Loss: 6.82177007690577585919\n",
      "Iteration 3822 => Loss: 6.82175342284761132561\n",
      "Iteration 3823 => Loss: 6.82173677103358055973\n",
      "Iteration 3824 => Loss: 6.82172012146338868632\n",
      "Iteration 3825 => Loss: 6.82170347413672129022\n",
      "Iteration 3826 => Loss: 6.82168682905328793709\n",
      "Iteration 3827 => Loss: 6.82167018621278309354\n",
      "Iteration 3828 => Loss: 6.82165354561489589713\n",
      "Iteration 3829 => Loss: 6.82163690725933324899\n",
      "Iteration 3830 => Loss: 6.82162027114578695119\n",
      "Iteration 3831 => Loss: 6.82160363727396568123\n",
      "Iteration 3832 => Loss: 6.82158700564354969487\n",
      "Iteration 3833 => Loss: 6.82157037625425122229\n",
      "Iteration 3834 => Loss: 6.82155374910577272374\n",
      "Iteration 3835 => Loss: 6.82153712419779179044\n",
      "Iteration 3836 => Loss: 6.82152050153002864619\n",
      "Iteration 3837 => Loss: 6.82150388110216621129\n",
      "Iteration 3838 => Loss: 6.82148726291390605780\n",
      "Iteration 3839 => Loss: 6.82147064696494798142\n",
      "Iteration 3840 => Loss: 6.82145403325499177782\n",
      "Iteration 3841 => Loss: 6.82143742178373635454\n",
      "Iteration 3842 => Loss: 6.82142081255087351366\n",
      "Iteration 3843 => Loss: 6.82140420555611459719\n",
      "Iteration 3844 => Loss: 6.82138760079913897272\n",
      "Iteration 3845 => Loss: 6.82137099827966153498\n",
      "Iteration 3846 => Loss: 6.82135439799737675060\n",
      "Iteration 3847 => Loss: 6.82133779995197730983\n",
      "Iteration 3848 => Loss: 6.82132120414317011381\n",
      "Iteration 3849 => Loss: 6.82130461057064074737\n",
      "Iteration 3850 => Loss: 6.82128801923410588159\n",
      "Iteration 3851 => Loss: 6.82127143013324932497\n",
      "Iteration 3852 => Loss: 6.82125484326778241950\n",
      "Iteration 3853 => Loss: 6.82123825863738719733\n",
      "Iteration 3854 => Loss: 6.82122167624177322409\n",
      "Iteration 3855 => Loss: 6.82120509608064384821\n",
      "Iteration 3856 => Loss: 6.82118851815369531266\n",
      "Iteration 3857 => Loss: 6.82117194246061764318\n",
      "Iteration 3858 => Loss: 6.82115536900111774088\n",
      "Iteration 3859 => Loss: 6.82113879777489184875\n",
      "Iteration 3860 => Loss: 6.82112222878163620976\n",
      "Iteration 3861 => Loss: 6.82110566202106394229\n",
      "Iteration 3862 => Loss: 6.82108909749285263757\n",
      "Iteration 3863 => Loss: 6.82107253519672074304\n",
      "Iteration 3864 => Loss: 6.82105597513235650808\n",
      "Iteration 3865 => Loss: 6.82103941729946150474\n",
      "Iteration 3866 => Loss: 6.82102286169773375235\n",
      "Iteration 3867 => Loss: 6.82100630832688015204\n",
      "Iteration 3868 => Loss: 6.82098975718659694678\n",
      "Iteration 3869 => Loss: 6.82097320827657238596\n",
      "Iteration 3870 => Loss: 6.82095666159652047611\n",
      "Iteration 3871 => Loss: 6.82094011714613390751\n",
      "Iteration 3872 => Loss: 6.82092357492510981132\n",
      "Iteration 3873 => Loss: 6.82090703493315597683\n",
      "Iteration 3874 => Loss: 6.82089049716996420614\n",
      "Iteration 3875 => Loss: 6.82087396163523962400\n",
      "Iteration 3876 => Loss: 6.82085742832867847341\n",
      "Iteration 3877 => Loss: 6.82084089724998055004\n",
      "Iteration 3878 => Loss: 6.82082436839885009050\n",
      "Iteration 3879 => Loss: 6.82080784177498333776\n",
      "Iteration 3880 => Loss: 6.82079131737808008751\n",
      "Iteration 3881 => Loss: 6.82077479520784102363\n",
      "Iteration 3882 => Loss: 6.82075827526395794820\n",
      "Iteration 3883 => Loss: 6.82074175754614664413\n",
      "Iteration 3884 => Loss: 6.82072524205409980169\n",
      "Iteration 3885 => Loss: 6.82070872878750655843\n",
      "Iteration 3886 => Loss: 6.82069221774608980269\n",
      "Iteration 3887 => Loss: 6.82067570892953156658\n",
      "Iteration 3888 => Loss: 6.82065920233754052759\n",
      "Iteration 3889 => Loss: 6.82064269796981115235\n",
      "Iteration 3890 => Loss: 6.82062619582604678925\n",
      "Iteration 3891 => Loss: 6.82060969590594634582\n",
      "Iteration 3892 => Loss: 6.82059319820921228228\n",
      "Iteration 3893 => Loss: 6.82057670273554439433\n",
      "Iteration 3894 => Loss: 6.82056020948464514220\n",
      "Iteration 3895 => Loss: 6.82054371845620632797\n",
      "Iteration 3896 => Loss: 6.82052722964993662913\n",
      "Iteration 3897 => Loss: 6.82051074306554561133\n",
      "Iteration 3898 => Loss: 6.82049425870271353034\n",
      "Iteration 3899 => Loss: 6.82047777656115261635\n",
      "Iteration 3900 => Loss: 6.82046129664055911235\n",
      "Iteration 3901 => Loss: 6.82044481894063725491\n",
      "Iteration 3902 => Loss: 6.82042834346109394517\n",
      "Iteration 3903 => Loss: 6.82041187020161299159\n",
      "Iteration 3904 => Loss: 6.82039539916190928892\n",
      "Iteration 3905 => Loss: 6.82037893034167819195\n",
      "Iteration 3906 => Loss: 6.82036246374062393727\n",
      "Iteration 3907 => Loss: 6.82034599935844809693\n",
      "Iteration 3908 => Loss: 6.82032953719483803212\n",
      "Iteration 3909 => Loss: 6.82031307724951396665\n",
      "Iteration 3910 => Loss: 6.82029661952216947896\n",
      "Iteration 3911 => Loss: 6.82028016401250436473\n",
      "Iteration 3912 => Loss: 6.82026371072022108422\n",
      "Iteration 3913 => Loss: 6.82024725964501499220\n",
      "Iteration 3914 => Loss: 6.82023081078659920706\n",
      "Iteration 3915 => Loss: 6.82021436414466197817\n",
      "Iteration 3916 => Loss: 6.82019791971891109483\n",
      "Iteration 3917 => Loss: 6.82018147750905789906\n",
      "Iteration 3918 => Loss: 6.82016503751478708750\n",
      "Iteration 3919 => Loss: 6.82014859973580644947\n",
      "Iteration 3920 => Loss: 6.82013216417181400431\n",
      "Iteration 3921 => Loss: 6.82011573082252375855\n",
      "Iteration 3922 => Loss: 6.82009929968762129704\n",
      "Iteration 3923 => Loss: 6.82008287076681884997\n",
      "Iteration 3924 => Loss: 6.82006644405980999579\n",
      "Iteration 3925 => Loss: 6.82005001956630874105\n",
      "Iteration 3926 => Loss: 6.82003359728600688783\n",
      "Iteration 3927 => Loss: 6.82001717721861133725\n",
      "Iteration 3928 => Loss: 6.82000075936381566777\n",
      "Iteration 3929 => Loss: 6.81998434372132855685\n",
      "Iteration 3930 => Loss: 6.81996793029084713567\n",
      "Iteration 3931 => Loss: 6.81995151907208008168\n",
      "Iteration 3932 => Loss: 6.81993511006472807878\n",
      "Iteration 3933 => Loss: 6.81991870326848648176\n",
      "Iteration 3934 => Loss: 6.81990229868306752081\n",
      "Iteration 3935 => Loss: 6.81988589630816122167\n",
      "Iteration 3936 => Loss: 6.81986949614348780813\n",
      "Iteration 3937 => Loss: 6.81985309818873020049\n",
      "Iteration 3938 => Loss: 6.81983670244359796442\n",
      "Iteration 3939 => Loss: 6.81982030890778823107\n",
      "Iteration 3940 => Loss: 6.81980391758101678334\n",
      "Iteration 3941 => Loss: 6.81978752846297897605\n",
      "Iteration 3942 => Loss: 6.81977114155337371670\n",
      "Iteration 3943 => Loss: 6.81975475685190524189\n",
      "Iteration 3944 => Loss: 6.81973837435827068276\n",
      "Iteration 3945 => Loss: 6.81972199407218848677\n",
      "Iteration 3946 => Loss: 6.81970561599334601510\n",
      "Iteration 3947 => Loss: 6.81968924012144928071\n",
      "Iteration 3948 => Loss: 6.81967286645620518470\n",
      "Iteration 3949 => Loss: 6.81965649499731352279\n",
      "Iteration 3950 => Loss: 6.81964012574447409065\n",
      "Iteration 3951 => Loss: 6.81962375869739556578\n",
      "Iteration 3952 => Loss: 6.81960739385578396110\n",
      "Iteration 3953 => Loss: 6.81959103121932397329\n",
      "Iteration 3954 => Loss: 6.81957467078774115521\n",
      "Iteration 3955 => Loss: 6.81955831256072730895\n",
      "Iteration 3956 => Loss: 6.81954195653797956567\n",
      "Iteration 3957 => Loss: 6.81952560271920749102\n",
      "Iteration 3958 => Loss: 6.81950925110411887431\n",
      "Iteration 3959 => Loss: 6.81949290169240374127\n",
      "Iteration 3960 => Loss: 6.81947655448377965115\n",
      "Iteration 3961 => Loss: 6.81946020947794373512\n",
      "Iteration 3962 => Loss: 6.81944386667459312434\n",
      "Iteration 3963 => Loss: 6.81942752607344093718\n",
      "Iteration 3964 => Loss: 6.81941118767418341662\n",
      "Iteration 3965 => Loss: 6.81939485147653101649\n",
      "Iteration 3966 => Loss: 6.81937851748017287434\n",
      "Iteration 3967 => Loss: 6.81936218568483543123\n",
      "Iteration 3968 => Loss: 6.81934585609020071928\n",
      "Iteration 3969 => Loss: 6.81932952869597830414\n",
      "Iteration 3970 => Loss: 6.81931320350187952783\n",
      "Iteration 3971 => Loss: 6.81929688050759885698\n",
      "Iteration 3972 => Loss: 6.81928055971284052816\n",
      "Iteration 3973 => Loss: 6.81926424111731588340\n",
      "Iteration 3974 => Loss: 6.81924792472071938931\n",
      "Iteration 3975 => Loss: 6.81923161052275439431\n",
      "Iteration 3976 => Loss: 6.81921529852313934583\n",
      "Iteration 3977 => Loss: 6.81919898872155716418\n",
      "Iteration 3978 => Loss: 6.81918268111773162587\n",
      "Iteration 3979 => Loss: 6.81916637571135897389\n",
      "Iteration 3980 => Loss: 6.81915007250212834577\n",
      "Iteration 3981 => Loss: 6.81913377148976085351\n",
      "Iteration 3982 => Loss: 6.81911747267396251004\n",
      "Iteration 3983 => Loss: 6.81910117605442511746\n",
      "Iteration 3984 => Loss: 6.81908488163085735323\n",
      "Iteration 3985 => Loss: 6.81906858940296878302\n",
      "Iteration 3986 => Loss: 6.81905229937045742616\n",
      "Iteration 3987 => Loss: 6.81903601153302840743\n",
      "Iteration 3988 => Loss: 6.81901972589038418704\n",
      "Iteration 3989 => Loss: 6.81900344244223699519\n",
      "Iteration 3990 => Loss: 6.81898716118828129851\n",
      "Iteration 3991 => Loss: 6.81897088212823199171\n",
      "Iteration 3992 => Loss: 6.81895460526177732419\n",
      "Iteration 3993 => Loss: 6.81893833058864107244\n",
      "Iteration 3994 => Loss: 6.81892205810851415038\n",
      "Iteration 3995 => Loss: 6.81890578782110790002\n",
      "Iteration 3996 => Loss: 6.81888951972612389341\n",
      "Iteration 3997 => Loss: 6.81887325382325748535\n",
      "Iteration 3998 => Loss: 6.81885699011223156418\n",
      "Iteration 3999 => Loss: 6.81884072859274237288\n",
      "Iteration 4000 => Loss: 6.81882446926449592439\n",
      "Iteration 4001 => Loss: 6.81880821212719201441\n",
      "Iteration 4002 => Loss: 6.81879195718053665587\n",
      "Iteration 4003 => Loss: 6.81877570442423763808\n",
      "Iteration 4004 => Loss: 6.81875945385799919762\n",
      "Iteration 4005 => Loss: 6.81874320548153356469\n",
      "Iteration 4006 => Loss: 6.81872695929452543595\n",
      "Iteration 4007 => Loss: 6.81871071529669325884\n",
      "Iteration 4008 => Loss: 6.81869447348774837536\n",
      "Iteration 4009 => Loss: 6.81867823386738614033\n",
      "Iteration 4010 => Loss: 6.81866199643531167851\n",
      "Iteration 4011 => Loss: 6.81864576119123633191\n",
      "Iteration 4012 => Loss: 6.81862952813485367898\n",
      "Iteration 4013 => Loss: 6.81861329726589193712\n",
      "Iteration 4014 => Loss: 6.81859706858402159213\n",
      "Iteration 4015 => Loss: 6.81858084208898151957\n",
      "Iteration 4016 => Loss: 6.81856461778045908062\n",
      "Iteration 4017 => Loss: 6.81854839565816028824\n",
      "Iteration 4018 => Loss: 6.81853217572179737260\n",
      "Iteration 4019 => Loss: 6.81851595797107101760\n",
      "Iteration 4020 => Loss: 6.81849974240568723616\n",
      "Iteration 4021 => Loss: 6.81848352902535204123\n",
      "Iteration 4022 => Loss: 6.81846731782977677483\n",
      "Iteration 4023 => Loss: 6.81845110881865146268\n",
      "Iteration 4024 => Loss: 6.81843490199169810495\n",
      "Iteration 4025 => Loss: 6.81841869734861294461\n",
      "Iteration 4026 => Loss: 6.81840249488910732367\n",
      "Iteration 4027 => Loss: 6.81838629461288725508\n",
      "Iteration 4028 => Loss: 6.81837009651964987000\n",
      "Iteration 4029 => Loss: 6.81835390060910473409\n",
      "Iteration 4030 => Loss: 6.81833770688096585388\n",
      "Iteration 4031 => Loss: 6.81832151533492947237\n",
      "Iteration 4032 => Loss: 6.81830532597071048428\n",
      "Iteration 4033 => Loss: 6.81828913878800424442\n",
      "Iteration 4034 => Loss: 6.81827295378652298297\n",
      "Iteration 4035 => Loss: 6.81825677096597626559\n",
      "Iteration 4036 => Loss: 6.81824059032606388797\n",
      "Iteration 4037 => Loss: 6.81822441186648742217\n",
      "Iteration 4038 => Loss: 6.81820823558696442745\n",
      "Iteration 4039 => Loss: 6.81819206148719558769\n",
      "Iteration 4040 => Loss: 6.81817588956688958035\n",
      "Iteration 4041 => Loss: 6.81815971982575241839\n",
      "Iteration 4042 => Loss: 6.81814355226348656203\n",
      "Iteration 4043 => Loss: 6.81812738687980424146\n",
      "Iteration 4044 => Loss: 6.81811122367440791692\n",
      "Iteration 4045 => Loss: 6.81809506264700004863\n",
      "Iteration 4046 => Loss: 6.81807890379729464314\n",
      "Iteration 4047 => Loss: 6.81806274712499416069\n",
      "Iteration 4048 => Loss: 6.81804659262980816692\n",
      "Iteration 4049 => Loss: 6.81803044031144267478\n",
      "Iteration 4050 => Loss: 6.81801429016959659180\n",
      "Iteration 4051 => Loss: 6.81799814220399102993\n",
      "Iteration 4052 => Loss: 6.81798199641432223217\n",
      "Iteration 4053 => Loss: 6.81796585280030154053\n",
      "Iteration 4054 => Loss: 6.81794971136163319159\n",
      "Iteration 4055 => Loss: 6.81793357209802408647\n",
      "Iteration 4056 => Loss: 6.81791743500918645537\n",
      "Iteration 4057 => Loss: 6.81790130009481742945\n",
      "Iteration 4058 => Loss: 6.81788516735463012708\n",
      "Iteration 4059 => Loss: 6.81786903678833144937\n",
      "Iteration 4060 => Loss: 6.81785290839563273835\n",
      "Iteration 4061 => Loss: 6.81783678217622846063\n",
      "Iteration 4062 => Loss: 6.81782065812983617548\n",
      "Iteration 4063 => Loss: 6.81780453625616278401\n",
      "Iteration 4064 => Loss: 6.81778841655491252283\n",
      "Iteration 4065 => Loss: 6.81777229902579229304\n",
      "Iteration 4066 => Loss: 6.81775618366850455487\n",
      "Iteration 4067 => Loss: 6.81774007048277042031\n",
      "Iteration 4068 => Loss: 6.81772395946828524416\n",
      "Iteration 4069 => Loss: 6.81770785062476125660\n",
      "Iteration 4070 => Loss: 6.81769174395190624693\n",
      "Iteration 4071 => Loss: 6.81767563944942978083\n",
      "Iteration 4072 => Loss: 6.81765953711702721307\n",
      "Iteration 4073 => Loss: 6.81764343695442587290\n",
      "Iteration 4074 => Loss: 6.81762733896132111511\n",
      "Iteration 4075 => Loss: 6.81761124313741628811\n",
      "Iteration 4076 => Loss: 6.81759514948242539845\n",
      "Iteration 4077 => Loss: 6.81757905799605623542\n",
      "Iteration 4078 => Loss: 6.81756296867801925288\n",
      "Iteration 4079 => Loss: 6.81754688152802135193\n",
      "Iteration 4080 => Loss: 6.81753079654576321644\n",
      "Iteration 4081 => Loss: 6.81751471373095530026\n",
      "Iteration 4082 => Loss: 6.81749863308331960354\n",
      "Iteration 4083 => Loss: 6.81748255460253904658\n",
      "Iteration 4084 => Loss: 6.81746647828833918226\n",
      "Iteration 4085 => Loss: 6.81745040414043046439\n",
      "Iteration 4086 => Loss: 6.81743433215850735962\n",
      "Iteration 4087 => Loss: 6.81741826234228387449\n",
      "Iteration 4088 => Loss: 6.81740219469147579190\n",
      "Iteration 4089 => Loss: 6.81738612920577846666\n",
      "Iteration 4090 => Loss: 6.81737006588491478709\n",
      "Iteration 4091 => Loss: 6.81735400472858099619\n",
      "Iteration 4092 => Loss: 6.81733794573648488324\n",
      "Iteration 4093 => Loss: 6.81732188890834134298\n",
      "Iteration 4094 => Loss: 6.81730583424385816471\n",
      "Iteration 4095 => Loss: 6.81728978174273514412\n",
      "Iteration 4096 => Loss: 6.81727373140470316315\n",
      "Iteration 4097 => Loss: 6.81725768322944247757\n",
      "Iteration 4098 => Loss: 6.81724163721668219296\n",
      "Iteration 4099 => Loss: 6.81722559336612210501\n",
      "Iteration 4100 => Loss: 6.81720955167746645031\n",
      "Iteration 4101 => Loss: 6.81719351215043634085\n",
      "Iteration 4102 => Loss: 6.81717747478473068412\n",
      "Iteration 4103 => Loss: 6.81716143958005993397\n",
      "Iteration 4104 => Loss: 6.81714540653613632060\n",
      "Iteration 4105 => Loss: 6.81712937565266585693\n",
      "Iteration 4106 => Loss: 6.81711334692935455593\n",
      "Iteration 4107 => Loss: 6.81709732036591908866\n",
      "Iteration 4108 => Loss: 6.81708129596206635625\n",
      "Iteration 4109 => Loss: 6.81706527371750148347\n",
      "Iteration 4110 => Loss: 6.81704925363193492416\n",
      "Iteration 4111 => Loss: 6.81703323570507357942\n",
      "Iteration 4112 => Loss: 6.81701721993663589672\n",
      "Iteration 4113 => Loss: 6.81700120632631545448\n",
      "Iteration 4114 => Loss: 6.81698519487384047011\n",
      "Iteration 4115 => Loss: 6.81696918557889741663\n",
      "Iteration 4116 => Loss: 6.81695317844121895234\n",
      "Iteration 4117 => Loss: 6.81693717346050043204\n",
      "Iteration 4118 => Loss: 6.81692117063644875685\n",
      "Iteration 4119 => Loss: 6.81690516996878326239\n",
      "Iteration 4120 => Loss: 6.81688917145721084978\n",
      "Iteration 4121 => Loss: 6.81687317510144374921\n",
      "Iteration 4122 => Loss: 6.81685718090117198642\n",
      "Iteration 4123 => Loss: 6.81684118885613443695\n",
      "Iteration 4124 => Loss: 6.81682519896601313292\n",
      "Iteration 4125 => Loss: 6.81680921123054517352\n",
      "Iteration 4126 => Loss: 6.81679322564941347906\n",
      "Iteration 4127 => Loss: 6.81677724222234893148\n",
      "Iteration 4128 => Loss: 6.81676126094904688557\n",
      "Iteration 4129 => Loss: 6.81674528182922312425\n",
      "Iteration 4130 => Loss: 6.81672930486258987770\n",
      "Iteration 4131 => Loss: 6.81671333004885315887\n",
      "Iteration 4132 => Loss: 6.81669735738772342160\n",
      "Iteration 4133 => Loss: 6.81668138687891023153\n",
      "Iteration 4134 => Loss: 6.81666541852212493069\n",
      "Iteration 4135 => Loss: 6.81664945231707619655\n",
      "Iteration 4136 => Loss: 6.81663348826347625931\n",
      "Iteration 4137 => Loss: 6.81661752636102935554\n",
      "Iteration 4138 => Loss: 6.81660156660945748541\n",
      "Iteration 4139 => Loss: 6.81658560900846044461\n",
      "Iteration 4140 => Loss: 6.81656965355775135151\n",
      "Iteration 4141 => Loss: 6.81655370025704243631\n",
      "Iteration 4142 => Loss: 6.81653774910603527104\n",
      "Iteration 4143 => Loss: 6.81652180010445363223\n",
      "Iteration 4144 => Loss: 6.81650585325199998010\n",
      "Iteration 4145 => Loss: 6.81648990854838299214\n",
      "Iteration 4146 => Loss: 6.81647396599331667488\n",
      "Iteration 4147 => Loss: 6.81645802558651503489\n",
      "Iteration 4148 => Loss: 6.81644208732768230874\n",
      "Iteration 4149 => Loss: 6.81642615121652628574\n",
      "Iteration 4150 => Loss: 6.81641021725276807786\n",
      "Iteration 4151 => Loss: 6.81639428543610659261\n",
      "Iteration 4152 => Loss: 6.81637835576626915923\n",
      "Iteration 4153 => Loss: 6.81636242824294313891\n",
      "Iteration 4154 => Loss: 6.81634650286586296630\n",
      "Iteration 4155 => Loss: 6.81633057963472399621\n",
      "Iteration 4156 => Loss: 6.81631465854924556425\n",
      "Iteration 4157 => Loss: 6.81629873960912568975\n",
      "Iteration 4158 => Loss: 6.81628282281408637289\n",
      "Iteration 4159 => Loss: 6.81626690816384606109\n",
      "Iteration 4160 => Loss: 6.81625099565810099733\n",
      "Iteration 4161 => Loss: 6.81623508529656696453\n",
      "Iteration 4162 => Loss: 6.81621917707895264016\n",
      "Iteration 4163 => Loss: 6.81620327100497735984\n",
      "Iteration 4164 => Loss: 6.81618736707434091926\n",
      "Iteration 4165 => Loss: 6.81617146528676620676\n",
      "Iteration 4166 => Loss: 6.81615556564195035349\n",
      "Iteration 4167 => Loss: 6.81613966813962424141\n",
      "Iteration 4168 => Loss: 6.81612377277947256715\n",
      "Iteration 4169 => Loss: 6.81610787956123598264\n",
      "Iteration 4170 => Loss: 6.81609198848460628994\n",
      "Iteration 4171 => Loss: 6.81607609954930016016\n",
      "Iteration 4172 => Loss: 6.81606021275503159984\n",
      "Iteration 4173 => Loss: 6.81604432810150662192\n",
      "Iteration 4174 => Loss: 6.81602844558844100931\n",
      "Iteration 4175 => Loss: 6.81601256521554610401\n",
      "Iteration 4176 => Loss: 6.81599668698253235988\n",
      "Iteration 4177 => Loss: 6.81598081088910578984\n",
      "Iteration 4178 => Loss: 6.81596493693499816402\n",
      "Iteration 4179 => Loss: 6.81594906511989506726\n",
      "Iteration 4180 => Loss: 6.81593319544352649331\n",
      "Iteration 4181 => Loss: 6.81591732790559756694\n",
      "Iteration 4182 => Loss: 6.81590146250581430110\n",
      "Iteration 4183 => Loss: 6.81588559924389869593\n",
      "Iteration 4184 => Loss: 6.81586973811956031710\n",
      "Iteration 4185 => Loss: 6.81585387913250251302\n",
      "Iteration 4186 => Loss: 6.81583802228245971833\n",
      "Iteration 4187 => Loss: 6.81582216756911485334\n",
      "Iteration 4188 => Loss: 6.81580631499220146452\n",
      "Iteration 4189 => Loss: 6.81579046455142378846\n",
      "Iteration 4190 => Loss: 6.81577461624648783811\n",
      "Iteration 4191 => Loss: 6.81575877007711827815\n",
      "Iteration 4192 => Loss: 6.81574292604301845699\n",
      "Iteration 4193 => Loss: 6.81572708414390238119\n",
      "Iteration 4194 => Loss: 6.81571124437948405728\n",
      "Iteration 4195 => Loss: 6.81569540674947926817\n",
      "Iteration 4196 => Loss: 6.81567957125359136228\n",
      "Iteration 4197 => Loss: 6.81566373789153789886\n",
      "Iteration 4198 => Loss: 6.81564790666303732536\n",
      "Iteration 4199 => Loss: 6.81563207756779387836\n",
      "Iteration 4200 => Loss: 6.81561625060551623534\n",
      "Iteration 4201 => Loss: 6.81560042577592817281\n",
      "Iteration 4202 => Loss: 6.81558460307873659190\n",
      "Iteration 4203 => Loss: 6.81556878251365194643\n",
      "Iteration 4204 => Loss: 6.81555296408039001932\n",
      "Iteration 4205 => Loss: 6.81553714777867281072\n",
      "Iteration 4206 => Loss: 6.81552133360818857000\n",
      "Iteration 4207 => Loss: 6.81550552156867350817\n",
      "Iteration 4208 => Loss: 6.81548971165983363818\n",
      "Iteration 4209 => Loss: 6.81547390388137497297\n",
      "Iteration 4210 => Loss: 6.81545809823301507180\n",
      "Iteration 4211 => Loss: 6.81544229471447238211\n",
      "Iteration 4212 => Loss: 6.81542649332545469321\n",
      "Iteration 4213 => Loss: 6.81541069406567157074\n",
      "Iteration 4214 => Loss: 6.81539489693483968580\n",
      "Iteration 4215 => Loss: 6.81537910193268015036\n",
      "Iteration 4216 => Loss: 6.81536330905889453646\n",
      "Iteration 4217 => Loss: 6.81534751831319240978\n",
      "Iteration 4218 => Loss: 6.81533172969530287588\n",
      "Iteration 4219 => Loss: 6.81531594320492750683\n",
      "Iteration 4220 => Loss: 6.81530015884178741459\n",
      "Iteration 4221 => Loss: 6.81528437660558772393\n",
      "Iteration 4222 => Loss: 6.81526859649604244140\n",
      "Iteration 4223 => Loss: 6.81525281851287356716\n",
      "Iteration 4224 => Loss: 6.81523704265579066686\n",
      "Iteration 4225 => Loss: 6.81522126892450064162\n",
      "Iteration 4226 => Loss: 6.81520549731871838617\n",
      "Iteration 4227 => Loss: 6.81518972783817034156\n",
      "Iteration 4228 => Loss: 6.81517396048255541530\n",
      "Iteration 4229 => Loss: 6.81515819525160271297\n",
      "Iteration 4230 => Loss: 6.81514243214500581303\n",
      "Iteration 4231 => Loss: 6.81512667116248938015\n",
      "Iteration 4232 => Loss: 6.81511091230376386818\n",
      "Iteration 4233 => Loss: 6.81509515556855216545\n",
      "Iteration 4234 => Loss: 6.81507940095656206125\n",
      "Iteration 4235 => Loss: 6.81506364846750223307\n",
      "Iteration 4236 => Loss: 6.81504789810109556925\n",
      "Iteration 4237 => Loss: 6.81503214985705252360\n",
      "Iteration 4238 => Loss: 6.81501640373508266180\n",
      "Iteration 4239 => Loss: 6.81500065973490176674\n",
      "Iteration 4240 => Loss: 6.81498491785623272676\n",
      "Iteration 4241 => Loss: 6.81496917809878333117\n",
      "Iteration 4242 => Loss: 6.81495344046226492196\n",
      "Iteration 4243 => Loss: 6.81493770494639594659\n",
      "Iteration 4244 => Loss: 6.81492197155088419436\n",
      "Iteration 4245 => Loss: 6.81490624027544988905\n",
      "Iteration 4246 => Loss: 6.81489051111981147812\n",
      "Iteration 4247 => Loss: 6.81487478408367497451\n",
      "Iteration 4248 => Loss: 6.81485905916675527294\n",
      "Iteration 4249 => Loss: 6.81484333636877526175\n",
      "Iteration 4250 => Loss: 6.81482761568943207209\n",
      "Iteration 4251 => Loss: 6.81481189712846191497\n",
      "Iteration 4252 => Loss: 6.81479618068556991517\n",
      "Iteration 4253 => Loss: 6.81478046636047007922\n",
      "Iteration 4254 => Loss: 6.81476475415286397919\n",
      "Iteration 4255 => Loss: 6.81474904406249137878\n",
      "Iteration 4256 => Loss: 6.81473333608905118552\n",
      "Iteration 4257 => Loss: 6.81471763023226362321\n",
      "Iteration 4258 => Loss: 6.81470192649184003386\n",
      "Iteration 4259 => Loss: 6.81468622486749797673\n",
      "Iteration 4260 => Loss: 6.81467052535895057019\n",
      "Iteration 4261 => Loss: 6.81465482796590737991\n",
      "Iteration 4262 => Loss: 6.81463913268809484691\n",
      "Iteration 4263 => Loss: 6.81462343952521987234\n",
      "Iteration 4264 => Loss: 6.81460774847700445633\n",
      "Iteration 4265 => Loss: 6.81459205954315105913\n",
      "Iteration 4266 => Loss: 6.81457637272339056267\n",
      "Iteration 4267 => Loss: 6.81456068801742453900\n",
      "Iteration 4268 => Loss: 6.81454500542497498827\n",
      "Iteration 4269 => Loss: 6.81452932494575769340\n",
      "Iteration 4270 => Loss: 6.81451364657949110182\n",
      "Iteration 4271 => Loss: 6.81449797032587678558\n",
      "Iteration 4272 => Loss: 6.81448229618464740298\n",
      "Iteration 4273 => Loss: 6.81446662415549830882\n",
      "Iteration 4274 => Loss: 6.81445095423816660229\n",
      "Iteration 4275 => Loss: 6.81443528643235385545\n",
      "Iteration 4276 => Loss: 6.81441962073778295661\n",
      "Iteration 4277 => Loss: 6.81440395715415814237\n",
      "Iteration 4278 => Loss: 6.81438829568120318925\n",
      "Iteration 4279 => Loss: 6.81437263631863832103\n",
      "Iteration 4280 => Loss: 6.81435697906617487973\n",
      "Iteration 4281 => Loss: 6.81434132392351976648\n",
      "Iteration 4282 => Loss: 6.81432567089040208685\n",
      "Iteration 4283 => Loss: 6.81431001996652696562\n",
      "Iteration 4284 => Loss: 6.81429437115162262018\n",
      "Iteration 4285 => Loss: 6.81427872444539239893\n",
      "Iteration 4286 => Loss: 6.81426307984755563751\n",
      "Iteration 4287 => Loss: 6.81424743735783255971\n",
      "Iteration 4288 => Loss: 6.81423179697593717208\n",
      "Iteration 4289 => Loss: 6.81421615870157815209\n",
      "Iteration 4290 => Loss: 6.81420052253448638169\n",
      "Iteration 4291 => Loss: 6.81418488847436609746\n",
      "Iteration 4292 => Loss: 6.81416925652093130594\n",
      "Iteration 4293 => Loss: 6.81415362667391200091\n",
      "Iteration 4294 => Loss: 6.81413799893300797805\n",
      "Iteration 4295 => Loss: 6.81412237329794834295\n",
      "Iteration 4296 => Loss: 6.81410674976844088491\n",
      "Iteration 4297 => Loss: 6.81409112834420493954\n",
      "Iteration 4298 => Loss: 6.81407550902496339518\n",
      "Iteration 4299 => Loss: 6.81405989181042226477\n",
      "Iteration 4300 => Loss: 6.81404427670029999575\n",
      "Iteration 4301 => Loss: 6.81402866369432125282\n",
      "Iteration 4302 => Loss: 6.81401305279219116073\n",
      "Iteration 4303 => Loss: 6.81399744399363349601\n",
      "Iteration 4304 => Loss: 6.81398183729835515976\n",
      "Iteration 4305 => Loss: 6.81396623270609147482\n",
      "Iteration 4306 => Loss: 6.81395063021654134872\n",
      "Iteration 4307 => Loss: 6.81393502982943477519\n",
      "Iteration 4308 => Loss: 6.81391943154447154996\n",
      "Iteration 4309 => Loss: 6.81390383536138788401\n",
      "Iteration 4310 => Loss: 6.81388824127988268486\n",
      "Iteration 4311 => Loss: 6.81387264929968328175\n",
      "Iteration 4312 => Loss: 6.81385705942050812212\n",
      "Iteration 4313 => Loss: 6.81384147164207032432\n",
      "Iteration 4314 => Loss: 6.81382588596408211856\n",
      "Iteration 4315 => Loss: 6.81381030238626994588\n",
      "Iteration 4316 => Loss: 6.81379472090834514830\n",
      "Iteration 4317 => Loss: 6.81377914153001817965\n",
      "Iteration 4318 => Loss: 6.81376356425102702730\n",
      "Iteration 4319 => Loss: 6.81374798907106704604\n",
      "Iteration 4320 => Loss: 6.81373241598986734147\n",
      "Iteration 4321 => Loss: 6.81371684500713481469\n",
      "Iteration 4322 => Loss: 6.81370127612259945948\n",
      "Iteration 4323 => Loss: 6.81368570933597350603\n",
      "Iteration 4324 => Loss: 6.81367014464696651999\n",
      "Iteration 4325 => Loss: 6.81365458205531027147\n",
      "Iteration 4326 => Loss: 6.81363902156070988525\n",
      "Iteration 4327 => Loss: 6.81362346316289180237\n",
      "Iteration 4328 => Loss: 6.81360790686156203577\n",
      "Iteration 4329 => Loss: 6.81359235265645146740\n",
      "Iteration 4330 => Loss: 6.81357680054727143926\n",
      "Iteration 4331 => Loss: 6.81356125053373595790\n",
      "Iteration 4332 => Loss: 6.81354570261556524713\n",
      "Iteration 4333 => Loss: 6.81353015679248041891\n",
      "Iteration 4334 => Loss: 6.81351461306419636799\n",
      "Iteration 4335 => Loss: 6.81349907143042798907\n",
      "Iteration 4336 => Loss: 6.81348353189089372961\n",
      "Iteration 4337 => Loss: 6.81346799444532003065\n",
      "Iteration 4338 => Loss: 6.81345245909342267510\n",
      "Iteration 4339 => Loss: 6.81343692583490323500\n",
      "Iteration 4340 => Loss: 6.81342139466949259230\n",
      "Iteration 4341 => Loss: 6.81340586559691629986\n",
      "Iteration 4342 => Loss: 6.81339033861687859428\n",
      "Iteration 4343 => Loss: 6.81337481372910414024\n",
      "Iteration 4344 => Loss: 6.81335929093330960882\n",
      "Iteration 4345 => Loss: 6.81334377022921344746\n",
      "Iteration 4346 => Loss: 6.81332825161653321544\n",
      "Iteration 4347 => Loss: 6.81331273509498558383\n",
      "Iteration 4348 => Loss: 6.81329722066429166460\n",
      "Iteration 4349 => Loss: 6.81328170832416724068\n",
      "Iteration 4350 => Loss: 6.81326619807432987130\n",
      "Iteration 4351 => Loss: 6.81325068991450866207\n",
      "Iteration 4352 => Loss: 6.81323518384440252049\n",
      "Iteration 4353 => Loss: 6.81321967986374588122\n",
      "Iteration 4354 => Loss: 6.81320417797225097445\n",
      "Iteration 4355 => Loss: 6.81318867816963713580\n",
      "Iteration 4356 => Loss: 6.81317318045561925999\n",
      "Iteration 4357 => Loss: 6.81315768482992822896\n",
      "Iteration 4358 => Loss: 6.81314219129227005567\n",
      "Iteration 4359 => Loss: 6.81312669984236229936\n",
      "Iteration 4360 => Loss: 6.81311121047993140110\n",
      "Iteration 4361 => Loss: 6.81309572320469492013\n",
      "Iteration 4362 => Loss: 6.81308023801637663297\n",
      "Iteration 4363 => Loss: 6.81306475491467722350\n",
      "Iteration 4364 => Loss: 6.81304927389933023818\n",
      "Iteration 4365 => Loss: 6.81303379497005590082\n",
      "Iteration 4366 => Loss: 6.81301831812656466525\n",
      "Iteration 4367 => Loss: 6.81300284336857497891\n",
      "Iteration 4368 => Loss: 6.81298737069581950010\n",
      "Iteration 4369 => Loss: 6.81297190010800779447\n",
      "Iteration 4370 => Loss: 6.81295643160485386858\n",
      "Iteration 4371 => Loss: 6.81294096518607439350\n",
      "Iteration 4372 => Loss: 6.81292550085141090932\n",
      "Iteration 4373 => Loss: 6.81291003860055432995\n",
      "Iteration 4374 => Loss: 6.81289457843324797182\n",
      "Iteration 4375 => Loss: 6.81287912034919429516\n",
      "Iteration 4376 => Loss: 6.81286366434812329373\n",
      "Iteration 4377 => Loss: 6.81284821042974542138\n",
      "Iteration 4378 => Loss: 6.81283275859378711914\n",
      "Iteration 4379 => Loss: 6.81281730883996328174\n",
      "Iteration 4380 => Loss: 6.81280186116799413298\n",
      "Iteration 4381 => Loss: 6.81278641557760522574\n",
      "Iteration 4382 => Loss: 6.81277097206850257294\n",
      "Iteration 4383 => Loss: 6.81275553064042060925\n",
      "Iteration 4384 => Loss: 6.81274009129307067667\n",
      "Iteration 4385 => Loss: 6.81272465402617211083\n",
      "Iteration 4386 => Loss: 6.81270921883944513553\n",
      "Iteration 4387 => Loss: 6.81269378573262063270\n",
      "Iteration 4388 => Loss: 6.81267835470539306897\n",
      "Iteration 4389 => Loss: 6.81266292575750576077\n",
      "Iteration 4390 => Loss: 6.81264749888867093830\n",
      "Iteration 4391 => Loss: 6.81263207409861504260\n",
      "Iteration 4392 => Loss: 6.81261665138703342848\n",
      "Iteration 4393 => Loss: 6.81260123075368095868\n",
      "Iteration 4394 => Loss: 6.81258581219824943531\n",
      "Iteration 4395 => Loss: 6.81257039572047151665\n",
      "Iteration 4396 => Loss: 6.81255498132006653833\n",
      "Iteration 4397 => Loss: 6.81253956899674939507\n",
      "Iteration 4398 => Loss: 6.81252415875025185699\n",
      "Iteration 4399 => Loss: 6.81250875058028260156\n",
      "Iteration 4400 => Loss: 6.81249334448655918806\n",
      "Iteration 4401 => Loss: 6.81247794046881338659\n",
      "Iteration 4402 => Loss: 6.81246253852675920371\n",
      "Iteration 4403 => Loss: 6.81244713866012219228\n",
      "Iteration 4404 => Loss: 6.81243174086861635885\n",
      "Iteration 4405 => Loss: 6.81241634515196015087\n",
      "Iteration 4406 => Loss: 6.81240095150988089756\n",
      "Iteration 4407 => Loss: 6.81238555994209260547\n",
      "Iteration 4408 => Loss: 6.81237017044832704471\n",
      "Iteration 4409 => Loss: 6.81235478302828845187\n",
      "Iteration 4410 => Loss: 6.81233939768171214979\n",
      "Iteration 4411 => Loss: 6.81232401440831036865\n",
      "Iteration 4412 => Loss: 6.81230863320780422043\n",
      "Iteration 4413 => Loss: 6.81229325407991748165\n",
      "Iteration 4414 => Loss: 6.81227787702437037609\n",
      "Iteration 4415 => Loss: 6.81226250204087957485\n",
      "Iteration 4416 => Loss: 6.81224712912917507168\n",
      "Iteration 4417 => Loss: 6.81223175828896909678\n",
      "Iteration 4418 => Loss: 6.81221638951998542666\n",
      "Iteration 4419 => Loss: 6.81220102282194162058\n",
      "Iteration 4420 => Loss: 6.81218565819456056687\n",
      "Iteration 4421 => Loss: 6.81217029563756781840\n",
      "Iteration 4422 => Loss: 6.81215493515067560537\n",
      "Iteration 4423 => Loss: 6.81213957673361303335\n",
      "Iteration 4424 => Loss: 6.81212422038610565522\n",
      "Iteration 4425 => Loss: 6.81210886610785504303\n",
      "Iteration 4426 => Loss: 6.81209351389860362502\n",
      "Iteration 4427 => Loss: 6.81207816375805919051\n",
      "Iteration 4428 => Loss: 6.81206281568595173326\n",
      "Iteration 4429 => Loss: 6.81204746968199614798\n",
      "Iteration 4430 => Loss: 6.81203212574591443484\n",
      "Iteration 4431 => Loss: 6.81201678387742592946\n",
      "Iteration 4432 => Loss: 6.81200144407625973741\n",
      "Iteration 4433 => Loss: 6.81198610634213341797\n",
      "Iteration 4434 => Loss: 6.81197077067476275403\n",
      "Iteration 4435 => Loss: 6.81195543707388129206\n",
      "Iteration 4436 => Loss: 6.81194010553920215045\n",
      "Iteration 4437 => Loss: 6.81192477607044999388\n",
      "Iteration 4438 => Loss: 6.81190944866734060525\n",
      "Iteration 4439 => Loss: 6.81189412332960397833\n",
      "Iteration 4440 => Loss: 6.81187880005695056695\n",
      "Iteration 4441 => Loss: 6.81186347884911924666\n",
      "Iteration 4442 => Loss: 6.81184815970581603040\n",
      "Iteration 4443 => Loss: 6.81183284262677091192\n",
      "Iteration 4444 => Loss: 6.81181752761169878596\n",
      "Iteration 4445 => Loss: 6.81180221466033142264\n",
      "Iteration 4446 => Loss: 6.81178690377238638121\n",
      "Iteration 4447 => Loss: 6.81177159494758033276\n",
      "Iteration 4448 => Loss: 6.81175628818563705380\n",
      "Iteration 4449 => Loss: 6.81174098348628920263\n",
      "Iteration 4450 => Loss: 6.81172568084924634491\n",
      "Iteration 4451 => Loss: 6.81171038027423403349\n",
      "Iteration 4452 => Loss: 6.81169508176098137398\n",
      "Iteration 4453 => Loss: 6.81167978530919437929\n",
      "Iteration 4454 => Loss: 6.81166449091861370135\n",
      "Iteration 4455 => Loss: 6.81164919858895068216\n",
      "Iteration 4456 => Loss: 6.81163390831992821006\n",
      "Iteration 4457 => Loss: 6.81161862011127627881\n",
      "Iteration 4458 => Loss: 6.81160333396270534223\n",
      "Iteration 4459 => Loss: 6.81158804987394361774\n",
      "Iteration 4460 => Loss: 6.81157276784472198727\n",
      "Iteration 4461 => Loss: 6.81155748787474824013\n",
      "Iteration 4462 => Loss: 6.81154220996375769914\n",
      "Iteration 4463 => Loss: 6.81152693411146081814\n",
      "Iteration 4464 => Loss: 6.81151166031759114361\n",
      "Iteration 4465 => Loss: 6.81149638858185557666\n",
      "Iteration 4466 => Loss: 6.81148111890400365098\n",
      "Iteration 4467 => Loss: 6.81146585128373160956\n",
      "Iteration 4468 => Loss: 6.81145058572077743975\n",
      "Iteration 4469 => Loss: 6.81143532221485248357\n",
      "Iteration 4470 => Loss: 6.81142006076569472839\n",
      "Iteration 4471 => Loss: 6.81140480137301285168\n",
      "Iteration 4472 => Loss: 6.81138954403653507086\n",
      "Iteration 4473 => Loss: 6.81137428875598516242\n",
      "Iteration 4474 => Loss: 6.81135903553108690289\n",
      "Iteration 4475 => Loss: 6.81134378436156318060\n",
      "Iteration 4476 => Loss: 6.81132853524713599569\n",
      "Iteration 4477 => Loss: 6.81131328818751846654\n",
      "Iteration 4478 => Loss: 6.81129804318245657413\n",
      "Iteration 4479 => Loss: 6.81128280023165011414\n",
      "Iteration 4480 => Loss: 6.81126755933483707395\n",
      "Iteration 4481 => Loss: 6.81125232049173767734\n",
      "Iteration 4482 => Loss: 6.81123708370206948359\n",
      "Iteration 4483 => Loss: 6.81122184896555982192\n",
      "Iteration 4484 => Loss: 6.81120661628193779791\n",
      "Iteration 4485 => Loss: 6.81119138565091741810\n",
      "Iteration 4486 => Loss: 6.81117615707222689991\n",
      "Iteration 4487 => Loss: 6.81116093054558824349\n",
      "Iteration 4488 => Loss: 6.81114570607072700170\n",
      "Iteration 4489 => Loss: 6.81113048364736251017\n",
      "Iteration 4490 => Loss: 6.81111526327522032176\n",
      "Iteration 4491 => Loss: 6.81110004495402243663\n",
      "Iteration 4492 => Loss: 6.81108482868349973671\n",
      "Iteration 4493 => Loss: 6.81106961446336800492\n",
      "Iteration 4494 => Loss: 6.81105440229335368230\n",
      "Iteration 4495 => Loss: 6.81103919217318587442\n",
      "Iteration 4496 => Loss: 6.81102398410257148242\n",
      "Iteration 4497 => Loss: 6.81100877808125559909\n",
      "Iteration 4498 => Loss: 6.81099357410894423737\n",
      "Iteration 4499 => Loss: 6.81097837218537183190\n",
      "Iteration 4500 => Loss: 6.81096317231026393557\n",
      "Iteration 4501 => Loss: 6.81094797448334077217\n",
      "Iteration 4502 => Loss: 6.81093277870432167731\n",
      "Iteration 4503 => Loss: 6.81091758497294108565\n",
      "Iteration 4504 => Loss: 6.81090239328890945103\n",
      "Iteration 4505 => Loss: 6.81088720365196031992\n",
      "Iteration 4506 => Loss: 6.81087201606181746882\n",
      "Iteration 4507 => Loss: 6.81085683051820378608\n",
      "Iteration 4508 => Loss: 6.81084164702084127185\n",
      "Iteration 4509 => Loss: 6.81082646556946347260\n",
      "Iteration 4510 => Loss: 6.81081128616377551310\n",
      "Iteration 4511 => Loss: 6.81079610880351982161\n",
      "Iteration 4512 => Loss: 6.81078093348841928645\n",
      "Iteration 4513 => Loss: 6.81076576021818791418\n",
      "Iteration 4514 => Loss: 6.81075058899255658673\n",
      "Iteration 4515 => Loss: 6.81073541981124996880\n",
      "Iteration 4516 => Loss: 6.81072025267399094872\n",
      "Iteration 4517 => Loss: 6.81070508758050330300\n",
      "Iteration 4518 => Loss: 6.81068992453051169633\n",
      "Iteration 4519 => Loss: 6.81067476352374967519\n",
      "Iteration 4520 => Loss: 6.81065960455992858158\n",
      "Iteration 4521 => Loss: 6.81064444763878018563\n",
      "Iteration 4522 => Loss: 6.81062929276003092838\n",
      "Iteration 4523 => Loss: 6.81061413992340458634\n",
      "Iteration 4524 => Loss: 6.81059898912861338971\n",
      "Iteration 4525 => Loss: 6.81058384037540420763\n",
      "Iteration 4526 => Loss: 6.81056869366348482941\n",
      "Iteration 4527 => Loss: 6.81055354899258880153\n",
      "Iteration 4528 => Loss: 6.81053840636243723594\n",
      "Iteration 4529 => Loss: 6.81052326577275923825\n",
      "Iteration 4530 => Loss: 6.81050812722327147952\n",
      "Iteration 4531 => Loss: 6.81049299071370572989\n",
      "Iteration 4532 => Loss: 6.81047785624379020675\n",
      "Iteration 4533 => Loss: 6.81046272381323714029\n",
      "Iteration 4534 => Loss: 6.81044759342179073514\n",
      "Iteration 4535 => Loss: 6.81043246506915611604\n",
      "Iteration 4536 => Loss: 6.81041733875506771767\n",
      "Iteration 4537 => Loss: 6.81040221447926352738\n",
      "Iteration 4538 => Loss: 6.81038709224144955812\n",
      "Iteration 4539 => Loss: 6.81037197204135402728\n",
      "Iteration 4540 => Loss: 6.81035685387871403407\n",
      "Iteration 4541 => Loss: 6.81034173775324269684\n",
      "Iteration 4542 => Loss: 6.81032662366467000936\n",
      "Iteration 4543 => Loss: 6.81031151161272596539\n",
      "Iteration 4544 => Loss: 6.81029640159712901237\n",
      "Iteration 4545 => Loss: 6.81028129361761447313\n",
      "Iteration 4546 => Loss: 6.81026618767388836062\n",
      "Iteration 4547 => Loss: 6.81025108376570198487\n",
      "Iteration 4548 => Loss: 6.81023598189276313519\n",
      "Iteration 4549 => Loss: 6.81022088205480269352\n",
      "Iteration 4550 => Loss: 6.81020578425154443636\n",
      "Iteration 4551 => Loss: 6.81019068848271924566\n",
      "Iteration 4552 => Loss: 6.81017559474805000974\n",
      "Iteration 4553 => Loss: 6.81016050304726672238\n",
      "Iteration 4554 => Loss: 6.81014541338008250193\n",
      "Iteration 4555 => Loss: 6.81013032574623444759\n",
      "Iteration 4556 => Loss: 6.81011524014544988859\n",
      "Iteration 4557 => Loss: 6.81010015657744816053\n",
      "Iteration 4558 => Loss: 6.81008507504196192173\n",
      "Iteration 4559 => Loss: 6.81006999553871139597\n",
      "Iteration 4560 => Loss: 6.81005491806742391248\n",
      "Iteration 4561 => Loss: 6.81003984262782946502\n",
      "Iteration 4562 => Loss: 6.81002476921965094192\n",
      "Iteration 4563 => Loss: 6.81000969784261478424\n",
      "Iteration 4564 => Loss: 6.80999462849644654483\n",
      "Iteration 4565 => Loss: 6.80997956118087621746\n",
      "Iteration 4566 => Loss: 6.80996449589562935500\n",
      "Iteration 4567 => Loss: 6.80994943264042618125\n",
      "Iteration 4568 => Loss: 6.80993437141500113086\n",
      "Iteration 4569 => Loss: 6.80991931221907798033\n",
      "Iteration 4570 => Loss: 6.80990425505237872983\n",
      "Iteration 4571 => Loss: 6.80988919991463426129\n",
      "Iteration 4572 => Loss: 6.80987414680557456848\n",
      "Iteration 4573 => Loss: 6.80985909572491721065\n",
      "Iteration 4574 => Loss: 6.80984404667239928699\n",
      "Iteration 4575 => Loss: 6.80982899964773658041\n",
      "Iteration 4576 => Loss: 6.80981395465067329553\n",
      "Iteration 4577 => Loss: 6.80979891168091544529\n",
      "Iteration 4578 => Loss: 6.80978387073819746433\n",
      "Iteration 4579 => Loss: 6.80976883182225112279\n",
      "Iteration 4580 => Loss: 6.80975379493279930898\n",
      "Iteration 4581 => Loss: 6.80973876006957024032\n",
      "Iteration 4582 => Loss: 6.80972372723229124603\n",
      "Iteration 4583 => Loss: 6.80970869642068876715\n",
      "Iteration 4584 => Loss: 6.80969366763448480384\n",
      "Iteration 4585 => Loss: 6.80967864087341467894\n",
      "Iteration 4586 => Loss: 6.80966361613719861623\n",
      "Iteration 4587 => Loss: 6.80964859342556572130\n",
      "Iteration 4588 => Loss: 6.80963357273825309335\n",
      "Iteration 4589 => Loss: 6.80961855407496763348\n",
      "Iteration 4590 => Loss: 6.80960353743546065175\n",
      "Iteration 4591 => Loss: 6.80958852281943904927\n",
      "Iteration 4592 => Loss: 6.80957351022663548434\n",
      "Iteration 4593 => Loss: 6.80955849965678616798\n",
      "Iteration 4594 => Loss: 6.80954349110960954761\n",
      "Iteration 4595 => Loss: 6.80952848458483561700\n",
      "Iteration 4596 => Loss: 6.80951348008219614627\n",
      "Iteration 4597 => Loss: 6.80949847760141047104\n",
      "Iteration 4598 => Loss: 6.80948347714221213778\n",
      "Iteration 4599 => Loss: 6.80946847870432314664\n",
      "Iteration 4600 => Loss: 6.80945348228747793229\n",
      "Iteration 4601 => Loss: 6.80943848789140027122\n",
      "Iteration 4602 => Loss: 6.80942349551581838085\n",
      "Iteration 4603 => Loss: 6.80940850516045781404\n",
      "Iteration 4604 => Loss: 6.80939351682505300545\n",
      "Iteration 4605 => Loss: 6.80937853050933306065\n",
      "Iteration 4606 => Loss: 6.80936354621300665713\n",
      "Iteration 4607 => Loss: 6.80934856393581799949\n",
      "Iteration 4608 => Loss: 6.80933358367750773965\n",
      "Iteration 4609 => Loss: 6.80931860543777567329\n",
      "Iteration 4610 => Loss: 6.80930362921636067597\n",
      "Iteration 4611 => Loss: 6.80928865501299274143\n",
      "Iteration 4612 => Loss: 6.80927368282740896888\n",
      "Iteration 4613 => Loss: 6.80925871265931981213\n",
      "Iteration 4614 => Loss: 6.80924374450846592310\n",
      "Iteration 4615 => Loss: 6.80922877837457374284\n",
      "Iteration 4616 => Loss: 6.80921381425736349513\n",
      "Iteration 4617 => Loss: 6.80919885215657139099\n",
      "Iteration 4618 => Loss: 6.80918389207192742418\n",
      "Iteration 4619 => Loss: 6.80916893400315625939\n",
      "Iteration 4620 => Loss: 6.80915397794997900860\n",
      "Iteration 4621 => Loss: 6.80913902391213543552\n",
      "Iteration 4622 => Loss: 6.80912407188935286939\n",
      "Iteration 4623 => Loss: 6.80910912188134531675\n",
      "Iteration 4624 => Loss: 6.80909417388786764036\n",
      "Iteration 4625 => Loss: 6.80907922790861785955\n",
      "Iteration 4626 => Loss: 6.80906428394335083709\n",
      "Iteration 4627 => Loss: 6.80904934199178146770\n",
      "Iteration 4628 => Loss: 6.80903440205364063331\n",
      "Iteration 4629 => Loss: 6.80901946412865566316\n",
      "Iteration 4630 => Loss: 6.80900452821655921554\n",
      "Iteration 4631 => Loss: 6.80898959431707329060\n",
      "Iteration 4632 => Loss: 6.80897466242994120478\n",
      "Iteration 4633 => Loss: 6.80895973255487341191\n",
      "Iteration 4634 => Loss: 6.80894480469161678116\n",
      "Iteration 4635 => Loss: 6.80892987883987821363\n",
      "Iteration 4636 => Loss: 6.80891495499940990754\n",
      "Iteration 4637 => Loss: 6.80890003316992586946\n",
      "Iteration 4638 => Loss: 6.80888511335115698131\n",
      "Iteration 4639 => Loss: 6.80887019554284389500\n",
      "Iteration 4640 => Loss: 6.80885527974469617618\n",
      "Iteration 4641 => Loss: 6.80884036595645980583\n",
      "Iteration 4642 => Loss: 6.80882545417786122499\n",
      "Iteration 4643 => Loss: 6.80881054440861710475\n",
      "Iteration 4644 => Loss: 6.80879563664846898519\n",
      "Iteration 4645 => Loss: 6.80878073089714330735\n",
      "Iteration 4646 => Loss: 6.80876582715437095317\n",
      "Iteration 4647 => Loss: 6.80875092541987569916\n",
      "Iteration 4648 => Loss: 6.80873602569339020363\n",
      "Iteration 4649 => Loss: 6.80872112797464534850\n",
      "Iteration 4650 => Loss: 6.80870623226336757483\n",
      "Iteration 4651 => Loss: 6.80869133855928598820\n",
      "Iteration 4652 => Loss: 6.80867644686214035232\n",
      "Iteration 4653 => Loss: 6.80866155717164556194\n",
      "Iteration 4654 => Loss: 6.80864666948754049258\n",
      "Iteration 4655 => Loss: 6.80863178380954980895\n",
      "Iteration 4656 => Loss: 6.80861690013740528116\n",
      "Iteration 4657 => Loss: 6.80860201847083512661\n",
      "Iteration 4658 => Loss: 6.80858713880957289177\n",
      "Iteration 4659 => Loss: 6.80857226115335212313\n",
      "Iteration 4660 => Loss: 6.80855738550189393266\n",
      "Iteration 4661 => Loss: 6.80854251185492476139\n",
      "Iteration 4662 => Loss: 6.80852764021218792578\n",
      "Iteration 4663 => Loss: 6.80851277057339920873\n",
      "Iteration 4664 => Loss: 6.80849790293829659760\n",
      "Iteration 4665 => Loss: 6.80848303730661186250\n",
      "Iteration 4666 => Loss: 6.80846817367806966814\n",
      "Iteration 4667 => Loss: 6.80845331205241510730\n",
      "Iteration 4668 => Loss: 6.80843845242934264661\n",
      "Iteration 4669 => Loss: 6.80842359480862402421\n",
      "Iteration 4670 => Loss: 6.80840873918996525305\n",
      "Iteration 4671 => Loss: 6.80839388557309455052\n",
      "Iteration 4672 => Loss: 6.80837903395776766757\n",
      "Iteration 4673 => Loss: 6.80836418434368617625\n",
      "Iteration 4674 => Loss: 6.80834933673058895209\n",
      "Iteration 4675 => Loss: 6.80833449111820598887\n",
      "Iteration 4676 => Loss: 6.80831964750627882665\n",
      "Iteration 4677 => Loss: 6.80830480589452768925\n",
      "Iteration 4678 => Loss: 6.80828996628268257041\n",
      "Iteration 4679 => Loss: 6.80827512867048234568\n",
      "Iteration 4680 => Loss: 6.80826029305764812705\n",
      "Iteration 4681 => Loss: 6.80824545944390990826\n",
      "Iteration 4682 => Loss: 6.80823062782900478851\n",
      "Iteration 4683 => Loss: 6.80821579821266276156\n",
      "Iteration 4684 => Loss: 6.80820097059461026845\n",
      "Iteration 4685 => Loss: 6.80818614497458174384\n",
      "Iteration 4686 => Loss: 6.80817132135230718148\n",
      "Iteration 4687 => Loss: 6.80815649972751568697\n",
      "Iteration 4688 => Loss: 6.80814168009993636588\n",
      "Iteration 4689 => Loss: 6.80812686246931075829\n",
      "Iteration 4690 => Loss: 6.80811204683535375892\n",
      "Iteration 4691 => Loss: 6.80809723319780957240\n",
      "Iteration 4692 => Loss: 6.80808242155639842252\n",
      "Iteration 4693 => Loss: 6.80806761191086362572\n",
      "Iteration 4694 => Loss: 6.80805280426092718216\n",
      "Iteration 4695 => Loss: 6.80803799860632619101\n",
      "Iteration 4696 => Loss: 6.80802319494678620515\n",
      "Iteration 4697 => Loss: 6.80800839328204165923\n",
      "Iteration 4698 => Loss: 6.80799359361181277706\n",
      "Iteration 4699 => Loss: 6.80797879593585175684\n",
      "Iteration 4700 => Loss: 6.80796400025387971056\n",
      "Iteration 4701 => Loss: 6.80794920656561952654\n",
      "Iteration 4702 => Loss: 6.80793441487081363306\n",
      "Iteration 4703 => Loss: 6.80791962516918758297\n",
      "Iteration 4704 => Loss: 6.80790483746047936364\n",
      "Iteration 4705 => Loss: 6.80789005174441275159\n",
      "Iteration 4706 => Loss: 6.80787526802072484600\n",
      "Iteration 4707 => Loss: 6.80786048628914208791\n",
      "Iteration 4708 => Loss: 6.80784570654939980017\n",
      "Iteration 4709 => Loss: 6.80783092880122797652\n",
      "Iteration 4710 => Loss: 6.80781615304435749891\n",
      "Iteration 4711 => Loss: 6.80780137927852635471\n",
      "Iteration 4712 => Loss: 6.80778660750345654407\n",
      "Iteration 4713 => Loss: 6.80777183771888960706\n",
      "Iteration 4714 => Loss: 6.80775706992454576749\n",
      "Iteration 4715 => Loss: 6.80774230412017189451\n",
      "Iteration 4716 => Loss: 6.80772754030548377102\n",
      "Iteration 4717 => Loss: 6.80771277848022116075\n",
      "Iteration 4718 => Loss: 6.80769801864411583381\n",
      "Iteration 4719 => Loss: 6.80768326079690400121\n",
      "Iteration 4720 => Loss: 6.80766850493831476854\n",
      "Iteration 4721 => Loss: 6.80765375106807368866\n",
      "Iteration 4722 => Loss: 6.80763899918591697258\n",
      "Iteration 4723 => Loss: 6.80762424929157639042\n",
      "Iteration 4724 => Loss: 6.80760950138478815319\n",
      "Iteration 4725 => Loss: 6.80759475546528403100\n",
      "Iteration 4726 => Loss: 6.80758001153279046491\n",
      "Iteration 4727 => Loss: 6.80756526958704455410\n",
      "Iteration 4728 => Loss: 6.80755052962777185144\n",
      "Iteration 4729 => Loss: 6.80753579165471478518\n",
      "Iteration 4730 => Loss: 6.80752105566760246091\n",
      "Iteration 4731 => Loss: 6.80750632166616309604\n",
      "Iteration 4732 => Loss: 6.80749158965013023703\n",
      "Iteration 4733 => Loss: 6.80747685961924187126\n",
      "Iteration 4734 => Loss: 6.80746213157321999887\n",
      "Iteration 4735 => Loss: 6.80744740551180527177\n",
      "Iteration 4736 => Loss: 6.80743268143473301279\n",
      "Iteration 4737 => Loss: 6.80741795934172433391\n",
      "Iteration 4738 => Loss: 6.80740323923252521610\n",
      "Iteration 4739 => Loss: 6.80738852110686476493\n",
      "Iteration 4740 => Loss: 6.80737380496446764511\n",
      "Iteration 4741 => Loss: 6.80735909080506829127\n",
      "Iteration 4742 => Loss: 6.80734437862840202627\n",
      "Iteration 4743 => Loss: 6.80732966843421127834\n",
      "Iteration 4744 => Loss: 6.80731496022221982400\n",
      "Iteration 4745 => Loss: 6.80730025399215854520\n",
      "Iteration 4746 => Loss: 6.80728554974375743569\n",
      "Iteration 4747 => Loss: 6.80727084747676158827\n",
      "Iteration 4748 => Loss: 6.80725614719089566762\n",
      "Iteration 4749 => Loss: 6.80724144888589499658\n",
      "Iteration 4750 => Loss: 6.80722675256149400980\n",
      "Iteration 4751 => Loss: 6.80721205821741914832\n",
      "Iteration 4752 => Loss: 6.80719736585340928769\n",
      "Iteration 4753 => Loss: 6.80718267546919708622\n",
      "Iteration 4754 => Loss: 6.80716798706451609036\n",
      "Iteration 4755 => Loss: 6.80715330063909718206\n",
      "Iteration 4756 => Loss: 6.80713861619267834868\n",
      "Iteration 4757 => Loss: 6.80712393372498958399\n",
      "Iteration 4758 => Loss: 6.80710925323575999357\n",
      "Iteration 4759 => Loss: 6.80709457472472667661\n",
      "Iteration 4760 => Loss: 6.80707989819163294953\n",
      "Iteration 4761 => Loss: 6.80706522363619459526\n",
      "Iteration 4762 => Loss: 6.80705055105815848293\n",
      "Iteration 4763 => Loss: 6.80703588045725105360\n",
      "Iteration 4764 => Loss: 6.80702121183320496556\n",
      "Iteration 4765 => Loss: 6.80700654518577064067\n",
      "Iteration 4766 => Loss: 6.80699188051465320370\n",
      "Iteration 4767 => Loss: 6.80697721781961018195\n",
      "Iteration 4768 => Loss: 6.80696255710036446374\n",
      "Iteration 4769 => Loss: 6.80694789835664781918\n",
      "Iteration 4770 => Loss: 6.80693324158819734748\n",
      "Iteration 4771 => Loss: 6.80691858679475281235\n",
      "Iteration 4772 => Loss: 6.80690393397604154302\n",
      "Iteration 4773 => Loss: 6.80688928313179708596\n",
      "Iteration 4774 => Loss: 6.80687463426175831671\n",
      "Iteration 4775 => Loss: 6.80685998736565167633\n",
      "Iteration 4776 => Loss: 6.80684534244321692853\n",
      "Iteration 4777 => Loss: 6.80683069949418761979\n",
      "Iteration 4778 => Loss: 6.80681605851829818477\n",
      "Iteration 4779 => Loss: 6.80680141951527950539\n",
      "Iteration 4780 => Loss: 6.80678678248486601632\n",
      "Iteration 4781 => Loss: 6.80677214742679659309\n",
      "Iteration 4782 => Loss: 6.80675751434080211766\n",
      "Iteration 4783 => Loss: 6.80674288322661702466\n",
      "Iteration 4784 => Loss: 6.80672825408397486058\n",
      "Iteration 4785 => Loss: 6.80671362691260917188\n",
      "Iteration 4786 => Loss: 6.80669900171225705776\n",
      "Iteration 4787 => Loss: 6.80668437848265472923\n",
      "Iteration 4788 => Loss: 6.80666975722353129186\n",
      "Iteration 4789 => Loss: 6.80665513793462295666\n",
      "Iteration 4790 => Loss: 6.80664052061566415830\n",
      "Iteration 4791 => Loss: 6.80662590526638666688\n",
      "Iteration 4792 => Loss: 6.80661129188653646338\n",
      "Iteration 4793 => Loss: 6.80659668047583910067\n",
      "Iteration 4794 => Loss: 6.80658207103402990157\n",
      "Iteration 4795 => Loss: 6.80656746356084330074\n",
      "Iteration 4796 => Loss: 6.80655285805601728555\n",
      "Iteration 4797 => Loss: 6.80653825451928273793\n",
      "Iteration 4798 => Loss: 6.80652365295036876347\n",
      "Iteration 4799 => Loss: 6.80650905334902400767\n",
      "Iteration 4800 => Loss: 6.80649445571498201701\n",
      "Iteration 4801 => Loss: 6.80647986004796301529\n",
      "Iteration 4802 => Loss: 6.80646526634771831255\n",
      "Iteration 4803 => Loss: 6.80645067461396457986\n",
      "Iteration 4804 => Loss: 6.80643608484645579182\n",
      "Iteration 4805 => Loss: 6.80642149704492194218\n",
      "Iteration 4806 => Loss: 6.80640691120909568923\n",
      "Iteration 4807 => Loss: 6.80639232733871057945\n",
      "Iteration 4808 => Loss: 6.80637774543349838297\n",
      "Iteration 4809 => Loss: 6.80636316549319886349\n",
      "Iteration 4810 => Loss: 6.80634858751755089656\n",
      "Iteration 4811 => Loss: 6.80633401150628625231\n",
      "Iteration 4812 => Loss: 6.80631943745914114174\n",
      "Iteration 4813 => Loss: 6.80630486537584999951\n",
      "Iteration 4814 => Loss: 6.80629029525614459573\n",
      "Iteration 4815 => Loss: 6.80627572709977091137\n",
      "Iteration 4816 => Loss: 6.80626116090645094658\n",
      "Iteration 4817 => Loss: 6.80624659667593068235\n",
      "Iteration 4818 => Loss: 6.80623203440793922425\n",
      "Iteration 4819 => Loss: 6.80621747410221278329\n",
      "Iteration 4820 => Loss: 6.80620291575849289956\n",
      "Iteration 4821 => Loss: 6.80618835937650601409\n",
      "Iteration 4822 => Loss: 6.80617380495600343693\n",
      "Iteration 4823 => Loss: 6.80615925249669651009\n",
      "Iteration 4824 => Loss: 6.80614470199834009634\n",
      "Iteration 4825 => Loss: 6.80613015346066951849\n",
      "Iteration 4826 => Loss: 6.80611560688341121761\n",
      "Iteration 4827 => Loss: 6.80610106226630140469\n",
      "Iteration 4828 => Loss: 6.80608651960908783707\n",
      "Iteration 4829 => Loss: 6.80607197891148985036\n",
      "Iteration 4830 => Loss: 6.80605744017326141915\n",
      "Iteration 4831 => Loss: 6.80604290339412276722\n",
      "Iteration 4832 => Loss: 6.80602836857381721103\n",
      "Iteration 4833 => Loss: 6.80601383571207474432\n",
      "Iteration 4834 => Loss: 6.80599930480864134807\n",
      "Iteration 4835 => Loss: 6.80598477586325945055\n",
      "Iteration 4836 => Loss: 6.80597024887564394646\n",
      "Iteration 4837 => Loss: 6.80595572384553637590\n",
      "Iteration 4838 => Loss: 6.80594120077268271984\n",
      "Iteration 4839 => Loss: 6.80592667965681119568\n",
      "Iteration 4840 => Loss: 6.80591216049766778440\n",
      "Iteration 4841 => Loss: 6.80589764329497537432\n",
      "Iteration 4842 => Loss: 6.80588312804847817006\n",
      "Iteration 4843 => Loss: 6.80586861475791149445\n",
      "Iteration 4844 => Loss: 6.80585410342301777575\n",
      "Iteration 4845 => Loss: 6.80583959404351901412\n",
      "Iteration 4846 => Loss: 6.80582508661916651960\n",
      "Iteration 4847 => Loss: 6.80581058114968406869\n",
      "Iteration 4848 => Loss: 6.80579607763482385963\n",
      "Iteration 4849 => Loss: 6.80578157607431233345\n",
      "Iteration 4850 => Loss: 6.80576707646787948391\n",
      "Iteration 4851 => Loss: 6.80575257881527750925\n",
      "Iteration 4852 => Loss: 6.80573808311622840961\n",
      "Iteration 4853 => Loss: 6.80572358937048260685\n",
      "Iteration 4854 => Loss: 6.80570909757776298932\n",
      "Iteration 4855 => Loss: 6.80569460773781820251\n",
      "Iteration 4856 => Loss: 6.80568011985038356926\n",
      "Iteration 4857 => Loss: 6.80566563391518553061\n",
      "Iteration 4858 => Loss: 6.80565114993197983750\n",
      "Iteration 4859 => Loss: 6.80563666790047694377\n",
      "Iteration 4860 => Loss: 6.80562218782043792942\n",
      "Iteration 4861 => Loss: 6.80560770969159190003\n",
      "Iteration 4862 => Loss: 6.80559323351366973753\n",
      "Iteration 4863 => Loss: 6.80557875928641475838\n",
      "Iteration 4864 => Loss: 6.80556428700956494993\n",
      "Iteration 4865 => Loss: 6.80554981668285652319\n",
      "Iteration 4866 => Loss: 6.80553534830602302463\n",
      "Iteration 4867 => Loss: 6.80552088187880421799\n",
      "Iteration 4868 => Loss: 6.80550641740093809062\n",
      "Iteration 4869 => Loss: 6.80549195487216618261\n",
      "Iteration 4870 => Loss: 6.80547749429221315864\n",
      "Iteration 4871 => Loss: 6.80546303566083121694\n",
      "Iteration 4872 => Loss: 6.80544857897774768674\n",
      "Iteration 4873 => Loss: 6.80543412424270410810\n",
      "Iteration 4874 => Loss: 6.80541967145544113293\n",
      "Iteration 4875 => Loss: 6.80540522061569141954\n",
      "Iteration 4876 => Loss: 6.80539077172318496167\n",
      "Iteration 4877 => Loss: 6.80537632477767395756\n",
      "Iteration 4878 => Loss: 6.80536187977888573641\n",
      "Iteration 4879 => Loss: 6.80534743672656805558\n",
      "Iteration 4880 => Loss: 6.80533299562044913245\n",
      "Iteration 4881 => Loss: 6.80531855646027405982\n",
      "Iteration 4882 => Loss: 6.80530411924577283145\n",
      "Iteration 4883 => Loss: 6.80528968397668965196\n",
      "Iteration 4884 => Loss: 6.80527525065276694960\n",
      "Iteration 4885 => Loss: 6.80526081927371695457\n",
      "Iteration 4886 => Loss: 6.80524638983931495773\n",
      "Iteration 4887 => Loss: 6.80523196234927052473\n",
      "Iteration 4888 => Loss: 6.80521753680333763015\n",
      "Iteration 4889 => Loss: 6.80520311320124626775\n",
      "Iteration 4890 => Loss: 6.80518869154272998401\n",
      "Iteration 4891 => Loss: 6.80517427182754008896\n",
      "Iteration 4892 => Loss: 6.80515985405541190545\n",
      "Iteration 4893 => Loss: 6.80514543822606743362\n",
      "Iteration 4894 => Loss: 6.80513102433926420076\n",
      "Iteration 4895 => Loss: 6.80511661239473308882\n",
      "Iteration 4896 => Loss: 6.80510220239221030880\n",
      "Iteration 4897 => Loss: 6.80508779433143828896\n",
      "Iteration 4898 => Loss: 6.80507338821215679303\n",
      "Iteration 4899 => Loss: 6.80505898403409670294\n",
      "Iteration 4900 => Loss: 6.80504458179699955878\n",
      "Iteration 4901 => Loss: 6.80503018150061400604\n",
      "Iteration 4902 => Loss: 6.80501578314466382125\n",
      "Iteration 4903 => Loss: 6.80500138672889054448\n",
      "Iteration 4904 => Loss: 6.80498699225304015670\n",
      "Iteration 4905 => Loss: 6.80497259971683821078\n",
      "Iteration 4906 => Loss: 6.80495820912003601677\n",
      "Iteration 4907 => Loss: 6.80494382046237067385\n",
      "Iteration 4908 => Loss: 6.80492943374358105757\n",
      "Iteration 4909 => Loss: 6.80491504896339449715\n",
      "Iteration 4910 => Loss: 6.80490066612156230264\n",
      "Iteration 4911 => Loss: 6.80488628521781890868\n",
      "Iteration 4912 => Loss: 6.80487190625190230264\n",
      "Iteration 4913 => Loss: 6.80485752922355047190\n",
      "Iteration 4914 => Loss: 6.80484315413251206195\n",
      "Iteration 4915 => Loss: 6.80482878097850818477\n",
      "Iteration 4916 => Loss: 6.80481440976129725584\n",
      "Iteration 4917 => Loss: 6.80480004048060216348\n",
      "Iteration 4918 => Loss: 6.80478567313617244139\n",
      "Iteration 4919 => Loss: 6.80477130772774696510\n",
      "Iteration 4920 => Loss: 6.80475694425505306384\n",
      "Iteration 4921 => Loss: 6.80474258271784204766\n",
      "Iteration 4922 => Loss: 6.80472822311584657484\n",
      "Iteration 4923 => Loss: 6.80471386544880729730\n",
      "Iteration 4924 => Loss: 6.80469950971647996596\n",
      "Iteration 4925 => Loss: 6.80468515591858036373\n",
      "Iteration 4926 => Loss: 6.80467080405485003070\n",
      "Iteration 4927 => Loss: 6.80465645412503405964\n",
      "Iteration 4928 => Loss: 6.80464210612888198426\n",
      "Iteration 4929 => Loss: 6.80462776006610958746\n",
      "Iteration 4930 => Loss: 6.80461341593648416648\n",
      "Iteration 4931 => Loss: 6.80459907373972061606\n",
      "Iteration 4932 => Loss: 6.80458473347558001620\n",
      "Iteration 4933 => Loss: 6.80457039514378525524\n",
      "Iteration 4934 => Loss: 6.80455605874408320233\n",
      "Iteration 4935 => Loss: 6.80454172427620740393\n",
      "Iteration 4936 => Loss: 6.80452739173990384103\n",
      "Iteration 4937 => Loss: 6.80451306113491316552\n",
      "Iteration 4938 => Loss: 6.80449873246097247659\n",
      "Iteration 4939 => Loss: 6.80448440571782331432\n",
      "Iteration 4940 => Loss: 6.80447008090519567247\n",
      "Iteration 4941 => Loss: 6.80445575802284263744\n",
      "Iteration 4942 => Loss: 6.80444143707050752568\n",
      "Iteration 4943 => Loss: 6.80442711804791056096\n",
      "Iteration 4944 => Loss: 6.80441280095480749424\n",
      "Iteration 4945 => Loss: 6.80439848579093453651\n",
      "Iteration 4946 => Loss: 6.80438417255603145151\n",
      "Iteration 4947 => Loss: 6.80436986124983089752\n",
      "Iteration 4948 => Loss: 6.80435555187208773731\n",
      "Iteration 4949 => Loss: 6.80434124442253285281\n",
      "Iteration 4950 => Loss: 6.80432693890090511957\n",
      "Iteration 4951 => Loss: 6.80431263530694785402\n",
      "Iteration 4952 => Loss: 6.80429833364040259625\n",
      "Iteration 4953 => Loss: 6.80428403390101532722\n",
      "Iteration 4954 => Loss: 6.80426973608850538255\n",
      "Iteration 4955 => Loss: 6.80425544020263295408\n",
      "Iteration 4956 => Loss: 6.80424114624313336464\n",
      "Iteration 4957 => Loss: 6.80422685420974548975\n",
      "Iteration 4958 => Loss: 6.80421256410221086952\n",
      "Iteration 4959 => Loss: 6.80419827592026482677\n",
      "Iteration 4960 => Loss: 6.80418398966365511882\n",
      "Iteration 4961 => Loss: 6.80416970533212062122\n",
      "Iteration 4962 => Loss: 6.80415542292540287406\n",
      "Iteration 4963 => Loss: 6.80414114244323808833\n",
      "Iteration 4964 => Loss: 6.80412686388536869231\n",
      "Iteration 4965 => Loss: 6.80411258725153800242\n",
      "Iteration 4966 => Loss: 6.80409831254148844693\n",
      "Iteration 4967 => Loss: 6.80408403975494913141\n",
      "Iteration 4968 => Loss: 6.80406976889166514866\n",
      "Iteration 4969 => Loss: 6.80405549995139757868\n",
      "Iteration 4970 => Loss: 6.80404123293385598714\n",
      "Iteration 4971 => Loss: 6.80402696783880234221\n",
      "Iteration 4972 => Loss: 6.80401270466596752584\n",
      "Iteration 4973 => Loss: 6.80399844341510195989\n",
      "Iteration 4974 => Loss: 6.80398418408593741447\n",
      "Iteration 4975 => Loss: 6.80396992667822342327\n",
      "Iteration 4976 => Loss: 6.80395567119169175641\n",
      "Iteration 4977 => Loss: 6.80394141762608928303\n",
      "Iteration 4978 => Loss: 6.80392716598116020776\n",
      "Iteration 4979 => Loss: 6.80391291625662475440\n",
      "Iteration 4980 => Loss: 6.80389866845225821379\n",
      "Iteration 4981 => Loss: 6.80388442256777992156\n",
      "Iteration 4982 => Loss: 6.80387017860293319416\n",
      "Iteration 4983 => Loss: 6.80385593655746045982\n",
      "Iteration 4984 => Loss: 6.80384169643111036407\n",
      "Iteration 4985 => Loss: 6.80382745822361201249\n",
      "Iteration 4986 => Loss: 6.80381322193472293236\n",
      "Iteration 4987 => Loss: 6.80379898756416778838\n",
      "Iteration 4988 => Loss: 6.80378475511169522605\n",
      "Iteration 4989 => Loss: 6.80377052457704589727\n",
      "Iteration 4990 => Loss: 6.80375629595996311849\n",
      "Iteration 4991 => Loss: 6.80374206926019109432\n",
      "Iteration 4992 => Loss: 6.80372784447746425940\n",
      "Iteration 4993 => Loss: 6.80371362161153037107\n",
      "Iteration 4994 => Loss: 6.80369940066212386398\n",
      "Iteration 4995 => Loss: 6.80368518162899427182\n",
      "Iteration 4996 => Loss: 6.80367096451188135831\n",
      "Iteration 4997 => Loss: 6.80365674931052577534\n",
      "Iteration 4998 => Loss: 6.80364253602466817483\n",
      "Iteration 4999 => Loss: 6.80362832465405276139\n",
      "Iteration 5000 => Loss: 6.80361411519842285145\n",
      "Iteration 5001 => Loss: 6.80359990765751465602\n",
      "Iteration 5002 => Loss: 6.80358570203108037333\n",
      "Iteration 5003 => Loss: 6.80357149831884644442\n",
      "Iteration 5004 => Loss: 6.80355729652057128476\n",
      "Iteration 5005 => Loss: 6.80354309663598666447\n",
      "Iteration 5006 => Loss: 6.80352889866483501180\n",
      "Iteration 5007 => Loss: 6.80351470260686053138\n",
      "Iteration 5008 => Loss: 6.80350050846180831599\n",
      "Iteration 5009 => Loss: 6.80348631622942345842\n",
      "Iteration 5010 => Loss: 6.80347212590943861699\n",
      "Iteration 5011 => Loss: 6.80345793750160154900\n",
      "Iteration 5012 => Loss: 6.80344375100564757730\n",
      "Iteration 5013 => Loss: 6.80342956642133067646\n",
      "Iteration 5014 => Loss: 6.80341538374838972203\n",
      "Iteration 5015 => Loss: 6.80340120298655826048\n",
      "Iteration 5016 => Loss: 6.80338702413558849003\n",
      "Iteration 5017 => Loss: 6.80337284719521928622\n",
      "Iteration 5018 => Loss: 6.80335867216519751821\n",
      "Iteration 5019 => Loss: 6.80334449904526206154\n",
      "Iteration 5020 => Loss: 6.80333032783515267994\n",
      "Iteration 5021 => Loss: 6.80331615853461180166\n",
      "Iteration 5022 => Loss: 6.80330199114339162492\n",
      "Iteration 5023 => Loss: 6.80328782566122658437\n",
      "Iteration 5024 => Loss: 6.80327366208786177282\n",
      "Iteration 5025 => Loss: 6.80325950042304139487\n",
      "Iteration 5026 => Loss: 6.80324534066650254971\n",
      "Iteration 5027 => Loss: 6.80323118281799565921\n",
      "Iteration 5028 => Loss: 6.80321702687725426983\n",
      "Iteration 5029 => Loss: 6.80320287284403413253\n",
      "Iteration 5030 => Loss: 6.80318872071806346469\n",
      "Iteration 5031 => Loss: 6.80317457049909446454\n",
      "Iteration 5032 => Loss: 6.80316042218687311305\n",
      "Iteration 5033 => Loss: 6.80314627578113473305\n",
      "Iteration 5034 => Loss: 6.80313213128162974641\n",
      "Iteration 5035 => Loss: 6.80311798868808814689\n",
      "Iteration 5036 => Loss: 6.80310384800027101448\n",
      "Iteration 5037 => Loss: 6.80308970921791189568\n",
      "Iteration 5038 => Loss: 6.80307557234075588326\n",
      "Iteration 5039 => Loss: 6.80306143736853474735\n",
      "Iteration 5040 => Loss: 6.80304730430101134431\n",
      "Iteration 5041 => Loss: 6.80303317313791833243\n",
      "Iteration 5042 => Loss: 6.80301904387900080451\n",
      "Iteration 5043 => Loss: 6.80300491652399941245\n",
      "Iteration 5044 => Loss: 6.80299079107266813082\n",
      "Iteration 5045 => Loss: 6.80297666752473251250\n",
      "Iteration 5046 => Loss: 6.80296254587995186114\n",
      "Iteration 5047 => Loss: 6.80294842613805883502\n",
      "Iteration 5048 => Loss: 6.80293430829880207966\n",
      "Iteration 5049 => Loss: 6.80292019236193379328\n",
      "Iteration 5050 => Loss: 6.80290607832717952874\n",
      "Iteration 5051 => Loss: 6.80289196619429947788\n",
      "Iteration 5052 => Loss: 6.80287785596303162805\n",
      "Iteration 5053 => Loss: 6.80286374763311219027\n",
      "Iteration 5054 => Loss: 6.80284964120429425094\n",
      "Iteration 5055 => Loss: 6.80283553667631846196\n",
      "Iteration 5056 => Loss: 6.80282143404892902794\n",
      "Iteration 5057 => Loss: 6.80280733332186482443\n",
      "Iteration 5058 => Loss: 6.80279323449488426689\n",
      "Iteration 5059 => Loss: 6.80277913756771557274\n",
      "Iteration 5060 => Loss: 6.80276504254011005202\n",
      "Iteration 5061 => Loss: 6.80275094941181190933\n",
      "Iteration 5062 => Loss: 6.80273685818256357294\n",
      "Iteration 5063 => Loss: 6.80272276885210747110\n",
      "Iteration 5064 => Loss: 6.80270868142019047298\n",
      "Iteration 5065 => Loss: 6.80269459588655500681\n",
      "Iteration 5066 => Loss: 6.80268051225094794177\n",
      "Iteration 5067 => Loss: 6.80266643051311525880\n",
      "Iteration 5068 => Loss: 6.80265235067279672165\n",
      "Iteration 5069 => Loss: 6.80263827272973298221\n",
      "Iteration 5070 => Loss: 6.80262419668367446235\n",
      "Iteration 5071 => Loss: 6.80261012253436536668\n",
      "Iteration 5072 => Loss: 6.80259605028154989981\n",
      "Iteration 5073 => Loss: 6.80258197992497049000\n",
      "Iteration 5074 => Loss: 6.80256791146437134188\n",
      "Iteration 5075 => Loss: 6.80255384489950021276\n",
      "Iteration 5076 => Loss: 6.80253978023010130727\n",
      "Iteration 5077 => Loss: 6.80252571745591794183\n",
      "Iteration 5078 => Loss: 6.80251165657668721565\n",
      "Iteration 5079 => Loss: 6.80249759759216665600\n",
      "Iteration 5080 => Loss: 6.80248354050209602661\n",
      "Iteration 5081 => Loss: 6.80246948530621775575\n",
      "Iteration 5082 => Loss: 6.80245543200427782438\n",
      "Iteration 5083 => Loss: 6.80244138059602310165\n",
      "Iteration 5084 => Loss: 6.80242733108119335128\n",
      "Iteration 5085 => Loss: 6.80241328345954254786\n",
      "Iteration 5086 => Loss: 6.80239923773080956693\n",
      "Iteration 5087 => Loss: 6.80238519389473328403\n",
      "Iteration 5088 => Loss: 6.80237115195107211463\n",
      "Iteration 5089 => Loss: 6.80235711189955960521\n",
      "Iteration 5090 => Loss: 6.80234307373994617762\n",
      "Iteration 5091 => Loss: 6.80232903747197248379\n",
      "Iteration 5092 => Loss: 6.80231500309538894555\n",
      "Iteration 5093 => Loss: 6.80230097060994243208\n",
      "Iteration 5094 => Loss: 6.80228694001537625979\n",
      "Iteration 5095 => Loss: 6.80227291131142575153\n",
      "Iteration 5096 => Loss: 6.80225888449784843459\n",
      "Iteration 5097 => Loss: 6.80224485957438762540\n",
      "Iteration 5098 => Loss: 6.80223083654078042315\n",
      "Iteration 5099 => Loss: 6.80221681539678524331\n",
      "Iteration 5100 => Loss: 6.80220279614213385599\n",
      "Iteration 5101 => Loss: 6.80218877877658023579\n",
      "Iteration 5102 => Loss: 6.80217476329987125183\n",
      "Iteration 5103 => Loss: 6.80216074971173778607\n",
      "Iteration 5104 => Loss: 6.80214673801195246483\n",
      "Iteration 5105 => Loss: 6.80213272820023640008\n",
      "Iteration 5106 => Loss: 6.80211872027633823734\n",
      "Iteration 5107 => Loss: 6.80210471424001639207\n",
      "Iteration 5108 => Loss: 6.80209071009100885163\n",
      "Iteration 5109 => Loss: 6.80207670782905804430\n",
      "Iteration 5110 => Loss: 6.80206270745391616828\n",
      "Iteration 5111 => Loss: 6.80204870896532476365\n",
      "Iteration 5112 => Loss: 6.80203471236303247593\n",
      "Iteration 5113 => Loss: 6.80202071764677729249\n",
      "Iteration 5114 => Loss: 6.80200672481631674060\n",
      "Iteration 5115 => Loss: 6.80199273387139058400\n",
      "Iteration 5116 => Loss: 6.80197874481174480366\n",
      "Iteration 5117 => Loss: 6.80196475763712804508\n",
      "Iteration 5118 => Loss: 6.80195077234728451288\n",
      "Iteration 5119 => Loss: 6.80193678894195574713\n",
      "Iteration 5120 => Loss: 6.80192280742089305789\n",
      "Iteration 5121 => Loss: 6.80190882778383887342\n",
      "Iteration 5122 => Loss: 6.80189485003054539192\n",
      "Iteration 5123 => Loss: 6.80188087416075504166\n",
      "Iteration 5124 => Loss: 6.80186690017421469179\n",
      "Iteration 5125 => Loss: 6.80185292807067121146\n",
      "Iteration 5126 => Loss: 6.80183895784986525257\n",
      "Iteration 5127 => Loss: 6.80182498951155078970\n",
      "Iteration 5128 => Loss: 6.80181102305547291564\n",
      "Iteration 5129 => Loss: 6.80179705848136695323\n",
      "Iteration 5130 => Loss: 6.80178309578900552879\n",
      "Iteration 5131 => Loss: 6.80176913497810264886\n",
      "Iteration 5132 => Loss: 6.80175517604842294617\n",
      "Iteration 5133 => Loss: 6.80174121899971240168\n",
      "Iteration 5134 => Loss: 6.80172726383172232545\n",
      "Iteration 5135 => Loss: 6.80171331054417738216\n",
      "Iteration 5136 => Loss: 6.80169935913684842177\n",
      "Iteration 5137 => Loss: 6.80168540960947254348\n",
      "Iteration 5138 => Loss: 6.80167146196179661644\n",
      "Iteration 5139 => Loss: 6.80165751619356040436\n",
      "Iteration 5140 => Loss: 6.80164357230452143455\n",
      "Iteration 5141 => Loss: 6.80162963029442657614\n",
      "Iteration 5142 => Loss: 6.80161569016301648105\n",
      "Iteration 5143 => Loss: 6.80160175191003624207\n",
      "Iteration 5144 => Loss: 6.80158781553524427466\n",
      "Iteration 5145 => Loss: 6.80157388103837678983\n",
      "Iteration 5146 => Loss: 6.80155994841918332128\n",
      "Iteration 5147 => Loss: 6.80154601767740896179\n",
      "Iteration 5148 => Loss: 6.80153208881280857412\n",
      "Iteration 5149 => Loss: 6.80151816182511659292\n",
      "Iteration 5150 => Loss: 6.80150423671409054549\n",
      "Iteration 5151 => Loss: 6.80149031347947374826\n",
      "Iteration 5152 => Loss: 6.80147639212101395856\n",
      "Iteration 5153 => Loss: 6.80146247263846071007\n",
      "Iteration 5154 => Loss: 6.80144855503155909560\n",
      "Iteration 5155 => Loss: 6.80143463930005154339\n",
      "Iteration 5156 => Loss: 6.80142072544369380438\n",
      "Iteration 5157 => Loss: 6.80140681346223008319\n",
      "Iteration 5158 => Loss: 6.80139290335540991350\n",
      "Iteration 5159 => Loss: 6.80137899512296595361\n",
      "Iteration 5160 => Loss: 6.80136508876466905349\n",
      "Iteration 5161 => Loss: 6.80135118428024476600\n",
      "Iteration 5162 => Loss: 6.80133728166945683569\n",
      "Iteration 5163 => Loss: 6.80132338093204857898\n",
      "Iteration 5164 => Loss: 6.80130948206776597686\n",
      "Iteration 5165 => Loss: 6.80129558507634879305\n",
      "Iteration 5166 => Loss: 6.80128168995756254844\n",
      "Iteration 5167 => Loss: 6.80126779671113457226\n",
      "Iteration 5168 => Loss: 6.80125390533683393812\n",
      "Iteration 5169 => Loss: 6.80124001583438531071\n",
      "Iteration 5170 => Loss: 6.80122612820355776364\n",
      "Iteration 5171 => Loss: 6.80121224244408395521\n",
      "Iteration 5172 => Loss: 6.80119835855572407723\n",
      "Iteration 5173 => Loss: 6.80118447653821345256\n",
      "Iteration 5174 => Loss: 6.80117059639130250304\n",
      "Iteration 5175 => Loss: 6.80115671811474520325\n",
      "Iteration 5176 => Loss: 6.80114284170828664600\n",
      "Iteration 5177 => Loss: 6.80112896717167636496\n",
      "Iteration 5178 => Loss: 6.80111509450465856474\n",
      "Iteration 5179 => Loss: 6.80110122370698899630\n",
      "Iteration 5180 => Loss: 6.80108735477840653516\n",
      "Iteration 5181 => Loss: 6.80107348771866604409\n",
      "Iteration 5182 => Loss: 6.80105962252751616859\n",
      "Iteration 5183 => Loss: 6.80104575920468601424\n",
      "Iteration 5184 => Loss: 6.80103189774995620098\n",
      "Iteration 5185 => Loss: 6.80101803816305316985\n",
      "Iteration 5186 => Loss: 6.80100418044373000725\n",
      "Iteration 5187 => Loss: 6.80099032459173891141\n",
      "Iteration 5188 => Loss: 6.80097647060681254061\n",
      "Iteration 5189 => Loss: 6.80096261848872707390\n",
      "Iteration 5190 => Loss: 6.80094876823720806414\n",
      "Iteration 5191 => Loss: 6.80093491985201215044\n",
      "Iteration 5192 => Loss: 6.80092107333289064286\n",
      "Iteration 5193 => Loss: 6.80090722867958508147\n",
      "Iteration 5194 => Loss: 6.80089338589185032902\n",
      "Iteration 5195 => Loss: 6.80087954496943059013\n",
      "Iteration 5196 => Loss: 6.80086570591207273395\n",
      "Iteration 5197 => Loss: 6.80085186871953073506\n",
      "Iteration 5198 => Loss: 6.80083803339155146261\n",
      "Iteration 5199 => Loss: 6.80082419992789244390\n",
      "Iteration 5200 => Loss: 6.80081036832828722538\n",
      "Iteration 5201 => Loss: 6.80079653859248356440\n",
      "Iteration 5202 => Loss: 6.80078271072025142274\n",
      "Iteration 5203 => Loss: 6.80076888471131812963\n",
      "Iteration 5204 => Loss: 6.80075506056544032418\n",
      "Iteration 5205 => Loss: 6.80074123828237109279\n",
      "Iteration 5206 => Loss: 6.80072741786185375190\n",
      "Iteration 5207 => Loss: 6.80071359930363783519\n",
      "Iteration 5208 => Loss: 6.80069978260747465271\n",
      "Iteration 5209 => Loss: 6.80068596777311196178\n",
      "Iteration 5210 => Loss: 6.80067215480029574337\n",
      "Iteration 5211 => Loss: 6.80065834368878530114\n",
      "Iteration 5212 => Loss: 6.80064453443832928059\n",
      "Iteration 5213 => Loss: 6.80063072704865412277\n",
      "Iteration 5214 => Loss: 6.80061692151953511853\n",
      "Iteration 5215 => Loss: 6.80060311785071291979\n",
      "Iteration 5216 => Loss: 6.80058931604193439568\n",
      "Iteration 5217 => Loss: 6.80057551609294996808\n",
      "Iteration 5218 => Loss: 6.80056171800350739431\n",
      "Iteration 5219 => Loss: 6.80054792177336775438\n",
      "Iteration 5220 => Loss: 6.80053412740226637112\n",
      "Iteration 5221 => Loss: 6.80052033488995366639\n",
      "Iteration 5222 => Loss: 6.80050654423618450295\n",
      "Iteration 5223 => Loss: 6.80049275544070663813\n",
      "Iteration 5224 => Loss: 6.80047896850327315832\n",
      "Iteration 5225 => Loss: 6.80046518342363004450\n",
      "Iteration 5226 => Loss: 6.80045140020152327764\n",
      "Iteration 5227 => Loss: 6.80043761883670949686\n",
      "Iteration 5228 => Loss: 6.80042383932894001219\n",
      "Iteration 5229 => Loss: 6.80041006167795192283\n",
      "Iteration 5230 => Loss: 6.80039628588350986149\n",
      "Iteration 5231 => Loss: 6.80038251194535270372\n",
      "Iteration 5232 => Loss: 6.80036873986323975316\n",
      "Iteration 5233 => Loss: 6.80035496963690988537\n",
      "Iteration 5234 => Loss: 6.80034120126612595669\n",
      "Iteration 5235 => Loss: 6.80032743475062950722\n",
      "Iteration 5236 => Loss: 6.80031367009016562974\n",
      "Iteration 5237 => Loss: 6.80029990728449629245\n",
      "Iteration 5238 => Loss: 6.80028614633336214723\n",
      "Iteration 5239 => Loss: 6.80027238723652693864\n",
      "Iteration 5240 => Loss: 6.80025862999372154860\n",
      "Iteration 5241 => Loss: 6.80024487460470972167\n",
      "Iteration 5242 => Loss: 6.80023112106923388609\n",
      "Iteration 5243 => Loss: 6.80021736938705156916\n",
      "Iteration 5244 => Loss: 6.80020361955790875186\n",
      "Iteration 5245 => Loss: 6.80018987158155674422\n",
      "Iteration 5246 => Loss: 6.80017612545774330357\n",
      "Iteration 5247 => Loss: 6.80016238118621973996\n",
      "Iteration 5248 => Loss: 6.80014863876674624521\n",
      "Iteration 5249 => Loss: 6.80013489819905725398\n",
      "Iteration 5250 => Loss: 6.80012115948291295808\n",
      "Iteration 5251 => Loss: 6.80010742261806999664\n",
      "Iteration 5252 => Loss: 6.80009368760425925160\n",
      "Iteration 5253 => Loss: 6.80007995444124180295\n",
      "Iteration 5254 => Loss: 6.80006622312877517800\n",
      "Iteration 5255 => Loss: 6.80005249366660269317\n",
      "Iteration 5256 => Loss: 6.80003876605447565851\n",
      "Iteration 5257 => Loss: 6.80002504029214449588\n",
      "Iteration 5258 => Loss: 6.80001131637935962715\n",
      "Iteration 5259 => Loss: 6.79999759431587857961\n",
      "Iteration 5260 => Loss: 6.79998387410144378151\n",
      "Iteration 5261 => Loss: 6.79997015573580920744\n",
      "Iteration 5262 => Loss: 6.79995643921872172655\n",
      "Iteration 5263 => Loss: 6.79994272454994685972\n",
      "Iteration 5264 => Loss: 6.79992901172921637709\n",
      "Iteration 5265 => Loss: 6.79991530075628691776\n",
      "Iteration 5266 => Loss: 6.79990159163091245631\n",
      "Iteration 5267 => Loss: 6.79988788435284696732\n",
      "Iteration 5268 => Loss: 6.79987417892183731993\n",
      "Iteration 5269 => Loss: 6.79986047533764015327\n",
      "Iteration 5270 => Loss: 6.79984677359999967194\n",
      "Iteration 5271 => Loss: 6.79983307370866718600\n",
      "Iteration 5272 => Loss: 6.79981937566339489365\n",
      "Iteration 5273 => Loss: 6.79980567946393588130\n",
      "Iteration 5274 => Loss: 6.79979198511004145900\n",
      "Iteration 5275 => Loss: 6.79977829260146737766\n",
      "Iteration 5276 => Loss: 6.79976460193795162468\n",
      "Iteration 5277 => Loss: 6.79975091311925972093\n",
      "Iteration 5278 => Loss: 6.79973722614513675921\n",
      "Iteration 5279 => Loss: 6.79972354101532960868\n",
      "Iteration 5280 => Loss: 6.79970985772960023752\n",
      "Iteration 5281 => Loss: 6.79969617628769640305\n",
      "Iteration 5282 => Loss: 6.79968249668936053354\n",
      "Iteration 5283 => Loss: 6.79966881893435637352\n",
      "Iteration 5284 => Loss: 6.79965514302242812761\n",
      "Iteration 5285 => Loss: 6.79964146895333509946\n",
      "Iteration 5286 => Loss: 6.79962779672681438825\n",
      "Iteration 5287 => Loss: 6.79961412634263595578\n",
      "Iteration 5288 => Loss: 6.79960045780053956577\n",
      "Iteration 5289 => Loss: 6.79958679110028541004\n",
      "Iteration 5290 => Loss: 6.79957312624161502868\n",
      "Iteration 5291 => Loss: 6.79955946322428594897\n",
      "Iteration 5292 => Loss: 6.79954580204804681642\n",
      "Iteration 5293 => Loss: 6.79953214271266048740\n",
      "Iteration 5294 => Loss: 6.79951848521786139656\n",
      "Iteration 5295 => Loss: 6.79950482956341240026\n",
      "Iteration 5296 => Loss: 6.79949117574906924943\n",
      "Iteration 5297 => Loss: 6.79947752377457881323\n",
      "Iteration 5298 => Loss: 6.79946387363968529627\n",
      "Iteration 5299 => Loss: 6.79945022534415688398\n",
      "Iteration 5300 => Loss: 6.79943657888773245190\n",
      "Iteration 5301 => Loss: 6.79942293427016331009\n",
      "Iteration 5302 => Loss: 6.79940929149121853214\n",
      "Iteration 5303 => Loss: 6.79939565055063699361\n",
      "Iteration 5304 => Loss: 6.79938201144817266908\n",
      "Iteration 5305 => Loss: 6.79936837418357686857\n",
      "Iteration 5306 => Loss: 6.79935473875660267851\n",
      "Iteration 5307 => Loss: 6.79934110516700584981\n",
      "Iteration 5308 => Loss: 6.79932747341453147527\n",
      "Iteration 5309 => Loss: 6.79931384349894596397\n",
      "Iteration 5310 => Loss: 6.79930021541998641510\n",
      "Iteration 5311 => Loss: 6.79928658917740680323\n",
      "Iteration 5312 => Loss: 6.79927296477097176108\n",
      "Iteration 5313 => Loss: 6.79925934220042638145\n",
      "Iteration 5314 => Loss: 6.79924572146551842167\n",
      "Iteration 5315 => Loss: 6.79923210256601162627\n",
      "Iteration 5316 => Loss: 6.79921848550164487079\n",
      "Iteration 5317 => Loss: 6.79920487027218545251\n",
      "Iteration 5318 => Loss: 6.79919125687738112873\n",
      "Iteration 5319 => Loss: 6.79917764531697521591\n",
      "Iteration 5320 => Loss: 6.79916403559073589946\n",
      "Iteration 5321 => Loss: 6.79915042769839583769\n",
      "Iteration 5322 => Loss: 6.79913682163973476236\n",
      "Iteration 5323 => Loss: 6.79912321741448444357\n",
      "Iteration 5324 => Loss: 6.79910961502240329679\n",
      "Iteration 5325 => Loss: 6.79909601446324796115\n",
      "Iteration 5326 => Loss: 6.79908241573677152303\n",
      "Iteration 5327 => Loss: 6.79906881884272173977\n",
      "Iteration 5328 => Loss: 6.79905522378085258595\n",
      "Iteration 5329 => Loss: 6.79904163055091981249\n",
      "Iteration 5330 => Loss: 6.79902803915267472945\n",
      "Iteration 5331 => Loss: 6.79901444958587664047\n",
      "Iteration 5332 => Loss: 6.79900086185027241470\n",
      "Iteration 5333 => Loss: 6.79898727594560892129\n",
      "Iteration 5334 => Loss: 6.79897369187165612203\n",
      "Iteration 5335 => Loss: 6.79896010962815822154\n",
      "Iteration 5336 => Loss: 6.79894652921486297714\n",
      "Iteration 5337 => Loss: 6.79893295063153324520\n",
      "Iteration 5338 => Loss: 6.79891937387792033576\n",
      "Iteration 5339 => Loss: 6.79890579895376934161\n",
      "Iteration 5340 => Loss: 6.79889222585884844818\n",
      "Iteration 5341 => Loss: 6.79887865459289830738\n",
      "Iteration 5342 => Loss: 6.79886508515568266375\n",
      "Iteration 5343 => Loss: 6.79885151754694394555\n",
      "Iteration 5344 => Loss: 6.79883795176644056824\n",
      "Iteration 5345 => Loss: 6.79882438781392650640\n",
      "Iteration 5346 => Loss: 6.79881082568916461639\n",
      "Iteration 5347 => Loss: 6.79879726539189199741\n",
      "Iteration 5348 => Loss: 6.79878370692187061763\n",
      "Iteration 5349 => Loss: 6.79877015027885800436\n",
      "Iteration 5350 => Loss: 6.79875659546260280308\n",
      "Iteration 5351 => Loss: 6.79874304247286431746\n",
      "Iteration 5352 => Loss: 6.79872949130938497575\n",
      "Iteration 5353 => Loss: 6.79871594197192319342\n",
      "Iteration 5354 => Loss: 6.79870239446024804408\n",
      "Iteration 5355 => Loss: 6.79868884877409573875\n",
      "Iteration 5356 => Loss: 6.79867530491322202835\n",
      "Iteration 5357 => Loss: 6.79866176287738888107\n",
      "Iteration 5358 => Loss: 6.79864822266634405423\n",
      "Iteration 5359 => Loss: 6.79863468427984773967\n",
      "Iteration 5360 => Loss: 6.79862114771764325383\n",
      "Iteration 5361 => Loss: 6.79860761297949345305\n",
      "Iteration 5362 => Loss: 6.79859408006515586465\n",
      "Iteration 5363 => Loss: 6.79858054897437469322\n",
      "Iteration 5364 => Loss: 6.79856701970690746606\n",
      "Iteration 5365 => Loss: 6.79855349226251259864\n",
      "Iteration 5366 => Loss: 6.79853996664094228919\n",
      "Iteration 5367 => Loss: 6.79852644284195495317\n",
      "Iteration 5368 => Loss: 6.79851292086529745973\n",
      "Iteration 5369 => Loss: 6.79849940071072289527\n",
      "Iteration 5370 => Loss: 6.79848588237799233980\n",
      "Iteration 5371 => Loss: 6.79847236586685532700\n",
      "Iteration 5372 => Loss: 6.79845885117707737777\n",
      "Iteration 5373 => Loss: 6.79844533830839825583\n",
      "Iteration 5374 => Loss: 6.79843182726057815302\n",
      "Iteration 5375 => Loss: 6.79841831803338170204\n",
      "Iteration 5376 => Loss: 6.79840481062655044298\n",
      "Iteration 5377 => Loss: 6.79839130503983923859\n",
      "Iteration 5378 => Loss: 6.79837780127300828070\n",
      "Iteration 5379 => Loss: 6.79836429932581065572\n",
      "Iteration 5380 => Loss: 6.79835079919800566728\n",
      "Iteration 5381 => Loss: 6.79833730088933929636\n",
      "Iteration 5382 => Loss: 6.79832380439957439933\n",
      "Iteration 5383 => Loss: 6.79831030972845962168\n",
      "Iteration 5384 => Loss: 6.79829681687575693161\n",
      "Iteration 5385 => Loss: 6.79828332584121497462\n",
      "Iteration 5386 => Loss: 6.79826983662458861346\n",
      "Iteration 5387 => Loss: 6.79825634922563981632\n",
      "Iteration 5388 => Loss: 6.79824286364411545236\n",
      "Iteration 5389 => Loss: 6.79822937987977482521\n",
      "Iteration 5390 => Loss: 6.79821589793237102128\n",
      "Iteration 5391 => Loss: 6.79820241780166512058\n",
      "Iteration 5392 => Loss: 6.79818893948740576860\n",
      "Iteration 5393 => Loss: 6.79817546298934871629\n",
      "Iteration 5394 => Loss: 6.79816198830725504365\n",
      "Iteration 5395 => Loss: 6.79814851544087517254\n",
      "Iteration 5396 => Loss: 6.79813504438996485391\n",
      "Iteration 5397 => Loss: 6.79812157515427006871\n",
      "Iteration 5398 => Loss: 6.79810810773356877235\n",
      "Iteration 5399 => Loss: 6.79809464212760072854\n",
      "Iteration 5400 => Loss: 6.79808117833611635916\n",
      "Iteration 5401 => Loss: 6.79806771635888562599\n",
      "Iteration 5402 => Loss: 6.79805425619565895090\n",
      "Iteration 5403 => Loss: 6.79804079784618497939\n",
      "Iteration 5404 => Loss: 6.79802734131023012054\n",
      "Iteration 5405 => Loss: 6.79801388658753058536\n",
      "Iteration 5406 => Loss: 6.79800043367787321102\n",
      "Iteration 5407 => Loss: 6.79798698258099243219\n",
      "Iteration 5408 => Loss: 6.79797353329664399979\n",
      "Iteration 5409 => Loss: 6.79796008582458988201\n",
      "Iteration 5410 => Loss: 6.79794664016458316524\n",
      "Iteration 5411 => Loss: 6.79793319631637782408\n",
      "Iteration 5412 => Loss: 6.79791975427973671486\n",
      "Iteration 5413 => Loss: 6.79790631405440493040\n",
      "Iteration 5414 => Loss: 6.79789287564015332066\n",
      "Iteration 5415 => Loss: 6.79787943903672342572\n",
      "Iteration 5416 => Loss: 6.79786600424387277286\n",
      "Iteration 5417 => Loss: 6.79785257126137132389\n",
      "Iteration 5418 => Loss: 6.79783914008895795433\n",
      "Iteration 5419 => Loss: 6.79782571072639907328\n",
      "Iteration 5420 => Loss: 6.79781228317344510259\n",
      "Iteration 5421 => Loss: 6.79779885742985889863\n",
      "Iteration 5422 => Loss: 6.79778543349539265961\n",
      "Iteration 5423 => Loss: 6.79777201136980213647\n",
      "Iteration 5424 => Loss: 6.79775859105284396833\n",
      "Iteration 5425 => Loss: 6.79774517254427035340\n",
      "Iteration 5426 => Loss: 6.79773175584384592440\n",
      "Iteration 5427 => Loss: 6.79771834095132376774\n",
      "Iteration 5428 => Loss: 6.79770492786645430527\n",
      "Iteration 5429 => Loss: 6.79769151658900927515\n",
      "Iteration 5430 => Loss: 6.79767810711872400020\n",
      "Iteration 5431 => Loss: 6.79766469945536844222\n",
      "Iteration 5432 => Loss: 6.79765129359869302306\n",
      "Iteration 5433 => Loss: 6.79763788954846503998\n",
      "Iteration 5434 => Loss: 6.79762448730443491485\n",
      "Iteration 5435 => Loss: 6.79761108686635573406\n",
      "Iteration 5436 => Loss: 6.79759768823398058402\n",
      "Iteration 5437 => Loss: 6.79758429140708031468\n",
      "Iteration 5438 => Loss: 6.79757089638540712428\n",
      "Iteration 5439 => Loss: 6.79755750316870432925\n",
      "Iteration 5440 => Loss: 6.79754411175674189138\n",
      "Iteration 5441 => Loss: 6.79753072214926934436\n",
      "Iteration 5442 => Loss: 6.79751733434605309725\n",
      "Iteration 5443 => Loss: 6.79750394834684179557\n",
      "Iteration 5444 => Loss: 6.79749056415139829568\n",
      "Iteration 5445 => Loss: 6.79747718175947390762\n",
      "Iteration 5446 => Loss: 6.79746380117082704686\n",
      "Iteration 5447 => Loss: 6.79745042238520813527\n",
      "Iteration 5448 => Loss: 6.79743704540239424006\n",
      "Iteration 5449 => Loss: 6.79742367022212956584\n",
      "Iteration 5450 => Loss: 6.79741029684416542267\n",
      "Iteration 5451 => Loss: 6.79739692526826733143\n",
      "Iteration 5452 => Loss: 6.79738355549418482582\n",
      "Iteration 5453 => Loss: 6.79737018752168786762\n",
      "Iteration 5454 => Loss: 6.79735682135052243780\n",
      "Iteration 5455 => Loss: 6.79734345698045405726\n",
      "Iteration 5456 => Loss: 6.79733009441123492422\n",
      "Iteration 5457 => Loss: 6.79731673364262167780\n",
      "Iteration 5458 => Loss: 6.79730337467436562804\n",
      "Iteration 5459 => Loss: 6.79729001750623673672\n",
      "Iteration 5460 => Loss: 6.79727666213799075479\n",
      "Iteration 5461 => Loss: 6.79726330856937632774\n",
      "Iteration 5462 => Loss: 6.79724995680016608190\n",
      "Iteration 5463 => Loss: 6.79723660683009445194\n",
      "Iteration 5464 => Loss: 6.79722325865893939323\n",
      "Iteration 5465 => Loss: 6.79720991228645043947\n",
      "Iteration 5466 => Loss: 6.79719656771238955884\n",
      "Iteration 5467 => Loss: 6.79718322493650628502\n",
      "Iteration 5468 => Loss: 6.79716988395856258620\n",
      "Iteration 5469 => Loss: 6.79715654477832131874\n",
      "Iteration 5470 => Loss: 6.79714320739553379269\n",
      "Iteration 5471 => Loss: 6.79712987180995931169\n",
      "Iteration 5472 => Loss: 6.79711653802135185032\n",
      "Iteration 5473 => Loss: 6.79710320602948581126\n",
      "Iteration 5474 => Loss: 6.79708987583409118827\n",
      "Iteration 5475 => Loss: 6.79707654743495215399\n",
      "Iteration 5476 => Loss: 6.79706322083181202487\n",
      "Iteration 5477 => Loss: 6.79704989602443188090\n",
      "Iteration 5478 => Loss: 6.79703657301257280210\n",
      "Iteration 5479 => Loss: 6.79702325179599142757\n",
      "Iteration 5480 => Loss: 6.79700993237443640282\n",
      "Iteration 5481 => Loss: 6.79699661474768568326\n",
      "Iteration 5482 => Loss: 6.79698329891548080894\n",
      "Iteration 5483 => Loss: 6.79696998487758641261\n",
      "Iteration 5484 => Loss: 6.79695667263376357425\n",
      "Iteration 5485 => Loss: 6.79694336218376005121\n",
      "Iteration 5486 => Loss: 6.79693005352733958802\n",
      "Iteration 5487 => Loss: 6.79691674666426859375\n",
      "Iteration 5488 => Loss: 6.79690344159429393756\n",
      "Iteration 5489 => Loss: 6.79689013831718114034\n",
      "Iteration 5490 => Loss: 6.79687683683268328849\n",
      "Iteration 5491 => Loss: 6.79686353714056679109\n",
      "Iteration 5492 => Loss: 6.79685023924058207001\n",
      "Iteration 5493 => Loss: 6.79683694313248576435\n",
      "Iteration 5494 => Loss: 6.79682364881604517137\n",
      "Iteration 5495 => Loss: 6.79681035629100982476\n",
      "Iteration 5496 => Loss: 6.79679706555715323901\n",
      "Iteration 5497 => Loss: 6.79678377661421873057\n",
      "Iteration 5498 => Loss: 6.79677048946196560308\n",
      "Iteration 5499 => Loss: 6.79675720410016115380\n",
      "Iteration 5500 => Loss: 6.79674392052856379820\n",
      "Iteration 5501 => Loss: 6.79673063874692573449\n",
      "Iteration 5502 => Loss: 6.79671735875500537816\n",
      "Iteration 5503 => Loss: 6.79670408055256469737\n",
      "Iteration 5504 => Loss: 6.79669080413937098939\n",
      "Iteration 5505 => Loss: 6.79667752951516845883\n",
      "Iteration 5506 => Loss: 6.79666425667973062019\n",
      "Iteration 5507 => Loss: 6.79665098563279457267\n",
      "Iteration 5508 => Loss: 6.79663771637414715343\n",
      "Iteration 5509 => Loss: 6.79662444890352457350\n",
      "Iteration 5510 => Loss: 6.79661118322069590647\n",
      "Iteration 5511 => Loss: 6.79659791932542400872\n",
      "Iteration 5512 => Loss: 6.79658465721745663757\n",
      "Iteration 5513 => Loss: 6.79657139689656286663\n",
      "Iteration 5514 => Loss: 6.79655813836249134141\n",
      "Iteration 5515 => Loss: 6.79654488161501912913\n",
      "Iteration 5516 => Loss: 6.79653162665389753982\n",
      "Iteration 5517 => Loss: 6.79651837347887077811\n",
      "Iteration 5518 => Loss: 6.79650512208971591122\n",
      "Iteration 5519 => Loss: 6.79649187248618780188\n",
      "Iteration 5520 => Loss: 6.79647862466803687198\n",
      "Iteration 5521 => Loss: 6.79646537863503752419\n",
      "Iteration 5522 => Loss: 6.79645213438694550945\n",
      "Iteration 5523 => Loss: 6.79643889192351213779\n",
      "Iteration 5524 => Loss: 6.79642565124450381830\n",
      "Iteration 5525 => Loss: 6.79641241234967186102\n",
      "Iteration 5526 => Loss: 6.79639917523879066863\n",
      "Iteration 5527 => Loss: 6.79638593991160711028\n",
      "Iteration 5528 => Loss: 6.79637270636788493050\n",
      "Iteration 5529 => Loss: 6.79635947460738165660\n",
      "Iteration 5530 => Loss: 6.79634624462985570403\n",
      "Iteration 5531 => Loss: 6.79633301643507881096\n",
      "Iteration 5532 => Loss: 6.79631979002280051105\n",
      "Iteration 5533 => Loss: 6.79630656539278277251\n",
      "Iteration 5534 => Loss: 6.79629334254478401078\n",
      "Iteration 5535 => Loss: 6.79628012147856530589\n",
      "Iteration 5536 => Loss: 6.79626690219388684966\n",
      "Iteration 5537 => Loss: 6.79625368469050261666\n",
      "Iteration 5538 => Loss: 6.79624046896818168051\n",
      "Iteration 5539 => Loss: 6.79622725502667979214\n",
      "Iteration 5540 => Loss: 6.79621404286576069609\n",
      "Iteration 5541 => Loss: 6.79620083248517570240\n",
      "Iteration 5542 => Loss: 6.79618762388470010194\n",
      "Iteration 5543 => Loss: 6.79617441706407543478\n",
      "Iteration 5544 => Loss: 6.79616121202307166271\n",
      "Iteration 5545 => Loss: 6.79614800876145253028\n",
      "Iteration 5546 => Loss: 6.79613480727897201206\n",
      "Iteration 5547 => Loss: 6.79612160757539207623\n",
      "Iteration 5548 => Loss: 6.79610840965047025009\n",
      "Iteration 5549 => Loss: 6.79609521350397294270\n",
      "Iteration 5550 => Loss: 6.79608201913566389862\n",
      "Iteration 5551 => Loss: 6.79606882654528643428\n",
      "Iteration 5552 => Loss: 6.79605563573261228782\n",
      "Iteration 5553 => Loss: 6.79604244669740431561\n",
      "Iteration 5554 => Loss: 6.79602925943941915676\n",
      "Iteration 5555 => Loss: 6.79601607395841877945\n",
      "Iteration 5556 => Loss: 6.79600289025416692823\n",
      "Iteration 5557 => Loss: 6.79598970832641491313\n",
      "Iteration 5558 => Loss: 6.79597652817492825505\n",
      "Iteration 5559 => Loss: 6.79596334979947247490\n",
      "Iteration 5560 => Loss: 6.79595017319979799453\n",
      "Iteration 5561 => Loss: 6.79593699837567388755\n",
      "Iteration 5562 => Loss: 6.79592382532685412855\n",
      "Iteration 5563 => Loss: 6.79591065405311223202\n",
      "Iteration 5564 => Loss: 6.79589748455419506712\n",
      "Iteration 5565 => Loss: 6.79588431682987259563\n",
      "Iteration 5566 => Loss: 6.79587115087990145668\n",
      "Iteration 5567 => Loss: 6.79585798670403651300\n",
      "Iteration 5568 => Loss: 6.79584482430205216730\n",
      "Iteration 5569 => Loss: 6.79583166367370328231\n",
      "Iteration 5570 => Loss: 6.79581850481874560899\n",
      "Iteration 5571 => Loss: 6.79580534773694555639\n",
      "Iteration 5572 => Loss: 6.79579219242806598089\n",
      "Iteration 5573 => Loss: 6.79577903889186085706\n",
      "Iteration 5574 => Loss: 6.79576588712810192305\n",
      "Iteration 5575 => Loss: 6.79575273713653693619\n",
      "Iteration 5576 => Loss: 6.79573958891694385187\n",
      "Iteration 5577 => Loss: 6.79572644246906865106\n",
      "Iteration 5578 => Loss: 6.79571329779267685467\n",
      "Iteration 5579 => Loss: 6.79570015488753309540\n",
      "Iteration 5580 => Loss: 6.79568701375339490056\n",
      "Iteration 5581 => Loss: 6.79567387439003045557\n",
      "Iteration 5582 => Loss: 6.79566073679718840594\n",
      "Iteration 5583 => Loss: 6.79564760097464937161\n",
      "Iteration 5584 => Loss: 6.79563446692215489264\n",
      "Iteration 5585 => Loss: 6.79562133463947937173\n",
      "Iteration 5586 => Loss: 6.79560820412637056620\n",
      "Iteration 5587 => Loss: 6.79559507538260731963\n",
      "Iteration 5588 => Loss: 6.79558194840794538294\n",
      "Iteration 5589 => Loss: 6.79556882320214228344\n",
      "Iteration 5590 => Loss: 6.79555569976495910112\n",
      "Iteration 5591 => Loss: 6.79554257809616135688\n",
      "Iteration 5592 => Loss: 6.79552945819551101891\n",
      "Iteration 5593 => Loss: 6.79551634006277094358\n",
      "Iteration 5594 => Loss: 6.79550322369768977637\n",
      "Iteration 5595 => Loss: 6.79549010910004369634\n",
      "Iteration 5596 => Loss: 6.79547699626959733621\n",
      "Iteration 5597 => Loss: 6.79546388520610111783\n",
      "Iteration 5598 => Loss: 6.79545077590931878575\n",
      "Iteration 5599 => Loss: 6.79543766837902296629\n",
      "Iteration 5600 => Loss: 6.79542456261496319314\n",
      "Iteration 5601 => Loss: 6.79541145861690942809\n",
      "Iteration 5602 => Loss: 6.79539835638461298117\n",
      "Iteration 5603 => Loss: 6.79538525591785624869\n",
      "Iteration 5604 => Loss: 6.79537215721638077071\n",
      "Iteration 5605 => Loss: 6.79535906027995117995\n",
      "Iteration 5606 => Loss: 6.79534596510833832639\n",
      "Iteration 5607 => Loss: 6.79533287170130240185\n",
      "Iteration 5608 => Loss: 6.79531978005860359815\n",
      "Iteration 5609 => Loss: 6.79530669018000654802\n",
      "Iteration 5610 => Loss: 6.79529360206526789057\n",
      "Iteration 5611 => Loss: 6.79528051571415758758\n",
      "Iteration 5612 => Loss: 6.79526743112643405453\n",
      "Iteration 5613 => Loss: 6.79525434830185748325\n",
      "Iteration 5614 => Loss: 6.79524126724019605916\n",
      "Iteration 5615 => Loss: 6.79522818794120109231\n",
      "Iteration 5616 => Loss: 6.79521511040464609721\n",
      "Iteration 5617 => Loss: 6.79520203463029126567\n",
      "Iteration 5618 => Loss: 6.79518896061790389496\n",
      "Iteration 5619 => Loss: 6.79517588836723351875\n",
      "Iteration 5620 => Loss: 6.79516281787805098702\n",
      "Iteration 5621 => Loss: 6.79514974915012359702\n",
      "Iteration 5622 => Loss: 6.79513668218319821790\n",
      "Iteration 5623 => Loss: 6.79512361697705369323\n",
      "Iteration 5624 => Loss: 6.79511055353144488578\n",
      "Iteration 5625 => Loss: 6.79509749184613554007\n",
      "Iteration 5626 => Loss: 6.79508443192088762430\n",
      "Iteration 5627 => Loss: 6.79507137375547465297\n",
      "Iteration 5628 => Loss: 6.79505831734963994251\n",
      "Iteration 5629 => Loss: 6.79504526270316322467\n",
      "Iteration 5630 => Loss: 6.79503220981580113857\n",
      "Iteration 5631 => Loss: 6.79501915868731654058\n",
      "Iteration 5632 => Loss: 6.79500610931746962251\n",
      "Iteration 5633 => Loss: 6.79499306170602590527\n",
      "Iteration 5634 => Loss: 6.79498001585275268610\n",
      "Iteration 5635 => Loss: 6.79496697175740838048\n",
      "Iteration 5636 => Loss: 6.79495392941975406842\n",
      "Iteration 5637 => Loss: 6.79494088883955793534\n",
      "Iteration 5638 => Loss: 6.79492785001658106125\n",
      "Iteration 5639 => Loss: 6.79491481295058630252\n",
      "Iteration 5640 => Loss: 6.79490177764133829186\n",
      "Iteration 5641 => Loss: 6.79488874408859455656\n",
      "Iteration 5642 => Loss: 6.79487571229212772295\n",
      "Iteration 5643 => Loss: 6.79486268225169531831\n",
      "Iteration 5644 => Loss: 6.79484965396706286356\n",
      "Iteration 5645 => Loss: 6.79483662743799143868\n",
      "Iteration 5646 => Loss: 6.79482360266424390005\n",
      "Iteration 5647 => Loss: 6.79481057964558843310\n",
      "Iteration 5648 => Loss: 6.79479755838178700600\n",
      "Iteration 5649 => Loss: 6.79478453887259536970\n",
      "Iteration 5650 => Loss: 6.79477152111779059140\n",
      "Iteration 5651 => Loss: 6.79475850511712842206\n",
      "Iteration 5652 => Loss: 6.79474549087037438255\n",
      "Iteration 5653 => Loss: 6.79473247837728422382\n",
      "Iteration 5654 => Loss: 6.79471946763763412491\n",
      "Iteration 5655 => Loss: 6.79470645865118338946\n",
      "Iteration 5656 => Loss: 6.79469345141769220930\n",
      "Iteration 5657 => Loss: 6.79468044593692521715\n",
      "Iteration 5658 => Loss: 6.79466744220864882209\n",
      "Iteration 5659 => Loss: 6.79465444023262854500\n",
      "Iteration 5660 => Loss: 6.79464144000862635409\n",
      "Iteration 5661 => Loss: 6.79462844153640688205\n",
      "Iteration 5662 => Loss: 6.79461544481571788623\n",
      "Iteration 5663 => Loss: 6.79460244984634620380\n",
      "Iteration 5664 => Loss: 6.79458945662805025023\n",
      "Iteration 5665 => Loss: 6.79457646516059199371\n",
      "Iteration 5666 => Loss: 6.79456347544373695513\n",
      "Iteration 5667 => Loss: 6.79455048747724799085\n",
      "Iteration 5668 => Loss: 6.79453750126087818728\n",
      "Iteration 5669 => Loss: 6.79452451679441171706\n",
      "Iteration 5670 => Loss: 6.79451153407759811387\n",
      "Iteration 5671 => Loss: 6.79449855311020822768\n",
      "Iteration 5672 => Loss: 6.79448557389200491485\n",
      "Iteration 5673 => Loss: 6.79447259642275636082\n",
      "Iteration 5674 => Loss: 6.79445962070221387563\n",
      "Iteration 5675 => Loss: 6.79444664673015630285\n",
      "Iteration 5676 => Loss: 6.79443367450634472249\n",
      "Iteration 5677 => Loss: 6.79442070403054021455\n",
      "Iteration 5678 => Loss: 6.79440773530250385903\n",
      "Iteration 5679 => Loss: 6.79439476832200828227\n",
      "Iteration 5680 => Loss: 6.79438180308881101155\n",
      "Iteration 5681 => Loss: 6.79436883960268467320\n",
      "Iteration 5682 => Loss: 6.79435587786338324179\n",
      "Iteration 5683 => Loss: 6.79434291787068112001\n",
      "Iteration 5684 => Loss: 6.79432995962434382875\n",
      "Iteration 5685 => Loss: 6.79431700312411912535\n",
      "Iteration 5686 => Loss: 6.79430404836979473515\n",
      "Iteration 5687 => Loss: 6.79429109536112019185\n",
      "Iteration 5688 => Loss: 6.79427814409786723360\n",
      "Iteration 5689 => Loss: 6.79426519457979072314\n",
      "Iteration 5690 => Loss: 6.79425224680666506316\n",
      "Iteration 5691 => Loss: 6.79423930077826376817\n",
      "Iteration 5692 => Loss: 6.79422635649432571370\n",
      "Iteration 5693 => Loss: 6.79421341395464128965\n",
      "Iteration 5694 => Loss: 6.79420047315896535878\n",
      "Iteration 5695 => Loss: 6.79418753410705011930\n",
      "Iteration 5696 => Loss: 6.79417459679868773748\n",
      "Iteration 5697 => Loss: 6.79416166123362685880\n",
      "Iteration 5698 => Loss: 6.79414872741162501057\n",
      "Iteration 5699 => Loss: 6.79413579533246103637\n",
      "Iteration 5700 => Loss: 6.79412286499589423983\n",
      "Iteration 5701 => Loss: 6.79410993640169991181\n",
      "Iteration 5702 => Loss: 6.79409700954962225694\n",
      "Iteration 5703 => Loss: 6.79408408443944811239\n",
      "Iteration 5704 => Loss: 6.79407116107093234092\n",
      "Iteration 5705 => Loss: 6.79405823944383424617\n",
      "Iteration 5706 => Loss: 6.79404531955793533626\n",
      "Iteration 5707 => Loss: 6.79403240141298780941\n",
      "Iteration 5708 => Loss: 6.79401948500876162740\n",
      "Iteration 5709 => Loss: 6.79400657034502764020\n",
      "Iteration 5710 => Loss: 6.79399365742153538150\n",
      "Iteration 5711 => Loss: 6.79398074623806813577\n",
      "Iteration 5712 => Loss: 6.79396783679438343029\n",
      "Iteration 5713 => Loss: 6.79395492909023968053\n",
      "Iteration 5714 => Loss: 6.79394202312541928279\n",
      "Iteration 5715 => Loss: 6.79392911889967709982\n",
      "Iteration 5716 => Loss: 6.79391621641277598798\n",
      "Iteration 5717 => Loss: 6.79390331566448857359\n",
      "Iteration 5718 => Loss: 6.79389041665457149577\n",
      "Iteration 5719 => Loss: 6.79387751938280537445\n",
      "Iteration 5720 => Loss: 6.79386462384894596056\n",
      "Iteration 5721 => Loss: 6.79385173005276499225\n",
      "Iteration 5722 => Loss: 6.79383883799401910863\n",
      "Iteration 5723 => Loss: 6.79382594767247471879\n",
      "Iteration 5724 => Loss: 6.79381305908790977810\n",
      "Iteration 5725 => Loss: 6.79380017224008181387\n",
      "Iteration 5726 => Loss: 6.79378728712875368245\n",
      "Iteration 5727 => Loss: 6.79377440375369623382\n",
      "Iteration 5728 => Loss: 6.79376152211467942976\n",
      "Iteration 5729 => Loss: 6.79374864221145902121\n",
      "Iteration 5730 => Loss: 6.79373576404380319360\n",
      "Iteration 5731 => Loss: 6.79372288761149079050\n",
      "Iteration 5732 => Loss: 6.79371001291427578650\n",
      "Iteration 5733 => Loss: 6.79369713995192192613\n",
      "Iteration 5734 => Loss: 6.79368426872420538842\n",
      "Iteration 5735 => Loss: 6.79367139923088814157\n",
      "Iteration 5736 => Loss: 6.79365853147172860105\n",
      "Iteration 5737 => Loss: 6.79364566544651005131\n",
      "Iteration 5738 => Loss: 6.79363280115498646694\n",
      "Iteration 5739 => Loss: 6.79361993859692514519\n",
      "Iteration 5740 => Loss: 6.79360707777209515967\n",
      "Iteration 5741 => Loss: 6.79359421868026647218\n",
      "Iteration 5742 => Loss: 6.79358136132119394546\n",
      "Iteration 5743 => Loss: 6.79356850569465642309\n",
      "Iteration 5744 => Loss: 6.79355565180041143236\n",
      "Iteration 5745 => Loss: 6.79354279963824048139\n",
      "Iteration 5746 => Loss: 6.79352994920789132749\n",
      "Iteration 5747 => Loss: 6.79351710050914103789\n",
      "Iteration 5748 => Loss: 6.79350425354174980441\n",
      "Iteration 5749 => Loss: 6.79349140830548847703\n",
      "Iteration 5750 => Loss: 6.79347856480012346481\n",
      "Iteration 5751 => Loss: 6.79346572302543094679\n",
      "Iteration 5752 => Loss: 6.79345288298116045667\n",
      "Iteration 5753 => Loss: 6.79344004466709083800\n",
      "Iteration 5754 => Loss: 6.79342720808297872992\n",
      "Iteration 5755 => Loss: 6.79341437322860475234\n",
      "Iteration 5756 => Loss: 6.79340154010372376803\n",
      "Iteration 5757 => Loss: 6.79338870870810662694\n",
      "Iteration 5758 => Loss: 6.79337587904152417906\n",
      "Iteration 5759 => Loss: 6.79336305110373928073\n",
      "Iteration 5760 => Loss: 6.79335022489452189376\n",
      "Iteration 5761 => Loss: 6.79333740041363132178\n",
      "Iteration 5762 => Loss: 6.79332457766084907291\n",
      "Iteration 5763 => Loss: 6.79331175663592468084\n",
      "Iteration 5764 => Loss: 6.79329893733864498273\n",
      "Iteration 5765 => Loss: 6.79328611976876128864\n",
      "Iteration 5766 => Loss: 6.79327330392604533671\n",
      "Iteration 5767 => Loss: 6.79326048981026264784\n",
      "Iteration 5768 => Loss: 6.79324767742118584835\n",
      "Iteration 5769 => Loss: 6.79323486675858045913\n",
      "Iteration 5770 => Loss: 6.79322205782220933656\n",
      "Iteration 5771 => Loss: 6.79320925061185043603\n",
      "Iteration 5772 => Loss: 6.79319644512725417940\n",
      "Iteration 5773 => Loss: 6.79318364136820562749\n",
      "Iteration 5774 => Loss: 6.79317083933446319577\n",
      "Iteration 5775 => Loss: 6.79315803902579951057\n",
      "Iteration 5776 => Loss: 6.79314524044196943464\n",
      "Iteration 5777 => Loss: 6.79313244358275003520\n",
      "Iteration 5778 => Loss: 6.79311964844791660312\n",
      "Iteration 5779 => Loss: 6.79310685503722400114\n",
      "Iteration 5780 => Loss: 6.79309406335044219105\n",
      "Iteration 5781 => Loss: 6.79308127338734824008\n",
      "Iteration 5782 => Loss: 6.79306848514769523462\n",
      "Iteration 5783 => Loss: 6.79305569863126379460\n",
      "Iteration 5784 => Loss: 6.79304291383781055913\n",
      "Iteration 5785 => Loss: 6.79303013076711081908\n",
      "Iteration 5786 => Loss: 6.79301734941893098352\n",
      "Iteration 5787 => Loss: 6.79300456979305078420\n",
      "Iteration 5788 => Loss: 6.79299179188921087302\n",
      "Iteration 5789 => Loss: 6.79297901570719986353\n",
      "Iteration 5790 => Loss: 6.79296624124678416479\n",
      "Iteration 5791 => Loss: 6.79295346850771686320\n",
      "Iteration 5792 => Loss: 6.79294069748978657231\n",
      "Iteration 5793 => Loss: 6.79292792819275437211\n",
      "Iteration 5794 => Loss: 6.79291516061638223078\n",
      "Iteration 5795 => Loss: 6.79290239476043566924\n",
      "Iteration 5796 => Loss: 6.79288963062470063647\n",
      "Iteration 5797 => Loss: 6.79287686820892577799\n",
      "Iteration 5798 => Loss: 6.79286410751288549648\n",
      "Iteration 5799 => Loss: 6.79285134853635508279\n",
      "Iteration 5800 => Loss: 6.79283859127909561693\n",
      "Iteration 5801 => Loss: 6.79282583574088061340\n",
      "Iteration 5802 => Loss: 6.79281308192147115221\n",
      "Iteration 5803 => Loss: 6.79280032982064252423\n",
      "Iteration 5804 => Loss: 6.79278757943816113851\n",
      "Iteration 5805 => Loss: 6.79277483077379251597\n",
      "Iteration 5806 => Loss: 6.79276208382731105928\n",
      "Iteration 5807 => Loss: 6.79274933859848228934\n",
      "Iteration 5808 => Loss: 6.79273659508706995069\n",
      "Iteration 5809 => Loss: 6.79272385329284400513\n",
      "Iteration 5810 => Loss: 6.79271111321558063167\n",
      "Iteration 5811 => Loss: 6.79269837485504446306\n",
      "Iteration 5812 => Loss: 6.79268563821099835565\n",
      "Iteration 5813 => Loss: 6.79267290328322204118\n",
      "Iteration 5814 => Loss: 6.79266017007147393514\n",
      "Iteration 5815 => Loss: 6.79264743857553021655\n",
      "Iteration 5816 => Loss: 6.79263470879515818268\n",
      "Iteration 5817 => Loss: 6.79262198073011980171\n",
      "Iteration 5818 => Loss: 6.79260925438018947631\n",
      "Iteration 5819 => Loss: 6.79259652974513805646\n",
      "Iteration 5820 => Loss: 6.79258380682473461576\n",
      "Iteration 5821 => Loss: 6.79257108561873668151\n",
      "Iteration 5822 => Loss: 6.79255836612693464360\n",
      "Iteration 5823 => Loss: 6.79254564834907448301\n",
      "Iteration 5824 => Loss: 6.79253293228493681966\n",
      "Iteration 5825 => Loss: 6.79252021793429783258\n",
      "Iteration 5826 => Loss: 6.79250750529691149637\n",
      "Iteration 5827 => Loss: 6.79249479437255754277\n",
      "Iteration 5828 => Loss: 6.79248208516100415721\n",
      "Iteration 5829 => Loss: 6.79246937766201597242\n",
      "Iteration 5830 => Loss: 6.79245667187535850928\n",
      "Iteration 5831 => Loss: 6.79244396780081505227\n",
      "Iteration 5832 => Loss: 6.79243126543813602325\n",
      "Iteration 5833 => Loss: 6.79241856478711003575\n",
      "Iteration 5834 => Loss: 6.79240586584749728161\n",
      "Iteration 5835 => Loss: 6.79239316861906328171\n",
      "Iteration 5836 => Loss: 6.79238047310158687964\n",
      "Iteration 5837 => Loss: 6.79236777929483004357\n",
      "Iteration 5838 => Loss: 6.79235508719856806437\n",
      "Iteration 5839 => Loss: 6.79234239681255935750\n",
      "Iteration 5840 => Loss: 6.79232970813658898379\n",
      "Iteration 5841 => Loss: 6.79231702117041535871\n",
      "Iteration 5842 => Loss: 6.79230433591381288494\n",
      "Iteration 5843 => Loss: 6.79229165236654264248\n",
      "Iteration 5844 => Loss: 6.79227897052838791581\n",
      "Iteration 5845 => Loss: 6.79226629039911333763\n",
      "Iteration 5846 => Loss: 6.79225361197848265249\n",
      "Iteration 5847 => Loss: 6.79224093526627292761\n",
      "Iteration 5848 => Loss: 6.79222826026225057205\n",
      "Iteration 5849 => Loss: 6.79221558696618554762\n",
      "Iteration 5850 => Loss: 6.79220291537784603975\n",
      "Iteration 5851 => Loss: 6.79219024549700911564\n",
      "Iteration 5852 => Loss: 6.79217757732343763166\n",
      "Iteration 5853 => Loss: 6.79216491085689888507\n",
      "Iteration 5854 => Loss: 6.79215224609717527215\n",
      "Iteration 5855 => Loss: 6.79213958304402343202\n",
      "Iteration 5856 => Loss: 6.79212692169722220825\n",
      "Iteration 5857 => Loss: 6.79211426205653800992\n",
      "Iteration 5858 => Loss: 6.79210160412174079880\n",
      "Iteration 5859 => Loss: 6.79208894789259876035\n",
      "Iteration 5860 => Loss: 6.79207629336888807359\n",
      "Iteration 5861 => Loss: 6.79206364055036981853\n",
      "Iteration 5862 => Loss: 6.79205098943682550328\n",
      "Iteration 5863 => Loss: 6.79203834002802242509\n",
      "Iteration 5864 => Loss: 6.79202569232372255215\n",
      "Iteration 5865 => Loss: 6.79201304632369673442\n",
      "Iteration 5866 => Loss: 6.79200040202773536180\n",
      "Iteration 5867 => Loss: 6.79198775943558530344\n",
      "Iteration 5868 => Loss: 6.79197511854702273837\n",
      "Iteration 5869 => Loss: 6.79196247936183006289\n",
      "Iteration 5870 => Loss: 6.79194984187975858703\n",
      "Iteration 5871 => Loss: 6.79193720610058537801\n",
      "Iteration 5872 => Loss: 6.79192457202409993755\n",
      "Iteration 5873 => Loss: 6.79191193965004380573\n",
      "Iteration 5874 => Loss: 6.79189930897820737243\n",
      "Iteration 5875 => Loss: 6.79188668000835882310\n",
      "Iteration 5876 => Loss: 6.79187405274025479684\n",
      "Iteration 5877 => Loss: 6.79186142717368213084\n",
      "Iteration 5878 => Loss: 6.79184880330840634599\n",
      "Iteration 5879 => Loss: 6.79183618114419385137\n",
      "Iteration 5880 => Loss: 6.79182356068082171419\n",
      "Iteration 5881 => Loss: 6.79181094191805634352\n",
      "Iteration 5882 => Loss: 6.79179832485566770117\n",
      "Iteration 5883 => Loss: 6.79178570949343818342\n",
      "Iteration 5884 => Loss: 6.79177309583111998847\n",
      "Iteration 5885 => Loss: 6.79176048386849995353\n",
      "Iteration 5886 => Loss: 6.79174787360534448766\n",
      "Iteration 5887 => Loss: 6.79173526504141644722\n",
      "Iteration 5888 => Loss: 6.79172265817649556396\n",
      "Iteration 5889 => Loss: 6.79171005301035268786\n",
      "Iteration 5890 => Loss: 6.79169744954275067528\n",
      "Iteration 5891 => Loss: 6.79168484777346925796\n",
      "Iteration 5892 => Loss: 6.79167224770228905584\n",
      "Iteration 5893 => Loss: 6.79165964932895249717\n",
      "Iteration 5894 => Loss: 6.79164705265325885364\n",
      "Iteration 5895 => Loss: 6.79163445767496742889\n",
      "Iteration 5896 => Loss: 6.79162186439384996106\n",
      "Iteration 5897 => Loss: 6.79160927280967019470\n",
      "Iteration 5898 => Loss: 6.79159668292221496699\n",
      "Iteration 5899 => Loss: 6.79158409473124535793\n",
      "Iteration 5900 => Loss: 6.79157150823653843474\n",
      "Iteration 5901 => Loss: 6.79155892343786327103\n",
      "Iteration 5902 => Loss: 6.79154634033498627588\n",
      "Iteration 5903 => Loss: 6.79153375892768362831\n",
      "Iteration 5904 => Loss: 6.79152117921572262560\n",
      "Iteration 5905 => Loss: 6.79150860119889010491\n",
      "Iteration 5906 => Loss: 6.79149602487693471176\n",
      "Iteration 5907 => Loss: 6.79148345024964772421\n",
      "Iteration 5908 => Loss: 6.79147087731678844591\n",
      "Iteration 5909 => Loss: 6.79145830607813216773\n",
      "Iteration 5910 => Loss: 6.79144573653346217412\n",
      "Iteration 5911 => Loss: 6.79143316868252888696\n",
      "Iteration 5912 => Loss: 6.79142060252511647889\n",
      "Iteration 5913 => Loss: 6.79140803806099491169\n",
      "Iteration 5914 => Loss: 6.79139547528993325898\n",
      "Iteration 5915 => Loss: 6.79138291421171214068\n",
      "Iteration 5916 => Loss: 6.79137035482609441317\n",
      "Iteration 5917 => Loss: 6.79135779713285003822\n",
      "Iteration 5918 => Loss: 6.79134524113176230031\n",
      "Iteration 5919 => Loss: 6.79133268682259672033\n",
      "Iteration 5920 => Loss: 6.79132013420511881918\n",
      "Iteration 5921 => Loss: 6.79130758327911188132\n",
      "Iteration 5922 => Loss: 6.79129503404434142766\n",
      "Iteration 5923 => Loss: 6.79128248650058186087\n",
      "Iteration 5924 => Loss: 6.79126994064760847181\n",
      "Iteration 5925 => Loss: 6.79125739648518766955\n",
      "Iteration 5926 => Loss: 6.79124485401308852772\n",
      "Iteration 5927 => Loss: 6.79123231323109077806\n",
      "Iteration 5928 => Loss: 6.79121977413896882325\n",
      "Iteration 5929 => Loss: 6.79120723673648907237\n",
      "Iteration 5930 => Loss: 6.79119470102342681628\n",
      "Iteration 5931 => Loss: 6.79118216699954757587\n",
      "Iteration 5932 => Loss: 6.79116963466463019472\n",
      "Iteration 5933 => Loss: 6.79115710401845085187\n",
      "Iteration 5934 => Loss: 6.79114457506077418003\n",
      "Iteration 5935 => Loss: 6.79113204779137458189\n",
      "Iteration 5936 => Loss: 6.79111952221002113106\n",
      "Iteration 5937 => Loss: 6.79110699831649533564\n",
      "Iteration 5938 => Loss: 6.79109447611056182836\n",
      "Iteration 5939 => Loss: 6.79108195559200833458\n",
      "Iteration 5940 => Loss: 6.79106943676058172343\n",
      "Iteration 5941 => Loss: 6.79105691961607416118\n",
      "Iteration 5942 => Loss: 6.79104440415825560962\n",
      "Iteration 5943 => Loss: 6.79103189038689869506\n",
      "Iteration 5944 => Loss: 6.79101937830176893840\n",
      "Iteration 5945 => Loss: 6.79100686790263896597\n",
      "Iteration 5946 => Loss: 6.79099435918929206224\n",
      "Iteration 5947 => Loss: 6.79098185216149197174\n",
      "Iteration 5948 => Loss: 6.79096934681901398534\n",
      "Iteration 5949 => Loss: 6.79095684316162984118\n",
      "Iteration 5950 => Loss: 6.79094434118912282372\n",
      "Iteration 5951 => Loss: 6.79093184090125046026\n",
      "Iteration 5952 => Loss: 6.79091934229779692345\n",
      "Iteration 5953 => Loss: 6.79090684537852951053\n",
      "Iteration 5954 => Loss: 6.79089435014322440054\n",
      "Iteration 5955 => Loss: 6.79088185659165421981\n",
      "Iteration 5956 => Loss: 6.79086936472358893013\n",
      "Iteration 5957 => Loss: 6.79085687453880471054\n",
      "Iteration 5958 => Loss: 6.79084438603707063464\n",
      "Iteration 5959 => Loss: 6.79083189921816821055\n",
      "Iteration 5960 => Loss: 6.79081941408186384734\n",
      "Iteration 5961 => Loss: 6.79080693062793283588\n",
      "Iteration 5962 => Loss: 6.79079444885614691430\n",
      "Iteration 5963 => Loss: 6.79078196876628137346\n",
      "Iteration 5964 => Loss: 6.79076949035811061606\n",
      "Iteration 5965 => Loss: 6.79075701363139838662\n",
      "Iteration 5966 => Loss: 6.79074453858592885780\n",
      "Iteration 5967 => Loss: 6.79073206522147732045\n",
      "Iteration 5968 => Loss: 6.79071959353781373636\n",
      "Iteration 5969 => Loss: 6.79070712353470984368\n",
      "Iteration 5970 => Loss: 6.79069465521193649238\n",
      "Iteration 5971 => Loss: 6.79068218856927163785\n",
      "Iteration 5972 => Loss: 6.79066972360648879459\n",
      "Iteration 5973 => Loss: 6.79065726032335881257\n",
      "Iteration 5974 => Loss: 6.79064479871965964719\n",
      "Iteration 5975 => Loss: 6.79063233879516303659\n",
      "Iteration 5976 => Loss: 6.79061988054963361350\n",
      "Iteration 5977 => Loss: 6.79060742398286532051\n",
      "Iteration 5978 => Loss: 6.79059496909461479675\n",
      "Iteration 5979 => Loss: 6.79058251588466443849\n",
      "Iteration 5980 => Loss: 6.79057006435277887846\n",
      "Iteration 5981 => Loss: 6.79055761449874317748\n",
      "Iteration 5982 => Loss: 6.79054516632232285644\n",
      "Iteration 5983 => Loss: 6.79053271982329853529\n",
      "Iteration 5984 => Loss: 6.79052027500144461669\n",
      "Iteration 5985 => Loss: 6.79050783185652662155\n",
      "Iteration 5986 => Loss: 6.79049539038832516979\n",
      "Iteration 5987 => Loss: 6.79048295059661111139\n",
      "Iteration 5988 => Loss: 6.79047051248116062538\n",
      "Iteration 5989 => Loss: 6.79045807604174100902\n",
      "Iteration 5990 => Loss: 6.79044564127813909948\n",
      "Iteration 5991 => Loss: 6.79043320819012308220\n",
      "Iteration 5992 => Loss: 6.79042077677746824804\n",
      "Iteration 5993 => Loss: 6.79040834703994278243\n",
      "Iteration 5994 => Loss: 6.79039591897732819348\n",
      "Iteration 5995 => Loss: 6.79038349258939266662\n",
      "Iteration 5996 => Loss: 6.79037106787591770996\n",
      "Iteration 5997 => Loss: 6.79035864483666884439\n",
      "Iteration 5998 => Loss: 6.79034622347143024257\n",
      "Iteration 5999 => Loss: 6.79033380377997275446\n",
      "Iteration 6000 => Loss: 6.79032138576206811820\n",
      "Iteration 6001 => Loss: 6.79030896941748451923\n",
      "Iteration 6002 => Loss: 6.79029655474602034104\n",
      "Iteration 6003 => Loss: 6.79028414174741712372\n",
      "Iteration 6004 => Loss: 6.79027173042147769166\n",
      "Iteration 6005 => Loss: 6.79025932076796046033\n",
      "Iteration 6006 => Loss: 6.79024691278665049055\n",
      "Iteration 6007 => Loss: 6.79023450647730886232\n",
      "Iteration 6008 => Loss: 6.79022210183971886011\n",
      "Iteration 6009 => Loss: 6.79020969887365666295\n",
      "Iteration 6010 => Loss: 6.79019729757890200261\n",
      "Iteration 6011 => Loss: 6.79018489795521684727\n",
      "Iteration 6012 => Loss: 6.79017250000238181684\n",
      "Iteration 6013 => Loss: 6.79016010372016864949\n",
      "Iteration 6014 => Loss: 6.79014770910836151785\n",
      "Iteration 6015 => Loss: 6.79013531616673216007\n",
      "Iteration 6016 => Loss: 6.79012292489504254434\n",
      "Iteration 6017 => Loss: 6.79011053529308750143\n",
      "Iteration 6018 => Loss: 6.79009814736062367047\n",
      "Iteration 6019 => Loss: 6.79008576109744144134\n",
      "Iteration 6020 => Loss: 6.79007337650330633494\n",
      "Iteration 6021 => Loss: 6.79006099357799097760\n",
      "Iteration 6022 => Loss: 6.79004861232128398285\n",
      "Iteration 6023 => Loss: 6.79003623273294909524\n",
      "Iteration 6024 => Loss: 6.79002385481276071744\n",
      "Iteration 6025 => Loss: 6.79001147856050746299\n",
      "Iteration 6026 => Loss: 6.78999910397594863554\n",
      "Iteration 6027 => Loss: 6.78998673105887107226\n",
      "Iteration 6028 => Loss: 6.78997435980903762953\n",
      "Iteration 6029 => Loss: 6.78996199022623336816\n",
      "Iteration 6030 => Loss: 6.78994962231023979626\n",
      "Iteration 6031 => Loss: 6.78993725606081355295\n",
      "Iteration 6032 => Loss: 6.78992489147774413993\n",
      "Iteration 6033 => Loss: 6.78991252856080063083\n",
      "Iteration 6034 => Loss: 6.78990016730976719828\n",
      "Iteration 6035 => Loss: 6.78988780772440758682\n",
      "Iteration 6036 => Loss: 6.78987544980450330456\n",
      "Iteration 6037 => Loss: 6.78986309354983141873\n",
      "Iteration 6038 => Loss: 6.78985073896016810835\n",
      "Iteration 6039 => Loss: 6.78983838603528244704\n",
      "Iteration 6040 => Loss: 6.78982603477495949562\n",
      "Iteration 6041 => Loss: 6.78981368517896832770\n",
      "Iteration 6042 => Loss: 6.78980133724707979326\n",
      "Iteration 6043 => Loss: 6.78978899097908250582\n",
      "Iteration 6044 => Loss: 6.78977664637474465081\n",
      "Iteration 6045 => Loss: 6.78976430343384063093\n",
      "Iteration 6046 => Loss: 6.78975196215614928974\n",
      "Iteration 6047 => Loss: 6.78973962254144502992\n",
      "Iteration 6048 => Loss: 6.78972728458951113595\n",
      "Iteration 6049 => Loss: 6.78971494830011224053\n",
      "Iteration 6050 => Loss: 6.78970261367303695721\n",
      "Iteration 6051 => Loss: 6.78969028070805169506\n",
      "Iteration 6052 => Loss: 6.78967794940492286315\n",
      "Iteration 6053 => Loss: 6.78966561976344440410\n",
      "Iteration 6054 => Loss: 6.78965329178338983240\n",
      "Iteration 6055 => Loss: 6.78964096546452200442\n",
      "Iteration 6056 => Loss: 6.78962864080663930366\n",
      "Iteration 6057 => Loss: 6.78961631780949925741\n",
      "Iteration 6058 => Loss: 6.78960399647287804470\n",
      "Iteration 6059 => Loss: 6.78959167679657049632\n",
      "Iteration 6060 => Loss: 6.78957935878033058685\n",
      "Iteration 6061 => Loss: 6.78956704242394426529\n",
      "Iteration 6062 => Loss: 6.78955472772719659247\n",
      "Iteration 6063 => Loss: 6.78954241468984509567\n",
      "Iteration 6064 => Loss: 6.78953010331168194114\n",
      "Iteration 6065 => Loss: 6.78951779359247531431\n",
      "Iteration 6066 => Loss: 6.78950548553200938784\n",
      "Iteration 6067 => Loss: 6.78949317913005412350\n",
      "Iteration 6068 => Loss: 6.78948087438638214763\n",
      "Iteration 6069 => Loss: 6.78946857130077940923\n",
      "Iteration 6070 => Loss: 6.78945626987301320554\n",
      "Iteration 6071 => Loss: 6.78944397010286948557\n",
      "Iteration 6072 => Loss: 6.78943167199012354018\n",
      "Iteration 6073 => Loss: 6.78941937553455066023\n",
      "Iteration 6074 => Loss: 6.78940708073591547844\n",
      "Iteration 6075 => Loss: 6.78939478759401993102\n",
      "Iteration 6076 => Loss: 6.78938249610861443983\n",
      "Iteration 6077 => Loss: 6.78937020627949561202\n",
      "Iteration 6078 => Loss: 6.78935791810642452759\n",
      "Iteration 6079 => Loss: 6.78934563158919335280\n",
      "Iteration 6080 => Loss: 6.78933334672756316763\n",
      "Iteration 6081 => Loss: 6.78932106352132613836\n",
      "Iteration 6082 => Loss: 6.78930878197024423315\n",
      "Iteration 6083 => Loss: 6.78929650207411139462\n",
      "Iteration 6084 => Loss: 6.78928422383268692641\n",
      "Iteration 6085 => Loss: 6.78927194724576299478\n",
      "Iteration 6086 => Loss: 6.78925967231310689698\n",
      "Iteration 6087 => Loss: 6.78924739903449925293\n",
      "Iteration 6088 => Loss: 6.78923512740972068258\n",
      "Iteration 6089 => Loss: 6.78922285743854203588\n",
      "Iteration 6090 => Loss: 6.78921058912073860370\n",
      "Iteration 6091 => Loss: 6.78919832245609722321\n",
      "Iteration 6092 => Loss: 6.78918605744438696803\n",
      "Iteration 6093 => Loss: 6.78917379408539201080\n",
      "Iteration 6094 => Loss: 6.78916153237888586602\n",
      "Iteration 6095 => Loss: 6.78914927232464204820\n",
      "Iteration 6096 => Loss: 6.78913701392244384181\n",
      "Iteration 6097 => Loss: 6.78912475717206032044\n",
      "Iteration 6098 => Loss: 6.78911250207328009765\n",
      "Iteration 6099 => Loss: 6.78910024862587935246\n",
      "Iteration 6100 => Loss: 6.78908799682962182942\n",
      "Iteration 6101 => Loss: 6.78907574668430058296\n",
      "Iteration 6102 => Loss: 6.78906349818968468668\n",
      "Iteration 6103 => Loss: 6.78905125134555831323\n",
      "Iteration 6104 => Loss: 6.78903900615169497712\n",
      "Iteration 6105 => Loss: 6.78902676260786375195\n",
      "Iteration 6106 => Loss: 6.78901452071386035669\n",
      "Iteration 6107 => Loss: 6.78900228046945031224\n",
      "Iteration 6108 => Loss: 6.78899004187441335034\n",
      "Iteration 6109 => Loss: 6.78897780492853453183\n",
      "Iteration 6110 => Loss: 6.78896556963157671305\n",
      "Iteration 6111 => Loss: 6.78895333598332584302\n",
      "Iteration 6112 => Loss: 6.78894110398357053526\n",
      "Iteration 6113 => Loss: 6.78892887363206831708\n",
      "Iteration 6114 => Loss: 6.78891664492860513747\n",
      "Iteration 6115 => Loss: 6.78890441787296605725\n",
      "Iteration 6116 => Loss: 6.78889219246492547910\n",
      "Iteration 6117 => Loss: 6.78887996870425158846\n",
      "Iteration 6118 => Loss: 6.78886774659074276883\n",
      "Iteration 6119 => Loss: 6.78885552612415832385\n",
      "Iteration 6120 => Loss: 6.78884330730427443257\n",
      "Iteration 6121 => Loss: 6.78883109013088681394\n",
      "Iteration 6122 => Loss: 6.78881887460376454158\n",
      "Iteration 6123 => Loss: 6.78880666072268557087\n",
      "Iteration 6124 => Loss: 6.78879444848742608087\n",
      "Iteration 6125 => Loss: 6.78878223789776580333\n",
      "Iteration 6126 => Loss: 6.78877002895348180544\n",
      "Iteration 6127 => Loss: 6.78875782165435648352\n",
      "Iteration 6128 => Loss: 6.78874561600016601659\n",
      "Iteration 6129 => Loss: 6.78873341199068836005\n",
      "Iteration 6130 => Loss: 6.78872120962570146929\n",
      "Iteration 6131 => Loss: 6.78870900890497974700\n",
      "Iteration 6132 => Loss: 6.78869680982831003035\n",
      "Iteration 6133 => Loss: 6.78868461239547382746\n",
      "Iteration 6134 => Loss: 6.78867241660623221833\n",
      "Iteration 6135 => Loss: 6.78866022246037648102\n",
      "Iteration 6136 => Loss: 6.78864802995768457095\n",
      "Iteration 6137 => Loss: 6.78863583909793089077\n",
      "Iteration 6138 => Loss: 6.78862364988089961315\n",
      "Iteration 6139 => Loss: 6.78861146230636425258\n",
      "Iteration 6140 => Loss: 6.78859927637410631718\n",
      "Iteration 6141 => Loss: 6.78858709208390553869\n",
      "Iteration 6142 => Loss: 6.78857490943553543161\n",
      "Iteration 6143 => Loss: 6.78856272842877839224\n",
      "Iteration 6144 => Loss: 6.78855054906341681686\n",
      "Iteration 6145 => Loss: 6.78853837133921889091\n",
      "Iteration 6146 => Loss: 6.78852619525597678063\n",
      "Iteration 6147 => Loss: 6.78851402081346133599\n",
      "Iteration 6148 => Loss: 6.78850184801145051239\n",
      "Iteration 6149 => Loss: 6.78848967684973025882\n",
      "Iteration 6150 => Loss: 6.78847750732806876073\n",
      "Iteration 6151 => Loss: 6.78846533944625818435\n",
      "Iteration 6152 => Loss: 6.78845317320406582695\n",
      "Iteration 6153 => Loss: 6.78844100860128030206\n",
      "Iteration 6154 => Loss: 6.78842884563766535422\n",
      "Iteration 6155 => Loss: 6.78841668431302025510\n",
      "Iteration 6156 => Loss: 6.78840452462711319015\n",
      "Iteration 6157 => Loss: 6.78839236657972300293\n",
      "Iteration 6158 => Loss: 6.78838021017063386608\n",
      "Iteration 6159 => Loss: 6.78836805539962107048\n",
      "Iteration 6160 => Loss: 6.78835590226646257150\n",
      "Iteration 6161 => Loss: 6.78834375077093721274\n",
      "Iteration 6162 => Loss: 6.78833160091282916682\n",
      "Iteration 6163 => Loss: 6.78831945269191461279\n",
      "Iteration 6164 => Loss: 6.78830730610797949964\n",
      "Iteration 6165 => Loss: 6.78829516116079112464\n",
      "Iteration 6166 => Loss: 6.78828301785013810132\n",
      "Iteration 6167 => Loss: 6.78827087617579838508\n",
      "Iteration 6168 => Loss: 6.78825873613754549041\n",
      "Iteration 6169 => Loss: 6.78824659773516980721\n",
      "Iteration 6170 => Loss: 6.78823446096843774455\n",
      "Iteration 6171 => Loss: 6.78822232583713969234\n",
      "Iteration 6172 => Loss: 6.78821019234105360596\n",
      "Iteration 6173 => Loss: 6.78819806047995566445\n",
      "Iteration 6174 => Loss: 6.78818593025362648774\n",
      "Iteration 6175 => Loss: 6.78817380166184491941\n",
      "Iteration 6176 => Loss: 6.78816167470439602027\n",
      "Iteration 6177 => Loss: 6.78814954938104619941\n",
      "Iteration 6178 => Loss: 6.78813742569159117579\n",
      "Iteration 6179 => Loss: 6.78812530363580446391\n",
      "Iteration 6180 => Loss: 6.78811318321346490734\n",
      "Iteration 6181 => Loss: 6.78810106442435401419\n",
      "Iteration 6182 => Loss: 6.78808894726825240440\n",
      "Iteration 6183 => Loss: 6.78807683174493270428\n",
      "Iteration 6184 => Loss: 6.78806471785418708009\n",
      "Iteration 6185 => Loss: 6.78805260559578815815\n",
      "Iteration 6186 => Loss: 6.78804049496951833476\n",
      "Iteration 6187 => Loss: 6.78802838597515378893\n",
      "Iteration 6188 => Loss: 6.78801627861247958151\n",
      "Iteration 6189 => Loss: 6.78800417288126656246\n",
      "Iteration 6190 => Loss: 6.78799206878130778620\n",
      "Iteration 6191 => Loss: 6.78797996631237143816\n",
      "Iteration 6192 => Loss: 6.78796786547424169100\n",
      "Iteration 6193 => Loss: 6.78795576626671692821\n",
      "Iteration 6194 => Loss: 6.78794366868954934802\n",
      "Iteration 6195 => Loss: 6.78793157274253200484\n",
      "Iteration 6196 => Loss: 6.78791947842544640679\n",
      "Iteration 6197 => Loss: 6.78790738573806962108\n",
      "Iteration 6198 => Loss: 6.78789529468018582037\n",
      "Iteration 6199 => Loss: 6.78788320525157740093\n",
      "Iteration 6200 => Loss: 6.78787111745200899549\n",
      "Iteration 6201 => Loss: 6.78785903128127632300\n",
      "Iteration 6202 => Loss: 6.78784694673916533247\n",
      "Iteration 6203 => Loss: 6.78783486382543710391\n",
      "Iteration 6204 => Loss: 6.78782278253987936267\n",
      "Iteration 6205 => Loss: 6.78781070288228161047\n",
      "Iteration 6206 => Loss: 6.78779862485242002634\n",
      "Iteration 6207 => Loss: 6.78778654845007789476\n",
      "Iteration 6208 => Loss: 6.78777447367502606568\n",
      "Iteration 6209 => Loss: 6.78776240052704782357\n",
      "Iteration 6210 => Loss: 6.78775032900593355834\n",
      "Iteration 6211 => Loss: 6.78773825911145234357\n",
      "Iteration 6212 => Loss: 6.78772619084339723372\n",
      "Iteration 6213 => Loss: 6.78771412420153463785\n",
      "Iteration 6214 => Loss: 6.78770205918565761039\n",
      "Iteration 6215 => Loss: 6.78768999579553788948\n",
      "Iteration 6216 => Loss: 6.78767793403096941773\n",
      "Iteration 6217 => Loss: 6.78766587389172215694\n",
      "Iteration 6218 => Loss: 6.78765381537757495067\n",
      "Iteration 6219 => Loss: 6.78764175848831374793\n",
      "Iteration 6220 => Loss: 6.78762970322372272136\n",
      "Iteration 6221 => Loss: 6.78761764958357716182\n",
      "Iteration 6222 => Loss: 6.78760559756766124195\n",
      "Iteration 6223 => Loss: 6.78759354717575469351\n",
      "Iteration 6224 => Loss: 6.78758149840764168914\n",
      "Iteration 6225 => Loss: 6.78756945126309663152\n",
      "Iteration 6226 => Loss: 6.78755740574190902237\n",
      "Iteration 6227 => Loss: 6.78754536184385592890\n",
      "Iteration 6228 => Loss: 6.78753331956872063557\n",
      "Iteration 6229 => Loss: 6.78752127891628109779\n",
      "Iteration 6230 => Loss: 6.78750923988632060002\n",
      "Iteration 6231 => Loss: 6.78749720247861798583\n",
      "Iteration 6232 => Loss: 6.78748516669296009240\n",
      "Iteration 6233 => Loss: 6.78747313252911688153\n",
      "Iteration 6234 => Loss: 6.78746109998688762488\n",
      "Iteration 6235 => Loss: 6.78744906906604583696\n",
      "Iteration 6236 => Loss: 6.78743703976636414410\n",
      "Iteration 6237 => Loss: 6.78742501208763826526\n",
      "Iteration 6238 => Loss: 6.78741298602963816222\n",
      "Iteration 6239 => Loss: 6.78740096159215511307\n",
      "Iteration 6240 => Loss: 6.78738893877495907958\n",
      "Iteration 6241 => Loss: 6.78737691757784666891\n",
      "Iteration 6242 => Loss: 6.78736489800058517829\n",
      "Iteration 6243 => Loss: 6.78735288004296855036\n",
      "Iteration 6244 => Loss: 6.78734086370476141781\n",
      "Iteration 6245 => Loss: 6.78732884898577104593\n",
      "Iteration 6246 => Loss: 6.78731683588575496202\n",
      "Iteration 6247 => Loss: 6.78730482440450888504\n",
      "Iteration 6248 => Loss: 6.78729281454181077038\n",
      "Iteration 6249 => Loss: 6.78728080629744034979\n",
      "Iteration 6250 => Loss: 6.78726879967118978954\n",
      "Iteration 6251 => Loss: 6.78725679466282283414\n",
      "Iteration 6252 => Loss: 6.78724479127213786711\n",
      "Iteration 6253 => Loss: 6.78723278949890751477\n",
      "Iteration 6254 => Loss: 6.78722078934291683794\n",
      "Iteration 6255 => Loss: 6.78720879080394734473\n",
      "Iteration 6256 => Loss: 6.78719679388178942503\n",
      "Iteration 6257 => Loss: 6.78718479857621037610\n",
      "Iteration 6258 => Loss: 6.78717280488699792329\n",
      "Iteration 6259 => Loss: 6.78716081281393712743\n",
      "Iteration 6260 => Loss: 6.78714882235681482570\n",
      "Iteration 6261 => Loss: 6.78713683351540630895\n",
      "Iteration 6262 => Loss: 6.78712484628949752619\n",
      "Iteration 6263 => Loss: 6.78711286067885755102\n",
      "Iteration 6264 => Loss: 6.78710087668328654331\n",
      "Iteration 6265 => Loss: 6.78708889430256068209\n",
      "Iteration 6266 => Loss: 6.78707691353646414001\n",
      "Iteration 6267 => Loss: 6.78706493438476687885\n",
      "Iteration 6268 => Loss: 6.78705295684726550576\n",
      "Iteration 6269 => Loss: 6.78704098092374508155\n",
      "Iteration 6270 => Loss: 6.78702900661397290349\n",
      "Iteration 6271 => Loss: 6.78701703391774557872\n",
      "Iteration 6272 => Loss: 6.78700506283483928627\n",
      "Iteration 6273 => Loss: 6.78699309336503198153\n",
      "Iteration 6274 => Loss: 6.78698112550811583077\n",
      "Iteration 6275 => Loss: 6.78696915926387145390\n",
      "Iteration 6276 => Loss: 6.78695719463207058908\n",
      "Iteration 6277 => Loss: 6.78694523161251961341\n",
      "Iteration 6278 => Loss: 6.78693327020497072510\n",
      "Iteration 6279 => Loss: 6.78692131040923296581\n",
      "Iteration 6280 => Loss: 6.78690935222507274460\n",
      "Iteration 6281 => Loss: 6.78689739565227689866\n",
      "Iteration 6282 => Loss: 6.78688544069063670605\n",
      "Iteration 6283 => Loss: 6.78687348733993012218\n",
      "Iteration 6284 => Loss: 6.78686153559993421425\n",
      "Iteration 6285 => Loss: 6.78684958547043581945\n",
      "Iteration 6286 => Loss: 6.78683763695121999859\n",
      "Iteration 6287 => Loss: 6.78682569004206381891\n",
      "Iteration 6288 => Loss: 6.78681374474276211117\n",
      "Iteration 6289 => Loss: 6.78680180105308661354\n",
      "Iteration 6290 => Loss: 6.78678985897282593953\n",
      "Iteration 6291 => Loss: 6.78677791850175626820\n",
      "Iteration 6292 => Loss: 6.78676597963966976579\n",
      "Iteration 6293 => Loss: 6.78675404238634794041\n",
      "Iteration 6294 => Loss: 6.78674210674156608292\n",
      "Iteration 6295 => Loss: 6.78673017270511014232\n",
      "Iteration 6296 => Loss: 6.78671824027678294300\n",
      "Iteration 6297 => Loss: 6.78670630945633934772\n",
      "Iteration 6298 => Loss: 6.78669438024357596362\n",
      "Iteration 6299 => Loss: 6.78668245263827518698\n",
      "Iteration 6300 => Loss: 6.78667052664021941411\n",
      "Iteration 6301 => Loss: 6.78665860224919548216\n",
      "Iteration 6302 => Loss: 6.78664667946498312290\n",
      "Iteration 6303 => Loss: 6.78663475828736739714\n",
      "Iteration 6304 => Loss: 6.78662283871612803665\n",
      "Iteration 6305 => Loss: 6.78661092075105454313\n",
      "Iteration 6306 => Loss: 6.78659900439192220745\n",
      "Iteration 6307 => Loss: 6.78658708963853207763\n",
      "Iteration 6308 => Loss: 6.78657517649065145093\n",
      "Iteration 6309 => Loss: 6.78656326494806183547\n",
      "Iteration 6310 => Loss: 6.78655135501056072656\n",
      "Iteration 6311 => Loss: 6.78653944667791986234\n",
      "Iteration 6312 => Loss: 6.78652753994993496178\n",
      "Iteration 6313 => Loss: 6.78651563482637421032\n",
      "Iteration 6314 => Loss: 6.78650373130703155056\n",
      "Iteration 6315 => Loss: 6.78649182939168760242\n",
      "Iteration 6316 => Loss: 6.78647992908013009128\n",
      "Iteration 6317 => Loss: 6.78646803037214407794\n",
      "Iteration 6318 => Loss: 6.78645613326750485328\n",
      "Iteration 6319 => Loss: 6.78644423776599747811\n",
      "Iteration 6320 => Loss: 6.78643234386742211228\n",
      "Iteration 6321 => Loss: 6.78642045157154516488\n",
      "Iteration 6322 => Loss: 6.78640856087815524944\n",
      "Iteration 6323 => Loss: 6.78639667178703209771\n",
      "Iteration 6324 => Loss: 6.78638478429797054048\n",
      "Iteration 6325 => Loss: 6.78637289841074853314\n",
      "Iteration 6326 => Loss: 6.78636101412515468922\n",
      "Iteration 6327 => Loss: 6.78634913144096518778\n",
      "Iteration 6328 => Loss: 6.78633725035796864233\n",
      "Iteration 6329 => Loss: 6.78632537087594744918\n",
      "Iteration 6330 => Loss: 6.78631349299468844549\n",
      "Iteration 6331 => Loss: 6.78630161671397402756\n",
      "Iteration 6332 => Loss: 6.78628974203359369710\n",
      "Iteration 6333 => Loss: 6.78627786895332008044\n",
      "Iteration 6334 => Loss: 6.78626599747294889653\n",
      "Iteration 6335 => Loss: 6.78625412759226076531\n",
      "Iteration 6336 => Loss: 6.78624225931104074760\n",
      "Iteration 6337 => Loss: 6.78623039262907035152\n",
      "Iteration 6338 => Loss: 6.78621852754613552605\n",
      "Iteration 6339 => Loss: 6.78620666406202577292\n",
      "Iteration 6340 => Loss: 6.78619480217651283027\n",
      "Iteration 6341 => Loss: 6.78618294188940396339\n",
      "Iteration 6342 => Loss: 6.78617108320045758774\n",
      "Iteration 6343 => Loss: 6.78615922610947652771\n",
      "Iteration 6344 => Loss: 6.78614737061623607417\n",
      "Iteration 6345 => Loss: 6.78613551672052661701\n",
      "Iteration 6346 => Loss: 6.78612366442213055251\n",
      "Iteration 6347 => Loss: 6.78611181372083027696\n",
      "Iteration 6348 => Loss: 6.78609996461641618026\n",
      "Iteration 6349 => Loss: 6.78608811710867332323\n",
      "Iteration 6350 => Loss: 6.78607627119736989130\n",
      "Iteration 6351 => Loss: 6.78606442688231314975\n",
      "Iteration 6352 => Loss: 6.78605258416327572490\n",
      "Iteration 6353 => Loss: 6.78604074304005244755\n",
      "Iteration 6354 => Loss: 6.78602890351241416766\n",
      "Iteration 6355 => Loss: 6.78601706558015305149\n",
      "Iteration 6356 => Loss: 6.78600522924305682437\n",
      "Iteration 6357 => Loss: 6.78599339450090965897\n",
      "Iteration 6358 => Loss: 6.78598156135349306339\n",
      "Iteration 6359 => Loss: 6.78596972980059387481\n",
      "Iteration 6360 => Loss: 6.78595789984199715406\n",
      "Iteration 6361 => Loss: 6.78594607147748529741\n",
      "Iteration 6362 => Loss: 6.78593424470684958294\n",
      "Iteration 6363 => Loss: 6.78592241952987684783\n",
      "Iteration 6364 => Loss: 6.78591059594634327112\n",
      "Iteration 6365 => Loss: 6.78589877395603124910\n",
      "Iteration 6366 => Loss: 6.78588695355874627069\n",
      "Iteration 6367 => Loss: 6.78587513475425563314\n",
      "Iteration 6368 => Loss: 6.78586331754234795000\n",
      "Iteration 6369 => Loss: 6.78585150192281183479\n",
      "Iteration 6370 => Loss: 6.78583968789543234834\n",
      "Iteration 6371 => Loss: 6.78582787545999277512\n",
      "Iteration 6372 => Loss: 6.78581606461628261684\n",
      "Iteration 6373 => Loss: 6.78580425536407627618\n",
      "Iteration 6374 => Loss: 6.78579244770317391300\n",
      "Iteration 6375 => Loss: 6.78578064163335437087\n",
      "Iteration 6376 => Loss: 6.78576883715440626332\n",
      "Iteration 6377 => Loss: 6.78575703426610665758\n",
      "Iteration 6378 => Loss: 6.78574523296825127261\n",
      "Iteration 6379 => Loss: 6.78573343326061539926\n",
      "Iteration 6380 => Loss: 6.78572163514299742104\n",
      "Iteration 6381 => Loss: 6.78570983861517795788\n",
      "Iteration 6382 => Loss: 6.78569804367694384695\n",
      "Iteration 6383 => Loss: 6.78568625032807215547\n",
      "Iteration 6384 => Loss: 6.78567445856835149698\n",
      "Iteration 6385 => Loss: 6.78566266839757581408\n",
      "Iteration 6386 => Loss: 6.78565087981552661489\n",
      "Iteration 6387 => Loss: 6.78563909282199961837\n",
      "Iteration 6388 => Loss: 6.78562730741674968726\n",
      "Iteration 6389 => Loss: 6.78561552359960096226\n",
      "Iteration 6390 => Loss: 6.78560374137031807606\n",
      "Iteration 6391 => Loss: 6.78559196072868520133\n",
      "Iteration 6392 => Loss: 6.78558018167450960334\n",
      "Iteration 6393 => Loss: 6.78556840420755147392\n",
      "Iteration 6394 => Loss: 6.78555662832761097292\n",
      "Iteration 6395 => Loss: 6.78554485403446427938\n",
      "Iteration 6396 => Loss: 6.78553308132791421770\n",
      "Iteration 6397 => Loss: 6.78552131020773252601\n",
      "Iteration 6398 => Loss: 6.78550954067371936418\n",
      "Iteration 6399 => Loss: 6.78549777272563492403\n",
      "Iteration 6400 => Loss: 6.78548600636329535263\n",
      "Iteration 6401 => Loss: 6.78547424158646972359\n",
      "Iteration 6402 => Loss: 6.78546247839494753862\n",
      "Iteration 6403 => Loss: 6.78545071678851652308\n",
      "Iteration 6404 => Loss: 6.78543895676696529051\n",
      "Iteration 6405 => Loss: 6.78542719833007712538\n",
      "Iteration 6406 => Loss: 6.78541544147763353578\n",
      "Iteration 6407 => Loss: 6.78540368620943912248\n",
      "Iteration 6408 => Loss: 6.78539193252525940636\n",
      "Iteration 6409 => Loss: 6.78538018042489010639\n",
      "Iteration 6410 => Loss: 6.78536842990811539522\n",
      "Iteration 6411 => Loss: 6.78535668097473010363\n",
      "Iteration 6412 => Loss: 6.78534493362450419340\n",
      "Iteration 6413 => Loss: 6.78533318785724581801\n",
      "Iteration 6414 => Loss: 6.78532144367272671559\n",
      "Iteration 6415 => Loss: 6.78530970107073194697\n",
      "Iteration 6416 => Loss: 6.78529796005105811929\n",
      "Iteration 6417 => Loss: 6.78528622061348674066\n",
      "Iteration 6418 => Loss: 6.78527448275781619458\n",
      "Iteration 6419 => Loss: 6.78526274648381111376\n",
      "Iteration 6420 => Loss: 6.78525101179126632900\n",
      "Iteration 6421 => Loss: 6.78523927867997755925\n",
      "Iteration 6422 => Loss: 6.78522754714972720080\n",
      "Iteration 6423 => Loss: 6.78521581720030386720\n",
      "Iteration 6424 => Loss: 6.78520408883148906654\n",
      "Iteration 6425 => Loss: 6.78519236204307141236\n",
      "Iteration 6426 => Loss: 6.78518063683483951820\n",
      "Iteration 6427 => Loss: 6.78516891320658732667\n",
      "Iteration 6428 => Loss: 6.78515719115808391138\n",
      "Iteration 6429 => Loss: 6.78514547068913209671\n",
      "Iteration 6430 => Loss: 6.78513375179951783167\n",
      "Iteration 6431 => Loss: 6.78512203448902706526\n",
      "Iteration 6432 => Loss: 6.78511031875743686470\n",
      "Iteration 6433 => Loss: 6.78509860460454561348\n",
      "Iteration 6434 => Loss: 6.78508689203014103697\n",
      "Iteration 6435 => Loss: 6.78507518103400197873\n",
      "Iteration 6436 => Loss: 6.78506347161591527595\n",
      "Iteration 6437 => Loss: 6.78505176377568286483\n",
      "Iteration 6438 => Loss: 6.78504005751307559535\n",
      "Iteration 6439 => Loss: 6.78502835282789540372\n",
      "Iteration 6440 => Loss: 6.78501664971992024533\n",
      "Iteration 6441 => Loss: 6.78500494818894139826\n",
      "Iteration 6442 => Loss: 6.78499324823474037061\n",
      "Iteration 6443 => Loss: 6.78498154985711110498\n",
      "Iteration 6444 => Loss: 6.78496985305583510950\n",
      "Iteration 6445 => Loss: 6.78495815783071432037\n",
      "Iteration 6446 => Loss: 6.78494646418151781120\n",
      "Iteration 6447 => Loss: 6.78493477210804396549\n",
      "Iteration 6448 => Loss: 6.78492308161007606770\n",
      "Iteration 6449 => Loss: 6.78491139268740450774\n",
      "Iteration 6450 => Loss: 6.78489970533982234002\n",
      "Iteration 6451 => Loss: 6.78488801956710396723\n",
      "Iteration 6452 => Loss: 6.78487633536904244380\n",
      "Iteration 6453 => Loss: 6.78486465274543171233\n",
      "Iteration 6454 => Loss: 6.78485297169605239276\n",
      "Iteration 6455 => Loss: 6.78484129222069665133\n",
      "Iteration 6456 => Loss: 6.78482961431915576611\n",
      "Iteration 6457 => Loss: 6.78481793799120591615\n",
      "Iteration 6458 => Loss: 6.78480626323664637312\n",
      "Iteration 6459 => Loss: 6.78479459005526397419\n",
      "Iteration 6460 => Loss: 6.78478291844683845113\n",
      "Iteration 6461 => Loss: 6.78477124841116374654\n",
      "Iteration 6462 => Loss: 6.78475957994803025031\n",
      "Iteration 6463 => Loss: 6.78474791305721502965\n",
      "Iteration 6464 => Loss: 6.78473624773852268532\n",
      "Iteration 6465 => Loss: 6.78472458399173117272\n",
      "Iteration 6466 => Loss: 6.78471292181662999354\n",
      "Iteration 6467 => Loss: 6.78470126121300687316\n",
      "Iteration 6468 => Loss: 6.78468960218065397783\n",
      "Iteration 6469 => Loss: 6.78467794471935015110\n",
      "Iteration 6470 => Loss: 6.78466628882889288832\n",
      "Iteration 6471 => Loss: 6.78465463450907613208\n",
      "Iteration 6472 => Loss: 6.78464298175966895599\n",
      "Iteration 6473 => Loss: 6.78463133058047951351\n",
      "Iteration 6474 => Loss: 6.78461968097127865462\n",
      "Iteration 6475 => Loss: 6.78460803293186476282\n",
      "Iteration 6476 => Loss: 6.78459638646203266887\n",
      "Iteration 6477 => Loss: 6.78458474156155766366\n",
      "Iteration 6478 => Loss: 6.78457309823023102524\n",
      "Iteration 6479 => Loss: 6.78456145646785113712\n",
      "Iteration 6480 => Loss: 6.78454981627419329016\n",
      "Iteration 6481 => Loss: 6.78453817764905320331\n",
      "Iteration 6482 => Loss: 6.78452654059222126648\n",
      "Iteration 6483 => Loss: 6.78451490510348165230\n",
      "Iteration 6484 => Loss: 6.78450327118263007975\n",
      "Iteration 6485 => Loss: 6.78449163882944628057\n",
      "Iteration 6486 => Loss: 6.78448000804372419736\n",
      "Iteration 6487 => Loss: 6.78446837882524889096\n",
      "Iteration 6488 => Loss: 6.78445675117381075125\n",
      "Iteration 6489 => Loss: 6.78444512508920993810\n",
      "Iteration 6490 => Loss: 6.78443350057121552510\n",
      "Iteration 6491 => Loss: 6.78442187761962589576\n",
      "Iteration 6492 => Loss: 6.78441025623423055180\n",
      "Iteration 6493 => Loss: 6.78439863641481899492\n",
      "Iteration 6494 => Loss: 6.78438701816117983867\n",
      "Iteration 6495 => Loss: 6.78437540147310347294\n",
      "Iteration 6496 => Loss: 6.78436378635037407037\n",
      "Iteration 6497 => Loss: 6.78435217279277846814\n",
      "Iteration 6498 => Loss: 6.78434056080011416157\n",
      "Iteration 6499 => Loss: 6.78432895037216709966\n",
      "Iteration 6500 => Loss: 6.78431734150872944866\n",
      "Iteration 6501 => Loss: 6.78430573420958449304\n",
      "Iteration 6502 => Loss: 6.78429412847451995816\n",
      "Iteration 6503 => Loss: 6.78428252430333778022\n",
      "Iteration 6504 => Loss: 6.78427092169581502645\n",
      "Iteration 6505 => Loss: 6.78425932065174386310\n",
      "Iteration 6506 => Loss: 6.78424772117090846280\n",
      "Iteration 6507 => Loss: 6.78423612325311253812\n",
      "Iteration 6508 => Loss: 6.78422452689813226812\n",
      "Iteration 6509 => Loss: 6.78421293210575981902\n",
      "Iteration 6510 => Loss: 6.78420133887579268617\n",
      "Iteration 6511 => Loss: 6.78418974720801681855\n",
      "Iteration 6512 => Loss: 6.78417815710220573067\n",
      "Iteration 6513 => Loss: 6.78416656855817468141\n",
      "Iteration 6514 => Loss: 6.78415498157569452076\n",
      "Iteration 6515 => Loss: 6.78414339615456629673\n",
      "Iteration 6516 => Loss: 6.78413181229457062926\n",
      "Iteration 6517 => Loss: 6.78412022999549613189\n",
      "Iteration 6518 => Loss: 6.78410864925715184626\n",
      "Iteration 6519 => Loss: 6.78409707007929974054\n",
      "Iteration 6520 => Loss: 6.78408549246174352731\n",
      "Iteration 6521 => Loss: 6.78407391640427359647\n",
      "Iteration 6522 => Loss: 6.78406234190667767336\n",
      "Iteration 6523 => Loss: 6.78405076896875680603\n",
      "Iteration 6524 => Loss: 6.78403919759028450898\n",
      "Iteration 6525 => Loss: 6.78402762777104761938\n",
      "Iteration 6526 => Loss: 6.78401605951085606705\n",
      "Iteration 6527 => Loss: 6.78400449280947803743\n",
      "Iteration 6528 => Loss: 6.78399292766672434851\n",
      "Iteration 6529 => Loss: 6.78398136408236673844\n",
      "Iteration 6530 => Loss: 6.78396980205620181437\n",
      "Iteration 6531 => Loss: 6.78395824158802884796\n",
      "Iteration 6532 => Loss: 6.78394668267762845915\n",
      "Iteration 6533 => Loss: 6.78393512532478570876\n",
      "Iteration 6534 => Loss: 6.78392356952930608571\n",
      "Iteration 6535 => Loss: 6.78391201529096221634\n",
      "Iteration 6536 => Loss: 6.78390046260955603685\n",
      "Iteration 6537 => Loss: 6.78388891148487704896\n",
      "Iteration 6538 => Loss: 6.78387736191671208985\n",
      "Iteration 6539 => Loss: 6.78386581390484888487\n",
      "Iteration 6540 => Loss: 6.78385426744908937025\n",
      "Iteration 6541 => Loss: 6.78384272254921061318\n",
      "Iteration 6542 => Loss: 6.78383117920500389175\n",
      "Iteration 6543 => Loss: 6.78381963741626758946\n",
      "Iteration 6544 => Loss: 6.78380809718279298437\n",
      "Iteration 6545 => Loss: 6.78379655850435714370\n",
      "Iteration 6546 => Loss: 6.78378502138076111549\n",
      "Iteration 6547 => Loss: 6.78377348581179262510\n",
      "Iteration 6548 => Loss: 6.78376195179724650330\n",
      "Iteration 6549 => Loss: 6.78375041933690603457\n",
      "Iteration 6550 => Loss: 6.78373888843056871423\n",
      "Iteration 6551 => Loss: 6.78372735907802049127\n",
      "Iteration 6552 => Loss: 6.78371583127905353194\n",
      "Iteration 6553 => Loss: 6.78370430503345911433\n",
      "Iteration 6554 => Loss: 6.78369278034103295738\n",
      "Iteration 6555 => Loss: 6.78368125720155035197\n",
      "Iteration 6556 => Loss: 6.78366973561481767518\n",
      "Iteration 6557 => Loss: 6.78365821558061199426\n",
      "Iteration 6558 => Loss: 6.78364669709873702175\n",
      "Iteration 6559 => Loss: 6.78363518016897959484\n",
      "Iteration 6560 => Loss: 6.78362366479112566253\n",
      "Iteration 6561 => Loss: 6.78361215096497360832\n",
      "Iteration 6562 => Loss: 6.78360063869031115757\n",
      "Iteration 6563 => Loss: 6.78358912796692781200\n",
      "Iteration 6564 => Loss: 6.78357761879461396148\n",
      "Iteration 6565 => Loss: 6.78356611117315910775\n",
      "Iteration 6566 => Loss: 6.78355460510236074612\n",
      "Iteration 6567 => Loss: 6.78354310058200571376\n",
      "Iteration 6568 => Loss: 6.78353159761188706511\n",
      "Iteration 6569 => Loss: 6.78352009619179252553\n",
      "Iteration 6570 => Loss: 6.78350859632151514944\n",
      "Iteration 6571 => Loss: 6.78349709800084710309\n",
      "Iteration 6572 => Loss: 6.78348560122957522367\n",
      "Iteration 6573 => Loss: 6.78347410600750233556\n",
      "Iteration 6574 => Loss: 6.78346261233441261140\n",
      "Iteration 6575 => Loss: 6.78345112021008933567\n",
      "Iteration 6576 => Loss: 6.78343962963433178004\n",
      "Iteration 6577 => Loss: 6.78342814060692944622\n",
      "Iteration 6578 => Loss: 6.78341665312767183593\n",
      "Iteration 6579 => Loss: 6.78340516719635999721\n",
      "Iteration 6580 => Loss: 6.78339368281277454997\n",
      "Iteration 6581 => Loss: 6.78338219997670588413\n",
      "Iteration 6582 => Loss: 6.78337071868795860041\n",
      "Iteration 6583 => Loss: 6.78335923894631243058\n",
      "Iteration 6584 => Loss: 6.78334776075155865271\n",
      "Iteration 6585 => Loss: 6.78333628410350097937\n",
      "Iteration 6586 => Loss: 6.78332480900191558959\n",
      "Iteration 6587 => Loss: 6.78331333544660886048\n",
      "Iteration 6588 => Loss: 6.78330186343735075383\n",
      "Iteration 6589 => Loss: 6.78329039297396096941\n",
      "Iteration 6590 => Loss: 6.78327892405620858085\n",
      "Iteration 6591 => Loss: 6.78326745668389374799\n",
      "Iteration 6592 => Loss: 6.78325599085681663070\n",
      "Iteration 6593 => Loss: 6.78324452657474896711\n",
      "Iteration 6594 => Loss: 6.78323306383750068704\n",
      "Iteration 6595 => Loss: 6.78322160264484974590\n",
      "Iteration 6596 => Loss: 6.78321014299660696167\n",
      "Iteration 6597 => Loss: 6.78319868489255028976\n",
      "Iteration 6598 => Loss: 6.78318722833246923187\n",
      "Iteration 6599 => Loss: 6.78317577331617016512\n",
      "Iteration 6600 => Loss: 6.78316431984342127492\n",
      "Iteration 6601 => Loss: 6.78315286791403959654\n",
      "Iteration 6602 => Loss: 6.78314141752779775629\n",
      "Iteration 6603 => Loss: 6.78312996868450124310\n",
      "Iteration 6604 => Loss: 6.78311852138393955869\n",
      "Iteration 6605 => Loss: 6.78310707562589332298\n",
      "Iteration 6606 => Loss: 6.78309563141017513033\n",
      "Iteration 6607 => Loss: 6.78308418873656382431\n",
      "Iteration 6608 => Loss: 6.78307274760484801845\n",
      "Iteration 6609 => Loss: 6.78306130801483053716\n",
      "Iteration 6610 => Loss: 6.78304986996629644125\n",
      "Iteration 6611 => Loss: 6.78303843345903967332\n",
      "Iteration 6612 => Loss: 6.78302699849284884692\n",
      "Iteration 6613 => Loss: 6.78301556506753122733\n",
      "Iteration 6614 => Loss: 6.78300413318286121722\n",
      "Iteration 6615 => Loss: 6.78299270283864252917\n",
      "Iteration 6616 => Loss: 6.78298127403465933583\n",
      "Iteration 6617 => Loss: 6.78296984677071268521\n",
      "Iteration 6618 => Loss: 6.78295842104658497362\n",
      "Iteration 6619 => Loss: 6.78294699686208435452\n",
      "Iteration 6620 => Loss: 6.78293557421698167786\n",
      "Iteration 6621 => Loss: 6.78292415311108687348\n",
      "Iteration 6622 => Loss: 6.78291273354419033126\n",
      "Iteration 6623 => Loss: 6.78290131551607622384\n",
      "Iteration 6624 => Loss: 6.78288989902654382291\n",
      "Iteration 6625 => Loss: 6.78287848407538795925\n",
      "Iteration 6626 => Loss: 6.78286707066239191732\n",
      "Iteration 6627 => Loss: 6.78285565878735496881\n",
      "Iteration 6628 => Loss: 6.78284424845007194449\n",
      "Iteration 6629 => Loss: 6.78283283965033145790\n",
      "Iteration 6630 => Loss: 6.78282143238792745166\n",
      "Iteration 6631 => Loss: 6.78281002666265564471\n",
      "Iteration 6632 => Loss: 6.78279862247430198607\n",
      "Iteration 6633 => Loss: 6.78278721982266574742\n",
      "Iteration 6634 => Loss: 6.78277581870753643045\n",
      "Iteration 6635 => Loss: 6.78276441912870886597\n",
      "Iteration 6636 => Loss: 6.78275302108597610840\n",
      "Iteration 6637 => Loss: 6.78274162457913298852\n",
      "Iteration 6638 => Loss: 6.78273022960796279079\n",
      "Iteration 6639 => Loss: 6.78271883617227100416\n",
      "Iteration 6640 => Loss: 6.78270744427184446579\n",
      "Iteration 6641 => Loss: 6.78269605390647623011\n",
      "Iteration 6642 => Loss: 6.78268466507596556880\n",
      "Iteration 6643 => Loss: 6.78267327778008954908\n",
      "Iteration 6644 => Loss: 6.78266189201866787073\n",
      "Iteration 6645 => Loss: 6.78265050779146871918\n",
      "Iteration 6646 => Loss: 6.78263912509830380060\n",
      "Iteration 6647 => Loss: 6.78262774393894485314\n",
      "Iteration 6648 => Loss: 6.78261636431320180662\n",
      "Iteration 6649 => Loss: 6.78260498622086860365\n",
      "Iteration 6650 => Loss: 6.78259360966173296958\n",
      "Iteration 6651 => Loss: 6.78258223463558973521\n",
      "Iteration 6652 => Loss: 6.78257086114222929041\n",
      "Iteration 6653 => Loss: 6.78255948918144913051\n",
      "Iteration 6654 => Loss: 6.78254811875304408630\n",
      "Iteration 6655 => Loss: 6.78253674985680454768\n",
      "Iteration 6656 => Loss: 6.78252538249252356906\n",
      "Iteration 6657 => Loss: 6.78251401665999242852\n",
      "Iteration 6658 => Loss: 6.78250265235901572680\n",
      "Iteration 6659 => Loss: 6.78249128958937319567\n",
      "Iteration 6660 => Loss: 6.78247992835086588315\n",
      "Iteration 6661 => Loss: 6.78246856864328684367\n",
      "Iteration 6662 => Loss: 6.78245721046642735530\n",
      "Iteration 6663 => Loss: 6.78244585382008935426\n",
      "Iteration 6664 => Loss: 6.78243449870405523683\n",
      "Iteration 6665 => Loss: 6.78242314511812871558\n",
      "Iteration 6666 => Loss: 6.78241179306209218680\n",
      "Iteration 6667 => Loss: 6.78240044253574492217\n",
      "Iteration 6668 => Loss: 6.78238909353888796971\n",
      "Iteration 6669 => Loss: 6.78237774607130639026\n",
      "Iteration 6670 => Loss: 6.78236640013280034367\n",
      "Iteration 6671 => Loss: 6.78235505572315489076\n",
      "Iteration 6672 => Loss: 6.78234371284217818499\n",
      "Iteration 6673 => Loss: 6.78233237148964640539\n",
      "Iteration 6674 => Loss: 6.78232103166537125816\n",
      "Iteration 6675 => Loss: 6.78230969336912892231\n",
      "Iteration 6676 => Loss: 6.78229835660072932768\n",
      "Iteration 6677 => Loss: 6.78228702135995842326\n",
      "Iteration 6678 => Loss: 6.78227568764660571077\n",
      "Iteration 6679 => Loss: 6.78226435546048112002\n",
      "Iteration 6680 => Loss: 6.78225302480136615912\n",
      "Iteration 6681 => Loss: 6.78224169566905654705\n",
      "Iteration 6682 => Loss: 6.78223036806335066728\n",
      "Iteration 6683 => Loss: 6.78221904198404335062\n",
      "Iteration 6684 => Loss: 6.78220771743091699335\n",
      "Iteration 6685 => Loss: 6.78219639440378152528\n",
      "Iteration 6686 => Loss: 6.78218507290242467178\n",
      "Iteration 6687 => Loss: 6.78217375292664481634\n",
      "Iteration 6688 => Loss: 6.78216243447623234886\n",
      "Iteration 6689 => Loss: 6.78215111755097499469\n",
      "Iteration 6690 => Loss: 6.78213980215067735458\n",
      "Iteration 6691 => Loss: 6.78212848827512626571\n",
      "Iteration 6692 => Loss: 6.78211717592412988154\n",
      "Iteration 6693 => Loss: 6.78210586509746793382\n",
      "Iteration 6694 => Loss: 6.78209455579493969424\n",
      "Iteration 6695 => Loss: 6.78208324801634354628\n",
      "Iteration 6696 => Loss: 6.78207194176147076803\n",
      "Iteration 6697 => Loss: 6.78206063703011530208\n",
      "Iteration 6698 => Loss: 6.78204933382207730830\n",
      "Iteration 6699 => Loss: 6.78203803213714362386\n",
      "Iteration 6700 => Loss: 6.78202673197511707315\n",
      "Iteration 6701 => Loss: 6.78201543333578271699\n",
      "Iteration 6702 => Loss: 6.78200413621894782068\n",
      "Iteration 6703 => Loss: 6.78199284062439211596\n",
      "Iteration 6704 => Loss: 6.78198154655192642082\n",
      "Iteration 6705 => Loss: 6.78197025400133046702\n",
      "Iteration 6706 => Loss: 6.78195896297240796713\n",
      "Iteration 6707 => Loss: 6.78194767346495552829\n",
      "Iteration 6708 => Loss: 6.78193638547876442857\n",
      "Iteration 6709 => Loss: 6.78192509901363038693\n",
      "Iteration 6710 => Loss: 6.78191381406934734599\n",
      "Iteration 6711 => Loss: 6.78190253064571013653\n",
      "Iteration 6712 => Loss: 6.78189124874252069475\n",
      "Iteration 6713 => Loss: 6.78187996835956496966\n",
      "Iteration 6714 => Loss: 6.78186868949664400930\n",
      "Iteration 6715 => Loss: 6.78185741215355175626\n",
      "Iteration 6716 => Loss: 6.78184613633007415956\n",
      "Iteration 6717 => Loss: 6.78183486202602026083\n",
      "Iteration 6718 => Loss: 6.78182358924118133814\n",
      "Iteration 6719 => Loss: 6.78181231797534866956\n",
      "Iteration 6720 => Loss: 6.78180104822831708589\n",
      "Iteration 6721 => Loss: 6.78178977999989296421\n",
      "Iteration 6722 => Loss: 6.78177851328985514812\n",
      "Iteration 6723 => Loss: 6.78176724809800735017\n",
      "Iteration 6724 => Loss: 6.78175598442415061839\n",
      "Iteration 6725 => Loss: 6.78174472226807178998\n",
      "Iteration 6726 => Loss: 6.78173346162957102479\n",
      "Iteration 6727 => Loss: 6.78172220250844226541\n",
      "Iteration 6728 => Loss: 6.78171094490448123082\n",
      "Iteration 6729 => Loss: 6.78169968881748186362\n",
      "Iteration 6730 => Loss: 6.78168843424723810642\n",
      "Iteration 6731 => Loss: 6.78167718119355455997\n",
      "Iteration 6732 => Loss: 6.78166592965621362055\n",
      "Iteration 6733 => Loss: 6.78165467963502432980\n",
      "Iteration 6734 => Loss: 6.78164343112977530126\n",
      "Iteration 6735 => Loss: 6.78163218414026225389\n",
      "Iteration 6736 => Loss: 6.78162093866628445937\n",
      "Iteration 6737 => Loss: 6.78160969470762964306\n",
      "Iteration 6738 => Loss: 6.78159845226410329389\n",
      "Iteration 6739 => Loss: 6.78158721133549313720\n",
      "Iteration 6740 => Loss: 6.78157597192160288557\n",
      "Iteration 6741 => Loss: 6.78156473402222825797\n",
      "Iteration 6742 => Loss: 6.78155349763715697975\n",
      "Iteration 6743 => Loss: 6.78154226276618921077\n",
      "Iteration 6744 => Loss: 6.78153102940912333452\n",
      "Iteration 6745 => Loss: 6.78151979756574885272\n",
      "Iteration 6746 => Loss: 6.78150856723586858976\n",
      "Iteration 6747 => Loss: 6.78149733841927471190\n",
      "Iteration 6748 => Loss: 6.78148611111576382626\n",
      "Iteration 6749 => Loss: 6.78147488532513609272\n",
      "Iteration 6750 => Loss: 6.78146366104718278933\n",
      "Iteration 6751 => Loss: 6.78145243828169874689\n",
      "Iteration 6752 => Loss: 6.78144121702848767796\n",
      "Iteration 6753 => Loss: 6.78142999728734707787\n",
      "Iteration 6754 => Loss: 6.78141877905805490201\n",
      "Iteration 6755 => Loss: 6.78140756234043440287\n",
      "Iteration 6756 => Loss: 6.78139634713425554224\n",
      "Iteration 6757 => Loss: 6.78138513343933180266\n",
      "Iteration 6758 => Loss: 6.78137392125545268584\n",
      "Iteration 6759 => Loss: 6.78136271058241568710\n",
      "Iteration 6760 => Loss: 6.78135150142001652540\n",
      "Iteration 6761 => Loss: 6.78134029376805269607\n",
      "Iteration 6762 => Loss: 6.78132908762632968802\n",
      "Iteration 6763 => Loss: 6.78131788299462456848\n",
      "Iteration 6764 => Loss: 6.78130667987274726727\n",
      "Iteration 6765 => Loss: 6.78129547826049439152\n",
      "Iteration 6766 => Loss: 6.78128427815765721931\n",
      "Iteration 6767 => Loss: 6.78127307956403146960\n",
      "Iteration 6768 => Loss: 6.78126188247942973675\n",
      "Iteration 6769 => Loss: 6.78125068690362020618\n",
      "Iteration 6770 => Loss: 6.78123949283642524222\n",
      "Iteration 6771 => Loss: 6.78122830027762990568\n",
      "Iteration 6772 => Loss: 6.78121710922703258007\n",
      "Iteration 6773 => Loss: 6.78120591968442454345\n",
      "Iteration 6774 => Loss: 6.78119473164962016654\n",
      "Iteration 6775 => Loss: 6.78118354512239385201\n",
      "Iteration 6776 => Loss: 6.78117236010255819423\n",
      "Iteration 6777 => Loss: 6.78116117658990091854\n",
      "Iteration 6778 => Loss: 6.78114999458422218481\n",
      "Iteration 6779 => Loss: 6.78113881408532570561\n",
      "Iteration 6780 => Loss: 6.78112763509300098264\n",
      "Iteration 6781 => Loss: 6.78111645760704107033\n",
      "Iteration 6782 => Loss: 6.78110528162725767487\n",
      "Iteration 6783 => Loss: 6.78109410715342697529\n",
      "Iteration 6784 => Loss: 6.78108293418536423047\n",
      "Iteration 6785 => Loss: 6.78107176272285538943\n",
      "Iteration 6786 => Loss: 6.78106059276570416472\n",
      "Iteration 6787 => Loss: 6.78104942431371071621\n",
      "Iteration 6788 => Loss: 6.78103825736665832835\n",
      "Iteration 6789 => Loss: 6.78102709192436314822\n",
      "Iteration 6790 => Loss: 6.78101592798660046668\n",
      "Iteration 6791 => Loss: 6.78100476555319175986\n",
      "Iteration 6792 => Loss: 6.78099360462391054227\n",
      "Iteration 6793 => Loss: 6.78098244519857473733\n",
      "Iteration 6794 => Loss: 6.78097128727696940587\n",
      "Iteration 6795 => Loss: 6.78096013085888493777\n",
      "Iteration 6796 => Loss: 6.78094897594413836828\n",
      "Iteration 6797 => Loss: 6.78093782253251742276\n",
      "Iteration 6798 => Loss: 6.78092667062381870835\n",
      "Iteration 6799 => Loss: 6.78091552021783439130\n",
      "Iteration 6800 => Loss: 6.78090437131437706597\n",
      "Iteration 6801 => Loss: 6.78089322391323445771\n",
      "Iteration 6802 => Loss: 6.78088207801419962095\n",
      "Iteration 6803 => Loss: 6.78087093361707982098\n",
      "Iteration 6804 => Loss: 6.78085979072166633586\n",
      "Iteration 6805 => Loss: 6.78084864932776820723\n",
      "Iteration 6806 => Loss: 6.78083750943515806142\n",
      "Iteration 6807 => Loss: 6.78082637104365737457\n",
      "Iteration 6808 => Loss: 6.78081523415305298386\n",
      "Iteration 6809 => Loss: 6.78080409876315126638\n",
      "Iteration 6810 => Loss: 6.78079296487373550661\n",
      "Iteration 6811 => Loss: 6.78078183248462096344\n",
      "Iteration 6812 => Loss: 6.78077070159559092133\n",
      "Iteration 6813 => Loss: 6.78075957220645086920\n",
      "Iteration 6814 => Loss: 6.78074844431699652603\n",
      "Iteration 6815 => Loss: 6.78073731792702538712\n",
      "Iteration 6816 => Loss: 6.78072619303633761234\n",
      "Iteration 6817 => Loss: 6.78071506964473158519\n",
      "Iteration 6818 => Loss: 6.78070394775199414283\n",
      "Iteration 6819 => Loss: 6.78069282735794320871\n",
      "Iteration 6820 => Loss: 6.78068170846237006089\n",
      "Iteration 6821 => Loss: 6.78067059106505709565\n",
      "Iteration 6822 => Loss: 6.78065947516582223642\n",
      "Iteration 6823 => Loss: 6.78064836076445409674\n",
      "Iteration 6824 => Loss: 6.78063724786074661921\n",
      "Iteration 6825 => Loss: 6.78062613645450529276\n",
      "Iteration 6826 => Loss: 6.78061502654553116543\n",
      "Iteration 6827 => Loss: 6.78060391813361373892\n",
      "Iteration 6828 => Loss: 6.78059281121856294305\n",
      "Iteration 6829 => Loss: 6.78058170580016561502\n",
      "Iteration 6830 => Loss: 6.78057060187821925012\n",
      "Iteration 6831 => Loss: 6.78055949945253289002\n",
      "Iteration 6832 => Loss: 6.78054839852289958912\n",
      "Iteration 6833 => Loss: 6.78053729908911506641\n",
      "Iteration 6834 => Loss: 6.78052620115098125808\n",
      "Iteration 6835 => Loss: 6.78051510470829388311\n",
      "Iteration 6836 => Loss: 6.78050400976085754223\n",
      "Iteration 6837 => Loss: 6.78049291630846262535\n",
      "Iteration 6838 => Loss: 6.78048182435091284503\n",
      "Iteration 6839 => Loss: 6.78047073388800480842\n",
      "Iteration 6840 => Loss: 6.78045964491953423448\n",
      "Iteration 6841 => Loss: 6.78044855744530661212\n",
      "Iteration 6842 => Loss: 6.78043747146511055490\n",
      "Iteration 6843 => Loss: 6.78042638697876309806\n",
      "Iteration 6844 => Loss: 6.78041530398604130880\n",
      "Iteration 6845 => Loss: 6.78040422248676311057\n",
      "Iteration 6846 => Loss: 6.78039314248070557056\n",
      "Iteration 6847 => Loss: 6.78038206396769105311\n",
      "Iteration 6848 => Loss: 6.78037098694749928995\n",
      "Iteration 6849 => Loss: 6.78035991141993843456\n",
      "Iteration 6850 => Loss: 6.78034883738480154136\n",
      "Iteration 6851 => Loss: 6.78033776484189854017\n",
      "Iteration 6852 => Loss: 6.78032669379101893270\n",
      "Iteration 6853 => Loss: 6.78031562423196643152\n",
      "Iteration 6854 => Loss: 6.78030455616453764378\n",
      "Iteration 6855 => Loss: 6.78029348958853095297\n",
      "Iteration 6856 => Loss: 6.78028242450375451256\n",
      "Iteration 6857 => Loss: 6.78027136090998006068\n",
      "Iteration 6858 => Loss: 6.78026029880703795527\n",
      "Iteration 6859 => Loss: 6.78024923819471236897\n",
      "Iteration 6860 => Loss: 6.78023817907281411976\n",
      "Iteration 6861 => Loss: 6.78022712144111849852\n",
      "Iteration 6862 => Loss: 6.78021606529944786956\n",
      "Iteration 6863 => Loss: 6.78020501064759262277\n",
      "Iteration 6864 => Loss: 6.78019395748535558255\n",
      "Iteration 6865 => Loss: 6.78018290581252447424\n",
      "Iteration 6866 => Loss: 6.78017185562891633310\n",
      "Iteration 6867 => Loss: 6.78016080693431444359\n",
      "Iteration 6868 => Loss: 6.78014975972852962371\n",
      "Iteration 6869 => Loss: 6.78013871401135315153\n",
      "Iteration 6870 => Loss: 6.78012766978258873962\n",
      "Iteration 6871 => Loss: 6.78011662704203654783\n",
      "Iteration 6872 => Loss: 6.78010558578949318331\n",
      "Iteration 6873 => Loss: 6.78009454602476235863\n",
      "Iteration 6874 => Loss: 6.78008350774763712820\n",
      "Iteration 6875 => Loss: 6.78007247095791854008\n",
      "Iteration 6876 => Loss: 6.78006143565541385954\n",
      "Iteration 6877 => Loss: 6.78005040183991702918\n",
      "Iteration 6878 => Loss: 6.78003936951122376797\n",
      "Iteration 6879 => Loss: 6.78002833866913956484\n",
      "Iteration 6880 => Loss: 6.78001730931345925057\n",
      "Iteration 6881 => Loss: 6.78000628144398831409\n",
      "Iteration 6882 => Loss: 6.77999525506052602708\n",
      "Iteration 6883 => Loss: 6.77998423016286277942\n",
      "Iteration 6884 => Loss: 6.77997320675081116548\n",
      "Iteration 6885 => Loss: 6.77996218482416601603\n",
      "Iteration 6886 => Loss: 6.77995116438272305004\n",
      "Iteration 6887 => Loss: 6.77994014542628953279\n",
      "Iteration 6888 => Loss: 6.77992912795465585418\n",
      "Iteration 6889 => Loss: 6.77991811196763016767\n",
      "Iteration 6890 => Loss: 6.77990709746500908039\n",
      "Iteration 6891 => Loss: 6.77989608444658831132\n",
      "Iteration 6892 => Loss: 6.77988507291218578388\n",
      "Iteration 6893 => Loss: 6.77987406286157501256\n",
      "Iteration 6894 => Loss: 6.77986305429457392080\n",
      "Iteration 6895 => Loss: 6.77985204721098178027\n",
      "Iteration 6896 => Loss: 6.77984104161058720450\n",
      "Iteration 6897 => Loss: 6.77983003749320278786\n",
      "Iteration 6898 => Loss: 6.77981903485862869019\n",
      "Iteration 6899 => Loss: 6.77980803370665618957\n",
      "Iteration 6900 => Loss: 6.77979703403708633402\n",
      "Iteration 6901 => Loss: 6.77978603584972638885\n",
      "Iteration 6902 => Loss: 6.77977503914437029664\n",
      "Iteration 6903 => Loss: 6.77976404392083065176\n",
      "Iteration 6904 => Loss: 6.77975305017888896231\n",
      "Iteration 6905 => Loss: 6.77974205791835515811\n",
      "Iteration 6906 => Loss: 6.77973106713902939902\n",
      "Iteration 6907 => Loss: 6.77972007784071539760\n",
      "Iteration 6908 => Loss: 6.77970909002320709646\n",
      "Iteration 6909 => Loss: 6.77969810368631176090\n",
      "Iteration 6910 => Loss: 6.77968711882982066896\n",
      "Iteration 6911 => Loss: 6.77967613545354463866\n",
      "Iteration 6912 => Loss: 6.77966515355727672443\n",
      "Iteration 6913 => Loss: 6.77965417314082063882\n",
      "Iteration 6914 => Loss: 6.77964319420397654170\n",
      "Iteration 6915 => Loss: 6.77963221674655081017\n",
      "Iteration 6916 => Loss: 6.77962124076833028141\n",
      "Iteration 6917 => Loss: 6.77961026626912932613\n",
      "Iteration 6918 => Loss: 6.77959929324873744605\n",
      "Iteration 6919 => Loss: 6.77958832170696101826\n",
      "Iteration 6920 => Loss: 6.77957735164360109081\n",
      "Iteration 6921 => Loss: 6.77956638305846581716\n",
      "Iteration 6922 => Loss: 6.77955541595134025812\n",
      "Iteration 6923 => Loss: 6.77954445032203523169\n",
      "Iteration 6924 => Loss: 6.77953348617034823320\n",
      "Iteration 6925 => Loss: 6.77952252349608475157\n",
      "Iteration 6926 => Loss: 6.77951156229903961759\n",
      "Iteration 6927 => Loss: 6.77950060257901920835\n",
      "Iteration 6928 => Loss: 6.77948964433582457190\n",
      "Iteration 6929 => Loss: 6.77947868756924876266\n",
      "Iteration 6930 => Loss: 6.77946773227909815773\n",
      "Iteration 6931 => Loss: 6.77945677846517558152\n",
      "Iteration 6932 => Loss: 6.77944582612728119386\n",
      "Iteration 6933 => Loss: 6.77943487526521249009\n",
      "Iteration 6934 => Loss: 6.77942392587877762367\n",
      "Iteration 6935 => Loss: 6.77941297796776431994\n",
      "Iteration 6936 => Loss: 6.77940203153199139052\n",
      "Iteration 6937 => Loss: 6.77939108657124833712\n",
      "Iteration 6938 => Loss: 6.77938014308534597774\n",
      "Iteration 6939 => Loss: 6.77936920107407647862\n",
      "Iteration 6940 => Loss: 6.77935826053723911144\n",
      "Iteration 6941 => Loss: 6.77934732147463936514\n",
      "Iteration 6942 => Loss: 6.77933638388607917591\n",
      "Iteration 6943 => Loss: 6.77932544777136403269\n",
      "Iteration 6944 => Loss: 6.77931451313028876626\n",
      "Iteration 6945 => Loss: 6.77930357996266330645\n",
      "Iteration 6946 => Loss: 6.77929264826827537860\n",
      "Iteration 6947 => Loss: 6.77928171804693224800\n",
      "Iteration 6948 => Loss: 6.77927078929844473265\n",
      "Iteration 6949 => Loss: 6.77925986202260322244\n",
      "Iteration 6950 => Loss: 6.77924893621921498266\n",
      "Iteration 6951 => Loss: 6.77923801188808017315\n",
      "Iteration 6952 => Loss: 6.77922708902899540107\n",
      "Iteration 6953 => Loss: 6.77921616764176615533\n",
      "Iteration 6954 => Loss: 6.77920524772619792486\n",
      "Iteration 6955 => Loss: 6.77919432928209264588\n",
      "Iteration 6956 => Loss: 6.77918341230924514917\n",
      "Iteration 6957 => Loss: 6.77917249680746181184\n",
      "Iteration 6958 => Loss: 6.77916158277654190556\n",
      "Iteration 6959 => Loss: 6.77915067021629003108\n",
      "Iteration 6960 => Loss: 6.77913975912649480193\n",
      "Iteration 6961 => Loss: 6.77912884950698746422\n",
      "Iteration 6962 => Loss: 6.77911794135754508517\n",
      "Iteration 6963 => Loss: 6.77910703467797759458\n",
      "Iteration 6964 => Loss: 6.77909612946807893508\n",
      "Iteration 6965 => Loss: 6.77908522572766880643\n",
      "Iteration 6966 => Loss: 6.77907432345653226946\n",
      "Iteration 6967 => Loss: 6.77906342265447570128\n",
      "Iteration 6968 => Loss: 6.77905252332130636717\n",
      "Iteration 6969 => Loss: 6.77904162545682176244\n",
      "Iteration 6970 => Loss: 6.77903072906083004057\n",
      "Iteration 6971 => Loss: 6.77901983413312514415\n",
      "Iteration 6972 => Loss: 6.77900894067350634487\n",
      "Iteration 6973 => Loss: 6.77899804868179423067\n",
      "Iteration 6974 => Loss: 6.77898715815776764515\n",
      "Iteration 6975 => Loss: 6.77897626910124806443\n",
      "Iteration 6976 => Loss: 6.77896538151203120748\n",
      "Iteration 6977 => Loss: 6.77895449538990835237\n",
      "Iteration 6978 => Loss: 6.77894361073469919887\n",
      "Iteration 6979 => Loss: 6.77893272754619236053\n",
      "Iteration 6980 => Loss: 6.77892184582420220806\n",
      "Iteration 6981 => Loss: 6.77891096556852357224\n",
      "Iteration 6982 => Loss: 6.77890008677895306022\n",
      "Iteration 6983 => Loss: 6.77888920945530948359\n",
      "Iteration 6984 => Loss: 6.77887833359738234407\n",
      "Iteration 6985 => Loss: 6.77886745920497535423\n",
      "Iteration 6986 => Loss: 6.77885658627789577935\n",
      "Iteration 6987 => Loss: 6.77884571481594289111\n",
      "Iteration 6988 => Loss: 6.77883484481892217843\n",
      "Iteration 6989 => Loss: 6.77882397628664001843\n",
      "Iteration 6990 => Loss: 6.77881310921887880738\n",
      "Iteration 6991 => Loss: 6.77880224361546979139\n",
      "Iteration 6992 => Loss: 6.77879137947619447857\n",
      "Iteration 6993 => Loss: 6.77878051680086013420\n",
      "Iteration 6994 => Loss: 6.77876965558928201716\n",
      "Iteration 6995 => Loss: 6.77875879584125229371\n",
      "Iteration 6996 => Loss: 6.77874793755656757099\n",
      "Iteration 6997 => Loss: 6.77873708073504133154\n",
      "Iteration 6998 => Loss: 6.77872622537646751795\n",
      "Iteration 6999 => Loss: 6.77871537148065250733\n",
      "Iteration 7000 => Loss: 6.77870451904740711768\n",
      "Iteration 7001 => Loss: 6.77869366807652973250\n",
      "Iteration 7002 => Loss: 6.77868281856781784711\n",
      "Iteration 7003 => Loss: 6.77867197052108316768\n",
      "Iteration 7004 => Loss: 6.77866112393611697229\n",
      "Iteration 7005 => Loss: 6.77865027881273185528\n",
      "Iteration 7006 => Loss: 6.77863943515072531198\n",
      "Iteration 7007 => Loss: 6.77862859294990460768\n",
      "Iteration 7008 => Loss: 6.77861775221006990222\n",
      "Iteration 7009 => Loss: 6.77860691293102757271\n",
      "Iteration 7010 => Loss: 6.77859607511258221990\n",
      "Iteration 7011 => Loss: 6.77858523875452245733\n",
      "Iteration 7012 => Loss: 6.77857440385667864291\n",
      "Iteration 7013 => Loss: 6.77856357041883406112\n",
      "Iteration 7014 => Loss: 6.77855273844079064816\n",
      "Iteration 7015 => Loss: 6.77854190792236188656\n",
      "Iteration 7016 => Loss: 6.77853107886333816623\n",
      "Iteration 7017 => Loss: 6.77852025126353829876\n",
      "Iteration 7018 => Loss: 6.77850942512275533858\n",
      "Iteration 7019 => Loss: 6.77849860044079388643\n",
      "Iteration 7020 => Loss: 6.77848777721746476033\n",
      "Iteration 7021 => Loss: 6.77847695545256456739\n",
      "Iteration 7022 => Loss: 6.77846613514589169114\n",
      "Iteration 7023 => Loss: 6.77845531629726405498\n",
      "Iteration 7024 => Loss: 6.77844449890647293699\n",
      "Iteration 7025 => Loss: 6.77843368297332560246\n",
      "Iteration 7026 => Loss: 6.77842286849763020484\n",
      "Iteration 7027 => Loss: 6.77841205547918157492\n",
      "Iteration 7028 => Loss: 6.77840124391779230706\n",
      "Iteration 7029 => Loss: 6.77839043381325456750\n",
      "Iteration 7030 => Loss: 6.77837962516538894420\n",
      "Iteration 7031 => Loss: 6.77836881797398138616\n",
      "Iteration 7032 => Loss: 6.77835801223884448774\n",
      "Iteration 7033 => Loss: 6.77834720795979084329\n",
      "Iteration 7034 => Loss: 6.77833640513660551363\n",
      "Iteration 7035 => Loss: 6.77832560376910464583\n",
      "Iteration 7036 => Loss: 6.77831480385708839975\n",
      "Iteration 7037 => Loss: 6.77830400540035427071\n",
      "Iteration 7038 => Loss: 6.77829320839871662940\n",
      "Iteration 7039 => Loss: 6.77828241285197918842\n",
      "Iteration 7040 => Loss: 6.77827161875994477214\n",
      "Iteration 7041 => Loss: 6.77826082612241176406\n",
      "Iteration 7042 => Loss: 6.77825003493918831765\n",
      "Iteration 7043 => Loss: 6.77823924521007903365\n",
      "Iteration 7044 => Loss: 6.77822845693488229557\n",
      "Iteration 7045 => Loss: 6.77821767011340892140\n",
      "Iteration 7046 => Loss: 6.77820688474545907098\n",
      "Iteration 7047 => Loss: 6.77819610083084267416\n",
      "Iteration 7048 => Loss: 6.77818531836935189716\n",
      "Iteration 7049 => Loss: 6.77817453736080377524\n",
      "Iteration 7050 => Loss: 6.77816375780500024462\n",
      "Iteration 7051 => Loss: 6.77815297970174146514\n",
      "Iteration 7052 => Loss: 6.77814220305083292573\n",
      "Iteration 7053 => Loss: 6.77813142785207212171\n",
      "Iteration 7054 => Loss: 6.77812065410527697651\n",
      "Iteration 7055 => Loss: 6.77810988181024232091\n",
      "Iteration 7056 => Loss: 6.77809911096677542020\n",
      "Iteration 7057 => Loss: 6.77808834157467909876\n",
      "Iteration 7058 => Loss: 6.77807757363376328641\n",
      "Iteration 7059 => Loss: 6.77806680714382370212\n",
      "Iteration 7060 => Loss: 6.77805604210467649295\n",
      "Iteration 7061 => Loss: 6.77804527851610760791\n",
      "Iteration 7062 => Loss: 6.77803451637795095763\n",
      "Iteration 7063 => Loss: 6.77802375568997828026\n",
      "Iteration 7064 => Loss: 6.77801299645201815736\n",
      "Iteration 7065 => Loss: 6.77800223866385476157\n",
      "Iteration 7066 => Loss: 6.77799148232531045721\n",
      "Iteration 7067 => Loss: 6.77798072743618540414\n",
      "Iteration 7068 => Loss: 6.77796997399628065040\n",
      "Iteration 7069 => Loss: 6.77795922200540612579\n",
      "Iteration 7070 => Loss: 6.77794847146335488475\n",
      "Iteration 7071 => Loss: 6.77793772236995017977\n",
      "Iteration 7072 => Loss: 6.77792697472498151257\n",
      "Iteration 7073 => Loss: 6.77791622852825792478\n",
      "Iteration 7074 => Loss: 6.77790548377958668169\n",
      "Iteration 7075 => Loss: 6.77789474047877504859\n",
      "Iteration 7076 => Loss: 6.77788399862561785625\n",
      "Iteration 7077 => Loss: 6.77787325821993036357\n",
      "Iteration 7078 => Loss: 6.77786251926151184222\n",
      "Iteration 7079 => Loss: 6.77785178175016600477\n",
      "Iteration 7080 => Loss: 6.77784104568570100469\n",
      "Iteration 7081 => Loss: 6.77783031106792233089\n",
      "Iteration 7082 => Loss: 6.77781957789663991321\n",
      "Iteration 7083 => Loss: 6.77780884617164680606\n",
      "Iteration 7084 => Loss: 6.77779811589275826833\n",
      "Iteration 7085 => Loss: 6.77778738705977268353\n",
      "Iteration 7086 => Loss: 6.77777665967249642875\n",
      "Iteration 7087 => Loss: 6.77776593373074209836\n",
      "Iteration 7088 => Loss: 6.77775520923430274678\n",
      "Iteration 7089 => Loss: 6.77774448618299807379\n",
      "Iteration 7090 => Loss: 6.77773376457662202199\n",
      "Iteration 7091 => Loss: 6.77772304441498185668\n",
      "Iteration 7092 => Loss: 6.77771232569788129041\n",
      "Iteration 7093 => Loss: 6.77770160842513380572\n",
      "Iteration 7094 => Loss: 6.77769089259654222701\n",
      "Iteration 7095 => Loss: 6.77768017821189783234\n",
      "Iteration 7096 => Loss: 6.77766946527103097964\n",
      "Iteration 7097 => Loss: 6.77765875377372495336\n",
      "Iteration 7098 => Loss: 6.77764804371979590059\n",
      "Iteration 7099 => Loss: 6.77763733510905463930\n",
      "Iteration 7100 => Loss: 6.77762662794128800670\n",
      "Iteration 7101 => Loss: 6.77761592221631836708\n",
      "Iteration 7102 => Loss: 6.77760521793394854484\n",
      "Iteration 7103 => Loss: 6.77759451509397958802\n",
      "Iteration 7104 => Loss: 6.77758381369621965007\n",
      "Iteration 7105 => Loss: 6.77757311374046800267\n",
      "Iteration 7106 => Loss: 6.77756241522654523379\n",
      "Iteration 7107 => Loss: 6.77755171815424262149\n",
      "Iteration 7108 => Loss: 6.77754102252336831924\n",
      "Iteration 7109 => Loss: 6.77753032833373847410\n",
      "Iteration 7110 => Loss: 6.77751963558514791686\n",
      "Iteration 7111 => Loss: 6.77750894427740835368\n",
      "Iteration 7112 => Loss: 6.77749825441031550355\n",
      "Iteration 7113 => Loss: 6.77748756598369617166\n",
      "Iteration 7114 => Loss: 6.77747687899733097794\n",
      "Iteration 7115 => Loss: 6.77746619345104317489\n",
      "Iteration 7116 => Loss: 6.77745550934463381054\n",
      "Iteration 7117 => Loss: 6.77744482667791192654\n",
      "Iteration 7118 => Loss: 6.77743414545067768273\n",
      "Iteration 7119 => Loss: 6.77742346566274189712\n",
      "Iteration 7120 => Loss: 6.77741278731389673595\n",
      "Iteration 7121 => Loss: 6.77740211040397166897\n",
      "Iteration 7122 => Loss: 6.77739143493276330332\n",
      "Iteration 7123 => Loss: 6.77738076090005936436\n",
      "Iteration 7124 => Loss: 6.77737008830570264450\n",
      "Iteration 7125 => Loss: 6.77735941714946132919\n",
      "Iteration 7126 => Loss: 6.77734874743116755269\n",
      "Iteration 7127 => Loss: 6.77733807915062236304\n",
      "Iteration 7128 => Loss: 6.77732741230762592011\n",
      "Iteration 7129 => Loss: 6.77731674690198548916\n",
      "Iteration 7130 => Loss: 6.77730608293350389459\n",
      "Iteration 7131 => Loss: 6.77729542040200438890\n",
      "Iteration 7132 => Loss: 6.77728475930727380927\n",
      "Iteration 7133 => Loss: 6.77727409964912830276\n",
      "Iteration 7134 => Loss: 6.77726344142738046372\n",
      "Iteration 7135 => Loss: 6.77725278464181624116\n",
      "Iteration 7136 => Loss: 6.77724212929226155211\n",
      "Iteration 7137 => Loss: 6.77723147537851300370\n",
      "Iteration 7138 => Loss: 6.77722082290038496666\n",
      "Iteration 7139 => Loss: 6.77721017185767582447\n",
      "Iteration 7140 => Loss: 6.77719952225019550696\n",
      "Iteration 7141 => Loss: 6.77718887407774861487\n",
      "Iteration 7142 => Loss: 6.77717822734014863073\n",
      "Iteration 7143 => Loss: 6.77716758203719660258\n",
      "Iteration 7144 => Loss: 6.77715693816869890753\n",
      "Iteration 7145 => Loss: 6.77714629573445748179\n",
      "Iteration 7146 => Loss: 6.77713565473429646602\n",
      "Iteration 7147 => Loss: 6.77712501516800536194\n",
      "Iteration 7148 => Loss: 6.77711437703539409938\n",
      "Iteration 7149 => Loss: 6.77710374033627349633\n",
      "Iteration 7150 => Loss: 6.77709310507044637717\n",
      "Iteration 7151 => Loss: 6.77708247123773066534\n",
      "Iteration 7152 => Loss: 6.77707183883791230983\n",
      "Iteration 7153 => Loss: 6.77706120787082433310\n",
      "Iteration 7154 => Loss: 6.77705057833624646690\n",
      "Iteration 7155 => Loss: 6.77703995023400906916\n",
      "Iteration 7156 => Loss: 6.77702932356390874702\n",
      "Iteration 7157 => Loss: 6.77701869832575010122\n",
      "Iteration 7158 => Loss: 6.77700807451934394976\n",
      "Iteration 7159 => Loss: 6.77699745214449755792\n",
      "Iteration 7160 => Loss: 6.77698683120101375010\n",
      "Iteration 7161 => Loss: 6.77697621168870156794\n",
      "Iteration 7162 => Loss: 6.77696559360737982303\n",
      "Iteration 7163 => Loss: 6.77695497695683446437\n",
      "Iteration 7164 => Loss: 6.77694436173688874447\n",
      "Iteration 7165 => Loss: 6.77693374794734193500\n",
      "Iteration 7166 => Loss: 6.77692313558800041307\n",
      "Iteration 7167 => Loss: 6.77691252465868476662\n",
      "Iteration 7168 => Loss: 6.77690191515918805010\n",
      "Iteration 7169 => Loss: 6.77689130708932019331\n",
      "Iteration 7170 => Loss: 6.77688070044889823151\n",
      "Iteration 7171 => Loss: 6.77687009523771344277\n",
      "Iteration 7172 => Loss: 6.77685949145558463869\n",
      "Iteration 7173 => Loss: 6.77684888910231642001\n",
      "Iteration 7174 => Loss: 6.77683828817771782838\n",
      "Iteration 7175 => Loss: 6.77682768868158991182\n",
      "Iteration 7176 => Loss: 6.77681709061374881742\n",
      "Iteration 7177 => Loss: 6.77680649397399559319\n",
      "Iteration 7178 => Loss: 6.77679589876214283350\n",
      "Iteration 7179 => Loss: 6.77678530497799691545\n",
      "Iteration 7180 => Loss: 6.77677471262136243979\n",
      "Iteration 7181 => Loss: 6.77676412169204844815\n",
      "Iteration 7182 => Loss: 6.77675353218985776493\n",
      "Iteration 7183 => Loss: 6.77674294411461364263\n",
      "Iteration 7184 => Loss: 6.77673235746611180019\n",
      "Iteration 7185 => Loss: 6.77672177224415683838\n",
      "Iteration 7186 => Loss: 6.77671118844856135155\n",
      "Iteration 7187 => Loss: 6.77670060607914059858\n",
      "Iteration 7188 => Loss: 6.77669002513568408119\n",
      "Iteration 7189 => Loss: 6.77667944561801594006\n",
      "Iteration 7190 => Loss: 6.77666886752593367049\n",
      "Iteration 7191 => Loss: 6.77665829085925874864\n",
      "Iteration 7192 => Loss: 6.77664771561778422893\n",
      "Iteration 7193 => Loss: 6.77663714180132714660\n",
      "Iteration 7194 => Loss: 6.77662656940970009600\n",
      "Iteration 7195 => Loss: 6.77661599844268813797\n",
      "Iteration 7196 => Loss: 6.77660542890012251860\n",
      "Iteration 7197 => Loss: 6.77659486078180073321\n",
      "Iteration 7198 => Loss: 6.77658429408754248158\n",
      "Iteration 7199 => Loss: 6.77657372881713726542\n",
      "Iteration 7200 => Loss: 6.77656316497040833724\n",
      "Iteration 7201 => Loss: 6.77655260254715763324\n",
      "Iteration 7202 => Loss: 6.77654204154719330688\n",
      "Iteration 7203 => Loss: 6.77653148197032262345\n",
      "Iteration 7204 => Loss: 6.77652092381635906548\n",
      "Iteration 7205 => Loss: 6.77651036708510901008\n",
      "Iteration 7206 => Loss: 6.77649981177637794616\n",
      "Iteration 7207 => Loss: 6.77648925788997669173\n",
      "Iteration 7208 => Loss: 6.77647870542571073571\n",
      "Iteration 7209 => Loss: 6.77646815438338023796\n",
      "Iteration 7210 => Loss: 6.77645760476281733276\n",
      "Iteration 7211 => Loss: 6.77644705656381152181\n",
      "Iteration 7212 => Loss: 6.77643650978617806402\n",
      "Iteration 7213 => Loss: 6.77642596442972156012\n",
      "Iteration 7214 => Loss: 6.77641542049425193994\n",
      "Iteration 7215 => Loss: 6.77640487797958179783\n",
      "Iteration 7216 => Loss: 6.77639433688551573454\n",
      "Iteration 7217 => Loss: 6.77638379721186279170\n",
      "Iteration 7218 => Loss: 6.77637325895843201096\n",
      "Iteration 7219 => Loss: 6.77636272212502799306\n",
      "Iteration 7220 => Loss: 6.77635218671146510871\n",
      "Iteration 7221 => Loss: 6.77634165271755595228\n",
      "Iteration 7222 => Loss: 6.77633112014309713089\n",
      "Iteration 7223 => Loss: 6.77632058898790212709\n",
      "Iteration 7224 => Loss: 6.77631005925177909432\n",
      "Iteration 7225 => Loss: 6.77629953093454862056\n",
      "Iteration 7226 => Loss: 6.77628900403600020752\n",
      "Iteration 7227 => Loss: 6.77627847855595710769\n",
      "Iteration 7228 => Loss: 6.77626795449422214546\n",
      "Iteration 7229 => Loss: 6.77625743185060613882\n",
      "Iteration 7230 => Loss: 6.77624691062491901761\n",
      "Iteration 7231 => Loss: 6.77623639081696005348\n",
      "Iteration 7232 => Loss: 6.77622587242655516349\n",
      "Iteration 7233 => Loss: 6.77621535545349917840\n",
      "Iteration 7234 => Loss: 6.77620483989760291621\n",
      "Iteration 7235 => Loss: 6.77619432575869140578\n",
      "Iteration 7236 => Loss: 6.77618381303655148429\n",
      "Iteration 7237 => Loss: 6.77617330173100373969\n",
      "Iteration 7238 => Loss: 6.77616279184185099638\n",
      "Iteration 7239 => Loss: 6.77615228336890940142\n",
      "Iteration 7240 => Loss: 6.77614177631198710827\n",
      "Iteration 7241 => Loss: 6.77613127067089227040\n",
      "Iteration 7242 => Loss: 6.77612076644543304127\n",
      "Iteration 7243 => Loss: 6.77611026363541490980\n",
      "Iteration 7244 => Loss: 6.77609976224065402306\n",
      "Iteration 7245 => Loss: 6.77608926226095586998\n",
      "Iteration 7246 => Loss: 6.77607876369613215672\n",
      "Iteration 7247 => Loss: 6.77606826654599370130\n",
      "Iteration 7248 => Loss: 6.77605777081033888720\n",
      "Iteration 7249 => Loss: 6.77604727648899451964\n",
      "Iteration 7250 => Loss: 6.77603678358175365304\n",
      "Iteration 7251 => Loss: 6.77602629208843776354\n",
      "Iteration 7252 => Loss: 6.77601580200884878735\n",
      "Iteration 7253 => Loss: 6.77600531334280198337\n",
      "Iteration 7254 => Loss: 6.77599482609009839962\n",
      "Iteration 7255 => Loss: 6.77598434025055773589\n",
      "Iteration 7256 => Loss: 6.77597385582397659931\n",
      "Iteration 7257 => Loss: 6.77596337281018357146\n",
      "Iteration 7258 => Loss: 6.77595289120896993040\n",
      "Iteration 7259 => Loss: 6.77594241102015715228\n",
      "Iteration 7260 => Loss: 6.77593193224354983784\n",
      "Iteration 7261 => Loss: 6.77592145487895525235\n",
      "Iteration 7262 => Loss: 6.77591097892619220744\n",
      "Iteration 7263 => Loss: 6.77590050438505731023\n",
      "Iteration 7264 => Loss: 6.77589003125537381322\n",
      "Iteration 7265 => Loss: 6.77587955953694187627\n",
      "Iteration 7266 => Loss: 6.77586908922956965284\n",
      "Iteration 7267 => Loss: 6.77585862033308572450\n",
      "Iteration 7268 => Loss: 6.77584815284727337570\n",
      "Iteration 7269 => Loss: 6.77583768677196296437\n",
      "Iteration 7270 => Loss: 6.77582722210695731491\n",
      "Iteration 7271 => Loss: 6.77581675885206546894\n",
      "Iteration 7272 => Loss: 6.77580629700709824448\n",
      "Iteration 7273 => Loss: 6.77579583657186557133\n",
      "Iteration 7274 => Loss: 6.77578537754617382660\n",
      "Iteration 7275 => Loss: 6.77577491992984182190\n",
      "Iteration 7276 => Loss: 6.77576446372267060525\n",
      "Iteration 7277 => Loss: 6.77575400892446833012\n",
      "Iteration 7278 => Loss: 6.77574355553506446626\n",
      "Iteration 7279 => Loss: 6.77573310355424318630\n",
      "Iteration 7280 => Loss: 6.77572265298183484816\n",
      "Iteration 7281 => Loss: 6.77571220381763872354\n",
      "Iteration 7282 => Loss: 6.77570175606147540037\n",
      "Iteration 7283 => Loss: 6.77569130971313704492\n",
      "Iteration 7284 => Loss: 6.77568086477245046240\n",
      "Iteration 7285 => Loss: 6.77567042123922025354\n",
      "Iteration 7286 => Loss: 6.77565997911325368364\n",
      "Iteration 7287 => Loss: 6.77564953839436245886\n",
      "Iteration 7288 => Loss: 6.77563909908236894353\n",
      "Iteration 7289 => Loss: 6.77562866117706708025\n",
      "Iteration 7290 => Loss: 6.77561822467828100969\n",
      "Iteration 7291 => Loss: 6.77560778958580378628\n",
      "Iteration 7292 => Loss: 6.77559735589946221523\n",
      "Iteration 7293 => Loss: 6.77558692361905645640\n",
      "Iteration 7294 => Loss: 6.77557649274440354503\n",
      "Iteration 7295 => Loss: 6.77556606327530985823\n",
      "Iteration 7296 => Loss: 6.77555563521159420759\n",
      "Iteration 7297 => Loss: 6.77554520855305053573\n",
      "Iteration 7298 => Loss: 6.77553478329950920056\n",
      "Iteration 7299 => Loss: 6.77552435945076947377\n",
      "Iteration 7300 => Loss: 6.77551393700663950881\n",
      "Iteration 7301 => Loss: 6.77550351596693989364\n",
      "Iteration 7302 => Loss: 6.77549309633147522902\n",
      "Iteration 7303 => Loss: 6.77548267810005810929\n",
      "Iteration 7304 => Loss: 6.77547226127249313521\n",
      "Iteration 7305 => Loss: 6.77546184584860089473\n",
      "Iteration 7306 => Loss: 6.77545143182818598859\n",
      "Iteration 7307 => Loss: 6.77544101921106634023\n",
      "Iteration 7308 => Loss: 6.77543060799704122132\n",
      "Iteration 7309 => Loss: 6.77542019818592944347\n",
      "Iteration 7310 => Loss: 6.77540978977754360102\n",
      "Iteration 7311 => Loss: 6.77539938277169007108\n",
      "Iteration 7312 => Loss: 6.77538897716818322436\n",
      "Iteration 7313 => Loss: 6.77537857296682766162\n",
      "Iteration 7314 => Loss: 6.77536817016744219444\n",
      "Iteration 7315 => Loss: 6.77535776876983231176\n",
      "Iteration 7316 => Loss: 6.77534736877381238429\n",
      "Iteration 7317 => Loss: 6.77533697017919767092\n",
      "Iteration 7318 => Loss: 6.77532657298578566696\n",
      "Iteration 7319 => Loss: 6.77531617719340406580\n",
      "Iteration 7320 => Loss: 6.77530578280184592188\n",
      "Iteration 7321 => Loss: 6.77529538981094425765\n",
      "Iteration 7322 => Loss: 6.77528499822049479206\n",
      "Iteration 7323 => Loss: 6.77527460803031367220\n",
      "Iteration 7324 => Loss: 6.77526421924020727516\n",
      "Iteration 7325 => Loss: 6.77525383184999618891\n",
      "Iteration 7326 => Loss: 6.77524344585948234965\n",
      "Iteration 7327 => Loss: 6.77523306126849256259\n",
      "Iteration 7328 => Loss: 6.77522267807681100038\n",
      "Iteration 7329 => Loss: 6.77521229628427956726\n",
      "Iteration 7330 => Loss: 6.77520191589068687676\n",
      "Iteration 7331 => Loss: 6.77519153689585262867\n",
      "Iteration 7332 => Loss: 6.77518115929959474641\n",
      "Iteration 7333 => Loss: 6.77517078310171250166\n",
      "Iteration 7334 => Loss: 6.77516040830202825873\n",
      "Iteration 7335 => Loss: 6.77515003490034573019\n",
      "Iteration 7336 => Loss: 6.77513966289648283947\n",
      "Iteration 7337 => Loss: 6.77512929229024862821\n",
      "Iteration 7338 => Loss: 6.77511892308145036168\n",
      "Iteration 7339 => Loss: 6.77510855526990951603\n",
      "Iteration 7340 => Loss: 6.77509818885542358657\n",
      "Iteration 7341 => Loss: 6.77508782383782026670\n",
      "Iteration 7342 => Loss: 6.77507746021690149263\n",
      "Iteration 7343 => Loss: 6.77506709799248074688\n",
      "Iteration 7344 => Loss: 6.77505673716437151199\n",
      "Iteration 7345 => Loss: 6.77504637773238815868\n",
      "Iteration 7346 => Loss: 6.77503601969633173496\n",
      "Iteration 7347 => Loss: 6.77502566305603171060\n",
      "Iteration 7348 => Loss: 6.77501530781128291636\n",
      "Iteration 7349 => Loss: 6.77500495396190238750\n",
      "Iteration 7350 => Loss: 6.77499460150770538291\n",
      "Iteration 7351 => Loss: 6.77498425044850538512\n",
      "Iteration 7352 => Loss: 6.77497390078410788306\n",
      "Iteration 7353 => Loss: 6.77496355251432813560\n",
      "Iteration 7354 => Loss: 6.77495320563898406618\n",
      "Iteration 7355 => Loss: 6.77494286015787849919\n",
      "Iteration 7356 => Loss: 6.77493251607082491716\n",
      "Iteration 7357 => Loss: 6.77492217337763769081\n",
      "Iteration 7358 => Loss: 6.77491183207813385536\n",
      "Iteration 7359 => Loss: 6.77490149217211978794\n",
      "Iteration 7360 => Loss: 6.77489115365940008928\n",
      "Iteration 7361 => Loss: 6.77488081653980867003\n",
      "Iteration 7362 => Loss: 6.77487048081314391368\n",
      "Iteration 7363 => Loss: 6.77486014647921219733\n",
      "Iteration 7364 => Loss: 6.77484981353784387892\n",
      "Iteration 7365 => Loss: 6.77483948198882224290\n",
      "Iteration 7366 => Loss: 6.77482915183198830533\n",
      "Iteration 7367 => Loss: 6.77481882306714400244\n",
      "Iteration 7368 => Loss: 6.77480849569409926403\n",
      "Iteration 7369 => Loss: 6.77479816971266846082\n",
      "Iteration 7370 => Loss: 6.77478784512266951623\n",
      "Iteration 7371 => Loss: 6.77477752192390525465\n",
      "Iteration 7372 => Loss: 6.77476720011619359951\n",
      "Iteration 7373 => Loss: 6.77475687969934536881\n",
      "Iteration 7374 => Loss: 6.77474656067317582142\n",
      "Iteration 7375 => Loss: 6.77473624303749488718\n",
      "Iteration 7376 => Loss: 6.77472592679211693678\n",
      "Iteration 7377 => Loss: 6.77471561193685900548\n",
      "Iteration 7378 => Loss: 6.77470529847151592406\n",
      "Iteration 7379 => Loss: 6.77469498639592071498\n",
      "Iteration 7380 => Loss: 6.77468467570987975535\n",
      "Iteration 7381 => Loss: 6.77467436641320475132\n",
      "Iteration 7382 => Loss: 6.77466405850570474456\n",
      "Iteration 7383 => Loss: 6.77465375198719943484\n",
      "Iteration 7384 => Loss: 6.77464344685749786379\n",
      "Iteration 7385 => Loss: 6.77463314311641173759\n",
      "Iteration 7386 => Loss: 6.77462284076375365061\n",
      "Iteration 7387 => Loss: 6.77461253979933886171\n",
      "Iteration 7388 => Loss: 6.77460224022298174162\n",
      "Iteration 7389 => Loss: 6.77459194203449843741\n",
      "Iteration 7390 => Loss: 6.77458164523369443799\n",
      "Iteration 7391 => Loss: 6.77457134982037523230\n",
      "Iteration 7392 => Loss: 6.77456105579437650732\n",
      "Iteration 7393 => Loss: 6.77455076315548598842\n",
      "Iteration 7394 => Loss: 6.77454047190353758623\n",
      "Iteration 7395 => Loss: 6.77453018203833501332\n",
      "Iteration 7396 => Loss: 6.77451989355969708129\n",
      "Iteration 7397 => Loss: 6.77450960646742661453\n",
      "Iteration 7398 => Loss: 6.77449932076134686554\n",
      "Iteration 7399 => Loss: 6.77448903644125799417\n",
      "Iteration 7400 => Loss: 6.77447875350698858199\n",
      "Iteration 7401 => Loss: 6.77446847195834145339\n",
      "Iteration 7402 => Loss: 6.77445819179514252539\n",
      "Iteration 7403 => Loss: 6.77444791301719284604\n",
      "Iteration 7404 => Loss: 6.77443763562429968061\n",
      "Iteration 7405 => Loss: 6.77442735961629871611\n",
      "Iteration 7406 => Loss: 6.77441708499298478330\n",
      "Iteration 7407 => Loss: 6.77440681175417758197\n",
      "Iteration 7408 => Loss: 6.77439653989968881831\n",
      "Iteration 7409 => Loss: 6.77438626942933996844\n",
      "Iteration 7410 => Loss: 6.77437600034292941587\n",
      "Iteration 7411 => Loss: 6.77436573264028218944\n",
      "Iteration 7412 => Loss: 6.77435546632121177169\n",
      "Iteration 7413 => Loss: 6.77434520138552542790\n",
      "Iteration 7414 => Loss: 6.77433493783304019331\n",
      "Iteration 7415 => Loss: 6.77432467566356777411\n",
      "Iteration 7416 => Loss: 6.77431441487692875825\n",
      "Iteration 7417 => Loss: 6.77430415547292241740\n",
      "Iteration 7418 => Loss: 6.77429389745138088585\n",
      "Iteration 7419 => Loss: 6.77428364081210077075\n",
      "Iteration 7420 => Loss: 6.77427338555490976546\n",
      "Iteration 7421 => Loss: 6.77426313167961602346\n",
      "Iteration 7422 => Loss: 6.77425287918602858639\n",
      "Iteration 7423 => Loss: 6.77424262807396715402\n",
      "Iteration 7424 => Loss: 6.77423237834324165618\n",
      "Iteration 7425 => Loss: 6.77422212999367268083\n",
      "Iteration 7426 => Loss: 6.77421188302507104595\n",
      "Iteration 7427 => Loss: 6.77420163743723868777\n",
      "Iteration 7428 => Loss: 6.77419139323001395780\n",
      "Iteration 7429 => Loss: 6.77418115040318280506\n",
      "Iteration 7430 => Loss: 6.77417090895657825200\n",
      "Iteration 7431 => Loss: 6.77416066889001733387\n",
      "Iteration 7432 => Loss: 6.77415043020329843415\n",
      "Iteration 7433 => Loss: 6.77414019289624569353\n",
      "Iteration 7434 => Loss: 6.77412995696866904183\n",
      "Iteration 7435 => Loss: 6.77411972242038551428\n",
      "Iteration 7436 => Loss: 6.77410948925121214614\n",
      "Iteration 7437 => Loss: 6.77409925746095087362\n",
      "Iteration 7438 => Loss: 6.77408902704942939010\n",
      "Iteration 7439 => Loss: 6.77407879801645851359\n",
      "Iteration 7440 => Loss: 6.77406857036184728571\n",
      "Iteration 7441 => Loss: 6.77405834408541540625\n",
      "Iteration 7442 => Loss: 6.77404811918697546957\n",
      "Iteration 7443 => Loss: 6.77403789566633740549\n",
      "Iteration 7444 => Loss: 6.77402767352332357831\n",
      "Iteration 7445 => Loss: 6.77401745275774480604\n",
      "Iteration 7446 => Loss: 6.77400723336940924213\n",
      "Iteration 7447 => Loss: 6.77399701535814546816\n",
      "Iteration 7448 => Loss: 6.77398679872375275579\n",
      "Iteration 7449 => Loss: 6.77397658346605346935\n",
      "Iteration 7450 => Loss: 6.77396636958485753865\n",
      "Iteration 7451 => Loss: 6.77395615707999265709\n",
      "Iteration 7452 => Loss: 6.77394594595125543179\n",
      "Iteration 7453 => Loss: 6.77393573619847000344\n",
      "Iteration 7454 => Loss: 6.77392552782145340728\n",
      "Iteration 7455 => Loss: 6.77391532082001113224\n",
      "Iteration 7456 => Loss: 6.77390511519396376627\n",
      "Iteration 7457 => Loss: 6.77389491094312834463\n",
      "Iteration 7458 => Loss: 6.77388470806731834983\n",
      "Iteration 7459 => Loss: 6.77387450656634193535\n",
      "Iteration 7460 => Loss: 6.77386430644002768275\n",
      "Iteration 7461 => Loss: 6.77385410768816953464\n",
      "Iteration 7462 => Loss: 6.77384391031059873711\n",
      "Iteration 7463 => Loss: 6.77383371430712521999\n",
      "Iteration 7464 => Loss: 6.77382351967756246580\n",
      "Iteration 7465 => Loss: 6.77381332642172750980\n",
      "Iteration 7466 => Loss: 6.77380313453943561086\n",
      "Iteration 7467 => Loss: 6.77379294403050202789\n",
      "Iteration 7468 => Loss: 6.77378275489473935522\n",
      "Iteration 7469 => Loss: 6.77377256713196107540\n",
      "Iteration 7470 => Loss: 6.77376238074198511185\n",
      "Iteration 7471 => Loss: 6.77375219572462761164\n",
      "Iteration 7472 => Loss: 6.77374201207970738636\n",
      "Iteration 7473 => Loss: 6.77373182980702193134\n",
      "Iteration 7474 => Loss: 6.77372164890640338086\n",
      "Iteration 7475 => Loss: 6.77371146937766521745\n",
      "Iteration 7476 => Loss: 6.77370129122061737093\n",
      "Iteration 7477 => Loss: 6.77369111443508220560\n",
      "Iteration 7478 => Loss: 6.77368093902085988134\n",
      "Iteration 7479 => Loss: 6.77367076497778342059\n",
      "Iteration 7480 => Loss: 6.77366059230565120686\n",
      "Iteration 7481 => Loss: 6.77365042100429626259\n",
      "Iteration 7482 => Loss: 6.77364025107352407673\n",
      "Iteration 7483 => Loss: 6.77363008251314724362\n",
      "Iteration 7484 => Loss: 6.77361991532298546304\n",
      "Iteration 7485 => Loss: 6.77360974950285665841\n",
      "Iteration 7486 => Loss: 6.77359958505256720684\n",
      "Iteration 7487 => Loss: 6.77358942197193769630\n",
      "Iteration 7488 => Loss: 6.77357926026079493198\n",
      "Iteration 7489 => Loss: 6.77356909991893019196\n",
      "Iteration 7490 => Loss: 6.77355894094618538048\n",
      "Iteration 7491 => Loss: 6.77354878334235088744\n",
      "Iteration 7492 => Loss: 6.77353862710726239982\n",
      "Iteration 7493 => Loss: 6.77352847224071918930\n",
      "Iteration 7494 => Loss: 6.77351831874255072563\n",
      "Iteration 7495 => Loss: 6.77350816661256782680\n",
      "Iteration 7496 => Loss: 6.77349801585058308717\n",
      "Iteration 7497 => Loss: 6.77348786645642242377\n",
      "Iteration 7498 => Loss: 6.77347771842988422009\n",
      "Iteration 7499 => Loss: 6.77346757177079261680\n",
      "Iteration 7500 => Loss: 6.77345742647896553734\n",
      "Iteration 7501 => Loss: 6.77344728255421557606\n",
      "Iteration 7502 => Loss: 6.77343713999636598544\n",
      "Iteration 7503 => Loss: 6.77342699880522136624\n",
      "Iteration 7504 => Loss: 6.77341685898060852367\n",
      "Iteration 7505 => Loss: 6.77340672052233561118\n",
      "Iteration 7506 => Loss: 6.77339658343022232856\n",
      "Iteration 7507 => Loss: 6.77338644770407416473\n",
      "Iteration 7508 => Loss: 6.77337631334372325398\n",
      "Iteration 7509 => Loss: 6.77336618034897863794\n",
      "Iteration 7510 => Loss: 6.77335604871965468732\n",
      "Iteration 7511 => Loss: 6.77334591845556932554\n",
      "Iteration 7512 => Loss: 6.77333578955653514697\n",
      "Iteration 7513 => Loss: 6.77332566202237362774\n",
      "Iteration 7514 => Loss: 6.77331553585289558583\n",
      "Iteration 7515 => Loss: 6.77330541104791983287\n",
      "Iteration 7516 => Loss: 6.77329528760727050951\n",
      "Iteration 7517 => Loss: 6.77328516553074244655\n",
      "Iteration 7518 => Loss: 6.77327504481817399551\n",
      "Iteration 7519 => Loss: 6.77326492546937153350\n",
      "Iteration 7520 => Loss: 6.77325480748414765486\n",
      "Iteration 7521 => Loss: 6.77324469086233094117\n",
      "Iteration 7522 => Loss: 6.77323457560372244046\n",
      "Iteration 7523 => Loss: 6.77322446170814806976\n",
      "Iteration 7524 => Loss: 6.77321434917542042342\n",
      "Iteration 7525 => Loss: 6.77320423800536186576\n",
      "Iteration 7526 => Loss: 6.77319412819778143842\n",
      "Iteration 7527 => Loss: 6.77318401975249528846\n",
      "Iteration 7528 => Loss: 6.77317391266932578020\n",
      "Iteration 7529 => Loss: 6.77316380694808994889\n",
      "Iteration 7530 => Loss: 6.77315370258859772434\n",
      "Iteration 7531 => Loss: 6.77314359959066791816\n",
      "Iteration 7532 => Loss: 6.77313349795411667742\n",
      "Iteration 7533 => Loss: 6.77312339767876281371\n",
      "Iteration 7534 => Loss: 6.77311329876442780318\n",
      "Iteration 7535 => Loss: 6.77310320121091180567\n",
      "Iteration 7536 => Loss: 6.77309310501804784366\n",
      "Iteration 7537 => Loss: 6.77308301018564318241\n",
      "Iteration 7538 => Loss: 6.77307291671352285078\n",
      "Iteration 7539 => Loss: 6.77306282460149855496\n",
      "Iteration 7540 => Loss: 6.77305273384938377745\n",
      "Iteration 7541 => Loss: 6.77304264445700088260\n",
      "Iteration 7542 => Loss: 6.77303255642416068838\n",
      "Iteration 7543 => Loss: 6.77302246975069000001\n",
      "Iteration 7544 => Loss: 6.77301238443639697095\n",
      "Iteration 7545 => Loss: 6.77300230048109774827\n",
      "Iteration 7546 => Loss: 6.77299221788461291993\n",
      "Iteration 7547 => Loss: 6.77298213664676218571\n",
      "Iteration 7548 => Loss: 6.77297205676735281088\n",
      "Iteration 7549 => Loss: 6.77296197824621781791\n",
      "Iteration 7550 => Loss: 6.77295190108315736666\n",
      "Iteration 7551 => Loss: 6.77294182527799559779\n",
      "Iteration 7552 => Loss: 6.77293175083054954655\n",
      "Iteration 7553 => Loss: 6.77292167774063536001\n",
      "Iteration 7554 => Loss: 6.77291160600807717884\n",
      "Iteration 7555 => Loss: 6.77290153563268493286\n",
      "Iteration 7556 => Loss: 6.77289146661427299279\n",
      "Iteration 7557 => Loss: 6.77288139895265750567\n",
      "Iteration 7558 => Loss: 6.77287133264766616492\n",
      "Iteration 7559 => Loss: 6.77286126769910712397\n",
      "Iteration 7560 => Loss: 6.77285120410680363534\n",
      "Iteration 7561 => Loss: 6.77284114187056651701\n",
      "Iteration 7562 => Loss: 6.77283108099021990967\n",
      "Iteration 7563 => Loss: 6.77282102146557285494\n",
      "Iteration 7564 => Loss: 6.77281096329645038168\n",
      "Iteration 7565 => Loss: 6.77280090648267218967\n",
      "Iteration 7566 => Loss: 6.77279085102404465601\n",
      "Iteration 7567 => Loss: 6.77278079692039458592\n",
      "Iteration 7568 => Loss: 6.77277074417152924468\n",
      "Iteration 7569 => Loss: 6.77276069277727632567\n",
      "Iteration 7570 => Loss: 6.77275064273745197596\n",
      "Iteration 7571 => Loss: 6.77274059405186701355\n",
      "Iteration 7572 => Loss: 6.77273054672034380275\n",
      "Iteration 7573 => Loss: 6.77272050074270204334\n",
      "Iteration 7574 => Loss: 6.77271045611874811243\n",
      "Iteration 7575 => Loss: 6.77270041284832302608\n",
      "Iteration 7576 => Loss: 6.77269037093122427962\n",
      "Iteration 7577 => Loss: 6.77268033036727334917\n",
      "Iteration 7578 => Loss: 6.77267029115628460545\n",
      "Iteration 7579 => Loss: 6.77266025329807952460\n",
      "Iteration 7580 => Loss: 6.77265021679248846453\n",
      "Iteration 7581 => Loss: 6.77264018163930803240\n",
      "Iteration 7582 => Loss: 6.77263014783836680976\n",
      "Iteration 7583 => Loss: 6.77262011538947650280\n",
      "Iteration 7584 => Loss: 6.77261008429246924578\n",
      "Iteration 7585 => Loss: 6.77260005454714253403\n",
      "Iteration 7586 => Loss: 6.77259002615333205455\n",
      "Iteration 7587 => Loss: 6.77257999911085040168\n",
      "Iteration 7588 => Loss: 6.77256997341950839342\n",
      "Iteration 7589 => Loss: 6.77255994907912661773\n",
      "Iteration 7590 => Loss: 6.77254992608952921529\n",
      "Iteration 7591 => Loss: 6.77253990445052966862\n",
      "Iteration 7592 => Loss: 6.77252988416195034205\n",
      "Iteration 7593 => Loss: 6.77251986522360471810\n",
      "Iteration 7594 => Loss: 6.77250984763530361477\n",
      "Iteration 7595 => Loss: 6.77249983139688538358\n",
      "Iteration 7596 => Loss: 6.77248981650814574351\n",
      "Iteration 7597 => Loss: 6.77247980296891860519\n",
      "Iteration 7598 => Loss: 6.77246979077901656296\n",
      "Iteration 7599 => Loss: 6.77245977993825665209\n",
      "Iteration 7600 => Loss: 6.77244977044646123687\n",
      "Iteration 7601 => Loss: 6.77243976230343935896\n",
      "Iteration 7602 => Loss: 6.77242975550902226445\n",
      "Iteration 7603 => Loss: 6.77241975006302077134\n",
      "Iteration 7604 => Loss: 6.77240974596525013851\n",
      "Iteration 7605 => Loss: 6.77239974321553894754\n",
      "Iteration 7606 => Loss: 6.77238974181369091099\n",
      "Iteration 7607 => Loss: 6.77237974175953461042\n",
      "Iteration 7608 => Loss: 6.77236974305288619291\n",
      "Iteration 7609 => Loss: 6.77235974569356535824\n",
      "Iteration 7610 => Loss: 6.77234974968139091800\n",
      "Iteration 7611 => Loss: 6.77233975501617546655\n",
      "Iteration 7612 => Loss: 6.77232976169774758546\n",
      "Iteration 7613 => Loss: 6.77231976972591986907\n",
      "Iteration 7614 => Loss: 6.77230977910050668811\n",
      "Iteration 7615 => Loss: 6.77229978982133307142\n",
      "Iteration 7616 => Loss: 6.77228980188821871877\n",
      "Iteration 7617 => Loss: 6.77227981530097711271\n",
      "Iteration 7618 => Loss: 6.77226983005942795302\n",
      "Iteration 7619 => Loss: 6.77225984616338827493\n",
      "Iteration 7620 => Loss: 6.77224986361268310731\n",
      "Iteration 7621 => Loss: 6.77223988240712593267\n",
      "Iteration 7622 => Loss: 6.77222990254654000353\n",
      "Iteration 7623 => Loss: 6.77221992403073791422\n",
      "Iteration 7624 => Loss: 6.77220994685954291725\n",
      "Iteration 7625 => Loss: 6.77219997103277204786\n",
      "Iteration 7626 => Loss: 6.77218999655024145312\n",
      "Iteration 7627 => Loss: 6.77218002341177438552\n",
      "Iteration 7628 => Loss: 6.77217005161719054485\n",
      "Iteration 7629 => Loss: 6.77216008116630696634\n",
      "Iteration 7630 => Loss: 6.77215011205894690249\n",
      "Iteration 7631 => Loss: 6.77214014429491495406\n",
      "Iteration 7632 => Loss: 6.77213017787404680803\n",
      "Iteration 7633 => Loss: 6.77212021279615061786\n",
      "Iteration 7634 => Loss: 6.77211024906105585330\n",
      "Iteration 7635 => Loss: 6.77210028666856533874\n",
      "Iteration 7636 => Loss: 6.77209032561852009025\n",
      "Iteration 7637 => Loss: 6.77208036591072204402\n",
      "Iteration 7638 => Loss: 6.77207040754498645896\n",
      "Iteration 7639 => Loss: 6.77206045052114991023\n",
      "Iteration 7640 => Loss: 6.77205049483902499219\n",
      "Iteration 7641 => Loss: 6.77204054049842252283\n",
      "Iteration 7642 => Loss: 6.77203058749917374826\n",
      "Iteration 7643 => Loss: 6.77202063584108771011\n",
      "Iteration 7644 => Loss: 6.77201068552399565448\n",
      "Iteration 7645 => Loss: 6.77200073654770573484\n",
      "Iteration 7646 => Loss: 6.77199078891204031549\n",
      "Iteration 7647 => Loss: 6.77198084261681731988\n",
      "Iteration 7648 => Loss: 6.77197089766186266502\n",
      "Iteration 7649 => Loss: 6.77196095404698628073\n",
      "Iteration 7650 => Loss: 6.77195101177201941312\n",
      "Iteration 7651 => Loss: 6.77194107083676932746\n",
      "Iteration 7652 => Loss: 6.77193113124106904621\n",
      "Iteration 7653 => Loss: 6.77192119298472050559\n",
      "Iteration 7654 => Loss: 6.77191125606755406352\n",
      "Iteration 7655 => Loss: 6.77190132048938941978\n",
      "Iteration 7656 => Loss: 6.77189138625005249139\n",
      "Iteration 7657 => Loss: 6.77188145334934699093\n",
      "Iteration 7658 => Loss: 6.77187152178709617090\n",
      "Iteration 7659 => Loss: 6.77186159156313305374\n",
      "Iteration 7660 => Loss: 6.77185166267726845746\n",
      "Iteration 7661 => Loss: 6.77184173512931764094\n",
      "Iteration 7662 => Loss: 6.77183180891910740939\n",
      "Iteration 7663 => Loss: 6.77182188404645390989\n",
      "Iteration 7664 => Loss: 6.77181196051117417767\n",
      "Iteration 7665 => Loss: 6.77180203831309235341\n",
      "Iteration 7666 => Loss: 6.77179211745203257777\n",
      "Iteration 7667 => Loss: 6.77178219792781366237\n",
      "Iteration 7668 => Loss: 6.77177227974023576706\n",
      "Iteration 7669 => Loss: 6.77176236288914701333\n",
      "Iteration 7670 => Loss: 6.77175244737435377829\n",
      "Iteration 7671 => Loss: 6.77174253319567487353\n",
      "Iteration 7672 => Loss: 6.77173262035292466976\n",
      "Iteration 7673 => Loss: 6.77172270884593974216\n",
      "Iteration 7674 => Loss: 6.77171279867452557966\n",
      "Iteration 7675 => Loss: 6.77170288983851698106\n",
      "Iteration 7676 => Loss: 6.77169298233771765894\n",
      "Iteration 7677 => Loss: 6.77168307617195441850\n",
      "Iteration 7678 => Loss: 6.77167317134104873588\n",
      "Iteration 7679 => Loss: 6.77166326784482031087\n",
      "Iteration 7680 => Loss: 6.77165336568309150778\n",
      "Iteration 7681 => Loss: 6.77164346485568291456\n",
      "Iteration 7682 => Loss: 6.77163356536240090833\n",
      "Iteration 7683 => Loss: 6.77162366720308295243\n",
      "Iteration 7684 => Loss: 6.77161377037753897667\n",
      "Iteration 7685 => Loss: 6.77160387488559667446\n",
      "Iteration 7686 => Loss: 6.77159398072707219285\n",
      "Iteration 7687 => Loss: 6.77158408790178789616\n",
      "Iteration 7688 => Loss: 6.77157419640955549056\n",
      "Iteration 7689 => Loss: 6.77156430625020799852\n",
      "Iteration 7690 => Loss: 6.77155441742355712620\n",
      "Iteration 7691 => Loss: 6.77154452992942790246\n",
      "Iteration 7692 => Loss: 6.77153464376764269161\n",
      "Iteration 7693 => Loss: 6.77152475893801408802\n",
      "Iteration 7694 => Loss: 6.77151487544037156141\n",
      "Iteration 7695 => Loss: 6.77150499327452326526\n",
      "Iteration 7696 => Loss: 6.77149511244030843926\n",
      "Iteration 7697 => Loss: 6.77148523293753346053\n",
      "Iteration 7698 => Loss: 6.77147535476601625248\n",
      "Iteration 7699 => Loss: 6.77146547792559161394\n",
      "Iteration 7700 => Loss: 6.77145560241606236929\n",
      "Iteration 7701 => Loss: 6.77144572823726864641\n",
      "Iteration 7702 => Loss: 6.77143585538901238152\n",
      "Iteration 7703 => Loss: 6.77142598387112837344\n",
      "Iteration 7704 => Loss: 6.77141611368343010469\n",
      "Iteration 7705 => Loss: 6.77140624482574260412\n",
      "Iteration 7706 => Loss: 6.77139637729788557152\n",
      "Iteration 7707 => Loss: 6.77138651109967337760\n",
      "Iteration 7708 => Loss: 6.77137664623093193939\n",
      "Iteration 7709 => Loss: 6.77136678269149339116\n",
      "Iteration 7710 => Loss: 6.77135692048115611641\n",
      "Iteration 7711 => Loss: 6.77134705959975757850\n",
      "Iteration 7712 => Loss: 6.77133720004711125995\n",
      "Iteration 7713 => Loss: 6.77132734182304218962\n",
      "Iteration 7714 => Loss: 6.77131748492737095546\n",
      "Iteration 7715 => Loss: 6.77130762935991459273\n",
      "Iteration 7716 => Loss: 6.77129777512049546573\n",
      "Iteration 7717 => Loss: 6.77128792220893593878\n",
      "Iteration 7718 => Loss: 6.77127807062506281710\n",
      "Iteration 7719 => Loss: 6.77126822036868603050\n",
      "Iteration 7720 => Loss: 6.77125837143963327236\n",
      "Iteration 7721 => Loss: 6.77124852383772601883\n",
      "Iteration 7722 => Loss: 6.77123867756277686425\n",
      "Iteration 7723 => Loss: 6.77122883261462149562\n",
      "Iteration 7724 => Loss: 6.77121898899307517183\n",
      "Iteration 7725 => Loss: 6.77120914669794959906\n",
      "Iteration 7726 => Loss: 6.77119930572907513522\n",
      "Iteration 7727 => Loss: 6.77118946608627858552\n",
      "Iteration 7728 => Loss: 6.77117962776937343250\n",
      "Iteration 7729 => Loss: 6.77116979077817937593\n",
      "Iteration 7730 => Loss: 6.77115995511251789196\n",
      "Iteration 7731 => Loss: 6.77115012077222289122\n",
      "Iteration 7732 => Loss: 6.77114028775709986263\n",
      "Iteration 7733 => Loss: 6.77113045606697294687\n",
      "Iteration 7734 => Loss: 6.77112062570167161368\n",
      "Iteration 7735 => Loss: 6.77111079666101023378\n",
      "Iteration 7736 => Loss: 6.77110096894481916507\n",
      "Iteration 7737 => Loss: 6.77109114255290922557\n",
      "Iteration 7738 => Loss: 6.77108131748510189141\n",
      "Iteration 7739 => Loss: 6.77107149374122307961\n",
      "Iteration 7740 => Loss: 6.77106167132109604267\n",
      "Iteration 7741 => Loss: 6.77105185022455202670\n",
      "Iteration 7742 => Loss: 6.77104203045138941519\n",
      "Iteration 7743 => Loss: 6.77103221200144744785\n",
      "Iteration 7744 => Loss: 6.77102239487453871902\n",
      "Iteration 7745 => Loss: 6.77101257907048825757\n",
      "Iteration 7746 => Loss: 6.77100276458912375688\n",
      "Iteration 7747 => Loss: 6.77099295143025692312\n",
      "Iteration 7748 => Loss: 6.77098313959371189696\n",
      "Iteration 7749 => Loss: 6.77097332907932436541\n",
      "Iteration 7750 => Loss: 6.77096351988689182377\n",
      "Iteration 7751 => Loss: 6.77095371201625173541\n",
      "Iteration 7752 => Loss: 6.77094390546722380009\n",
      "Iteration 7753 => Loss: 6.77093410023963304667\n",
      "Iteration 7754 => Loss: 6.77092429633328940497\n",
      "Iteration 7755 => Loss: 6.77091449374802856198\n",
      "Iteration 7756 => Loss: 6.77090469248366044752\n",
      "Iteration 7757 => Loss: 6.77089489254001986041\n",
      "Iteration 7758 => Loss: 6.77088509391692472406\n",
      "Iteration 7759 => Loss: 6.77087529661419029736\n",
      "Iteration 7760 => Loss: 6.77086550063164427371\n",
      "Iteration 7761 => Loss: 6.77085570596910280017\n",
      "Iteration 7762 => Loss: 6.77084591262640067555\n",
      "Iteration 7763 => Loss: 6.77083612060334960603\n",
      "Iteration 7764 => Loss: 6.77082632989977373228\n",
      "Iteration 7765 => Loss: 6.77081654051549630680\n",
      "Iteration 7766 => Loss: 6.77080675245033969389\n",
      "Iteration 7767 => Loss: 6.77079696570412359335\n",
      "Iteration 7768 => Loss: 6.77078718027667569856\n",
      "Iteration 7769 => Loss: 6.77077739616781304477\n",
      "Iteration 7770 => Loss: 6.77076761337735710811\n",
      "Iteration 7771 => Loss: 6.77075783190513380561\n",
      "Iteration 7772 => Loss: 6.77074805175096727794\n",
      "Iteration 7773 => Loss: 6.77073827291467544853\n",
      "Iteration 7774 => Loss: 6.77072849539608867531\n",
      "Iteration 7775 => Loss: 6.77071871919501599990\n",
      "Iteration 7776 => Loss: 6.77070894431128866842\n",
      "Iteration 7777 => Loss: 6.77069917074472638063\n",
      "Iteration 7778 => Loss: 6.77068939849515327722\n",
      "Iteration 7779 => Loss: 6.77067962756240149247\n",
      "Iteration 7780 => Loss: 6.77066985794627296258\n",
      "Iteration 7781 => Loss: 6.77066008964660071001\n",
      "Iteration 7782 => Loss: 6.77065032266320709908\n",
      "Iteration 7783 => Loss: 6.77064055699591982318\n",
      "Iteration 7784 => Loss: 6.77063079264455591755\n",
      "Iteration 7785 => Loss: 6.77062102960893863468\n",
      "Iteration 7786 => Loss: 6.77061126788889033890\n",
      "Iteration 7787 => Loss: 6.77060150748423783540\n",
      "Iteration 7788 => Loss: 6.77059174839479460672\n",
      "Iteration 7789 => Loss: 6.77058199062039189897\n",
      "Iteration 7790 => Loss: 6.77057223416085829371\n",
      "Iteration 7791 => Loss: 6.77056247901599217442\n",
      "Iteration 7792 => Loss: 6.77055272518564610351\n",
      "Iteration 7793 => Loss: 6.77054297266962645807\n",
      "Iteration 7794 => Loss: 6.77053322146775737878\n",
      "Iteration 7795 => Loss: 6.77052347157986034176\n",
      "Iteration 7796 => Loss: 6.77051372300576570495\n",
      "Iteration 7797 => Loss: 6.77050397574529405631\n",
      "Iteration 7798 => Loss: 6.77049422979826243107\n",
      "Iteration 7799 => Loss: 6.77048448516449941081\n",
      "Iteration 7800 => Loss: 6.77047474184382913620\n",
      "Iteration 7801 => Loss: 6.77046499983606508977\n",
      "Iteration 7802 => Loss: 6.77045525914103940579\n",
      "Iteration 7803 => Loss: 6.77044551975858155402\n",
      "Iteration 7804 => Loss: 6.77043578168849435883\n",
      "Iteration 7805 => Loss: 6.77042604493062150084\n",
      "Iteration 7806 => Loss: 6.77041630948477379803\n",
      "Iteration 7807 => Loss: 6.77040657535077805562\n",
      "Iteration 7808 => Loss: 6.77039684252846196699\n",
      "Iteration 7809 => Loss: 6.77038711101764167921\n",
      "Iteration 7810 => Loss: 6.77037738081814133295\n",
      "Iteration 7811 => Loss: 6.77036765192978684524\n",
      "Iteration 7812 => Loss: 6.77035792435240679765\n",
      "Iteration 7813 => Loss: 6.77034819808580490275\n",
      "Iteration 7814 => Loss: 6.77033847312983194655\n",
      "Iteration 7815 => Loss: 6.77032874948429341799\n",
      "Iteration 7816 => Loss: 6.77031902714901256957\n",
      "Iteration 7817 => Loss: 6.77030930612381887101\n",
      "Iteration 7818 => Loss: 6.77029958640853113394\n",
      "Iteration 7819 => Loss: 6.77028986800298682169\n",
      "Iteration 7820 => Loss: 6.77028015090699319956\n",
      "Iteration 7821 => Loss: 6.77027043512037529638\n",
      "Iteration 7822 => Loss: 6.77026072064296347008\n",
      "Iteration 7823 => Loss: 6.77025100747457475592\n",
      "Iteration 7824 => Loss: 6.77024129561504039998\n",
      "Iteration 7825 => Loss: 6.77023158506417921387\n",
      "Iteration 7826 => Loss: 6.77022187582181622645\n",
      "Iteration 7827 => Loss: 6.77021216788777202567\n",
      "Iteration 7828 => Loss: 6.77020246126187075220\n",
      "Iteration 7829 => Loss: 6.77019275594394009943\n",
      "Iteration 7830 => Loss: 6.77018305193380243168\n",
      "Iteration 7831 => Loss: 6.77017334923128277779\n",
      "Iteration 7832 => Loss: 6.77016364783620083756\n",
      "Iteration 7833 => Loss: 6.77015394774838341618\n",
      "Iteration 7834 => Loss: 6.77014424896765287798\n",
      "Iteration 7835 => Loss: 6.77013455149383158727\n",
      "Iteration 7836 => Loss: 6.77012485532675079014\n",
      "Iteration 7837 => Loss: 6.77011516046622396914\n",
      "Iteration 7838 => Loss: 6.77010546691208414671\n",
      "Iteration 7839 => Loss: 6.77009577466414835811\n",
      "Iteration 7840 => Loss: 6.77008608372224696126\n",
      "Iteration 7841 => Loss: 6.77007639408620054411\n",
      "Iteration 7842 => Loss: 6.77006670575582703009\n",
      "Iteration 7843 => Loss: 6.77005701873096654708\n",
      "Iteration 7844 => Loss: 6.77004733301142547219\n",
      "Iteration 7845 => Loss: 6.77003764859704748602\n",
      "Iteration 7846 => Loss: 6.77002796548762919571\n",
      "Iteration 7847 => Loss: 6.77001828368302138728\n",
      "Iteration 7848 => Loss: 6.77000860318303043783\n",
      "Iteration 7849 => Loss: 6.76999892398749114619\n",
      "Iteration 7850 => Loss: 6.76998924609623031756\n",
      "Iteration 7851 => Loss: 6.76997956950906054630\n",
      "Iteration 7852 => Loss: 6.76996989422580952578\n",
      "Iteration 7853 => Loss: 6.76996022024630317304\n",
      "Iteration 7854 => Loss: 6.76995054757037273419\n",
      "Iteration 7855 => Loss: 6.76994087619782991538\n",
      "Iteration 7856 => Loss: 6.76993120612850773909\n",
      "Iteration 7857 => Loss: 6.76992153736222590510\n",
      "Iteration 7858 => Loss: 6.76991186989881743585\n",
      "Iteration 7859 => Loss: 6.76990220373809759025\n",
      "Iteration 7860 => Loss: 6.76989253887989317349\n",
      "Iteration 7861 => Loss: 6.76988287532402832625\n",
      "Iteration 7862 => Loss: 6.76987321307032452467\n",
      "Iteration 7863 => Loss: 6.76986355211861745573\n",
      "Iteration 7864 => Loss: 6.76985389246871704927\n",
      "Iteration 7865 => Loss: 6.76984423412045899227\n",
      "Iteration 7866 => Loss: 6.76983457707366742540\n",
      "Iteration 7867 => Loss: 6.76982492132815938390\n",
      "Iteration 7868 => Loss: 6.76981526688376611389\n",
      "Iteration 7869 => Loss: 6.76980561374030731514\n",
      "Iteration 7870 => Loss: 6.76979596189761334557\n",
      "Iteration 7871 => Loss: 6.76978631135550301678\n",
      "Iteration 7872 => Loss: 6.76977666211381023942\n",
      "Iteration 7873 => Loss: 6.76976701417234316693\n",
      "Iteration 7874 => Loss: 6.76975736753094192721\n",
      "Iteration 7875 => Loss: 6.76974772218942533186\n",
      "Iteration 7876 => Loss: 6.76973807814762551516\n",
      "Iteration 7877 => Loss: 6.76972843540535595963\n",
      "Iteration 7878 => Loss: 6.76971879396244879956\n",
      "Iteration 7879 => Loss: 6.76970915381872018202\n",
      "Iteration 7880 => Loss: 6.76969951497401112306\n",
      "Iteration 7881 => Loss: 6.76968987742812888797\n",
      "Iteration 7882 => Loss: 6.76968024118091271646\n",
      "Iteration 7883 => Loss: 6.76967060623217786741\n",
      "Iteration 7884 => Loss: 6.76966097258175203422\n",
      "Iteration 7885 => Loss: 6.76965134022946379844\n",
      "Iteration 7886 => Loss: 6.76964170917513907710\n",
      "Iteration 7887 => Loss: 6.76963207941859757000\n",
      "Iteration 7888 => Loss: 6.76962245095966430597\n",
      "Iteration 7889 => Loss: 6.76961282379816609023\n",
      "Iteration 7890 => Loss: 6.76960319793393239252\n",
      "Iteration 7891 => Loss: 6.76959357336678291261\n",
      "Iteration 7892 => Loss: 6.76958395009654356755\n",
      "Iteration 7893 => Loss: 6.76957432812304382708\n",
      "Iteration 7894 => Loss: 6.76956470744609983825\n",
      "Iteration 7895 => Loss: 6.76955508806554284718\n",
      "Iteration 7896 => Loss: 6.76954546998120143542\n",
      "Iteration 7897 => Loss: 6.76953585319290507272\n",
      "Iteration 7898 => Loss: 6.76952623770046368890\n",
      "Iteration 7899 => Loss: 6.76951662350370675370\n",
      "Iteration 7900 => Loss: 6.76950701060247084229\n",
      "Iteration 7901 => Loss: 6.76949739899657298992\n",
      "Iteration 7902 => Loss: 6.76948778868583467272\n",
      "Iteration 7903 => Loss: 6.76947817967009335405\n",
      "Iteration 7904 => Loss: 6.76946857194916784550\n",
      "Iteration 7905 => Loss: 6.76945896552287429415\n",
      "Iteration 7906 => Loss: 6.76944936039105282788\n",
      "Iteration 7907 => Loss: 6.76943975655352581100\n",
      "Iteration 7908 => Loss: 6.76943015401011294330\n",
      "Iteration 7909 => Loss: 6.76942055276064724723\n",
      "Iteration 7910 => Loss: 6.76941095280494931075\n",
      "Iteration 7911 => Loss: 6.76940135414284416271\n",
      "Iteration 7912 => Loss: 6.76939175677415949650\n",
      "Iteration 7913 => Loss: 6.76938216069872389369\n",
      "Iteration 7914 => Loss: 6.76937256591635971859\n",
      "Iteration 7915 => Loss: 6.76936297242688844733\n",
      "Iteration 7916 => Loss: 6.76935338023014576692\n",
      "Iteration 7917 => Loss: 6.76934378932594871259\n",
      "Iteration 7918 => Loss: 6.76933419971413474769\n",
      "Iteration 7919 => Loss: 6.76932461139451469023\n",
      "Iteration 7920 => Loss: 6.76931502436692156266\n",
      "Iteration 7921 => Loss: 6.76930543863117772929\n",
      "Iteration 7922 => Loss: 6.76929585418712420619\n",
      "Iteration 7923 => Loss: 6.76928627103456825864\n",
      "Iteration 7924 => Loss: 6.76927668917334202092\n",
      "Iteration 7925 => Loss: 6.76926710860327318642\n",
      "Iteration 7926 => Loss: 6.76925752932418411945\n",
      "Iteration 7927 => Loss: 6.76924795133590873064\n",
      "Iteration 7928 => Loss: 6.76923837463826583161\n",
      "Iteration 7929 => Loss: 6.76922879923108133937\n",
      "Iteration 7930 => Loss: 6.76921922511418827639\n",
      "Iteration 7931 => Loss: 6.76920965228741078334\n",
      "Iteration 7932 => Loss: 6.76920008075056323094\n",
      "Iteration 7933 => Loss: 6.76919051050348663523\n",
      "Iteration 7934 => Loss: 6.76918094154599980783\n",
      "Iteration 7935 => Loss: 6.76917137387793044212\n",
      "Iteration 7936 => Loss: 6.76916180749910267878\n",
      "Iteration 7937 => Loss: 6.76915224240934776390\n",
      "Iteration 7938 => Loss: 6.76914267860849250269\n",
      "Iteration 7939 => Loss: 6.76913311609635659494\n",
      "Iteration 7940 => Loss: 6.76912355487277128674\n",
      "Iteration 7941 => Loss: 6.76911399493756604784\n",
      "Iteration 7942 => Loss: 6.76910443629055524895\n",
      "Iteration 7943 => Loss: 6.76909487893157457705\n",
      "Iteration 7944 => Loss: 6.76908532286045172555\n",
      "Iteration 7945 => Loss: 6.76907576807700817056\n",
      "Iteration 7946 => Loss: 6.76906621458107160549\n",
      "Iteration 7947 => Loss: 6.76905666237247150008\n",
      "Iteration 7948 => Loss: 6.76904711145103021863\n",
      "Iteration 7949 => Loss: 6.76903756181657456636\n",
      "Iteration 7950 => Loss: 6.76902801346894023027\n",
      "Iteration 7951 => Loss: 6.76901846640793802834\n",
      "Iteration 7952 => Loss: 6.76900892063340720028\n",
      "Iteration 7953 => Loss: 6.76899937614517099860\n",
      "Iteration 7954 => Loss: 6.76898983294305534031\n",
      "Iteration 7955 => Loss: 6.76898029102688614245\n",
      "Iteration 7956 => Loss: 6.76897075039648932204\n",
      "Iteration 7957 => Loss: 6.76896121105169257248\n",
      "Iteration 7958 => Loss: 6.76895167299233158076\n",
      "Iteration 7959 => Loss: 6.76894213621821716487\n",
      "Iteration 7960 => Loss: 6.76893260072918678816\n",
      "Iteration 7961 => Loss: 6.76892306652506370312\n",
      "Iteration 7962 => Loss: 6.76891353360567382680\n",
      "Iteration 7963 => Loss: 6.76890400197084218803\n",
      "Iteration 7964 => Loss: 6.76889447162040269745\n",
      "Iteration 7965 => Loss: 6.76888494255417860757\n",
      "Iteration 7966 => Loss: 6.76887541477199849993\n",
      "Iteration 7967 => Loss: 6.76886588827368562704\n",
      "Iteration 7968 => Loss: 6.76885636305907034682\n",
      "Iteration 7969 => Loss: 6.76884683912797857630\n",
      "Iteration 7970 => Loss: 6.76883731648023534433\n",
      "Iteration 7971 => Loss: 6.76882779511566834429\n",
      "Iteration 7972 => Loss: 6.76881827503411237501\n",
      "Iteration 7973 => Loss: 6.76880875623538358354\n",
      "Iteration 7974 => Loss: 6.76879923871931143964\n",
      "Iteration 7975 => Loss: 6.76878972248573607118\n",
      "Iteration 7976 => Loss: 6.76878020753446385527\n",
      "Iteration 7977 => Loss: 6.76877069386533758433\n",
      "Iteration 7978 => Loss: 6.76876118147817784632\n",
      "Iteration 7979 => Loss: 6.76875167037280611737\n",
      "Iteration 7980 => Loss: 6.76874216054905897266\n",
      "Iteration 7981 => Loss: 6.76873265200677209918\n",
      "Iteration 7982 => Loss: 6.76872314474575009768\n",
      "Iteration 7983 => Loss: 6.76871363876583576058\n",
      "Iteration 7984 => Loss: 6.76870413406685500490\n",
      "Iteration 7985 => Loss: 6.76869463064862930679\n",
      "Iteration 7986 => Loss: 6.76868512851099524141\n",
      "Iteration 7987 => Loss: 6.76867562765376540312\n",
      "Iteration 7988 => Loss: 6.76866612807678436070\n",
      "Iteration 7989 => Loss: 6.76865662977987359028\n",
      "Iteration 7990 => Loss: 6.76864713276285367982\n",
      "Iteration 7991 => Loss: 6.76863763702556298085\n",
      "Iteration 7992 => Loss: 6.76862814256781941680\n",
      "Iteration 7993 => Loss: 6.76861864938945601011\n",
      "Iteration 7994 => Loss: 6.76860915749030134236\n",
      "Iteration 7995 => Loss: 6.76859966687018310694\n",
      "Iteration 7996 => Loss: 6.76859017752891745090\n",
      "Iteration 7997 => Loss: 6.76858068946634805485\n",
      "Iteration 7998 => Loss: 6.76857120268229284221\n",
      "Iteration 7999 => Loss: 6.76856171717658483544\n",
      "Iteration 8000 => Loss: 6.76855223294904817521\n",
      "Iteration 8001 => Loss: 6.76854274999951410763\n",
      "Iteration 8002 => Loss: 6.76853326832780144429\n",
      "Iteration 8003 => Loss: 6.76852378793375564214\n",
      "Iteration 8004 => Loss: 6.76851430881718219013\n",
      "Iteration 8005 => Loss: 6.76850483097792920972\n",
      "Iteration 8006 => Loss: 6.76849535441580929529\n",
      "Iteration 8007 => Loss: 6.76848587913066435107\n",
      "Iteration 8008 => Loss: 6.76847640512230341869\n",
      "Iteration 8009 => Loss: 6.76846693239057994873\n",
      "Iteration 8010 => Loss: 6.76845746093530298282\n",
      "Iteration 8011 => Loss: 6.76844799075629399709\n",
      "Iteration 8012 => Loss: 6.76843852185340111305\n",
      "Iteration 8013 => Loss: 6.76842905422644314228\n",
      "Iteration 8014 => Loss: 6.76841958787525488361\n",
      "Iteration 8015 => Loss: 6.76841012279965603682\n",
      "Iteration 8016 => Loss: 6.76840065899946807804\n",
      "Iteration 8017 => Loss: 6.76839119647453468787\n",
      "Iteration 8018 => Loss: 6.76838173522467734244\n",
      "Iteration 8019 => Loss: 6.76837227524972284698\n",
      "Iteration 8020 => Loss: 6.76836281654950067121\n",
      "Iteration 8021 => Loss: 6.76835335912383939672\n",
      "Iteration 8022 => Loss: 6.76834390297256671687\n",
      "Iteration 8023 => Loss: 6.76833444809551476595\n",
      "Iteration 8024 => Loss: 6.76832499449250413193\n",
      "Iteration 8025 => Loss: 6.76831554216336250818\n",
      "Iteration 8026 => Loss: 6.76830609110793091077\n",
      "Iteration 8027 => Loss: 6.76829664132602903948\n",
      "Iteration 8028 => Loss: 6.76828719281747837044\n",
      "Iteration 8029 => Loss: 6.76827774558212347245\n",
      "Iteration 8030 => Loss: 6.76826829961978138073\n",
      "Iteration 8031 => Loss: 6.76825885493028600592\n",
      "Iteration 8032 => Loss: 6.76824941151345527146\n",
      "Iteration 8033 => Loss: 6.76823996936913374611\n",
      "Iteration 8034 => Loss: 6.76823052849714379420\n",
      "Iteration 8035 => Loss: 6.76822108889730245096\n",
      "Iteration 8036 => Loss: 6.76821165056945606153\n",
      "Iteration 8037 => Loss: 6.76820221351342343752\n",
      "Iteration 8038 => Loss: 6.76819277772903316048\n",
      "Iteration 8039 => Loss: 6.76818334321611470017\n",
      "Iteration 8040 => Loss: 6.76817390997449930268\n",
      "Iteration 8041 => Loss: 6.76816447800401466139\n",
      "Iteration 8042 => Loss: 6.76815504730448580517\n",
      "Iteration 8043 => Loss: 6.76814561787574753282\n",
      "Iteration 8044 => Loss: 6.76813618971762664955\n",
      "Iteration 8045 => Loss: 6.76812676282994640786\n",
      "Iteration 8046 => Loss: 6.76811733721254160656\n",
      "Iteration 8047 => Loss: 6.76810791286524171539\n",
      "Iteration 8048 => Loss: 6.76809848978786998686\n",
      "Iteration 8049 => Loss: 6.76808906798025944340\n",
      "Iteration 8050 => Loss: 6.76807964744223955478\n",
      "Iteration 8051 => Loss: 6.76807022817363801437\n",
      "Iteration 8052 => Loss: 6.76806081017428251556\n",
      "Iteration 8053 => Loss: 6.76805139344400075174\n",
      "Iteration 8054 => Loss: 6.76804197798263018626\n",
      "Iteration 8055 => Loss: 6.76803256378999140708\n",
      "Iteration 8056 => Loss: 6.76802315086591477211\n",
      "Iteration 8057 => Loss: 6.76801373921022886293\n",
      "Iteration 8058 => Loss: 6.76800432882276847835\n",
      "Iteration 8059 => Loss: 6.76799491970335509450\n",
      "Iteration 8060 => Loss: 6.76798551185182351020\n",
      "Iteration 8061 => Loss: 6.76797610526799697794\n",
      "Iteration 8062 => Loss: 6.76796669995170852019\n",
      "Iteration 8063 => Loss: 6.76795729590279115939\n",
      "Iteration 8064 => Loss: 6.76794789312106903623\n",
      "Iteration 8065 => Loss: 6.76793849160636895590\n",
      "Iteration 8066 => Loss: 6.76792909135853548719\n",
      "Iteration 8067 => Loss: 6.76791969237737234266\n",
      "Iteration 8068 => Loss: 6.76791029466272675563\n",
      "Iteration 8069 => Loss: 6.76790089821442730766\n",
      "Iteration 8070 => Loss: 6.76789150303229725125\n",
      "Iteration 8071 => Loss: 6.76788210911617049703\n",
      "Iteration 8072 => Loss: 6.76787271646587296203\n",
      "Iteration 8073 => Loss: 6.76786332508123678053\n",
      "Iteration 8074 => Loss: 6.76785393496208786956\n",
      "Iteration 8075 => Loss: 6.76784454610826102794\n",
      "Iteration 8076 => Loss: 6.76783515851958483722\n",
      "Iteration 8077 => Loss: 6.76782577219588610262\n",
      "Iteration 8078 => Loss: 6.76781638713698630028\n",
      "Iteration 8079 => Loss: 6.76780700334272999896\n",
      "Iteration 8080 => Loss: 6.76779762081294578024\n",
      "Iteration 8081 => Loss: 6.76778823954745067937\n",
      "Iteration 8082 => Loss: 6.76777885954608660057\n",
      "Iteration 8083 => Loss: 6.76776948080867590818\n",
      "Iteration 8084 => Loss: 6.76776010333505162464\n",
      "Iteration 8085 => Loss: 6.76775072712504499606\n",
      "Iteration 8086 => Loss: 6.76774135217847838675\n",
      "Iteration 8087 => Loss: 6.76773197849519281277\n",
      "Iteration 8088 => Loss: 6.76772260607500886209\n",
      "Iteration 8089 => Loss: 6.76771323491775333991\n",
      "Iteration 8090 => Loss: 6.76770386502327436773\n",
      "Iteration 8091 => Loss: 6.76769449639137921082\n",
      "Iteration 8092 => Loss: 6.76768512902190888525\n",
      "Iteration 8093 => Loss: 6.76767576291469730165\n",
      "Iteration 8094 => Loss: 6.76766639806956682435\n",
      "Iteration 8095 => Loss: 6.76765703448635314032\n",
      "Iteration 8096 => Loss: 6.76764767216487861390\n",
      "Iteration 8097 => Loss: 6.76763831110497893206\n",
      "Iteration 8098 => Loss: 6.76762895130648001185\n",
      "Iteration 8099 => Loss: 6.76761959276921665207\n",
      "Iteration 8100 => Loss: 6.76761023549302098701\n",
      "Iteration 8101 => Loss: 6.76760087947771271644\n",
      "Iteration 8102 => Loss: 6.76759152472313196824\n",
      "Iteration 8103 => Loss: 6.76758217122910821217\n",
      "Iteration 8104 => Loss: 6.76757281899546203618\n",
      "Iteration 8105 => Loss: 6.76756346802203179180\n",
      "Iteration 8106 => Loss: 6.76755411830864161971\n",
      "Iteration 8107 => Loss: 6.76754476985513253595\n",
      "Iteration 8108 => Loss: 6.76753542266133045757\n",
      "Iteration 8109 => Loss: 6.76752607672706041342\n",
      "Iteration 8110 => Loss: 6.76751673205215720230\n",
      "Iteration 8111 => Loss: 6.76750738863644674126\n",
      "Iteration 8112 => Loss: 6.76749804647976382910\n",
      "Iteration 8113 => Loss: 6.76748870558193438285\n",
      "Iteration 8114 => Loss: 6.76747936594279320133\n",
      "Iteration 8115 => Loss: 6.76747002756217241881\n",
      "Iteration 8116 => Loss: 6.76746069043989439962\n",
      "Iteration 8117 => Loss: 6.76745135457579660709\n",
      "Iteration 8118 => Loss: 6.76744201996970673463\n",
      "Iteration 8119 => Loss: 6.76743268662145780468\n",
      "Iteration 8120 => Loss: 6.76742335453087662245\n",
      "Iteration 8121 => Loss: 6.76741402369779798676\n",
      "Iteration 8122 => Loss: 6.76740469412204426192\n",
      "Iteration 8123 => Loss: 6.76739536580346001671\n",
      "Iteration 8124 => Loss: 6.76738603874185784548\n",
      "Iteration 8125 => Loss: 6.76737671293708142883\n",
      "Iteration 8126 => Loss: 6.76736738838896201287\n",
      "Iteration 8127 => Loss: 6.76735806509732018554\n",
      "Iteration 8128 => Loss: 6.76734874306199873928\n",
      "Iteration 8129 => Loss: 6.76733942228281559750\n",
      "Iteration 8130 => Loss: 6.76733010275961177626\n",
      "Iteration 8131 => Loss: 6.76732078449221496896\n",
      "Iteration 8132 => Loss: 6.76731146748045109263\n",
      "Iteration 8133 => Loss: 6.76730215172416205149\n",
      "Iteration 8134 => Loss: 6.76729283722316665717\n",
      "Iteration 8135 => Loss: 6.76728352397730947843\n",
      "Iteration 8136 => Loss: 6.76727421198640222144\n",
      "Iteration 8137 => Loss: 6.76726490125029389588\n",
      "Iteration 8138 => Loss: 6.76725559176880242518\n",
      "Iteration 8139 => Loss: 6.76724628354176438449\n",
      "Iteration 8140 => Loss: 6.76723697656901546083\n",
      "Iteration 8141 => Loss: 6.76722767085037713031\n",
      "Iteration 8142 => Loss: 6.76721836638568507993\n",
      "Iteration 8143 => Loss: 6.76720906317477410852\n",
      "Iteration 8144 => Loss: 6.76719976121747368580\n",
      "Iteration 8145 => Loss: 6.76719046051360262339\n",
      "Iteration 8146 => Loss: 6.76718116106301614820\n",
      "Iteration 8147 => Loss: 6.76717186286552330188\n",
      "Iteration 8148 => Loss: 6.76716256592095977140\n",
      "Iteration 8149 => Loss: 6.76715327022916035560\n",
      "Iteration 8150 => Loss: 6.76714397578996518234\n",
      "Iteration 8151 => Loss: 6.76713468260319483960\n",
      "Iteration 8152 => Loss: 6.76712539066867524440\n",
      "Iteration 8153 => Loss: 6.76711609998624297191\n",
      "Iteration 8154 => Loss: 6.76710681055573726184\n",
      "Iteration 8155 => Loss: 6.76709752237698403121\n",
      "Iteration 8156 => Loss: 6.76708823544981097342\n",
      "Iteration 8157 => Loss: 6.76707894977405466364\n",
      "Iteration 8158 => Loss: 6.76706966534953746617\n",
      "Iteration 8159 => Loss: 6.76706038217610039709\n",
      "Iteration 8160 => Loss: 6.76705110025357292614\n",
      "Iteration 8161 => Loss: 6.76704181958178541123\n",
      "Iteration 8162 => Loss: 6.76703254016057176301\n",
      "Iteration 8163 => Loss: 6.76702326198975701033\n",
      "Iteration 8164 => Loss: 6.76701398506917772835\n",
      "Iteration 8165 => Loss: 6.76700470939865983411\n",
      "Iteration 8166 => Loss: 6.76699543497804256731\n",
      "Iteration 8167 => Loss: 6.76698616180715895041\n",
      "Iteration 8168 => Loss: 6.76697688988583134773\n",
      "Iteration 8169 => Loss: 6.76696761921388834082\n",
      "Iteration 8170 => Loss: 6.76695834979118071573\n",
      "Iteration 8171 => Loss: 6.76694908161752284315\n",
      "Iteration 8172 => Loss: 6.76693981469275218643\n",
      "Iteration 8173 => Loss: 6.76693054901671064982\n",
      "Iteration 8174 => Loss: 6.76692128458920727496\n",
      "Iteration 8175 => Loss: 6.76691202141009107152\n",
      "Iteration 8176 => Loss: 6.76690275947918973287\n",
      "Iteration 8177 => Loss: 6.76689349879633095242\n",
      "Iteration 8178 => Loss: 6.76688423936134952896\n",
      "Iteration 8179 => Loss: 6.76687498117408203768\n",
      "Iteration 8180 => Loss: 6.76686572423435173107\n",
      "Iteration 8181 => Loss: 6.76685646854200051337\n",
      "Iteration 8182 => Loss: 6.76684721409684986071\n",
      "Iteration 8183 => Loss: 6.76683796089873101920\n",
      "Iteration 8184 => Loss: 6.76682870894748944579\n",
      "Iteration 8185 => Loss: 6.76681945824294572844\n",
      "Iteration 8186 => Loss: 6.76681020878494177140\n",
      "Iteration 8187 => Loss: 6.76680096057329816261\n",
      "Iteration 8188 => Loss: 6.76679171360784881273\n",
      "Iteration 8189 => Loss: 6.76678246788843384962\n",
      "Iteration 8190 => Loss: 6.76677322341487652579\n",
      "Iteration 8191 => Loss: 6.76676398018701608095\n",
      "Iteration 8192 => Loss: 6.76675473820467932029\n",
      "Iteration 8193 => Loss: 6.76674549746770015446\n",
      "Iteration 8194 => Loss: 6.76673625797591071773\n",
      "Iteration 8195 => Loss: 6.76672701972914580892\n",
      "Iteration 8196 => Loss: 6.76671778272723312142\n",
      "Iteration 8197 => Loss: 6.76670854697000034861\n",
      "Iteration 8198 => Loss: 6.76669931245729472380\n",
      "Iteration 8199 => Loss: 6.76669007918894482856\n",
      "Iteration 8200 => Loss: 6.76668084716477036267\n",
      "Iteration 8201 => Loss: 6.76667161638461500672\n",
      "Iteration 8202 => Loss: 6.76666238684831178318\n",
      "Iteration 8203 => Loss: 6.76665315855568128001\n",
      "Iteration 8204 => Loss: 6.76664393150656806597\n",
      "Iteration 8205 => Loss: 6.76663470570080161082\n",
      "Iteration 8206 => Loss: 6.76662548113821493700\n",
      "Iteration 8207 => Loss: 6.76661625781863218521\n",
      "Iteration 8208 => Loss: 6.76660703574189703602\n",
      "Iteration 8209 => Loss: 6.76659781490783451829\n",
      "Iteration 8210 => Loss: 6.76658859531628387174\n",
      "Iteration 8211 => Loss: 6.76657937696707101338\n",
      "Iteration 8212 => Loss: 6.76657015986003607111\n",
      "Iteration 8213 => Loss: 6.76656094399500052106\n",
      "Iteration 8214 => Loss: 6.76655172937180715564\n",
      "Iteration 8215 => Loss: 6.76654251599028455644\n",
      "Iteration 8216 => Loss: 6.76653330385026396954\n",
      "Iteration 8217 => Loss: 6.76652409295158285829\n",
      "Iteration 8218 => Loss: 6.76651488329406713973\n",
      "Iteration 8219 => Loss: 6.76650567487755871809\n",
      "Iteration 8220 => Loss: 6.76649646770188351041\n",
      "Iteration 8221 => Loss: 6.76648726176687187461\n",
      "Iteration 8222 => Loss: 6.76647805707236571493\n",
      "Iteration 8223 => Loss: 6.76646885361819272475\n",
      "Iteration 8224 => Loss: 6.76645965140418415018\n",
      "Iteration 8225 => Loss: 6.76645045043017656639\n",
      "Iteration 8226 => Loss: 6.76644125069599855493\n",
      "Iteration 8227 => Loss: 6.76643205220148047374\n",
      "Iteration 8228 => Loss: 6.76642285494646689159\n",
      "Iteration 8229 => Loss: 6.76641365893078550187\n",
      "Iteration 8230 => Loss: 6.76640446415426399795\n",
      "Iteration 8231 => Loss: 6.76639527061673984321\n",
      "Iteration 8232 => Loss: 6.76638607831804961279\n",
      "Iteration 8233 => Loss: 6.76637688725801478284\n",
      "Iteration 8234 => Loss: 6.76636769743648169850\n",
      "Iteration 8235 => Loss: 6.76635850885327716497\n",
      "Iteration 8236 => Loss: 6.76634932150823331654\n",
      "Iteration 8237 => Loss: 6.76634013540118939289\n",
      "Iteration 8238 => Loss: 6.76633095053196775837\n",
      "Iteration 8239 => Loss: 6.76632176690041209355\n",
      "Iteration 8240 => Loss: 6.76631258450634742729\n",
      "Iteration 8241 => Loss: 6.76630340334961477566\n",
      "Iteration 8242 => Loss: 6.76629422343004627294\n",
      "Iteration 8243 => Loss: 6.76628504474747050068\n",
      "Iteration 8244 => Loss: 6.76627586730171870499\n",
      "Iteration 8245 => Loss: 6.76626669109262657287\n",
      "Iteration 8246 => Loss: 6.76625751612003600854\n",
      "Iteration 8247 => Loss: 6.76624834238377115270\n",
      "Iteration 8248 => Loss: 6.76623916988366769232\n",
      "Iteration 8249 => Loss: 6.76622999861956220258\n",
      "Iteration 8250 => Loss: 6.76622082859128237686\n",
      "Iteration 8251 => Loss: 6.76621165979866656670\n",
      "Iteration 8252 => Loss: 6.76620249224154335366\n",
      "Iteration 8253 => Loss: 6.76619332591975641833\n",
      "Iteration 8254 => Loss: 6.76618416083312279596\n",
      "Iteration 8255 => Loss: 6.76617499698149060805\n",
      "Iteration 8256 => Loss: 6.76616583436468754797\n",
      "Iteration 8257 => Loss: 6.76615667298254308548\n",
      "Iteration 8258 => Loss: 6.76614751283490267753\n",
      "Iteration 8259 => Loss: 6.76613835392158957660\n",
      "Iteration 8260 => Loss: 6.76612919624244302241\n",
      "Iteration 8261 => Loss: 6.76612003979729159653\n",
      "Iteration 8262 => Loss: 6.76611088458596920958\n",
      "Iteration 8263 => Loss: 6.76610173060831776581\n",
      "Iteration 8264 => Loss: 6.76609257786416051772\n",
      "Iteration 8265 => Loss: 6.76608342635334203408\n",
      "Iteration 8266 => Loss: 6.76607427607568645556\n",
      "Iteration 8267 => Loss: 6.76606512703103568640\n",
      "Iteration 8268 => Loss: 6.76605597921921386728\n",
      "Iteration 8269 => Loss: 6.76604683264006290244\n",
      "Iteration 8270 => Loss: 6.76603768729341492616\n",
      "Iteration 8271 => Loss: 6.76602854317910384907\n",
      "Iteration 8272 => Loss: 6.76601940029695736456\n",
      "Iteration 8273 => Loss: 6.76601025864682004141\n",
      "Iteration 8274 => Loss: 6.76600111822852046117\n",
      "Iteration 8275 => Loss: 6.76599197904189164632\n",
      "Iteration 8276 => Loss: 6.76598284108676661930\n",
      "Iteration 8277 => Loss: 6.76597370436298550800\n",
      "Iteration 8278 => Loss: 6.76596456887037689398\n",
      "Iteration 8279 => Loss: 6.76595543460878001696\n",
      "Iteration 8280 => Loss: 6.76594630157802434667\n",
      "Iteration 8281 => Loss: 6.76593716977794112921\n",
      "Iteration 8282 => Loss: 6.76592803920837759790\n",
      "Iteration 8283 => Loss: 6.76591890986914723527\n",
      "Iteration 8284 => Loss: 6.76590978176010882095\n",
      "Iteration 8285 => Loss: 6.76590065488107406111\n",
      "Iteration 8286 => Loss: 6.76589152923188308364\n",
      "Iteration 8287 => Loss: 6.76588240481238312185\n",
      "Iteration 8288 => Loss: 6.76587328162239920459\n",
      "Iteration 8289 => Loss: 6.76586415966175902525\n",
      "Iteration 8290 => Loss: 6.76585503893031336986\n",
      "Iteration 8291 => Loss: 6.76584591942787927366\n",
      "Iteration 8292 => Loss: 6.76583680115430396995\n",
      "Iteration 8293 => Loss: 6.76582768410940804671\n",
      "Iteration 8294 => Loss: 6.76581856829304317813\n",
      "Iteration 8295 => Loss: 6.76580945370502995218\n",
      "Iteration 8296 => Loss: 6.76580034034521826669\n",
      "Iteration 8297 => Loss: 6.76579122821342693328\n",
      "Iteration 8298 => Loss: 6.76578211730948808622\n",
      "Iteration 8299 => Loss: 6.76577300763325872879\n",
      "Iteration 8300 => Loss: 6.76576389918454612626\n",
      "Iteration 8301 => Loss: 6.76575479196320017650\n",
      "Iteration 8302 => Loss: 6.76574568596905567830\n",
      "Iteration 8303 => Loss: 6.76573658120194565413\n",
      "Iteration 8304 => Loss: 6.76572747766170046191\n",
      "Iteration 8305 => Loss: 6.76571837534816111770\n",
      "Iteration 8306 => Loss: 6.76570927426115975578\n",
      "Iteration 8307 => Loss: 6.76570017440052762225\n",
      "Iteration 8308 => Loss: 6.76569107576609951593\n",
      "Iteration 8309 => Loss: 6.76568197835771822923\n",
      "Iteration 8310 => Loss: 6.76567288217521145555\n",
      "Iteration 8311 => Loss: 6.76566378721841843458\n",
      "Iteration 8312 => Loss: 6.76565469348717041242\n",
      "Iteration 8313 => Loss: 6.76564560098130662880\n",
      "Iteration 8314 => Loss: 6.76563650970065477708\n",
      "Iteration 8315 => Loss: 6.76562741964505676151\n",
      "Iteration 8316 => Loss: 6.76561833081434027548\n",
      "Iteration 8317 => Loss: 6.76560924320834189416\n",
      "Iteration 8318 => Loss: 6.76560015682690174543\n",
      "Iteration 8319 => Loss: 6.76559107166985729265\n",
      "Iteration 8320 => Loss: 6.76558198773703356466\n",
      "Iteration 8321 => Loss: 6.76557290502827601841\n",
      "Iteration 8322 => Loss: 6.76556382354340613006\n",
      "Iteration 8323 => Loss: 6.76555474328226669201\n",
      "Iteration 8324 => Loss: 6.76554566424470404939\n",
      "Iteration 8325 => Loss: 6.76553658643053079658\n",
      "Iteration 8326 => Loss: 6.76552750983960482500\n",
      "Iteration 8327 => Loss: 6.76551843447174316992\n",
      "Iteration 8328 => Loss: 6.76550936032678684739\n",
      "Iteration 8329 => Loss: 6.76550028740457154441\n",
      "Iteration 8330 => Loss: 6.76549121570493738886\n",
      "Iteration 8331 => Loss: 6.76548214522770940960\n",
      "Iteration 8332 => Loss: 6.76547307597274105717\n",
      "Iteration 8333 => Loss: 6.76546400793984137323\n",
      "Iteration 8334 => Loss: 6.76545494112886647287\n",
      "Iteration 8335 => Loss: 6.76544587553965026672\n",
      "Iteration 8336 => Loss: 6.76543681117201423092\n",
      "Iteration 8337 => Loss: 6.76542774802580648696\n",
      "Iteration 8338 => Loss: 6.76541868610085916913\n",
      "Iteration 8339 => Loss: 6.76540962539699997080\n",
      "Iteration 8340 => Loss: 6.76540056591408145437\n",
      "Iteration 8341 => Loss: 6.76539150765192154324\n",
      "Iteration 8342 => Loss: 6.76538245061036569439\n",
      "Iteration 8343 => Loss: 6.76537339478925403569\n",
      "Iteration 8344 => Loss: 6.76536434018840271420\n",
      "Iteration 8345 => Loss: 6.76535528680766784504\n",
      "Iteration 8346 => Loss: 6.76534623464687800976\n",
      "Iteration 8347 => Loss: 6.76533718370585646085\n",
      "Iteration 8348 => Loss: 6.76532813398445753705\n",
      "Iteration 8349 => Loss: 6.76531908548251070812\n",
      "Iteration 8350 => Loss: 6.76531003819984633196\n",
      "Iteration 8351 => Loss: 6.76530099213630720101\n",
      "Iteration 8352 => Loss: 6.76529194729172456135\n",
      "Iteration 8353 => Loss: 6.76528290366593498817\n",
      "Iteration 8354 => Loss: 6.76527386125878038570\n",
      "Iteration 8355 => Loss: 6.76526482007008311825\n",
      "Iteration 8356 => Loss: 6.76525578009968331372\n",
      "Iteration 8357 => Loss: 6.76524674134742642906\n",
      "Iteration 8358 => Loss: 6.76523770381314371036\n",
      "Iteration 8359 => Loss: 6.76522866749666995645\n",
      "Iteration 8360 => Loss: 6.76521963239783818977\n",
      "Iteration 8361 => Loss: 6.76521059851648320915\n",
      "Iteration 8362 => Loss: 6.76520156585245402425\n",
      "Iteration 8363 => Loss: 6.76519253440556322943\n",
      "Iteration 8364 => Loss: 6.76518350417567315702\n",
      "Iteration 8365 => Loss: 6.76517447516259906592\n",
      "Iteration 8366 => Loss: 6.76516544736618996581\n",
      "Iteration 8367 => Loss: 6.76515642078627088551\n",
      "Iteration 8368 => Loss: 6.76514739542269261108\n",
      "Iteration 8369 => Loss: 6.76513837127527928317\n",
      "Iteration 8370 => Loss: 6.76512934834387191785\n",
      "Iteration 8371 => Loss: 6.76512032662829998486\n",
      "Iteration 8372 => Loss: 6.76511130612840982934\n",
      "Iteration 8373 => Loss: 6.76510228684403003285\n",
      "Iteration 8374 => Loss: 6.76509326877500338782\n",
      "Iteration 8375 => Loss: 6.76508425192115847580\n",
      "Iteration 8376 => Loss: 6.76507523628233720103\n",
      "Iteration 8377 => Loss: 6.76506622185837169781\n",
      "Iteration 8378 => Loss: 6.76505720864909942946\n",
      "Iteration 8379 => Loss: 6.76504819665436674114\n",
      "Iteration 8380 => Loss: 6.76503918587399422080\n",
      "Iteration 8381 => Loss: 6.76503017630782288450\n",
      "Iteration 8382 => Loss: 6.76502116795569286012\n",
      "Iteration 8383 => Loss: 6.76501216081744338737\n",
      "Iteration 8384 => Loss: 6.76500315489290571236\n",
      "Iteration 8385 => Loss: 6.76499415018191374571\n",
      "Iteration 8386 => Loss: 6.76498514668430939167\n",
      "Iteration 8387 => Loss: 6.76497614439992656088\n",
      "Iteration 8388 => Loss: 6.76496714332860271668\n",
      "Iteration 8389 => Loss: 6.76495814347017265789\n",
      "Iteration 8390 => Loss: 6.76494914482446763060\n",
      "Iteration 8391 => Loss: 6.76494014739134108538\n",
      "Iteration 8392 => Loss: 6.76493115117061449837\n",
      "Iteration 8393 => Loss: 6.76492215616212977380\n",
      "Iteration 8394 => Loss: 6.76491316236572703957\n",
      "Iteration 8395 => Loss: 6.76490416978123310088\n",
      "Iteration 8396 => Loss: 6.76489517840849519104\n",
      "Iteration 8397 => Loss: 6.76488618824734277979\n",
      "Iteration 8398 => Loss: 6.76487719929762043591\n",
      "Iteration 8399 => Loss: 6.76486821155915851733\n",
      "Iteration 8400 => Loss: 6.76485922503179004650\n",
      "Iteration 8401 => Loss: 6.76485023971535426313\n",
      "Iteration 8402 => Loss: 6.76484125560969840052\n",
      "Iteration 8403 => Loss: 6.76483227271464748753\n",
      "Iteration 8404 => Loss: 6.76482329103004520476\n",
      "Iteration 8405 => Loss: 6.76481431055572635103\n",
      "Iteration 8406 => Loss: 6.76480533129152306060\n",
      "Iteration 8407 => Loss: 6.76479635323727901408\n",
      "Iteration 8408 => Loss: 6.76478737639282279304\n",
      "Iteration 8409 => Loss: 6.76477840075800163078\n",
      "Iteration 8410 => Loss: 6.76476942633265121430\n",
      "Iteration 8411 => Loss: 6.76476045311660456605\n",
      "Iteration 8412 => Loss: 6.76475148110968937942\n",
      "Iteration 8413 => Loss: 6.76474251031176532223\n",
      "Iteration 8414 => Loss: 6.76473354072264676518\n",
      "Iteration 8415 => Loss: 6.76472457234219159972\n",
      "Iteration 8416 => Loss: 6.76471560517021863745\n",
      "Iteration 8417 => Loss: 6.76470663920657599988\n",
      "Iteration 8418 => Loss: 6.76469767445109937398\n",
      "Iteration 8419 => Loss: 6.76468871090362178222\n",
      "Iteration 8420 => Loss: 6.76467974856398157613\n",
      "Iteration 8421 => Loss: 6.76467078743202154811\n",
      "Iteration 8422 => Loss: 6.76466182750756583886\n",
      "Iteration 8423 => Loss: 6.76465286879047233981\n",
      "Iteration 8424 => Loss: 6.76464391128055897440\n",
      "Iteration 8425 => Loss: 6.76463495497767119957\n",
      "Iteration 8426 => Loss: 6.76462599988165091958\n",
      "Iteration 8427 => Loss: 6.76461704599232316326\n",
      "Iteration 8428 => Loss: 6.76460809330953694030\n",
      "Iteration 8429 => Loss: 6.76459914183312438496\n",
      "Iteration 8430 => Loss: 6.76459019156292473696\n",
      "Iteration 8431 => Loss: 6.76458124249877545964\n",
      "Iteration 8432 => Loss: 6.76457229464051135182\n",
      "Iteration 8433 => Loss: 6.76456334798797520591\n",
      "Iteration 8434 => Loss: 6.76455440254100004438\n",
      "Iteration 8435 => Loss: 6.76454545829942333057\n",
      "Iteration 8436 => Loss: 6.76453651526308341602\n",
      "Iteration 8437 => Loss: 6.76452757343181865224\n",
      "Iteration 8438 => Loss: 6.76451863280546650259\n",
      "Iteration 8439 => Loss: 6.76450969338386798313\n",
      "Iteration 8440 => Loss: 6.76450075516685345178\n",
      "Iteration 8441 => Loss: 6.76449181815425770736\n",
      "Iteration 8442 => Loss: 6.76448288234593864132\n",
      "Iteration 8443 => Loss: 6.76447394774170795984\n",
      "Iteration 8444 => Loss: 6.76446501434142621889\n",
      "Iteration 8445 => Loss: 6.76445608214491400645\n",
      "Iteration 8446 => Loss: 6.76444715115202122035\n",
      "Iteration 8447 => Loss: 6.76443822136257733035\n",
      "Iteration 8448 => Loss: 6.76442929277641713526\n",
      "Iteration 8449 => Loss: 6.76442036539340030288\n",
      "Iteration 8450 => Loss: 6.76441143921333765121\n",
      "Iteration 8451 => Loss: 6.76440251423607730175\n",
      "Iteration 8452 => Loss: 6.76439359046146382326\n",
      "Iteration 8453 => Loss: 6.76438466788933023821\n",
      "Iteration 8454 => Loss: 6.76437574651951134541\n",
      "Iteration 8455 => Loss: 6.76436682635184904910\n",
      "Iteration 8456 => Loss: 6.76435790738617814810\n",
      "Iteration 8457 => Loss: 6.76434898962233965847\n",
      "Iteration 8458 => Loss: 6.76434007306016926719\n",
      "Iteration 8459 => Loss: 6.76433115769950443763\n",
      "Iteration 8460 => Loss: 6.76432224354019151491\n",
      "Iteration 8461 => Loss: 6.76431333058205375153\n",
      "Iteration 8462 => Loss: 6.76430441882494726258\n",
      "Iteration 8463 => Loss: 6.76429550826869352420\n",
      "Iteration 8464 => Loss: 6.76428659891314154606\n",
      "Iteration 8465 => Loss: 6.76427769075812346244\n",
      "Iteration 8466 => Loss: 6.76426878380348206576\n",
      "Iteration 8467 => Loss: 6.76425987804904771394\n",
      "Iteration 8468 => Loss: 6.76425097349467208119\n",
      "Iteration 8469 => Loss: 6.76424207014018730177\n",
      "Iteration 8470 => Loss: 6.76423316798542550998\n",
      "Iteration 8471 => Loss: 6.76422426703023216277\n",
      "Iteration 8472 => Loss: 6.76421536727443584169\n",
      "Iteration 8473 => Loss: 6.76420646871789266186\n",
      "Iteration 8474 => Loss: 6.76419757136042321122\n",
      "Iteration 8475 => Loss: 6.76418867520187827580\n",
      "Iteration 8476 => Loss: 6.76417978024208910171\n",
      "Iteration 8477 => Loss: 6.76417088648089670500\n",
      "Iteration 8478 => Loss: 6.76416199391813854902\n",
      "Iteration 8479 => Loss: 6.76415310255365298531\n",
      "Iteration 8480 => Loss: 6.76414421238728369445\n",
      "Iteration 8481 => Loss: 6.76413532341886103438\n",
      "Iteration 8482 => Loss: 6.76412643564823223841\n",
      "Iteration 8483 => Loss: 6.76411754907522233538\n",
      "Iteration 8484 => Loss: 6.76410866369968477585\n",
      "Iteration 8485 => Loss: 6.76409977952145347047\n",
      "Iteration 8486 => Loss: 6.76409089654036588257\n",
      "Iteration 8487 => Loss: 6.76408201475625769916\n",
      "Iteration 8488 => Loss: 6.76407313416897082448\n",
      "Iteration 8489 => Loss: 6.76406425477834627458\n",
      "Iteration 8490 => Loss: 6.76405537658422240099\n",
      "Iteration 8491 => Loss: 6.76404649958642512075\n",
      "Iteration 8492 => Loss: 6.76403762378481587803\n",
      "Iteration 8493 => Loss: 6.76402874917921792530\n",
      "Iteration 8494 => Loss: 6.76401987576947494318\n",
      "Iteration 8495 => Loss: 6.76401100355541906595\n",
      "Iteration 8496 => Loss: 6.76400213253689752690\n",
      "Iteration 8497 => Loss: 6.76399326271374778941\n",
      "Iteration 8498 => Loss: 6.76398439408580109955\n",
      "Iteration 8499 => Loss: 6.76397552665291712515\n",
      "Iteration 8500 => Loss: 6.76396666041490846055\n",
      "Iteration 8501 => Loss: 6.76395779537163210904\n",
      "Iteration 8502 => Loss: 6.76394893152291931671\n",
      "Iteration 8503 => Loss: 6.76394006886861109962\n",
      "Iteration 8504 => Loss: 6.76393120740854403294\n",
      "Iteration 8505 => Loss: 6.76392234714255469186\n",
      "Iteration 8506 => Loss: 6.76391348807049563874\n",
      "Iteration 8507 => Loss: 6.76390463019219545515\n",
      "Iteration 8508 => Loss: 6.76389577350749160445\n",
      "Iteration 8509 => Loss: 6.76388691801622776723\n",
      "Iteration 8510 => Loss: 6.76387806371824407137\n",
      "Iteration 8511 => Loss: 6.76386921061337353933\n",
      "Iteration 8512 => Loss: 6.76386035870146962168\n",
      "Iteration 8513 => Loss: 6.76385150798235201819\n",
      "Iteration 8514 => Loss: 6.76384265845587240307\n",
      "Iteration 8515 => Loss: 6.76383381012186646331\n",
      "Iteration 8516 => Loss: 6.76382496298017255043\n",
      "Iteration 8517 => Loss: 6.76381611703063345686\n",
      "Iteration 8518 => Loss: 6.76380727227308842231\n",
      "Iteration 8519 => Loss: 6.76379842870737224558\n",
      "Iteration 8520 => Loss: 6.76378958633333304817\n",
      "Iteration 8521 => Loss: 6.76378074515079852347\n",
      "Iteration 8522 => Loss: 6.76377190515961768114\n",
      "Iteration 8523 => Loss: 6.76376306635961999092\n",
      "Iteration 8524 => Loss: 6.76375422875065623884\n",
      "Iteration 8525 => Loss: 6.76374539233256211190\n",
      "Iteration 8526 => Loss: 6.76373655710517418527\n",
      "Iteration 8527 => Loss: 6.76372772306833791589\n",
      "Iteration 8528 => Loss: 6.76371889022187922080\n",
      "Iteration 8529 => Loss: 6.76371005856565421510\n",
      "Iteration 8530 => Loss: 6.76370122809949680942\n",
      "Iteration 8531 => Loss: 6.76369239882323824986\n",
      "Iteration 8532 => Loss: 6.76368357073673642788\n",
      "Iteration 8533 => Loss: 6.76367474383981015507\n",
      "Iteration 8534 => Loss: 6.76366591813231377017\n",
      "Iteration 8535 => Loss: 6.76365709361408029565\n",
      "Iteration 8536 => Loss: 6.76364827028495074757\n",
      "Iteration 8537 => Loss: 6.76363944814476791834\n",
      "Iteration 8538 => Loss: 6.76363062719336660678\n",
      "Iteration 8539 => Loss: 6.76362180743058694077\n",
      "Iteration 8540 => Loss: 6.76361298885627526545\n",
      "Iteration 8541 => Loss: 6.76360417147026904416\n",
      "Iteration 8542 => Loss: 6.76359535527240307573\n",
      "Iteration 8543 => Loss: 6.76358654026252548164\n",
      "Iteration 8544 => Loss: 6.76357772644046129074\n",
      "Iteration 8545 => Loss: 6.76356891380606750630\n",
      "Iteration 8546 => Loss: 6.76356010235917270990\n",
      "Iteration 8547 => Loss: 6.76355129209962235848\n",
      "Iteration 8548 => Loss: 6.76354248302725480357\n",
      "Iteration 8549 => Loss: 6.76353367514191994303\n",
      "Iteration 8550 => Loss: 6.76352486844343570027\n",
      "Iteration 8551 => Loss: 6.76351606293165819039\n",
      "Iteration 8552 => Loss: 6.76350725860642576492\n",
      "Iteration 8553 => Loss: 6.76349845546757499903\n",
      "Iteration 8554 => Loss: 6.76348965351495401421\n",
      "Iteration 8555 => Loss: 6.76348085274839228020\n",
      "Iteration 8556 => Loss: 6.76347205316772726036\n",
      "Iteration 8557 => Loss: 6.76346325477281506977\n",
      "Iteration 8558 => Loss: 6.76345445756348606636\n",
      "Iteration 8559 => Loss: 6.76344566153958126620\n",
      "Iteration 8560 => Loss: 6.76343686670093457991\n",
      "Iteration 8561 => Loss: 6.76342807304740478713\n",
      "Iteration 8562 => Loss: 6.76341928057881514036\n",
      "Iteration 8563 => Loss: 6.76341048929501287290\n",
      "Iteration 8564 => Loss: 6.76340169919583988900\n",
      "Iteration 8565 => Loss: 6.76339291028112388204\n",
      "Iteration 8566 => Loss: 6.76338412255071563806\n",
      "Iteration 8567 => Loss: 6.76337533600446949578\n",
      "Iteration 8568 => Loss: 6.76336655064220071409\n",
      "Iteration 8569 => Loss: 6.76335776646375919086\n",
      "Iteration 8570 => Loss: 6.76334898346898505395\n",
      "Iteration 8571 => Loss: 6.76334020165772376032\n",
      "Iteration 8572 => Loss: 6.76333142102980566790\n",
      "Iteration 8573 => Loss: 6.76332264158509044449\n",
      "Iteration 8574 => Loss: 6.76331386332340045442\n",
      "Iteration 8575 => Loss: 6.76330508624457582556\n",
      "Iteration 8576 => Loss: 6.76329631034846645576\n",
      "Iteration 8577 => Loss: 6.76328753563491336109\n",
      "Iteration 8578 => Loss: 6.76327876210375489308\n",
      "Iteration 8579 => Loss: 6.76326998975482851506\n",
      "Iteration 8580 => Loss: 6.76326121858797169040\n",
      "Iteration 8581 => Loss: 6.76325244860302987604\n",
      "Iteration 8582 => Loss: 6.76324367979984586441\n",
      "Iteration 8583 => Loss: 6.76323491217826422428\n",
      "Iteration 8584 => Loss: 6.76322614573811708993\n",
      "Iteration 8585 => Loss: 6.76321738047925080650\n",
      "Iteration 8586 => Loss: 6.76320861640150106098\n",
      "Iteration 8587 => Loss: 6.76319985350471597485\n",
      "Iteration 8588 => Loss: 6.76319109178872324151\n",
      "Iteration 8589 => Loss: 6.76318233125337986422\n",
      "Iteration 8590 => Loss: 6.76317357189851353638\n",
      "Iteration 8591 => Loss: 6.76316481372397237948\n",
      "Iteration 8592 => Loss: 6.76315605672959829775\n",
      "Iteration 8593 => Loss: 6.76314730091522609001\n",
      "Iteration 8594 => Loss: 6.76313854628070298958\n",
      "Iteration 8595 => Loss: 6.76312979282587622976\n",
      "Iteration 8596 => Loss: 6.76312104055057083940\n",
      "Iteration 8597 => Loss: 6.76311228945462961093\n",
      "Iteration 8598 => Loss: 6.76310353953790244219\n",
      "Iteration 8599 => Loss: 6.76309479080022857289\n",
      "Iteration 8600 => Loss: 6.76308604324144724274\n",
      "Iteration 8601 => Loss: 6.76307729686140035597\n",
      "Iteration 8602 => Loss: 6.76306855165993336954\n",
      "Iteration 8603 => Loss: 6.76305980763688019408\n",
      "Iteration 8604 => Loss: 6.76305106479208451020\n",
      "Iteration 8605 => Loss: 6.76304232312538200489\n",
      "Iteration 8606 => Loss: 6.76303358263662701688\n",
      "Iteration 8607 => Loss: 6.76302484332564901592\n",
      "Iteration 8608 => Loss: 6.76301610519229345897\n",
      "Iteration 8609 => Loss: 6.76300736823641113205\n",
      "Iteration 8610 => Loss: 6.76299863245782528764\n",
      "Iteration 8611 => Loss: 6.76298989785638937633\n",
      "Iteration 8612 => Loss: 6.76298116443193997327\n",
      "Iteration 8613 => Loss: 6.76297243218432164724\n",
      "Iteration 8614 => Loss: 6.76296370111337719067\n",
      "Iteration 8615 => Loss: 6.76295497121894317871\n",
      "Iteration 8616 => Loss: 6.76294624250085885109\n",
      "Iteration 8617 => Loss: 6.76293751495897677017\n",
      "Iteration 8618 => Loss: 6.76292878859312907025\n",
      "Iteration 8619 => Loss: 6.76292006340315854374\n",
      "Iteration 8620 => Loss: 6.76291133938890531851\n",
      "Iteration 8621 => Loss: 6.76290261655021840426\n",
      "Iteration 8622 => Loss: 6.76289389488693082342\n",
      "Iteration 8623 => Loss: 6.76288517439888980931\n",
      "Iteration 8624 => Loss: 6.76287645508593815435\n",
      "Iteration 8625 => Loss: 6.76286773694791509826\n",
      "Iteration 8626 => Loss: 6.76285901998465366347\n",
      "Iteration 8627 => Loss: 6.76285030419601351781\n",
      "Iteration 8628 => Loss: 6.76284158958181613741\n",
      "Iteration 8629 => Loss: 6.76283287614192474280\n",
      "Iteration 8630 => Loss: 6.76282416387615903375\n",
      "Iteration 8631 => Loss: 6.76281545278438311897\n",
      "Iteration 8632 => Loss: 6.76280674286642025095\n",
      "Iteration 8633 => Loss: 6.76279803412212654479\n",
      "Iteration 8634 => Loss: 6.76278932655132347662\n",
      "Iteration 8635 => Loss: 6.76278062015387870787\n",
      "Iteration 8636 => Loss: 6.76277191492961637920\n",
      "Iteration 8637 => Loss: 6.76276321087838816482\n",
      "Iteration 8638 => Loss: 6.76275450800001820539\n",
      "Iteration 8639 => Loss: 6.76274580629437949142\n",
      "Iteration 8640 => Loss: 6.76273710576129083449\n",
      "Iteration 8641 => Loss: 6.76272840640059591522\n",
      "Iteration 8642 => Loss: 6.76271970821213663783\n",
      "Iteration 8643 => Loss: 6.76271101119576734106\n",
      "Iteration 8644 => Loss: 6.76270231535132282374\n",
      "Iteration 8645 => Loss: 6.76269362067863433197\n",
      "Iteration 8646 => Loss: 6.76268492717755975718\n",
      "Iteration 8647 => Loss: 6.76267623484793212185\n",
      "Iteration 8648 => Loss: 6.76266754368960487653\n",
      "Iteration 8649 => Loss: 6.76265885370240660279\n",
      "Iteration 8650 => Loss: 6.76265016488618186941\n",
      "Iteration 8651 => Loss: 6.76264147724077524515\n",
      "Iteration 8652 => Loss: 6.76263279076603396334\n",
      "Iteration 8653 => Loss: 6.76262410546179548732\n",
      "Iteration 8654 => Loss: 6.76261542132789639226\n",
      "Iteration 8655 => Loss: 6.76260673836419190508\n",
      "Iteration 8656 => Loss: 6.76259805657051416006\n",
      "Iteration 8657 => Loss: 6.76258937594671305504\n",
      "Iteration 8658 => Loss: 6.76258069649262161249\n",
      "Iteration 8659 => Loss: 6.76257201820809239479\n",
      "Iteration 8660 => Loss: 6.76256334109295487167\n",
      "Iteration 8661 => Loss: 6.76255466514706693459\n",
      "Iteration 8662 => Loss: 6.76254599037025894148\n",
      "Iteration 8663 => Loss: 6.76253731676237723747\n",
      "Iteration 8664 => Loss: 6.76252864432326639132\n",
      "Iteration 8665 => Loss: 6.76251997305277363637\n",
      "Iteration 8666 => Loss: 6.76251130295072577781\n",
      "Iteration 8667 => Loss: 6.76250263401697981891\n",
      "Iteration 8668 => Loss: 6.76249396625137322303\n",
      "Iteration 8669 => Loss: 6.76248529965374700623\n",
      "Iteration 8670 => Loss: 6.76247663422394484911\n",
      "Iteration 8671 => Loss: 6.76246796996181398498\n",
      "Iteration 8672 => Loss: 6.76245930686718743630\n",
      "Iteration 8673 => Loss: 6.76245064493991510091\n",
      "Iteration 8674 => Loss: 6.76244198417984154759\n",
      "Iteration 8675 => Loss: 6.76243332458680335151\n",
      "Iteration 8676 => Loss: 6.76242466616065041052\n",
      "Iteration 8677 => Loss: 6.76241600890121841161\n",
      "Iteration 8678 => Loss: 6.76240735280834748266\n",
      "Iteration 8679 => Loss: 6.76239869788189285060\n",
      "Iteration 8680 => Loss: 6.76239004412168398517\n",
      "Iteration 8681 => Loss: 6.76238139152757433692\n",
      "Iteration 8682 => Loss: 6.76237274009940403374\n",
      "Iteration 8683 => Loss: 6.76236408983700698627\n",
      "Iteration 8684 => Loss: 6.76235544074024019778\n",
      "Iteration 8685 => Loss: 6.76234679280893313802\n",
      "Iteration 8686 => Loss: 6.76233814604294458661\n",
      "Iteration 8687 => Loss: 6.76232950044210756602\n",
      "Iteration 8688 => Loss: 6.76232085600625865141\n",
      "Iteration 8689 => Loss: 6.76231221273525306970\n",
      "Iteration 8690 => Loss: 6.76230357062893094877\n",
      "Iteration 8691 => Loss: 6.76229492968713064016\n",
      "Iteration 8692 => Loss: 6.76228628990969671264\n",
      "Iteration 8693 => Loss: 6.76227765129647195863\n",
      "Iteration 8694 => Loss: 6.76226901384730538780\n",
      "Iteration 8695 => Loss: 6.76226037756203179896\n",
      "Iteration 8696 => Loss: 6.76225174244050020178\n",
      "Iteration 8697 => Loss: 6.76224310848255250050\n",
      "Iteration 8698 => Loss: 6.76223447568803415209\n",
      "Iteration 8699 => Loss: 6.76222584405677817898\n",
      "Iteration 8700 => Loss: 6.76221721358863891993\n",
      "Iteration 8701 => Loss: 6.76220858428345916735\n",
      "Iteration 8702 => Loss: 6.76219995614107372006\n",
      "Iteration 8703 => Loss: 6.76219132916133425226\n",
      "Iteration 8704 => Loss: 6.76218270334408000366\n",
      "Iteration 8705 => Loss: 6.76217407868915909575\n",
      "Iteration 8706 => Loss: 6.76216545519640188644\n",
      "Iteration 8707 => Loss: 6.76215683286567070809\n",
      "Iteration 8708 => Loss: 6.76214821169679680679\n",
      "Iteration 8709 => Loss: 6.76213959168962386315\n",
      "Iteration 8710 => Loss: 6.76213097284399733411\n",
      "Iteration 8711 => Loss: 6.76212235515975823574\n",
      "Iteration 8712 => Loss: 6.76211373863675735407\n",
      "Iteration 8713 => Loss: 6.76210512327483037609\n",
      "Iteration 8714 => Loss: 6.76209650907382897600\n",
      "Iteration 8715 => Loss: 6.76208789603358884079\n",
      "Iteration 8716 => Loss: 6.76207928415395898014\n",
      "Iteration 8717 => Loss: 6.76207067343477508103\n",
      "Iteration 8718 => Loss: 6.76206206387588615314\n",
      "Iteration 8719 => Loss: 6.76205345547714209431\n",
      "Iteration 8720 => Loss: 6.76204484823837770335\n",
      "Iteration 8721 => Loss: 6.76203624215944287812\n",
      "Iteration 8722 => Loss: 6.76202763724017419378\n",
      "Iteration 8723 => Loss: 6.76201903348041888364\n",
      "Iteration 8724 => Loss: 6.76201043088001529924\n",
      "Iteration 8725 => Loss: 6.76200182943882577291\n",
      "Iteration 8726 => Loss: 6.76199322915667089262\n",
      "Iteration 8727 => Loss: 6.76198463003340499711\n",
      "Iteration 8728 => Loss: 6.76197603206887709604\n",
      "Iteration 8729 => Loss: 6.76196743526292554094\n",
      "Iteration 8730 => Loss: 6.76195883961538868334\n",
      "Iteration 8731 => Loss: 6.76195024512612086198\n",
      "Iteration 8732 => Loss: 6.76194165179496486928\n",
      "Iteration 8733 => Loss: 6.76193305962175017498\n",
      "Iteration 8734 => Loss: 6.76192446860633911143\n",
      "Iteration 8735 => Loss: 6.76191587874856470108\n",
      "Iteration 8736 => Loss: 6.76190729004827328907\n",
      "Iteration 8737 => Loss: 6.76189870250531388507\n",
      "Iteration 8738 => Loss: 6.76189011611952217606\n",
      "Iteration 8739 => Loss: 6.76188153089074894808\n",
      "Iteration 8740 => Loss: 6.76187294681883521719\n",
      "Iteration 8741 => Loss: 6.76186436390362732851\n",
      "Iteration 8742 => Loss: 6.76185578214496629812\n",
      "Iteration 8743 => Loss: 6.76184720154270202386\n",
      "Iteration 8744 => Loss: 6.76183862209667285725\n",
      "Iteration 8745 => Loss: 6.76183004380672070255\n",
      "Iteration 8746 => Loss: 6.76182146667269101670\n",
      "Iteration 8747 => Loss: 6.76181289069443902662\n",
      "Iteration 8748 => Loss: 6.76180431587179864295\n",
      "Iteration 8749 => Loss: 6.76179574220461443446\n",
      "Iteration 8750 => Loss: 6.76178716969273718718\n",
      "Iteration 8751 => Loss: 6.76177859833599814721\n",
      "Iteration 8752 => Loss: 6.76177002813425342964\n",
      "Iteration 8753 => Loss: 6.76176145908734760326\n",
      "Iteration 8754 => Loss: 6.76175289119511901959\n",
      "Iteration 8755 => Loss: 6.76174432445741668829\n",
      "Iteration 8756 => Loss: 6.76173575887408428997\n",
      "Iteration 8757 => Loss: 6.76172719444496017616\n",
      "Iteration 8758 => Loss: 6.76171863116989779741\n",
      "Iteration 8759 => Loss: 6.76171006904873284071\n",
      "Iteration 8760 => Loss: 6.76170150808131698028\n",
      "Iteration 8761 => Loss: 6.76169294826748856764\n",
      "Iteration 8762 => Loss: 6.76168438960709838881\n",
      "Iteration 8763 => Loss: 6.76167583209999101257\n",
      "Iteration 8764 => Loss: 6.76166727574600834316\n",
      "Iteration 8765 => Loss: 6.76165872054499228483\n",
      "Iteration 8766 => Loss: 6.76165016649678829452\n",
      "Iteration 8767 => Loss: 6.76164161360124715827\n",
      "Iteration 8768 => Loss: 6.76163306185820900396\n",
      "Iteration 8769 => Loss: 6.76162451126751662400\n",
      "Iteration 8770 => Loss: 6.76161596182901813989\n",
      "Iteration 8771 => Loss: 6.76160741354256167313\n",
      "Iteration 8772 => Loss: 6.76159886640797758162\n",
      "Iteration 8773 => Loss: 6.76159032042512286864\n",
      "Iteration 8774 => Loss: 6.76158177559384210298\n",
      "Iteration 8775 => Loss: 6.76157323191398074158\n",
      "Iteration 8776 => Loss: 6.76156468938538246505\n",
      "Iteration 8777 => Loss: 6.76155614800788118401\n",
      "Iteration 8778 => Loss: 6.76154760778133834265\n",
      "Iteration 8779 => Loss: 6.76153906870559140430\n",
      "Iteration 8780 => Loss: 6.76153053078048849045\n",
      "Iteration 8781 => Loss: 6.76152199400586706446\n",
      "Iteration 8782 => Loss: 6.76151345838157791235\n",
      "Iteration 8783 => Loss: 6.76150492390745760929\n",
      "Iteration 8784 => Loss: 6.76149639058337204034\n",
      "Iteration 8785 => Loss: 6.76148785840914268164\n",
      "Iteration 8786 => Loss: 6.76147932738462653646\n",
      "Iteration 8787 => Loss: 6.76147079750966817357\n",
      "Iteration 8788 => Loss: 6.76146226878411127359\n",
      "Iteration 8789 => Loss: 6.76145374120779418803\n",
      "Iteration 8790 => Loss: 6.76144521478057747288\n",
      "Iteration 8791 => Loss: 6.76143668950229237424\n",
      "Iteration 8792 => Loss: 6.76142816537279234268\n",
      "Iteration 8793 => Loss: 6.76141964239192017061\n",
      "Iteration 8794 => Loss: 6.76141112055951509774\n",
      "Iteration 8795 => Loss: 6.76140259987543768005\n",
      "Iteration 8796 => Loss: 6.76139408033951294641\n",
      "Iteration 8797 => Loss: 6.76138556195160322915\n",
      "Iteration 8798 => Loss: 6.76137704471154066255\n",
      "Iteration 8799 => Loss: 6.76136852861917869717\n",
      "Iteration 8800 => Loss: 6.76136001367436545451\n",
      "Iteration 8801 => Loss: 6.76135149987693484519\n",
      "Iteration 8802 => Loss: 6.76134298722674831339\n",
      "Iteration 8803 => Loss: 6.76133447572362911160\n",
      "Iteration 8804 => Loss: 6.76132596536744578941\n",
      "Iteration 8805 => Loss: 6.76131745615802959293\n",
      "Iteration 8806 => Loss: 6.76130894809523041999\n",
      "Iteration 8807 => Loss: 6.76130044117889461575\n",
      "Iteration 8808 => Loss: 6.76129193540886852531\n",
      "Iteration 8809 => Loss: 6.76128343078499050023\n",
      "Iteration 8810 => Loss: 6.76127492730711310287\n",
      "Iteration 8811 => Loss: 6.76126642497508090202\n",
      "Iteration 8812 => Loss: 6.76125792378873846644\n",
      "Iteration 8813 => Loss: 6.76124942374793125310\n",
      "Iteration 8814 => Loss: 6.76124092485250294260\n",
      "Iteration 8815 => Loss: 6.76123242710230609731\n",
      "Iteration 8816 => Loss: 6.76122393049717729241\n",
      "Iteration 8817 => Loss: 6.76121543503696997846\n",
      "Iteration 8818 => Loss: 6.76120694072152694787\n",
      "Iteration 8819 => Loss: 6.76119844755068655218\n",
      "Iteration 8820 => Loss: 6.76118995552431112372\n",
      "Iteration 8821 => Loss: 6.76118146464223457315\n",
      "Iteration 8822 => Loss: 6.76117297490429969287\n",
      "Iteration 8823 => Loss: 6.76116448631035726891\n",
      "Iteration 8824 => Loss: 6.76115599886026874543\n",
      "Iteration 8825 => Loss: 6.76114751255384849316\n",
      "Iteration 8826 => Loss: 6.76113902739096239713\n",
      "Iteration 8827 => Loss: 6.76113054337145324979\n",
      "Iteration 8828 => Loss: 6.76112206049517094897\n",
      "Iteration 8829 => Loss: 6.76111357876194940530\n",
      "Iteration 8830 => Loss: 6.76110509817164206936\n",
      "Iteration 8831 => Loss: 6.76109661872410150352\n",
      "Iteration 8832 => Loss: 6.76108814041916517112\n",
      "Iteration 8833 => Loss: 6.76107966325668030549\n",
      "Iteration 8834 => Loss: 6.76107118723649325176\n",
      "Iteration 8835 => Loss: 6.76106271235844769052\n",
      "Iteration 8836 => Loss: 6.76105423862239707233\n",
      "Iteration 8837 => Loss: 6.76104576602818063691\n",
      "Iteration 8838 => Loss: 6.76103729457565272298\n",
      "Iteration 8839 => Loss: 6.76102882426464990573\n",
      "Iteration 8840 => Loss: 6.76102035509502474753\n",
      "Iteration 8841 => Loss: 6.76101188706661204719\n",
      "Iteration 8842 => Loss: 6.76100342017927502525\n",
      "Iteration 8843 => Loss: 6.76099495443284759233\n",
      "Iteration 8844 => Loss: 6.76098648982718142264\n",
      "Iteration 8845 => Loss: 6.76097802636212286131\n",
      "Iteration 8846 => Loss: 6.76096956403751203624\n",
      "Iteration 8847 => Loss: 6.76096110285320506250\n",
      "Iteration 8848 => Loss: 6.76095264280904473253\n",
      "Iteration 8849 => Loss: 6.76094418390487295056\n",
      "Iteration 8850 => Loss: 6.76093572614053961445\n",
      "Iteration 8851 => Loss: 6.76092726951588662843\n",
      "Iteration 8852 => Loss: 6.76091881403077277213\n",
      "Iteration 8853 => Loss: 6.76091035968502840348\n",
      "Iteration 8854 => Loss: 6.76090190647851407846\n",
      "Iteration 8855 => Loss: 6.76089345441106726042\n",
      "Iteration 8856 => Loss: 6.76088500348253429451\n",
      "Iteration 8857 => Loss: 6.76087655369276685491\n",
      "Iteration 8858 => Loss: 6.76086810504161217494\n",
      "Iteration 8859 => Loss: 6.76085965752891127067\n",
      "Iteration 8860 => Loss: 6.76085121115451403995\n",
      "Iteration 8861 => Loss: 6.76084276591826505154\n",
      "Iteration 8862 => Loss: 6.76083432182001331512\n",
      "Iteration 8863 => Loss: 6.76082587885960517582\n",
      "Iteration 8864 => Loss: 6.76081743703688609060\n",
      "Iteration 8865 => Loss: 6.76080899635170151640\n",
      "Iteration 8866 => Loss: 6.76080055680389868655\n",
      "Iteration 8867 => Loss: 6.76079211839333371614\n",
      "Iteration 8868 => Loss: 6.76078368111983785127\n",
      "Iteration 8869 => Loss: 6.76077524498326098978\n",
      "Iteration 8870 => Loss: 6.76076680998346635221\n",
      "Iteration 8871 => Loss: 6.76075837612028163193\n",
      "Iteration 8872 => Loss: 6.76074994339356205586\n",
      "Iteration 8873 => Loss: 6.76074151180315219278\n",
      "Iteration 8874 => Loss: 6.76073308134889838783\n",
      "Iteration 8875 => Loss: 6.76072465203065231520\n",
      "Iteration 8876 => Loss: 6.76071622384825676733\n",
      "Iteration 8877 => Loss: 6.76070779680155808933\n",
      "Iteration 8878 => Loss: 6.76069937089040884359\n",
      "Iteration 8879 => Loss: 6.76069094611464826983\n",
      "Iteration 8880 => Loss: 6.76068252247412093681\n",
      "Iteration 8881 => Loss: 6.76067409996869095323\n",
      "Iteration 8882 => Loss: 6.76066567859818690067\n",
      "Iteration 8883 => Loss: 6.76065725836246667058\n",
      "Iteration 8884 => Loss: 6.76064883926137216719\n",
      "Iteration 8885 => Loss: 6.76064042129475151199\n",
      "Iteration 8886 => Loss: 6.76063200446245282649\n",
      "Iteration 8887 => Loss: 6.76062358876432067945\n",
      "Iteration 8888 => Loss: 6.76061517420021385050\n",
      "Iteration 8889 => Loss: 6.76060676076996092121\n",
      "Iteration 8890 => Loss: 6.76059834847342333575\n",
      "Iteration 8891 => Loss: 6.76058993731043678110\n",
      "Iteration 8892 => Loss: 6.76058152728086181327\n",
      "Iteration 8893 => Loss: 6.76057311838453589559\n",
      "Iteration 8894 => Loss: 6.76056471062130981409\n",
      "Iteration 8895 => Loss: 6.76055630399103346662\n",
      "Iteration 8896 => Loss: 6.76054789849354786924\n",
      "Iteration 8897 => Loss: 6.76053949412870203162\n",
      "Iteration 8898 => Loss: 6.76053109089634851614\n",
      "Iteration 8899 => Loss: 6.76052268879633189158\n",
      "Iteration 8900 => Loss: 6.76051428782849939125\n",
      "Iteration 8901 => Loss: 6.76050588799268936668\n",
      "Iteration 8902 => Loss: 6.76049748928876592657\n",
      "Iteration 8903 => Loss: 6.76048909171657363970\n",
      "Iteration 8904 => Loss: 6.76048069527594197581\n",
      "Iteration 8905 => Loss: 6.76047229996673859631\n",
      "Iteration 8906 => Loss: 6.76046390578879741184\n",
      "Iteration 8907 => Loss: 6.76045551274198519565\n",
      "Iteration 8908 => Loss: 6.76044712082612431203\n",
      "Iteration 8909 => Loss: 6.76043873004107975788\n",
      "Iteration 8910 => Loss: 6.76043034038669876651\n",
      "Iteration 8911 => Loss: 6.76042195186281880126\n",
      "Iteration 8912 => Loss: 6.76041356446929420088\n",
      "Iteration 8913 => Loss: 6.76040517820596864595\n",
      "Iteration 8914 => Loss: 6.76039679307269647524\n",
      "Iteration 8915 => Loss: 6.76038840906931781660\n",
      "Iteration 8916 => Loss: 6.76038002619569056151\n",
      "Iteration 8917 => Loss: 6.76037164445165039695\n",
      "Iteration 8918 => Loss: 6.76036326383705077347\n",
      "Iteration 8919 => Loss: 6.76035488435174514166\n",
      "Iteration 8920 => Loss: 6.76034650599556918849\n",
      "Iteration 8921 => Loss: 6.76033812876838435812\n",
      "Iteration 8922 => Loss: 6.76032975267002811393\n",
      "Iteration 8923 => Loss: 6.76032137770035124191\n",
      "Iteration 8924 => Loss: 6.76031300385920097540\n",
      "Iteration 8925 => Loss: 6.76030463114643076494\n",
      "Iteration 8926 => Loss: 6.76029625956187540936\n",
      "Iteration 8927 => Loss: 6.76028788910539990553\n",
      "Iteration 8928 => Loss: 6.76027951977683994045\n",
      "Iteration 8929 => Loss: 6.76027115157605162921\n",
      "Iteration 8930 => Loss: 6.76026278450287865240\n",
      "Iteration 8931 => Loss: 6.76025441855716646700\n",
      "Iteration 8932 => Loss: 6.76024605373876674719\n",
      "Iteration 8933 => Loss: 6.76023769004753471989\n",
      "Iteration 8934 => Loss: 6.76022932748329630215\n",
      "Iteration 8935 => Loss: 6.76022096604592448443\n",
      "Iteration 8936 => Loss: 6.76021260573525317739\n",
      "Iteration 8937 => Loss: 6.76020424655113849610\n",
      "Iteration 8938 => Loss: 6.76019588849341257486\n",
      "Iteration 8939 => Loss: 6.76018753156195195686\n",
      "Iteration 8940 => Loss: 6.76017917575657989460\n",
      "Iteration 8941 => Loss: 6.76017082107715339134\n",
      "Iteration 8942 => Loss: 6.76016246752352412130\n",
      "Iteration 8943 => Loss: 6.76015411509553043601\n",
      "Iteration 8944 => Loss: 6.76014576379303111509\n",
      "Iteration 8945 => Loss: 6.76013741361586717460\n",
      "Iteration 8946 => Loss: 6.76012906456389472964\n",
      "Iteration 8947 => Loss: 6.76012071663695657264\n",
      "Iteration 8948 => Loss: 6.76011236983490348962\n",
      "Iteration 8949 => Loss: 6.76010402415758093753\n",
      "Iteration 8950 => Loss: 6.76009567960483614968\n",
      "Iteration 8951 => Loss: 6.76008733617652524117\n",
      "Iteration 8952 => Loss: 6.76007899387248833989\n",
      "Iteration 8953 => Loss: 6.76007065269257889639\n",
      "Iteration 8954 => Loss: 6.76006231263664414399\n",
      "Iteration 8955 => Loss: 6.76005397370453309236\n",
      "Iteration 8956 => Loss: 6.76004563589609475116\n",
      "Iteration 8957 => Loss: 6.76003729921117368917\n",
      "Iteration 8958 => Loss: 6.76002896364962335696\n",
      "Iteration 8959 => Loss: 6.76002062921129365236\n",
      "Iteration 8960 => Loss: 6.76001229589602736780\n",
      "Iteration 8961 => Loss: 6.76000396370367351295\n",
      "Iteration 8962 => Loss: 6.75999563263408642655\n",
      "Iteration 8963 => Loss: 6.75998730268711156555\n",
      "Iteration 8964 => Loss: 6.75997897386259705144\n",
      "Iteration 8965 => Loss: 6.75997064616039455842\n",
      "Iteration 8966 => Loss: 6.75996231958034865528\n",
      "Iteration 8967 => Loss: 6.75995399412230746350\n",
      "Iteration 8968 => Loss: 6.75994566978612798636\n",
      "Iteration 8969 => Loss: 6.75993734657164679902\n",
      "Iteration 8970 => Loss: 6.75992902447872623384\n",
      "Iteration 8971 => Loss: 6.75992070350719753691\n",
      "Iteration 8972 => Loss: 6.75991238365693014600\n",
      "Iteration 8973 => Loss: 6.75990406492776685354\n",
      "Iteration 8974 => Loss: 6.75989574731954068199\n",
      "Iteration 8975 => Loss: 6.75988743083211840457\n",
      "Iteration 8976 => Loss: 6.75987911546534281371\n",
      "Iteration 8977 => Loss: 6.75987080121906824814\n",
      "Iteration 8978 => Loss: 6.75986248809313305941\n",
      "Iteration 8979 => Loss: 6.75985417608739513895\n",
      "Iteration 8980 => Loss: 6.75984586520169816737\n",
      "Iteration 8981 => Loss: 6.75983755543589470705\n",
      "Iteration 8982 => Loss: 6.75982924678984353761\n",
      "Iteration 8983 => Loss: 6.75982093926336613521\n",
      "Iteration 8984 => Loss: 6.75981263285633637850\n",
      "Iteration 8985 => Loss: 6.75980432756859972443\n",
      "Iteration 8986 => Loss: 6.75979602339999363636\n",
      "Iteration 8987 => Loss: 6.75978772035037600574\n",
      "Iteration 8988 => Loss: 6.75977941841959761859\n",
      "Iteration 8989 => Loss: 6.75977111760750926095\n",
      "Iteration 8990 => Loss: 6.75976281791394839615\n",
      "Iteration 8991 => Loss: 6.75975451933877291566\n",
      "Iteration 8992 => Loss: 6.75974622188183271732\n",
      "Iteration 8993 => Loss: 6.75973792554297681079\n",
      "Iteration 8994 => Loss: 6.75972963032205420575\n",
      "Iteration 8995 => Loss: 6.75972133621890503008\n",
      "Iteration 8996 => Loss: 6.75971304323339339248\n",
      "Iteration 8997 => Loss: 6.75970475136536386174\n",
      "Iteration 8998 => Loss: 6.75969646061465745390\n",
      "Iteration 8999 => Loss: 6.75968817098114094222\n",
      "Iteration 9000 => Loss: 6.75967988246464912550\n",
      "Iteration 9001 => Loss: 6.75967159506502746069\n",
      "Iteration 9002 => Loss: 6.75966330878214005651\n",
      "Iteration 9003 => Loss: 6.75965502361583059354\n",
      "Iteration 9004 => Loss: 6.75964673956594275239\n",
      "Iteration 9005 => Loss: 6.75963845663233975358\n",
      "Iteration 9006 => Loss: 6.75963017481485284321\n",
      "Iteration 9007 => Loss: 6.75962189411334968270\n",
      "Iteration 9008 => Loss: 6.75961361452766684721\n",
      "Iteration 9009 => Loss: 6.75960533605766666909\n",
      "Iteration 9010 => Loss: 6.75959705870318039445\n",
      "Iteration 9011 => Loss: 6.75958878246407390833\n",
      "Iteration 9012 => Loss: 6.75958050734019177952\n",
      "Iteration 9013 => Loss: 6.75957223333138124133\n",
      "Iteration 9014 => Loss: 6.75956396043749574432\n",
      "Iteration 9015 => Loss: 6.75955568865837896908\n",
      "Iteration 9016 => Loss: 6.75954741799389235979\n",
      "Iteration 9017 => Loss: 6.75953914844387515615\n",
      "Iteration 9018 => Loss: 6.75953088000817459147\n",
      "Iteration 9019 => Loss: 6.75952261268665566263\n",
      "Iteration 9020 => Loss: 6.75951434647915139209\n",
      "Iteration 9021 => Loss: 6.75950608138552588855\n",
      "Iteration 9022 => Loss: 6.75949781740561839172\n",
      "Iteration 9023 => Loss: 6.75948955453928590487\n",
      "Iteration 9024 => Loss: 6.75948129278636944406\n",
      "Iteration 9025 => Loss: 6.75947303214673045346\n",
      "Iteration 9026 => Loss: 6.75946477262021083732\n",
      "Iteration 9027 => Loss: 6.75945651420666582254\n",
      "Iteration 9028 => Loss: 6.75944825690593642520\n",
      "Iteration 9029 => Loss: 6.75944000071788231310\n",
      "Iteration 9030 => Loss: 6.75943174564235071955\n",
      "Iteration 9031 => Loss: 6.75942349167919065422\n",
      "Iteration 9032 => Loss: 6.75941523882825290315\n",
      "Iteration 9033 => Loss: 6.75940698708938825234\n",
      "Iteration 9034 => Loss: 6.75939873646244748784\n",
      "Iteration 9035 => Loss: 6.75939048694728228384\n",
      "Iteration 9036 => Loss: 6.75938223854372655097\n",
      "Iteration 9037 => Loss: 6.75937399125165860880\n",
      "Iteration 9038 => Loss: 6.75936574507089904529\n",
      "Iteration 9039 => Loss: 6.75935750000132884452\n",
      "Iteration 9040 => Loss: 6.75934925604277037081\n",
      "Iteration 9041 => Loss: 6.75934101319509661465\n",
      "Iteration 9042 => Loss: 6.75933277145814148668\n",
      "Iteration 9043 => Loss: 6.75932453083175843744\n",
      "Iteration 9044 => Loss: 6.75931629131580091752\n",
      "Iteration 9045 => Loss: 6.75930805291012504199\n",
      "Iteration 9046 => Loss: 6.75929981561457182693\n",
      "Iteration 9047 => Loss: 6.75929157942899117018\n",
      "Iteration 9048 => Loss: 6.75928334435324007501\n",
      "Iteration 9049 => Loss: 6.75927511038716577474\n",
      "Iteration 9050 => Loss: 6.75926687753062083175\n",
      "Iteration 9051 => Loss: 6.75925864578345247935\n",
      "Iteration 9052 => Loss: 6.75925041514550795085\n",
      "Iteration 9053 => Loss: 6.75924218561665224314\n",
      "Iteration 9054 => Loss: 6.75923395719672104320\n",
      "Iteration 9055 => Loss: 6.75922572988556780160\n",
      "Iteration 9056 => Loss: 6.75921750368304774526\n",
      "Iteration 9057 => Loss: 6.75920927858900544294\n",
      "Iteration 9058 => Loss: 6.75920105460330145064\n",
      "Iteration 9059 => Loss: 6.75919283172577589625\n",
      "Iteration 9060 => Loss: 6.75918460995628755938\n",
      "Iteration 9061 => Loss: 6.75917638929467923248\n",
      "Iteration 9062 => Loss: 6.75916816974081235969\n",
      "Iteration 9063 => Loss: 6.75915995129452351620\n",
      "Iteration 9064 => Loss: 6.75915173395567325798\n",
      "Iteration 9065 => Loss: 6.75914351772411059471\n",
      "Iteration 9066 => Loss: 6.75913530259968720060\n",
      "Iteration 9067 => Loss: 6.75912708858225030895\n",
      "Iteration 9068 => Loss: 6.75911887567165248214\n",
      "Iteration 9069 => Loss: 6.75911066386774805892\n",
      "Iteration 9070 => Loss: 6.75910245317038338442\n",
      "Iteration 9071 => Loss: 6.75909424357941279737\n",
      "Iteration 9072 => Loss: 6.75908603509467820203\n",
      "Iteration 9073 => Loss: 6.75907782771604015437\n",
      "Iteration 9074 => Loss: 6.75906962144334677589\n",
      "Iteration 9075 => Loss: 6.75906141627645418168\n",
      "Iteration 9076 => Loss: 6.75905321221520782871\n",
      "Iteration 9077 => Loss: 6.75904500925945761480\n",
      "Iteration 9078 => Loss: 6.75903680740905521418\n",
      "Iteration 9079 => Loss: 6.75902860666384963650\n",
      "Iteration 9080 => Loss: 6.75902040702370054959\n",
      "Iteration 9081 => Loss: 6.75901220848845163403\n",
      "Iteration 9082 => Loss: 6.75900401105795811674\n",
      "Iteration 9083 => Loss: 6.75899581473206367832\n",
      "Iteration 9084 => Loss: 6.75898761951063153930\n",
      "Iteration 9085 => Loss: 6.75897942539350271574\n",
      "Iteration 9086 => Loss: 6.75897123238053154637\n",
      "Iteration 9087 => Loss: 6.75896304047156970540\n",
      "Iteration 9088 => Loss: 6.75895484966646886704\n",
      "Iteration 9089 => Loss: 6.75894665996508425820\n",
      "Iteration 9090 => Loss: 6.75893847136725689495\n",
      "Iteration 9091 => Loss: 6.75893028387284200420\n",
      "Iteration 9092 => Loss: 6.75892209748169214834\n",
      "Iteration 9093 => Loss: 6.75891391219366255427\n",
      "Iteration 9094 => Loss: 6.75890572800860489622\n",
      "Iteration 9095 => Loss: 6.75889754492635930205\n",
      "Iteration 9096 => Loss: 6.75888936294678721595\n",
      "Iteration 9097 => Loss: 6.75888118206974031210\n",
      "Iteration 9098 => Loss: 6.75887300229506760019\n",
      "Iteration 9099 => Loss: 6.75886482362261808987\n",
      "Iteration 9100 => Loss: 6.75885664605223990264\n",
      "Iteration 9101 => Loss: 6.75884846958379537085\n",
      "Iteration 9102 => Loss: 6.75884029421713083963\n",
      "Iteration 9103 => Loss: 6.75883211995209709499\n",
      "Iteration 9104 => Loss: 6.75882394678854492298\n",
      "Iteration 9105 => Loss: 6.75881577472633221504\n",
      "Iteration 9106 => Loss: 6.75880760376530087541\n",
      "Iteration 9107 => Loss: 6.75879943390531057190\n",
      "Iteration 9108 => Loss: 6.75879126514620320876\n",
      "Iteration 9109 => Loss: 6.75878309748784023014\n",
      "Iteration 9110 => Loss: 6.75877493093006798119\n",
      "Iteration 9111 => Loss: 6.75876676547274524154\n",
      "Iteration 9112 => Loss: 6.75875860111571835631\n",
      "Iteration 9113 => Loss: 6.75875043785883544700\n",
      "Iteration 9114 => Loss: 6.75874227570194552328\n",
      "Iteration 9115 => Loss: 6.75873411464491802292\n",
      "Iteration 9116 => Loss: 6.75872595468759040926\n",
      "Iteration 9117 => Loss: 6.75871779582981613288\n",
      "Iteration 9118 => Loss: 6.75870963807145397340\n",
      "Iteration 9119 => Loss: 6.75870148141234583505\n",
      "Iteration 9120 => Loss: 6.75869332585234605659\n",
      "Iteration 9121 => Loss: 6.75868517139131164129\n",
      "Iteration 9122 => Loss: 6.75867701802908982245\n",
      "Iteration 9123 => Loss: 6.75866886576553138610\n",
      "Iteration 9124 => Loss: 6.75866071460049955277\n",
      "Iteration 9125 => Loss: 6.75865256453383267399\n",
      "Iteration 9126 => Loss: 6.75864441556538508848\n",
      "Iteration 9127 => Loss: 6.75863626769501824043\n",
      "Iteration 9128 => Loss: 6.75862812092257225771\n",
      "Iteration 9129 => Loss: 6.75861997524790680814\n",
      "Iteration 9130 => Loss: 6.75861183067086912502\n",
      "Iteration 9131 => Loss: 6.75860368719131532345\n",
      "Iteration 9132 => Loss: 6.75859554480909974217\n",
      "Iteration 9133 => Loss: 6.75858740352406872631\n",
      "Iteration 9134 => Loss: 6.75857926333607395009\n",
      "Iteration 9135 => Loss: 6.75857112424497152858\n",
      "Iteration 9136 => Loss: 6.75856298625061580054\n",
      "Iteration 9137 => Loss: 6.75855484935285044656\n",
      "Iteration 9138 => Loss: 6.75854671355153424628\n",
      "Iteration 9139 => Loss: 6.75853857884651443300\n",
      "Iteration 9140 => Loss: 6.75853044523765156271\n",
      "Iteration 9141 => Loss: 6.75852231272479286872\n",
      "Iteration 9142 => Loss: 6.75851418130779446614\n",
      "Iteration 9143 => Loss: 6.75850605098649293012\n",
      "Iteration 9144 => Loss: 6.75849792176076569206\n",
      "Iteration 9145 => Loss: 6.75848979363044488622\n",
      "Iteration 9146 => Loss: 6.75848166659539728585\n",
      "Iteration 9147 => Loss: 6.75847354065546035429\n",
      "Iteration 9148 => Loss: 6.75846541581050086478\n",
      "Iteration 9149 => Loss: 6.75845729206035983339\n",
      "Iteration 9150 => Loss: 6.75844916940489426338\n",
      "Iteration 9151 => Loss: 6.75844104784396115804\n",
      "Iteration 9152 => Loss: 6.75843292737740508613\n",
      "Iteration 9153 => Loss: 6.75842480800508926819\n",
      "Iteration 9154 => Loss: 6.75841668972685472028\n",
      "Iteration 9155 => Loss: 6.75840857254255400477\n",
      "Iteration 9156 => Loss: 6.75840045645205300673\n",
      "Iteration 9157 => Loss: 6.75839234145519540675\n",
      "Iteration 9158 => Loss: 6.75838422755183287904\n",
      "Iteration 9159 => Loss: 6.75837611474181354509\n",
      "Iteration 9160 => Loss: 6.75836800302500328996\n",
      "Iteration 9161 => Loss: 6.75835989240124135335\n",
      "Iteration 9162 => Loss: 6.75835178287038740308\n",
      "Iteration 9163 => Loss: 6.75834367443229666605\n",
      "Iteration 9164 => Loss: 6.75833556708681992831\n",
      "Iteration 9165 => Loss: 6.75832746083380264679\n",
      "Iteration 9166 => Loss: 6.75831935567311159474\n",
      "Iteration 9167 => Loss: 6.75831125160458690004\n",
      "Iteration 9168 => Loss: 6.75830314862808645415\n",
      "Iteration 9169 => Loss: 6.75829504674346015491\n",
      "Iteration 9170 => Loss: 6.75828694595056678196\n",
      "Iteration 9171 => Loss: 6.75827884624925445678\n",
      "Iteration 9172 => Loss: 6.75827074763937396540\n",
      "Iteration 9173 => Loss: 6.75826265012078941652\n",
      "Iteration 9174 => Loss: 6.75825455369334626710\n",
      "Iteration 9175 => Loss: 6.75824645835689441498\n",
      "Iteration 9176 => Loss: 6.75823836411128731072\n",
      "Iteration 9177 => Loss: 6.75823027095638728667\n",
      "Iteration 9178 => Loss: 6.75822217889202470076\n",
      "Iteration 9179 => Loss: 6.75821408791808586614\n",
      "Iteration 9180 => Loss: 6.75820599803439581166\n",
      "Iteration 9181 => Loss: 6.75819790924082131056\n",
      "Iteration 9182 => Loss: 6.75818982153721936612\n",
      "Iteration 9183 => Loss: 6.75818173492342921804\n",
      "Iteration 9184 => Loss: 6.75817364939930786960\n",
      "Iteration 9185 => Loss: 6.75816556496471765314\n",
      "Iteration 9186 => Loss: 6.75815748161950047290\n",
      "Iteration 9187 => Loss: 6.75814939936351866123\n",
      "Iteration 9188 => Loss: 6.75814131819662211598\n",
      "Iteration 9189 => Loss: 6.75813323811865984680\n",
      "Iteration 9190 => Loss: 6.75812515912948885699\n",
      "Iteration 9191 => Loss: 6.75811708122897059070\n",
      "Iteration 9192 => Loss: 6.75810900441693895857\n",
      "Iteration 9193 => Loss: 6.75810092869326428655\n",
      "Iteration 9194 => Loss: 6.75809285405779647249\n",
      "Iteration 9195 => Loss: 6.75808478051037297973\n",
      "Iteration 9196 => Loss: 6.75807670805087834509\n",
      "Iteration 9197 => Loss: 6.75806863667913670923\n",
      "Iteration 9198 => Loss: 6.75806056639502017447\n",
      "Iteration 9199 => Loss: 6.75805249719836975686\n",
      "Iteration 9200 => Loss: 6.75804442908904245968\n",
      "Iteration 9201 => Loss: 6.75803636206689350985\n",
      "Iteration 9202 => Loss: 6.75802829613177902246\n",
      "Iteration 9203 => Loss: 6.75802023128355688897\n",
      "Iteration 9204 => Loss: 6.75801216752206812544\n",
      "Iteration 9205 => Loss: 6.75800410484716884696\n",
      "Iteration 9206 => Loss: 6.75799604325871783317\n",
      "Iteration 9207 => Loss: 6.75798798275656320556\n",
      "Iteration 9208 => Loss: 6.75797992334057262553\n",
      "Iteration 9209 => Loss: 6.75797186501057822738\n",
      "Iteration 9210 => Loss: 6.75796380776644856070\n",
      "Iteration 9211 => Loss: 6.75795575160802819425\n",
      "Iteration 9212 => Loss: 6.75794769653518212493\n",
      "Iteration 9213 => Loss: 6.75793964254775492151\n",
      "Iteration 9214 => Loss: 6.75793158964560358726\n",
      "Iteration 9215 => Loss: 6.75792353782858512545\n",
      "Iteration 9216 => Loss: 6.75791548709654499305\n",
      "Iteration 9217 => Loss: 6.75790743744934374604\n",
      "Iteration 9218 => Loss: 6.75789938888683217044\n",
      "Iteration 9219 => Loss: 6.75789134140886904589\n",
      "Iteration 9220 => Loss: 6.75788329501529538845\n",
      "Iteration 9221 => Loss: 6.75787524970597974772\n",
      "Iteration 9222 => Loss: 6.75786720548076491610\n",
      "Iteration 9223 => Loss: 6.75785916233951944321\n",
      "Iteration 9224 => Loss: 6.75785112028208612145\n",
      "Iteration 9225 => Loss: 6.75784307930831396050\n",
      "Iteration 9226 => Loss: 6.75783503941806529269\n",
      "Iteration 9227 => Loss: 6.75782700061119623314\n",
      "Iteration 9228 => Loss: 6.75781896288754957425\n",
      "Iteration 9229 => Loss: 6.75781092624699475380\n",
      "Iteration 9230 => Loss: 6.75780289068937989327\n",
      "Iteration 9231 => Loss: 6.75779485621454600874\n",
      "Iteration 9232 => Loss: 6.75778682282236342616\n",
      "Iteration 9233 => Loss: 6.75777879051268648425\n",
      "Iteration 9234 => Loss: 6.75777075928536241634\n",
      "Iteration 9235 => Loss: 6.75776272914024467298\n",
      "Iteration 9236 => Loss: 6.75775470007718581655\n",
      "Iteration 9237 => Loss: 6.75774667209604551488\n",
      "Iteration 9238 => Loss: 6.75773864519668254758\n",
      "Iteration 9239 => Loss: 6.75773061937893881890\n",
      "Iteration 9240 => Loss: 6.75772259464268110207\n",
      "Iteration 9241 => Loss: 6.75771457098774419592\n",
      "Iteration 9242 => Loss: 6.75770654841400286728\n",
      "Iteration 9243 => Loss: 6.75769852692130523764\n",
      "Iteration 9244 => Loss: 6.75769050650950298120\n",
      "Iteration 9245 => Loss: 6.75768248717845754214\n",
      "Iteration 9246 => Loss: 6.75767446892801348923\n",
      "Iteration 9247 => Loss: 6.75766645175802427303\n",
      "Iteration 9248 => Loss: 6.75765843566835311407\n",
      "Iteration 9249 => Loss: 6.75765042065885257472\n",
      "Iteration 9250 => Loss: 6.75764240672937610555\n",
      "Iteration 9251 => Loss: 6.75763439387977804529\n",
      "Iteration 9252 => Loss: 6.75762638210990118637\n",
      "Iteration 9253 => Loss: 6.75761837141961940745\n",
      "Iteration 9254 => Loss: 6.75761036180877905366\n",
      "Iteration 9255 => Loss: 6.75760235327723268739\n",
      "Iteration 9256 => Loss: 6.75759434582483820009\n",
      "Iteration 9257 => Loss: 6.75758633945144460142\n",
      "Iteration 9258 => Loss: 6.75757833415691511192\n",
      "Iteration 9259 => Loss: 6.75757032994110051760\n",
      "Iteration 9260 => Loss: 6.75756232680385338085\n",
      "Iteration 9261 => Loss: 6.75755432474502182316\n",
      "Iteration 9262 => Loss: 6.75754632376447705866\n",
      "Iteration 9263 => Loss: 6.75753832386206543248\n",
      "Iteration 9264 => Loss: 6.75753032503763328975\n",
      "Iteration 9265 => Loss: 6.75752232729105184461\n",
      "Iteration 9266 => Loss: 6.75751433062215944858\n",
      "Iteration 9267 => Loss: 6.75750633503082820397\n",
      "Iteration 9268 => Loss: 6.75749834051689912684\n",
      "Iteration 9269 => Loss: 6.75749034708023188500\n",
      "Iteration 9270 => Loss: 6.75748235472068170537\n",
      "Iteration 9271 => Loss: 6.75747436343810203851\n",
      "Iteration 9272 => Loss: 6.75746637323234100592\n",
      "Iteration 9273 => Loss: 6.75745838410327070989\n",
      "Iteration 9274 => Loss: 6.75745039605073483102\n",
      "Iteration 9275 => Loss: 6.75744240907458326717\n",
      "Iteration 9276 => Loss: 6.75743442317468101521\n",
      "Iteration 9277 => Loss: 6.75742643835088419024\n",
      "Iteration 9278 => Loss: 6.75741845460303824922\n",
      "Iteration 9279 => Loss: 6.75741047193100374813\n",
      "Iteration 9280 => Loss: 6.75740249033463058481\n",
      "Iteration 9281 => Loss: 6.75739450981378375616\n",
      "Iteration 9282 => Loss: 6.75738653036830960730\n",
      "Iteration 9283 => Loss: 6.75737855199806780604\n",
      "Iteration 9284 => Loss: 6.75737057470291713202\n",
      "Iteration 9285 => Loss: 6.75736259848270393036\n",
      "Iteration 9286 => Loss: 6.75735462333728076345\n",
      "Iteration 9287 => Loss: 6.75734664926651884542\n",
      "Iteration 9288 => Loss: 6.75733867627025386327\n",
      "Iteration 9289 => Loss: 6.75733070434836147200\n",
      "Iteration 9290 => Loss: 6.75732273350067647044\n",
      "Iteration 9291 => Loss: 6.75731476372707007272\n",
      "Iteration 9292 => Loss: 6.75730679502739040032\n",
      "Iteration 9293 => Loss: 6.75729882740148823927\n",
      "Iteration 9294 => Loss: 6.75729086084922858646\n",
      "Iteration 9295 => Loss: 6.75728289537045778701\n",
      "Iteration 9296 => Loss: 6.75727493096504172598\n",
      "Iteration 9297 => Loss: 6.75726696763283118941\n",
      "Iteration 9298 => Loss: 6.75725900537367341059\n",
      "Iteration 9299 => Loss: 6.75725104418743782730\n",
      "Iteration 9300 => Loss: 6.75724308407396900833\n",
      "Iteration 9301 => Loss: 6.75723512503312484512\n",
      "Iteration 9302 => Loss: 6.75722716706476500548\n",
      "Iteration 9303 => Loss: 6.75721921016874293997\n",
      "Iteration 9304 => Loss: 6.75721125434491387551\n",
      "Iteration 9305 => Loss: 6.75720329959312859813\n",
      "Iteration 9306 => Loss: 6.75719534591324233475\n",
      "Iteration 9307 => Loss: 6.75718739330512541130\n",
      "Iteration 9308 => Loss: 6.75717944176861795569\n",
      "Iteration 9309 => Loss: 6.75717149130357697118\n",
      "Iteration 9310 => Loss: 6.75716354190987456008\n",
      "Iteration 9311 => Loss: 6.75715559358734019213\n",
      "Iteration 9312 => Loss: 6.75714764633584596965\n",
      "Iteration 9313 => Loss: 6.75713970015525333679\n",
      "Iteration 9314 => Loss: 6.75713175504539709237\n",
      "Iteration 9315 => Loss: 6.75712381100615022689\n",
      "Iteration 9316 => Loss: 6.75711586803736885543\n",
      "Iteration 9317 => Loss: 6.75710792613889132951\n",
      "Iteration 9318 => Loss: 6.75709998531059330418\n",
      "Iteration 9319 => Loss: 6.75709204555232112455\n",
      "Iteration 9320 => Loss: 6.75708410686392824118\n",
      "Iteration 9321 => Loss: 6.75707616924527698643\n",
      "Iteration 9322 => Loss: 6.75706823269622347539\n",
      "Iteration 9323 => Loss: 6.75706029721662027043\n",
      "Iteration 9324 => Loss: 6.75705236280631815760\n",
      "Iteration 9325 => Loss: 6.75704442946517946922\n",
      "Iteration 9326 => Loss: 6.75703649719306653765\n",
      "Iteration 9327 => Loss: 6.75702856598981771441\n",
      "Iteration 9328 => Loss: 6.75702063585531043088\n",
      "Iteration 9329 => Loss: 6.75701270678938037406\n",
      "Iteration 9330 => Loss: 6.75700477879189076447\n",
      "Iteration 9331 => Loss: 6.75699685186270659898\n",
      "Iteration 9332 => Loss: 6.75698892600167067002\n",
      "Iteration 9333 => Loss: 6.75698100120865419171\n",
      "Iteration 9334 => Loss: 6.75697307748349906831\n",
      "Iteration 9335 => Loss: 6.75696515482605875036\n",
      "Iteration 9336 => Loss: 6.75695723323620622836\n",
      "Iteration 9337 => Loss: 6.75694931271378695925\n",
      "Iteration 9338 => Loss: 6.75694139325866238721\n",
      "Iteration 9339 => Loss: 6.75693347487067708101\n",
      "Iteration 9340 => Loss: 6.75692555754970491932\n",
      "Iteration 9341 => Loss: 6.75691764129557892460\n",
      "Iteration 9342 => Loss: 6.75690972610818096911\n",
      "Iteration 9343 => Loss: 6.75690181198735118073\n",
      "Iteration 9344 => Loss: 6.75689389893294301004\n",
      "Iteration 9345 => Loss: 6.75688598694483122387\n",
      "Iteration 9346 => Loss: 6.75687807602286039099\n",
      "Iteration 9347 => Loss: 6.75687016616687508019\n",
      "Iteration 9348 => Loss: 6.75686225737674917013\n",
      "Iteration 9349 => Loss: 6.75685434965233433502\n",
      "Iteration 9350 => Loss: 6.75684644299348757812\n",
      "Iteration 9351 => Loss: 6.75683853740006146182\n",
      "Iteration 9352 => Loss: 6.75683063287191298940\n",
      "Iteration 9353 => Loss: 6.75682272940890182866\n",
      "Iteration 9354 => Loss: 6.75681482701088498288\n",
      "Iteration 9355 => Loss: 6.75680692567771323809\n",
      "Iteration 9356 => Loss: 6.75679902540925247934\n",
      "Iteration 9357 => Loss: 6.75679112620534727540\n",
      "Iteration 9358 => Loss: 6.75678322806586262317\n",
      "Iteration 9359 => Loss: 6.75677533099065552591\n",
      "Iteration 9360 => Loss: 6.75676743497957765783\n",
      "Iteration 9361 => Loss: 6.75675954003248424584\n",
      "Iteration 9362 => Loss: 6.75675164614923851047\n",
      "Iteration 9363 => Loss: 6.75674375332969745500\n",
      "Iteration 9364 => Loss: 6.75673586157371452998\n",
      "Iteration 9365 => Loss: 6.75672797088114052144\n",
      "Iteration 9366 => Loss: 6.75672008125184486715\n",
      "Iteration 9367 => Loss: 6.75671219268567391225\n",
      "Iteration 9368 => Loss: 6.75670430518248466001\n",
      "Iteration 9369 => Loss: 6.75669641874214033095\n",
      "Iteration 9370 => Loss: 6.75668853336449615199\n",
      "Iteration 9371 => Loss: 6.75668064904940290916\n",
      "Iteration 9372 => Loss: 6.75667276579672382297\n",
      "Iteration 9373 => Loss: 6.75666488360631678489\n",
      "Iteration 9374 => Loss: 6.75665700247803435730\n",
      "Iteration 9375 => Loss: 6.75664912241172732621\n",
      "Iteration 9376 => Loss: 6.75664124340726779394\n",
      "Iteration 9377 => Loss: 6.75663336546449855291\n",
      "Iteration 9378 => Loss: 6.75662548858329170542\n",
      "Iteration 9379 => Loss: 6.75661761276348293848\n",
      "Iteration 9380 => Loss: 6.75660973800495234798\n",
      "Iteration 9381 => Loss: 6.75660186430754272635\n",
      "Iteration 9382 => Loss: 6.75659399167111640594\n",
      "Iteration 9383 => Loss: 6.75658612009552594913\n",
      "Iteration 9384 => Loss: 6.75657824958063102372\n",
      "Iteration 9385 => Loss: 6.75657038012628330392\n",
      "Iteration 9386 => Loss: 6.75656251173235311569\n",
      "Iteration 9387 => Loss: 6.75655464439868502780\n",
      "Iteration 9388 => Loss: 6.75654677812514581348\n",
      "Iteration 9389 => Loss: 6.75653891291158448240\n",
      "Iteration 9390 => Loss: 6.75653104875785981420\n",
      "Iteration 9391 => Loss: 6.75652318566383858212\n",
      "Iteration 9392 => Loss: 6.75651532362936091403\n",
      "Iteration 9393 => Loss: 6.75650746265429447135\n",
      "Iteration 9394 => Loss: 6.75649960273849181647\n",
      "Iteration 9395 => Loss: 6.75649174388181972262\n",
      "Iteration 9396 => Loss: 6.75648388608412631129\n",
      "Iteration 9397 => Loss: 6.75647602934527391483\n",
      "Iteration 9398 => Loss: 6.75646817366511509562\n",
      "Iteration 9399 => Loss: 6.75646031904350952146\n",
      "Iteration 9400 => Loss: 6.75645246548031863654\n",
      "Iteration 9401 => Loss: 6.75644461297539411504\n",
      "Iteration 9402 => Loss: 6.75643676152859296025\n",
      "Iteration 9403 => Loss: 6.75642891113977395179\n",
      "Iteration 9404 => Loss: 6.75642106180880031019\n",
      "Iteration 9405 => Loss: 6.75641321353552015694\n",
      "Iteration 9406 => Loss: 6.75640536631979671256\n",
      "Iteration 9407 => Loss: 6.75639752016148698033\n",
      "Iteration 9408 => Loss: 6.75638967506044973987\n",
      "Iteration 9409 => Loss: 6.75638183101653755358\n",
      "Iteration 9410 => Loss: 6.75637398802961097743\n",
      "Iteration 9411 => Loss: 6.75636614609953323196\n",
      "Iteration 9412 => Loss: 6.75635830522614710958\n",
      "Iteration 9413 => Loss: 6.75635046540932737713\n",
      "Iteration 9414 => Loss: 6.75634262664891682704\n",
      "Iteration 9415 => Loss: 6.75633478894478578525\n",
      "Iteration 9416 => Loss: 6.75632695229678326143\n",
      "Iteration 9417 => Loss: 6.75631911670476981158\n",
      "Iteration 9418 => Loss: 6.75631128216859977442\n",
      "Iteration 9419 => Loss: 6.75630344868814081138\n",
      "Iteration 9420 => Loss: 6.75629561626323926760\n",
      "Iteration 9421 => Loss: 6.75628778489375214633\n",
      "Iteration 9422 => Loss: 6.75627995457955421443\n",
      "Iteration 9423 => Loss: 6.75627212532049092886\n",
      "Iteration 9424 => Loss: 6.75626429711641396381\n",
      "Iteration 9425 => Loss: 6.75625646996718387527\n",
      "Iteration 9426 => Loss: 6.75624864387267454191\n",
      "Iteration 9427 => Loss: 6.75624081883272342708\n",
      "Iteration 9428 => Loss: 6.75623299484720618580\n",
      "Iteration 9429 => Loss: 6.75622517191596383412\n",
      "Iteration 9430 => Loss: 6.75621735003886314530\n",
      "Iteration 9431 => Loss: 6.75620952921576378714\n",
      "Iteration 9432 => Loss: 6.75620170944651921019\n",
      "Iteration 9433 => Loss: 6.75619389073098908227\n",
      "Iteration 9434 => Loss: 6.75618607306903307119\n",
      "Iteration 9435 => Loss: 6.75617825646050107480\n",
      "Iteration 9436 => Loss: 6.75617044090526341904\n",
      "Iteration 9437 => Loss: 6.75616262640317088994\n",
      "Iteration 9438 => Loss: 6.75615481295408493168\n",
      "Iteration 9439 => Loss: 6.75614700055786165933\n",
      "Iteration 9440 => Loss: 6.75613918921436429343\n",
      "Iteration 9441 => Loss: 6.75613137892343562640\n",
      "Iteration 9442 => Loss: 6.75612356968494864873\n",
      "Iteration 9443 => Loss: 6.75611576149875769914\n",
      "Iteration 9444 => Loss: 6.75610795436472155728\n",
      "Iteration 9445 => Loss: 6.75610014828269722642\n",
      "Iteration 9446 => Loss: 6.75609234325253815712\n",
      "Iteration 9447 => Loss: 6.75608453927411289897\n",
      "Iteration 9448 => Loss: 6.75607673634727046164\n",
      "Iteration 9449 => Loss: 6.75606893447187406565\n",
      "Iteration 9450 => Loss: 6.75606113364778604335\n",
      "Iteration 9451 => Loss: 6.75605333387485629260\n",
      "Iteration 9452 => Loss: 6.75604553515294625754\n",
      "Iteration 9453 => Loss: 6.75603773748191738235\n",
      "Iteration 9454 => Loss: 6.75602994086162134124\n",
      "Iteration 9455 => Loss: 6.75602214529192313108\n",
      "Iteration 9456 => Loss: 6.75601435077267620244\n",
      "Iteration 9457 => Loss: 6.75600655730374466401\n",
      "Iteration 9458 => Loss: 6.75599876488498107818\n",
      "Iteration 9459 => Loss: 6.75599097351624156005\n",
      "Iteration 9460 => Loss: 6.75598318319739554738\n",
      "Iteration 9461 => Loss: 6.75597539392829471439\n",
      "Iteration 9462 => Loss: 6.75596760570879872887\n",
      "Iteration 9463 => Loss: 6.75595981853876370593\n",
      "Iteration 9464 => Loss: 6.75595203241805020156\n",
      "Iteration 9465 => Loss: 6.75594424734652054809\n",
      "Iteration 9466 => Loss: 6.75593646332403441335\n",
      "Iteration 9467 => Loss: 6.75592868035044080699\n",
      "Iteration 9468 => Loss: 6.75592089842560294954\n",
      "Iteration 9469 => Loss: 6.75591311754937695611\n",
      "Iteration 9470 => Loss: 6.75590533772162604720\n",
      "Iteration 9471 => Loss: 6.75589755894220811427\n",
      "Iteration 9472 => Loss: 6.75588978121098993057\n",
      "Iteration 9473 => Loss: 6.75588200452781340033\n",
      "Iteration 9474 => Loss: 6.75587422889254085590\n",
      "Iteration 9475 => Loss: 6.75586645430504084686\n",
      "Iteration 9476 => Loss: 6.75585868076516860015\n",
      "Iteration 9477 => Loss: 6.75585090827278289538\n",
      "Iteration 9478 => Loss: 6.75584313682773451859\n",
      "Iteration 9479 => Loss: 6.75583536642989468390\n",
      "Iteration 9480 => Loss: 6.75582759707911417735\n",
      "Iteration 9481 => Loss: 6.75581982877525799580\n",
      "Iteration 9482 => Loss: 6.75581206151817337258\n",
      "Iteration 9483 => Loss: 6.75580429530773596269\n",
      "Iteration 9484 => Loss: 6.75579653014379388765\n",
      "Iteration 9485 => Loss: 6.75578876602620148617\n",
      "Iteration 9486 => Loss: 6.75578100295483263693\n",
      "Iteration 9487 => Loss: 6.75577324092953634960\n",
      "Iteration 9488 => Loss: 6.75576547995016607473\n",
      "Iteration 9489 => Loss: 6.75575772001659480281\n",
      "Iteration 9490 => Loss: 6.75574996112867509623\n",
      "Iteration 9491 => Loss: 6.75574220328627106369\n",
      "Iteration 9492 => Loss: 6.75573444648923260303\n",
      "Iteration 9493 => Loss: 6.75572669073741760570\n",
      "Iteration 9494 => Loss: 6.75571893603069728584\n",
      "Iteration 9495 => Loss: 6.75571118236892864672\n",
      "Iteration 9496 => Loss: 6.75570342975195803348\n",
      "Iteration 9497 => Loss: 6.75569567817965754841\n",
      "Iteration 9498 => Loss: 6.75568792765187886573\n",
      "Iteration 9499 => Loss: 6.75568017816848698232\n",
      "Iteration 9500 => Loss: 6.75567242972933801326\n",
      "Iteration 9501 => Loss: 6.75566468233428540913\n",
      "Iteration 9502 => Loss: 6.75565693598320216040\n",
      "Iteration 9503 => Loss: 6.75564919067593816493\n",
      "Iteration 9504 => Loss: 6.75564144641235664324\n",
      "Iteration 9505 => Loss: 6.75563370319231459860\n",
      "Iteration 9506 => Loss: 6.75562596101567525153\n",
      "Iteration 9507 => Loss: 6.75561821988229027625\n",
      "Iteration 9508 => Loss: 6.75561047979201934055\n",
      "Iteration 9509 => Loss: 6.75560274074473277039\n",
      "Iteration 9510 => Loss: 6.75559500274028668088\n",
      "Iteration 9511 => Loss: 6.75558726577853185802\n",
      "Iteration 9512 => Loss: 6.75557952985933329870\n",
      "Iteration 9513 => Loss: 6.75557179498255244710\n",
      "Iteration 9514 => Loss: 6.75556406114804364194\n",
      "Iteration 9515 => Loss: 6.75555632835567010375\n",
      "Iteration 9516 => Loss: 6.75554859660529327670\n",
      "Iteration 9517 => Loss: 6.75554086589677371677\n",
      "Iteration 9518 => Loss: 6.75553313622996487453\n",
      "Iteration 9519 => Loss: 6.75552540760472464143\n",
      "Iteration 9520 => Loss: 6.75551768002091712617\n",
      "Iteration 9521 => Loss: 6.75550995347840377292\n",
      "Iteration 9522 => Loss: 6.75550222797704957856\n",
      "Iteration 9523 => Loss: 6.75549450351670177639\n",
      "Iteration 9524 => Loss: 6.75548678009722713966\n",
      "Iteration 9525 => Loss: 6.75547905771848355982\n",
      "Iteration 9526 => Loss: 6.75547133638032715197\n",
      "Iteration 9527 => Loss: 6.75546361608262824205\n",
      "Iteration 9528 => Loss: 6.75545589682523761610\n",
      "Iteration 9529 => Loss: 6.75544817860801227738\n",
      "Iteration 9530 => Loss: 6.75544046143081988731\n",
      "Iteration 9531 => Loss: 6.75543274529352189006\n",
      "Iteration 9532 => Loss: 6.75542503019597440073\n",
      "Iteration 9533 => Loss: 6.75541731613802820533\n",
      "Iteration 9534 => Loss: 6.75540960311956162343\n",
      "Iteration 9535 => Loss: 6.75540189114042100016\n",
      "Iteration 9536 => Loss: 6.75539418020047133240\n",
      "Iteration 9537 => Loss: 6.75538647029956962342\n",
      "Iteration 9538 => Loss: 6.75537876143757909375\n",
      "Iteration 9539 => Loss: 6.75537105361435230577\n",
      "Iteration 9540 => Loss: 6.75536334682976224997\n",
      "Iteration 9541 => Loss: 6.75535564108366592961\n",
      "Iteration 9542 => Loss: 6.75534793637591235438\n",
      "Iteration 9543 => Loss: 6.75534023270636385661\n",
      "Iteration 9544 => Loss: 6.75533253007489253861\n",
      "Iteration 9545 => Loss: 6.75532482848135540365\n",
      "Iteration 9546 => Loss: 6.75531712792560234959\n",
      "Iteration 9547 => Loss: 6.75530942840750281420\n",
      "Iteration 9548 => Loss: 6.75530172992690847167\n",
      "Iteration 9549 => Loss: 6.75529403248369142432\n",
      "Iteration 9550 => Loss: 6.75528633607770689906\n",
      "Iteration 9551 => Loss: 6.75527864070881012282\n",
      "Iteration 9552 => Loss: 6.75527094637685987522\n",
      "Iteration 9553 => Loss: 6.75526325308172559403\n",
      "Iteration 9554 => Loss: 6.75525556082326605889\n",
      "Iteration 9555 => Loss: 6.75524786960133916125\n",
      "Iteration 9556 => Loss: 6.75524017941580368074\n",
      "Iteration 9557 => Loss: 6.75523249026651750881\n",
      "Iteration 9558 => Loss: 6.75522480215334564235\n",
      "Iteration 9559 => Loss: 6.75521711507614952552\n",
      "Iteration 9560 => Loss: 6.75520942903479593156\n",
      "Iteration 9561 => Loss: 6.75520174402912321199\n",
      "Iteration 9562 => Loss: 6.75519406005901057455\n",
      "Iteration 9563 => Loss: 6.75518637712431591069\n",
      "Iteration 9564 => Loss: 6.75517869522489355916\n",
      "Iteration 9565 => Loss: 6.75517101436061206954\n",
      "Iteration 9566 => Loss: 6.75516333453132400422\n",
      "Iteration 9567 => Loss: 6.75515565573689613643\n",
      "Iteration 9568 => Loss: 6.75514797797718902217\n",
      "Iteration 9569 => Loss: 6.75514030125205167110\n",
      "Iteration 9570 => Loss: 6.75513262556135529735\n",
      "Iteration 9571 => Loss: 6.75512495090497022687\n",
      "Iteration 9572 => Loss: 6.75511727728273392302\n",
      "Iteration 9573 => Loss: 6.75510960469452470534\n",
      "Iteration 9574 => Loss: 6.75510193314019691257\n",
      "Iteration 9575 => Loss: 6.75509426261960665983\n",
      "Iteration 9576 => Loss: 6.75508659313261805579\n",
      "Iteration 9577 => Loss: 6.75507892467910675549\n",
      "Iteration 9578 => Loss: 6.75507125725891111045\n",
      "Iteration 9579 => Loss: 6.75506359087189522938\n",
      "Iteration 9580 => Loss: 6.75505592551793387912\n",
      "Iteration 9581 => Loss: 6.75504826119687873387\n",
      "Iteration 9582 => Loss: 6.75504059790859123780\n",
      "Iteration 9583 => Loss: 6.75503293565293194689\n",
      "Iteration 9584 => Loss: 6.75502527442975786443\n",
      "Iteration 9585 => Loss: 6.75501761423894109271\n",
      "Iteration 9586 => Loss: 6.75500995508032886505\n",
      "Iteration 9587 => Loss: 6.75500229695379594830\n",
      "Iteration 9588 => Loss: 6.75499463985919490483\n",
      "Iteration 9589 => Loss: 6.75498698379638007339\n",
      "Iteration 9590 => Loss: 6.75497932876523332624\n",
      "Iteration 9591 => Loss: 6.75497167476559479127\n",
      "Iteration 9592 => Loss: 6.75496402179732857718\n",
      "Iteration 9593 => Loss: 6.75495636986031211535\n",
      "Iteration 9594 => Loss: 6.75494871895438553366\n",
      "Iteration 9595 => Loss: 6.75494106907942271079\n",
      "Iteration 9596 => Loss: 6.75493342023528420270\n",
      "Iteration 9597 => Loss: 6.75492577242181813091\n",
      "Iteration 9598 => Loss: 6.75491812563890814403\n",
      "Iteration 9599 => Loss: 6.75491047988639348176\n",
      "Iteration 9600 => Loss: 6.75490283516414802278\n",
      "Iteration 9601 => Loss: 6.75489519147202788218\n",
      "Iteration 9602 => Loss: 6.75488754880989805685\n",
      "Iteration 9603 => Loss: 6.75487990717761643822\n",
      "Iteration 9604 => Loss: 6.75487226657504358229\n",
      "Iteration 9605 => Loss: 6.75486462700205159138\n",
      "Iteration 9606 => Loss: 6.75485698845847970517\n",
      "Iteration 9607 => Loss: 6.75484935094420446688\n",
      "Iteration 9608 => Loss: 6.75484171445909442610\n",
      "Iteration 9609 => Loss: 6.75483407900299503979\n",
      "Iteration 9610 => Loss: 6.75482644457577041663\n",
      "Iteration 9611 => Loss: 6.75481881117728732988\n",
      "Iteration 9612 => Loss: 6.75481117880740544734\n",
      "Iteration 9613 => Loss: 6.75480354746599154225\n",
      "Iteration 9614 => Loss: 6.75479591715289817699\n",
      "Iteration 9615 => Loss: 6.75478828786798946027\n",
      "Iteration 9616 => Loss: 6.75478065961112505988\n",
      "Iteration 9617 => Loss: 6.75477303238217174908\n",
      "Iteration 9618 => Loss: 6.75476540618099541291\n",
      "Iteration 9619 => Loss: 6.75475778100743884380\n",
      "Iteration 9620 => Loss: 6.75475015686137592041\n",
      "Iteration 9621 => Loss: 6.75474253374267252781\n",
      "Iteration 9622 => Loss: 6.75473491165118122836\n",
      "Iteration 9623 => Loss: 6.75472729058676968350\n",
      "Iteration 9624 => Loss: 6.75471967054929312013\n",
      "Iteration 9625 => Loss: 6.75471205153862275239\n",
      "Iteration 9626 => Loss: 6.75470443355460847812\n",
      "Iteration 9627 => Loss: 6.75469681659712772870\n",
      "Iteration 9628 => Loss: 6.75468920066602507291\n",
      "Iteration 9629 => Loss: 6.75468158576116817216\n",
      "Iteration 9630 => Loss: 6.75467397188242646422\n",
      "Iteration 9631 => Loss: 6.75466635902965339966\n",
      "Iteration 9632 => Loss: 6.75465874720271397535\n",
      "Iteration 9633 => Loss: 6.75465113640146341822\n",
      "Iteration 9634 => Loss: 6.75464352662577116604\n",
      "Iteration 9635 => Loss: 6.75463591787550310386\n",
      "Iteration 9636 => Loss: 6.75462831015050291228\n",
      "Iteration 9637 => Loss: 6.75462070345065423993\n",
      "Iteration 9638 => Loss: 6.75461309777580520830\n",
      "Iteration 9639 => Loss: 6.75460549312582614334\n",
      "Iteration 9640 => Loss: 6.75459788950056783108\n",
      "Iteration 9641 => Loss: 6.75459028689989704475\n",
      "Iteration 9642 => Loss: 6.75458268532368055759\n",
      "Iteration 9643 => Loss: 6.75457508477177803741\n",
      "Iteration 9644 => Loss: 6.75456748524405092837\n",
      "Iteration 9645 => Loss: 6.75455988674035623376\n",
      "Iteration 9646 => Loss: 6.75455228926057404948\n",
      "Iteration 9647 => Loss: 6.75454469280453828617\n",
      "Iteration 9648 => Loss: 6.75453709737213259245\n",
      "Iteration 9649 => Loss: 6.75452950296320508983\n",
      "Iteration 9650 => Loss: 6.75452190957762699242\n",
      "Iteration 9651 => Loss: 6.75451431721526418528\n",
      "Iteration 9652 => Loss: 6.75450672587596656626\n",
      "Iteration 9653 => Loss: 6.75449913555961067857\n",
      "Iteration 9654 => Loss: 6.75449154626604375551\n",
      "Iteration 9655 => Loss: 6.75448395799513789939\n",
      "Iteration 9656 => Loss: 6.75447637074674478441\n",
      "Iteration 9657 => Loss: 6.75446878452074006560\n",
      "Iteration 9658 => Loss: 6.75446119931697630534\n",
      "Iteration 9659 => Loss: 6.75445361513533004683\n",
      "Iteration 9660 => Loss: 6.75444603197563697705\n",
      "Iteration 9661 => Loss: 6.75443844983778340918\n",
      "Iteration 9662 => Loss: 6.75443086872163078738\n",
      "Iteration 9663 => Loss: 6.75442328862702101588\n",
      "Iteration 9664 => Loss: 6.75441570955383330244\n",
      "Iteration 9665 => Loss: 6.75440813150192997938\n",
      "Iteration 9666 => Loss: 6.75440055447116627363\n",
      "Iteration 9667 => Loss: 6.75439297846141162296\n",
      "Iteration 9668 => Loss: 6.75438540347251947793\n",
      "Iteration 9669 => Loss: 6.75437782950436282903\n",
      "Iteration 9670 => Loss: 6.75437025655680223224\n",
      "Iteration 9671 => Loss: 6.75436268462969291448\n",
      "Iteration 9672 => Loss: 6.75435511372289987264\n",
      "Iteration 9673 => Loss: 6.75434754383628899177\n",
      "Iteration 9674 => Loss: 6.75433997496972171604\n",
      "Iteration 9675 => Loss: 6.75433240712305416054\n",
      "Iteration 9676 => Loss: 6.75432484029615931576\n",
      "Iteration 9677 => Loss: 6.75431727448888619136\n",
      "Iteration 9678 => Loss: 6.75430970970111932417\n",
      "Iteration 9679 => Loss: 6.75430214593270683565\n",
      "Iteration 9680 => Loss: 6.75429458318350572910\n",
      "Iteration 9681 => Loss: 6.75428702145338366591\n",
      "Iteration 9682 => Loss: 6.75427946074221186024\n",
      "Iteration 9683 => Loss: 6.75427190104984553898\n",
      "Iteration 9684 => Loss: 6.75426434237614703449\n",
      "Iteration 9685 => Loss: 6.75425678472098400817\n",
      "Iteration 9686 => Loss: 6.75424922808421079878\n",
      "Iteration 9687 => Loss: 6.75424167246569684409\n",
      "Iteration 9688 => Loss: 6.75423411786530536460\n",
      "Iteration 9689 => Loss: 6.75422656428289780450\n",
      "Iteration 9690 => Loss: 6.75421901171832939070\n",
      "Iteration 9691 => Loss: 6.75421146017147400187\n",
      "Iteration 9692 => Loss: 6.75420390964218508856\n",
      "Iteration 9693 => Loss: 6.75419636013033564126\n",
      "Iteration 9694 => Loss: 6.75418881163578355142\n",
      "Iteration 9695 => Loss: 6.75418126415839470411\n",
      "Iteration 9696 => Loss: 6.75417371769802254988\n",
      "Iteration 9697 => Loss: 6.75416617225453475015\n",
      "Iteration 9698 => Loss: 6.75415862782780340723\n",
      "Iteration 9699 => Loss: 6.75415108441768285985\n",
      "Iteration 9700 => Loss: 6.75414354202403544036\n",
      "Iteration 9701 => Loss: 6.75413600064672348111\n",
      "Iteration 9702 => Loss: 6.75412846028561197897\n",
      "Iteration 9703 => Loss: 6.75412092094057125991\n",
      "Iteration 9704 => Loss: 6.75411338261145832718\n",
      "Iteration 9705 => Loss: 6.75410584529812751953\n",
      "Iteration 9706 => Loss: 6.75409830900045893287\n",
      "Iteration 9707 => Loss: 6.75409077371830246506\n",
      "Iteration 9708 => Loss: 6.75408323945152755385\n",
      "Iteration 9709 => Loss: 6.75407570619999919614\n",
      "Iteration 9710 => Loss: 6.75406817396356817795\n",
      "Iteration 9711 => Loss: 6.75406064274211015430\n",
      "Iteration 9712 => Loss: 6.75405311253548568118\n",
      "Iteration 9713 => Loss: 6.75404558334355709093\n",
      "Iteration 9714 => Loss: 6.75403805516618493954\n",
      "Iteration 9715 => Loss: 6.75403052800324399385\n",
      "Iteration 9716 => Loss: 6.75402300185458237536\n",
      "Iteration 9717 => Loss: 6.75401547672006863365\n",
      "Iteration 9718 => Loss: 6.75400795259957309469\n",
      "Iteration 9719 => Loss: 6.75400042949294832084\n",
      "Iteration 9720 => Loss: 6.75399290740006019718\n",
      "Iteration 9721 => Loss: 6.75398538632078349053\n",
      "Iteration 9722 => Loss: 6.75397786625495921697\n",
      "Iteration 9723 => Loss: 6.75397034720247368966\n",
      "Iteration 9724 => Loss: 6.75396282916318657641\n",
      "Iteration 9725 => Loss: 6.75395531213694955142\n",
      "Iteration 9726 => Loss: 6.75394779612363382881\n",
      "Iteration 9727 => Loss: 6.75394028112310174095\n",
      "Iteration 9728 => Loss: 6.75393276713521029109\n",
      "Iteration 9729 => Loss: 6.75392525415982891701\n",
      "Iteration 9730 => Loss: 6.75391774219683060920\n",
      "Iteration 9731 => Loss: 6.75391023124606615369\n",
      "Iteration 9732 => Loss: 6.75390272130740765277\n",
      "Iteration 9733 => Loss: 6.75389521238069967524\n",
      "Iteration 9734 => Loss: 6.75388770446583652785\n",
      "Iteration 9735 => Loss: 6.75388019756265300941\n",
      "Iteration 9736 => Loss: 6.75387269167103365675\n",
      "Iteration 9737 => Loss: 6.75386518679082925587\n",
      "Iteration 9738 => Loss: 6.75385768292190746820\n",
      "Iteration 9739 => Loss: 6.75385018006413773151\n",
      "Iteration 9740 => Loss: 6.75384267821737527271\n",
      "Iteration 9741 => Loss: 6.75383517738149130594\n",
      "Iteration 9742 => Loss: 6.75382767755634283446\n",
      "Iteration 9743 => Loss: 6.75382017874179307881\n",
      "Iteration 9744 => Loss: 6.75381268093770792404\n",
      "Iteration 9745 => Loss: 6.75380518414395591975\n",
      "Iteration 9746 => Loss: 6.75379768836039318103\n",
      "Iteration 9747 => Loss: 6.75379019358689447472\n",
      "Iteration 9748 => Loss: 6.75378269982331680410\n",
      "Iteration 9749 => Loss: 6.75377520706951983698\n",
      "Iteration 9750 => Loss: 6.75376771532538011655\n",
      "Iteration 9751 => Loss: 6.75376022459074754067\n",
      "Iteration 9752 => Loss: 6.75375273486549154711\n",
      "Iteration 9753 => Loss: 6.75374524614948068546\n",
      "Iteration 9754 => Loss: 6.75373775844256840628\n",
      "Iteration 9755 => Loss: 6.75373027174463391731\n",
      "Iteration 9756 => Loss: 6.75372278605551823460\n",
      "Iteration 9757 => Loss: 6.75371530137511566494\n",
      "Iteration 9758 => Loss: 6.75370781770325834259\n",
      "Iteration 9759 => Loss: 6.75370033503984412704\n",
      "Iteration 9760 => Loss: 6.75369285338470692892\n",
      "Iteration 9761 => Loss: 6.75368537273772684415\n",
      "Iteration 9762 => Loss: 6.75367789309876709325\n",
      "Iteration 9763 => Loss: 6.75367041446768645585\n",
      "Iteration 9764 => Loss: 6.75366293684435081701\n",
      "Iteration 9765 => Loss: 6.75365546022862783815\n",
      "Iteration 9766 => Loss: 6.75364798462037629889\n",
      "Iteration 9767 => Loss: 6.75364051001946563701\n",
      "Iteration 9768 => Loss: 6.75363303642575640851\n",
      "Iteration 9769 => Loss: 6.75362556383912515656\n",
      "Iteration 9770 => Loss: 6.75361809225941467361\n",
      "Iteration 9771 => Loss: 6.75361062168650061466\n",
      "Iteration 9772 => Loss: 6.75360315212024886478\n",
      "Iteration 9773 => Loss: 6.75359568356052530902\n",
      "Iteration 9774 => Loss: 6.75358821600718339795\n",
      "Iteration 9775 => Loss: 6.75358074946009523387\n",
      "Iteration 9776 => Loss: 6.75357328391913203092\n",
      "Iteration 9777 => Loss: 6.75356581938414546329\n",
      "Iteration 9778 => Loss: 6.75355835585500852147\n",
      "Iteration 9779 => Loss: 6.75355089333157909692\n",
      "Iteration 9780 => Loss: 6.75354343181373018012\n",
      "Iteration 9781 => Loss: 6.75353597130131433346\n",
      "Iteration 9782 => Loss: 6.75352851179421342920\n",
      "Iteration 9783 => Loss: 6.75352105329227114794\n",
      "Iteration 9784 => Loss: 6.75351359579537025013\n",
      "Iteration 9785 => Loss: 6.75350613930336507451\n",
      "Iteration 9786 => Loss: 6.75349868381611795343\n",
      "Iteration 9787 => Loss: 6.75349122933350809461\n",
      "Iteration 9788 => Loss: 6.75348377585538361956\n",
      "Iteration 9789 => Loss: 6.75347632338161840693\n",
      "Iteration 9790 => Loss: 6.75346887191207390089\n",
      "Iteration 9791 => Loss: 6.75346142144661243378\n",
      "Iteration 9792 => Loss: 6.75345397198509811432\n",
      "Iteration 9793 => Loss: 6.75344652352741192658\n",
      "Iteration 9794 => Loss: 6.75343907607340110388\n",
      "Iteration 9795 => Loss: 6.75343162962293241947\n",
      "Iteration 9796 => Loss: 6.75342418417587353474\n",
      "Iteration 9797 => Loss: 6.75341673973209122295\n",
      "Iteration 9798 => Loss: 6.75340929629145225732\n",
      "Iteration 9799 => Loss: 6.75340185385380831207\n",
      "Iteration 9800 => Loss: 6.75339441241904037128\n",
      "Iteration 9801 => Loss: 6.75338697198699833280\n",
      "Iteration 9802 => Loss: 6.75337953255756495707\n",
      "Iteration 9803 => Loss: 6.75337209413058836560\n",
      "Iteration 9804 => Loss: 6.75336465670594421340\n",
      "Iteration 9805 => Loss: 6.75335722028349305646\n",
      "Iteration 9806 => Loss: 6.75334978486310077983\n",
      "Iteration 9807 => Loss: 6.75334235044463060404\n",
      "Iteration 9808 => Loss: 6.75333491702795196687\n",
      "Iteration 9809 => Loss: 6.75332748461292009523\n",
      "Iteration 9810 => Loss: 6.75332005319941330868\n",
      "Iteration 9811 => Loss: 6.75331262278728683413\n",
      "Iteration 9812 => Loss: 6.75330519337641543842\n",
      "Iteration 9813 => Loss: 6.75329776496664635488\n",
      "Iteration 9814 => Loss: 6.75329033755786145576\n",
      "Iteration 9815 => Loss: 6.75328291114992040889\n",
      "Iteration 9816 => Loss: 6.75327548574269265202\n",
      "Iteration 9817 => Loss: 6.75326806133603341209\n",
      "Iteration 9818 => Loss: 6.75326063792981390321\n",
      "Iteration 9819 => Loss: 6.75325321552389823410\n",
      "Iteration 9820 => Loss: 6.75324579411815673069\n",
      "Iteration 9821 => Loss: 6.75323837371244284355\n",
      "Iteration 9822 => Loss: 6.75323095430664022132\n",
      "Iteration 9823 => Loss: 6.75322353590059787365\n",
      "Iteration 9824 => Loss: 6.75321611849418257378\n",
      "Iteration 9825 => Loss: 6.75320870208726997674\n",
      "Iteration 9826 => Loss: 6.75320128667971175673\n",
      "Iteration 9827 => Loss: 6.75319387227138268059\n",
      "Iteration 9828 => Loss: 6.75318645886214774521\n",
      "Iteration 9829 => Loss: 6.75317904645186839474\n",
      "Iteration 9830 => Loss: 6.75317163504041584332\n",
      "Iteration 9831 => Loss: 6.75316422462764887058\n",
      "Iteration 9832 => Loss: 6.75315681521343247340\n",
      "Iteration 9833 => Loss: 6.75314940679764230680\n",
      "Iteration 9834 => Loss: 6.75314199938013270952\n",
      "Iteration 9835 => Loss: 6.75313459296076867844\n",
      "Iteration 9836 => Loss: 6.75312718753942764494\n",
      "Iteration 9837 => Loss: 6.75311978311596483593\n",
      "Iteration 9838 => Loss: 6.75311237969024880101\n",
      "Iteration 9839 => Loss: 6.75310497726214364889\n",
      "Iteration 9840 => Loss: 6.75309757583152059368\n",
      "Iteration 9841 => Loss: 6.75309017539823930321\n",
      "Iteration 9842 => Loss: 6.75308277596215766891\n",
      "Iteration 9843 => Loss: 6.75307537752316022761\n",
      "Iteration 9844 => Loss: 6.75306798008110398257\n",
      "Iteration 9845 => Loss: 6.75306058363584860160\n",
      "Iteration 9846 => Loss: 6.75305318818726529884\n",
      "Iteration 9847 => Loss: 6.75304579373522351204\n",
      "Iteration 9848 => Loss: 6.75303840027958379721\n",
      "Iteration 9849 => Loss: 6.75303100782020315762\n",
      "Iteration 9850 => Loss: 6.75302361635697234732\n",
      "Iteration 9851 => Loss: 6.75301622588973415873\n",
      "Iteration 9852 => Loss: 6.75300883641835802962\n",
      "Iteration 9853 => Loss: 6.75300144794272316773\n",
      "Iteration 9854 => Loss: 6.75299406046267858272\n",
      "Iteration 9855 => Loss: 6.75298667397809992963\n",
      "Iteration 9856 => Loss: 6.75297928848884776443\n",
      "Iteration 9857 => Loss: 6.75297190399479685396\n",
      "Iteration 9858 => Loss: 6.75296452049579976062\n",
      "Iteration 9859 => Loss: 6.75295713799173302760\n",
      "Iteration 9860 => Loss: 6.75294975648246342814\n",
      "Iteration 9861 => Loss: 6.75294237596784974187\n",
      "Iteration 9862 => Loss: 6.75293499644775696567\n",
      "Iteration 9863 => Loss: 6.75292761792205986637\n",
      "Iteration 9864 => Loss: 6.75292024039062077634\n",
      "Iteration 9865 => Loss: 6.75291286385329936337\n",
      "Iteration 9866 => Loss: 6.75290548830997217067\n",
      "Iteration 9867 => Loss: 6.75289811376049264879\n",
      "Iteration 9868 => Loss: 6.75289074020474355819\n",
      "Iteration 9869 => Loss: 6.75288336764257390854\n",
      "Iteration 9870 => Loss: 6.75287599607386201939\n",
      "Iteration 9871 => Loss: 6.75286862549846755854\n",
      "Iteration 9872 => Loss: 6.75286125591625818743\n",
      "Iteration 9873 => Loss: 6.75285388732710334381\n",
      "Iteration 9874 => Loss: 6.75284651973086536003\n",
      "Iteration 9875 => Loss: 6.75283915312741633841\n",
      "Iteration 9876 => Loss: 6.75283178751660884132\n",
      "Iteration 9877 => Loss: 6.75282442289832651738\n",
      "Iteration 9878 => Loss: 6.75281705927242281717\n",
      "Iteration 9879 => Loss: 6.75280969663876273756\n",
      "Iteration 9880 => Loss: 6.75280233499722903900\n",
      "Iteration 9881 => Loss: 6.75279497434766984298\n",
      "Iteration 9882 => Loss: 6.75278761468995547546\n",
      "Iteration 9883 => Loss: 6.75278025602396425597\n",
      "Iteration 9884 => Loss: 6.75277289834955141146\n",
      "Iteration 9885 => Loss: 6.75276554166658460332\n",
      "Iteration 9886 => Loss: 6.75275818597492616391\n",
      "Iteration 9887 => Loss: 6.75275083127445885367\n",
      "Iteration 9888 => Loss: 6.75274347756502901774\n",
      "Iteration 9889 => Loss: 6.75273612484651675203\n",
      "Iteration 9890 => Loss: 6.75272877311877905981\n",
      "Iteration 9891 => Loss: 6.75272142238168893158\n",
      "Iteration 9892 => Loss: 6.75271407263511402874\n",
      "Iteration 9893 => Loss: 6.75270672387892023636\n",
      "Iteration 9894 => Loss: 6.75269937611296278135\n",
      "Iteration 9895 => Loss: 6.75269202933712886505\n",
      "Iteration 9896 => Loss: 6.75268468355126572078\n",
      "Iteration 9897 => Loss: 6.75267733875524989173\n",
      "Iteration 9898 => Loss: 6.75266999494894548661\n",
      "Iteration 9899 => Loss: 6.75266265213221750230\n",
      "Iteration 9900 => Loss: 6.75265531030493892928\n",
      "Iteration 9901 => Loss: 6.75264796946697209989\n",
      "Iteration 9902 => Loss: 6.75264062961818023467\n",
      "Iteration 9903 => Loss: 6.75263329075843543592\n",
      "Iteration 9904 => Loss: 6.75262595288760270051\n",
      "Iteration 9905 => Loss: 6.75261861600554613716\n",
      "Iteration 9906 => Loss: 6.75261128011213695999\n",
      "Iteration 9907 => Loss: 6.75260394520723927769\n",
      "Iteration 9908 => Loss: 6.75259661129072075170\n",
      "Iteration 9909 => Loss: 6.75258927836245259613\n",
      "Iteration 9910 => Loss: 6.75258194642229181426\n",
      "Iteration 9911 => Loss: 6.75257461547010962022\n",
      "Iteration 9912 => Loss: 6.75256728550577633996\n",
      "Iteration 9913 => Loss: 6.75255995652915785854\n",
      "Iteration 9914 => Loss: 6.75255262854011917284\n",
      "Iteration 9915 => Loss: 6.75254530153852439156\n",
      "Iteration 9916 => Loss: 6.75253797552424650519\n",
      "Iteration 9917 => Loss: 6.75253065049714695789\n",
      "Iteration 9918 => Loss: 6.75252332645709962833\n",
      "Iteration 9919 => Loss: 6.75251600340396418432\n",
      "Iteration 9920 => Loss: 6.75250868133761006362\n",
      "Iteration 9921 => Loss: 6.75250136025790137495\n",
      "Iteration 9922 => Loss: 6.75249404016471732604\n",
      "Iteration 9923 => Loss: 6.75248672105791136744\n",
      "Iteration 9924 => Loss: 6.75247940293735471329\n",
      "Iteration 9925 => Loss: 6.75247208580292301860\n",
      "Iteration 9926 => Loss: 6.75246476965446706942\n",
      "Iteration 9927 => Loss: 6.75245745449186340892\n",
      "Iteration 9928 => Loss: 6.75245014031498591578\n",
      "Iteration 9929 => Loss: 6.75244282712368981691\n",
      "Iteration 9930 => Loss: 6.75243551491784277374\n",
      "Iteration 9931 => Loss: 6.75242820369731955310\n",
      "Iteration 9932 => Loss: 6.75242089346198692823\n",
      "Iteration 9933 => Loss: 6.75241358421170456694\n",
      "Iteration 9934 => Loss: 6.75240627594634545972\n",
      "Iteration 9935 => Loss: 6.75239896866577460344\n",
      "Iteration 9936 => Loss: 6.75239166236986143588\n",
      "Iteration 9937 => Loss: 6.75238435705846651302\n",
      "Iteration 9938 => Loss: 6.75237705273147881258\n",
      "Iteration 9939 => Loss: 6.75236974938873935059\n",
      "Iteration 9940 => Loss: 6.75236244703012555846\n",
      "Iteration 9941 => Loss: 6.75235514565550420940\n",
      "Iteration 9942 => Loss: 6.75234784526475007027\n",
      "Iteration 9943 => Loss: 6.75234054585771392709\n",
      "Iteration 9944 => Loss: 6.75233324743427854031\n",
      "Iteration 9945 => Loss: 6.75232594999431068317\n",
      "Iteration 9946 => Loss: 6.75231865353766469440\n",
      "Iteration 9947 => Loss: 6.75231135806422333445\n",
      "Iteration 9948 => Loss: 6.75230406357384360660\n",
      "Iteration 9949 => Loss: 6.75229677006640205406\n",
      "Iteration 9950 => Loss: 6.75228947754175656826\n",
      "Iteration 9951 => Loss: 6.75228218599978546877\n",
      "Iteration 9952 => Loss: 6.75227489544034220614\n",
      "Iteration 9953 => Loss: 6.75226760586330954084\n",
      "Iteration 9954 => Loss: 6.75226031726853825887\n",
      "Iteration 9955 => Loss: 6.75225302965591911430\n",
      "Iteration 9956 => Loss: 6.75224574302529667591\n",
      "Iteration 9957 => Loss: 6.75223845737655103960\n",
      "Iteration 9958 => Loss: 6.75223117270954809044\n",
      "Iteration 9959 => Loss: 6.75222388902415016076\n",
      "Iteration 9960 => Loss: 6.75221660632023379378\n",
      "Iteration 9961 => Loss: 6.75220932459765954547\n",
      "Iteration 9962 => Loss: 6.75220204385630395905\n",
      "Iteration 9963 => Loss: 6.75219476409602581413\n",
      "Iteration 9964 => Loss: 6.75218748531669277213\n",
      "Iteration 9965 => Loss: 6.75218020751817515901\n",
      "Iteration 9966 => Loss: 6.75217293070034507707\n",
      "Iteration 9967 => Loss: 6.75216565486306574684\n",
      "Iteration 9968 => Loss: 6.75215838000620749426\n",
      "Iteration 9969 => Loss: 6.75215110612963531622\n",
      "Iteration 9970 => Loss: 6.75214383323321598596\n",
      "Iteration 9971 => Loss: 6.75213656131682160577\n",
      "Iteration 9972 => Loss: 6.75212929038032161344\n",
      "Iteration 9973 => Loss: 6.75212202042357834131\n",
      "Iteration 9974 => Loss: 6.75211475144646211533\n",
      "Iteration 9975 => Loss: 6.75210748344883882055\n",
      "Iteration 9976 => Loss: 6.75210021643058677654\n",
      "Iteration 9977 => Loss: 6.75209295039156209839\n",
      "Iteration 9978 => Loss: 6.75208568533162978298\n",
      "Iteration 9979 => Loss: 6.75207842125067259076\n",
      "Iteration 9980 => Loss: 6.75207115814854930136\n",
      "Iteration 9981 => Loss: 6.75206389602513290527\n",
      "Iteration 9982 => Loss: 6.75205663488028040575\n",
      "Iteration 9983 => Loss: 6.75204937471387101056\n",
      "Iteration 9984 => Loss: 6.75204211552577238109\n",
      "Iteration 9985 => Loss: 6.75203485731584862606\n",
      "Iteration 9986 => Loss: 6.75202760008396474234\n",
      "Iteration 9987 => Loss: 6.75202034382999549678\n",
      "Iteration 9988 => Loss: 6.75201308855381299168\n",
      "Iteration 9989 => Loss: 6.75200583425527689485\n",
      "Iteration 9990 => Loss: 6.75199858093425309136\n",
      "Iteration 9991 => Loss: 6.75199132859061901257\n",
      "Iteration 9992 => Loss: 6.75198407722423965538\n",
      "Iteration 9993 => Loss: 6.75197682683498090483\n",
      "Iteration 9994 => Loss: 6.75196957742271308689\n",
      "Iteration 9995 => Loss: 6.75196232898730297478\n",
      "Iteration 9996 => Loss: 6.75195508152861822992\n",
      "Iteration 9997 => Loss: 6.75194783504653361916\n",
      "Iteration 9998 => Loss: 6.75194058954091325120\n",
      "Iteration 9999 => Loss: 6.75193334501162478745\n",
      "Iteration 10000 => Loss: 6.75192610145853322479\n",
      "Iteration 10001 => Loss: 6.75191885888151333006\n",
      "Iteration 10002 => Loss: 6.75191161728043098833\n",
      "Iteration 10003 => Loss: 6.75190437665515386101\n",
      "Iteration 10004 => Loss: 6.75189713700555849130\n",
      "Iteration 10005 => Loss: 6.75188989833149832975\n",
      "Iteration 10006 => Loss: 6.75188266063285080776\n",
      "Iteration 10007 => Loss: 6.75187542390948181037\n",
      "Iteration 10008 => Loss: 6.75186818816126965714\n",
      "Iteration 10009 => Loss: 6.75186095338807046318\n",
      "Iteration 10010 => Loss: 6.75185371958975721896\n",
      "Iteration 10011 => Loss: 6.75184648676619847407\n",
      "Iteration 10012 => Loss: 6.75183925491726899537\n",
      "Iteration 10013 => Loss: 6.75183202404282312159\n",
      "Iteration 10014 => Loss: 6.75182479414274894225\n",
      "Iteration 10015 => Loss: 6.75181756521689280248\n",
      "Iteration 10016 => Loss: 6.75181033726513746274\n",
      "Iteration 10017 => Loss: 6.75180311028735857803\n",
      "Iteration 10018 => Loss: 6.75179588428340515804\n",
      "Iteration 10019 => Loss: 6.75178865925316173957\n",
      "Iteration 10020 => Loss: 6.75178143519648710225\n",
      "Iteration 10021 => Loss: 6.75177421211326489470\n",
      "Iteration 10022 => Loss: 6.75176699000334235024\n",
      "Iteration 10023 => Loss: 6.75175976886660134113\n",
      "Iteration 10024 => Loss: 6.75175254870291041698\n",
      "Iteration 10025 => Loss: 6.75174532951213457466\n",
      "Iteration 10026 => Loss: 6.75173811129414858101\n",
      "Iteration 10027 => Loss: 6.75173089404882720288\n",
      "Iteration 10028 => Loss: 6.75172367777601323269\n",
      "Iteration 10029 => Loss: 6.75171646247560186538\n",
      "Iteration 10030 => Loss: 6.75170924814745188058\n",
      "Iteration 10031 => Loss: 6.75170203479143715697\n",
      "Iteration 10032 => Loss: 6.75169482240741736234\n",
      "Iteration 10033 => Loss: 6.75168761099526282266\n",
      "Iteration 10034 => Loss: 6.75168040055485008111\n",
      "Iteration 10035 => Loss: 6.75167319108605479272\n",
      "Iteration 10036 => Loss: 6.75166598258872063809\n",
      "Iteration 10037 => Loss: 6.75165877506273748310\n",
      "Iteration 10038 => Loss: 6.75165156850797298915\n",
      "Iteration 10039 => Loss: 6.75164436292428948860\n",
      "Iteration 10040 => Loss: 6.75163715831156441283\n",
      "Iteration 10041 => Loss: 6.75162995466965387692\n",
      "Iteration 10042 => Loss: 6.75162275199843708862\n",
      "Iteration 10043 => Loss: 6.75161555029778437387\n",
      "Iteration 10044 => Loss: 6.75160834956755273595\n",
      "Iteration 10045 => Loss: 6.75160114980762404713\n",
      "Iteration 10046 => Loss: 6.75159395101786596882\n",
      "Iteration 10047 => Loss: 6.75158675319813905702\n",
      "Iteration 10048 => Loss: 6.75157955634832696035\n",
      "Iteration 10049 => Loss: 6.75157236046829201115\n",
      "Iteration 10050 => Loss: 6.75156516555789476541\n",
      "Iteration 10051 => Loss: 6.75155797161701443088\n",
      "Iteration 10052 => Loss: 6.75155077864552399802\n",
      "Iteration 10053 => Loss: 6.75154358664328224648\n",
      "Iteration 10054 => Loss: 6.75153639561015950221\n",
      "Iteration 10055 => Loss: 6.75152920554603674930\n",
      "Iteration 10056 => Loss: 6.75152201645077187919\n",
      "Iteration 10057 => Loss: 6.75151482832423699421\n",
      "Iteration 10058 => Loss: 6.75150764116630330847\n",
      "Iteration 10059 => Loss: 6.75150045497684025975\n",
      "Iteration 10060 => Loss: 6.75149326975571284493\n",
      "Iteration 10061 => Loss: 6.75148608550280204810\n",
      "Iteration 10062 => Loss: 6.75147890221797020160\n",
      "Iteration 10063 => Loss: 6.75147171990107786144\n",
      "Iteration 10064 => Loss: 6.75146453855201400529\n",
      "Iteration 10065 => Loss: 6.75145735817062764283\n",
      "Iteration 10066 => Loss: 6.75145017875680242270\n",
      "Iteration 10067 => Loss: 6.75144300031040600629\n",
      "Iteration 10068 => Loss: 6.75143582283130161414\n",
      "Iteration 10069 => Loss: 6.75142864631936667763\n",
      "Iteration 10070 => Loss: 6.75142147077446352910\n",
      "Iteration 10071 => Loss: 6.75141429619647226446\n",
      "Iteration 10072 => Loss: 6.75140712258525255152\n",
      "Iteration 10073 => Loss: 6.75139994994067116352\n",
      "Iteration 10074 => Loss: 6.75139277826261174908\n",
      "Iteration 10075 => Loss: 6.75138560755093131149\n",
      "Iteration 10076 => Loss: 6.75137843780550728212\n",
      "Iteration 10077 => Loss: 6.75137126902621442781\n",
      "Iteration 10078 => Loss: 6.75136410121290264641\n",
      "Iteration 10079 => Loss: 6.75135693436545825108\n",
      "Iteration 10080 => Loss: 6.75134976848375067959\n",
      "Iteration 10081 => Loss: 6.75134260356764581701\n",
      "Iteration 10082 => Loss: 6.75133543961700866021\n",
      "Iteration 10083 => Loss: 6.75132827663171486421\n",
      "Iteration 10084 => Loss: 6.75132111461164186039\n",
      "Iteration 10085 => Loss: 6.75131395355664665203\n",
      "Iteration 10086 => Loss: 6.75130679346661022322\n",
      "Iteration 10087 => Loss: 6.75129963434138158362\n",
      "Iteration 10088 => Loss: 6.75129247618085948091\n",
      "Iteration 10089 => Loss: 6.75128531898489558927\n",
      "Iteration 10090 => Loss: 6.75127816275336556373\n",
      "Iteration 10091 => Loss: 6.75127100748613706571\n",
      "Iteration 10092 => Loss: 6.75126385318308042116\n",
      "Iteration 10093 => Loss: 6.75125669984406684421\n",
      "Iteration 10094 => Loss: 6.75124954746896399627\n",
      "Iteration 10095 => Loss: 6.75124239605765108507\n",
      "Iteration 10096 => Loss: 6.75123524560998511390\n",
      "Iteration 10097 => Loss: 6.75122809612584440231\n",
      "Iteration 10098 => Loss: 6.75122094760509927625\n",
      "Iteration 10099 => Loss: 6.75121380004761739713\n",
      "Iteration 10100 => Loss: 6.75120665345326642637\n",
      "Iteration 10101 => Loss: 6.75119950782192379535\n",
      "Iteration 10102 => Loss: 6.75119236315345272459\n",
      "Iteration 10103 => Loss: 6.75118521944772265186\n",
      "Iteration 10104 => Loss: 6.75117807670461367309\n",
      "Iteration 10105 => Loss: 6.75117093492398723242\n",
      "Iteration 10106 => Loss: 6.75116379410571898489\n",
      "Iteration 10107 => Loss: 6.75115665424967303920\n",
      "Iteration 10108 => Loss: 6.75114951535572593855\n",
      "Iteration 10109 => Loss: 6.75114237742374356799\n",
      "Iteration 10110 => Loss: 6.75113524045359447712\n",
      "Iteration 10111 => Loss: 6.75112810444516231456\n",
      "Iteration 10112 => Loss: 6.75112096939830230724\n",
      "Iteration 10113 => Loss: 6.75111383531288833382\n",
      "Iteration 10114 => Loss: 6.75110670218879249660\n",
      "Iteration 10115 => Loss: 6.75109957002588867425\n",
      "Iteration 10116 => Loss: 6.75109243882404452819\n",
      "Iteration 10117 => Loss: 6.75108530858313038436\n",
      "Iteration 10118 => Loss: 6.75107817930301834508\n",
      "Iteration 10119 => Loss: 6.75107105098357518358\n",
      "Iteration 10120 => Loss: 6.75106392362467211399\n",
      "Iteration 10121 => Loss: 6.75105679722618301497\n",
      "Iteration 10122 => Loss: 6.75104967178797910066\n",
      "Iteration 10123 => Loss: 6.75104254730991826250\n",
      "Iteration 10124 => Loss: 6.75103542379189125455\n",
      "Iteration 10125 => Loss: 6.75102830123375863280\n",
      "Iteration 10126 => Loss: 6.75102117963538894685\n",
      "Iteration 10127 => Loss: 6.75101405899665074628\n",
      "Iteration 10128 => Loss: 6.75100693931742412701\n",
      "Iteration 10129 => Loss: 6.75099982059758296771\n",
      "Iteration 10130 => Loss: 6.75099270283697894257\n",
      "Iteration 10131 => Loss: 6.75098558603550102930\n",
      "Iteration 10132 => Loss: 6.75097847019300267846\n",
      "Iteration 10133 => Loss: 6.75097135530936220960\n",
      "Iteration 10134 => Loss: 6.75096424138446415952\n",
      "Iteration 10135 => Loss: 6.75095712841816730787\n",
      "Iteration 10136 => Loss: 6.75095001641033842787\n",
      "Iteration 10137 => Loss: 6.75094290536086027998\n",
      "Iteration 10138 => Loss: 6.75093579526958986747\n",
      "Iteration 10139 => Loss: 6.75092868613640639808\n",
      "Iteration 10140 => Loss: 6.75092157796117575685\n",
      "Iteration 10141 => Loss: 6.75091447074378248061\n",
      "Iteration 10142 => Loss: 6.75090736448407380266\n",
      "Iteration 10143 => Loss: 6.75090025918194047705\n",
      "Iteration 10144 => Loss: 6.75089315483725105338\n",
      "Iteration 10145 => Loss: 6.75088605144986786399\n",
      "Iteration 10146 => Loss: 6.75087894901966745209\n",
      "Iteration 10147 => Loss: 6.75087184754651747909\n",
      "Iteration 10148 => Loss: 6.75086474703029537636\n",
      "Iteration 10149 => Loss: 6.75085764747087058169\n",
      "Iteration 10150 => Loss: 6.75085054886811075647\n",
      "Iteration 10151 => Loss: 6.75084345122188711485\n",
      "Iteration 10152 => Loss: 6.75083635453206554189\n",
      "Iteration 10153 => Loss: 6.75082925879853057438\n",
      "Iteration 10154 => Loss: 6.75082216402114809739\n",
      "Iteration 10155 => Loss: 6.75081507019978310780\n",
      "Iteration 10156 => Loss: 6.75080797733431037244\n",
      "Iteration 10157 => Loss: 6.75080088542460465817\n",
      "Iteration 10158 => Loss: 6.75079379447053717911\n",
      "Iteration 10159 => Loss: 6.75078670447196493853\n",
      "Iteration 10160 => Loss: 6.75077961542878579593\n",
      "Iteration 10161 => Loss: 6.75077252734084520824\n",
      "Iteration 10162 => Loss: 6.75076544020802771229\n",
      "Iteration 10163 => Loss: 6.75075835403019741676\n",
      "Iteration 10164 => Loss: 6.75075126880723797029\n",
      "Iteration 10165 => Loss: 6.75074418453900992887\n",
      "Iteration 10166 => Loss: 6.75073710122538717116\n",
      "Iteration 10167 => Loss: 6.75073001886624091128\n",
      "Iteration 10168 => Loss: 6.75072293746144147519\n",
      "Iteration 10169 => Loss: 6.75071585701086895881\n",
      "Iteration 10170 => Loss: 6.75070877751438391812\n",
      "Iteration 10171 => Loss: 6.75070169897185490271\n",
      "Iteration 10172 => Loss: 6.75069462138317000210\n",
      "Iteration 10173 => Loss: 6.75068754474818621958\n",
      "Iteration 10174 => Loss: 6.75068046906678098651\n",
      "Iteration 10175 => Loss: 6.75067339433882374067\n",
      "Iteration 10176 => Loss: 6.75066632056418214347\n",
      "Iteration 10177 => Loss: 6.75065924774273629083\n",
      "Iteration 10178 => Loss: 6.75065217587435562052\n",
      "Iteration 10179 => Loss: 6.75064510495890601760\n",
      "Iteration 10180 => Loss: 6.75063803499626580162\n",
      "Iteration 10181 => Loss: 6.75063096598630085765\n",
      "Iteration 10182 => Loss: 6.75062389792889216977\n",
      "Iteration 10183 => Loss: 6.75061683082389496491\n",
      "Iteration 10184 => Loss: 6.75060976467119910893\n",
      "Iteration 10185 => Loss: 6.75060269947066249330\n",
      "Iteration 10186 => Loss: 6.75059563522215633213\n",
      "Iteration 10187 => Loss: 6.75058857192557226767\n",
      "Iteration 10188 => Loss: 6.75058150958075930959\n",
      "Iteration 10189 => Loss: 6.75057444818760377103\n",
      "Iteration 10190 => Loss: 6.75056738774596887254\n",
      "Iteration 10191 => Loss: 6.75056032825572582823\n",
      "Iteration 10192 => Loss: 6.75055326971674762859\n",
      "Iteration 10193 => Loss: 6.75054621212891436954\n",
      "Iteration 10194 => Loss: 6.75053915549209015978\n",
      "Iteration 10195 => Loss: 6.75053209980614976615\n",
      "Iteration 10196 => Loss: 6.75052504507095818553\n",
      "Iteration 10197 => Loss: 6.75051799128639906655\n",
      "Iteration 10198 => Loss: 6.75051093845233385338\n",
      "Iteration 10199 => Loss: 6.75050388656863908921\n",
      "Iteration 10200 => Loss: 6.75049683563518776452\n",
      "Iteration 10201 => Loss: 6.75048978565184487621\n",
      "Iteration 10202 => Loss: 6.75048273661849407290\n",
      "Iteration 10203 => Loss: 6.75047568853499857511\n",
      "Iteration 10204 => Loss: 6.75046864140123226150\n",
      "Iteration 10205 => Loss: 6.75046159521707256346\n",
      "Iteration 10206 => Loss: 6.75045454998238181332\n",
      "Iteration 10207 => Loss: 6.75044750569703388976\n",
      "Iteration 10208 => Loss: 6.75044046236091332958\n",
      "Iteration 10209 => Loss: 6.75043341997387447151\n",
      "Iteration 10210 => Loss: 6.75042637853580274054\n",
      "Iteration 10211 => Loss: 6.75041933804656757445\n",
      "Iteration 10212 => Loss: 6.75041229850603308194\n",
      "Iteration 10213 => Loss: 6.75040525991407847073\n",
      "Iteration 10214 => Loss: 6.75039822227057584314\n",
      "Iteration 10215 => Loss: 6.75039118557539286058\n",
      "Iteration 10216 => Loss: 6.75038414982841050715\n",
      "Iteration 10217 => Loss: 6.75037711502949644427\n",
      "Iteration 10218 => Loss: 6.75037008117852010969\n",
      "Iteration 10219 => Loss: 6.75036304827534916484\n",
      "Iteration 10220 => Loss: 6.75035601631987081106\n",
      "Iteration 10221 => Loss: 6.75034898531194382798\n",
      "Iteration 10222 => Loss: 6.75034195525145186423\n",
      "Iteration 10223 => Loss: 6.75033492613825192308\n",
      "Iteration 10224 => Loss: 6.75032789797223120587\n",
      "Iteration 10225 => Loss: 6.75032087075326003855\n",
      "Iteration 10226 => Loss: 6.75031384448120075348\n",
      "Iteration 10227 => Loss: 6.75030681915593699927\n",
      "Iteration 10228 => Loss: 6.75029979477733466098\n",
      "Iteration 10229 => Loss: 6.75029277134526406456\n",
      "Iteration 10230 => Loss: 6.75028574885960708230\n",
      "Iteration 10231 => Loss: 6.75027872732022338198\n",
      "Iteration 10232 => Loss: 6.75027170672700194132\n",
      "Iteration 10233 => Loss: 6.75026468707980153994\n",
      "Iteration 10234 => Loss: 6.75025766837848983926\n",
      "Iteration 10235 => Loss: 6.75025065062296558693\n",
      "Iteration 10236 => Loss: 6.75024363381307335175\n",
      "Iteration 10237 => Loss: 6.75023661794870299957\n",
      "Iteration 10238 => Loss: 6.75022960302971686275\n",
      "Iteration 10239 => Loss: 6.75022258905598881995\n",
      "Iteration 10240 => Loss: 6.75021557602740251980\n",
      "Iteration 10241 => Loss: 6.75020856394381763010\n",
      "Iteration 10242 => Loss: 6.75020155280510980589\n",
      "Iteration 10243 => Loss: 6.75019454261115825489\n",
      "Iteration 10244 => Loss: 6.75018753336183063851\n",
      "Iteration 10245 => Loss: 6.75018052505699550636\n",
      "Iteration 10246 => Loss: 6.75017351769653028981\n",
      "Iteration 10247 => Loss: 6.75016651128030442663\n",
      "Iteration 10248 => Loss: 6.75015950580820867089\n",
      "Iteration 10249 => Loss: 6.75015250128008581498\n",
      "Iteration 10250 => Loss: 6.75014549769582572480\n",
      "Iteration 10251 => Loss: 6.75013849505530227901\n",
      "Iteration 10252 => Loss: 6.75013149335838313903\n",
      "Iteration 10253 => Loss: 6.75012449260495195347\n",
      "Iteration 10254 => Loss: 6.75011749279486839015\n",
      "Iteration 10255 => Loss: 6.75011049392799744595\n",
      "Iteration 10256 => Loss: 6.75010349600423875671\n",
      "Iteration 10257 => Loss: 6.75009649902343955574\n",
      "Iteration 10258 => Loss: 6.75008950298549770253\n",
      "Iteration 10259 => Loss: 6.75008250789026487126\n",
      "Iteration 10260 => Loss: 6.75007551373762293423\n",
      "Iteration 10261 => Loss: 6.75006852052743955284\n",
      "Iteration 10262 => Loss: 6.75006152825959748753\n",
      "Iteration 10263 => Loss: 6.75005453693396084702\n",
      "Iteration 10264 => Loss: 6.75004754655040706268\n",
      "Iteration 10265 => Loss: 6.75004055710880734864\n",
      "Iteration 10266 => Loss: 6.75003356860903735992\n",
      "Iteration 10267 => Loss: 6.75002658105097186336\n",
      "Iteration 10268 => Loss: 6.75001959443447585585\n",
      "Iteration 10269 => Loss: 6.75001260875942321604\n",
      "Iteration 10270 => Loss: 6.75000562402569137532\n",
      "Iteration 10271 => Loss: 6.74999864023315510053\n",
      "Iteration 10272 => Loss: 6.74999165738168649398\n",
      "Iteration 10273 => Loss: 6.74998467547115588161\n",
      "Iteration 10274 => Loss: 6.74997769450144247116\n",
      "Iteration 10275 => Loss: 6.74997071447240948316\n",
      "Iteration 10276 => Loss: 6.74996373538393790170\n",
      "Iteration 10277 => Loss: 6.74995675723589627637\n",
      "Iteration 10278 => Loss: 6.74994978002817269669\n",
      "Iteration 10279 => Loss: 6.74994280376061528415\n",
      "Iteration 10280 => Loss: 6.74993582843311745734\n",
      "Iteration 10281 => Loss: 6.74992885404554421314\n",
      "Iteration 10282 => Loss: 6.74992188059776498932\n",
      "Iteration 10283 => Loss: 6.74991490808966521087\n",
      "Iteration 10284 => Loss: 6.74990793652110898648\n",
      "Iteration 10285 => Loss: 6.74990096589196753030\n",
      "Iteration 10286 => Loss: 6.74989399620212715547\n",
      "Iteration 10287 => Loss: 6.74988702745144841799\n",
      "Iteration 10288 => Loss: 6.74988005963981052560\n",
      "Iteration 10289 => Loss: 6.74987309276708558059\n",
      "Iteration 10290 => Loss: 6.74986612683315367889\n",
      "Iteration 10291 => Loss: 6.74985916183786649469\n",
      "Iteration 10292 => Loss: 6.74985219778112632838\n",
      "Iteration 10293 => Loss: 6.74984523466279107140\n",
      "Iteration 10294 => Loss: 6.74983827248272927335\n",
      "Iteration 10295 => Loss: 6.74983131124082902375\n",
      "Iteration 10296 => Loss: 6.74982435093695620765\n",
      "Iteration 10297 => Loss: 6.74981739157098470372\n",
      "Iteration 10298 => Loss: 6.74981043314278217338\n",
      "Iteration 10299 => Loss: 6.74980347565223315343\n",
      "Iteration 10300 => Loss: 6.74979651909921152253\n",
      "Iteration 10301 => Loss: 6.74978956348357606032\n",
      "Iteration 10302 => Loss: 6.74978260880521574450\n",
      "Iteration 10303 => Loss: 6.74977565506399912465\n",
      "Iteration 10304 => Loss: 6.74976870225979119766\n",
      "Iteration 10305 => Loss: 6.74976175039248538212\n",
      "Iteration 10306 => Loss: 6.74975479946194489855\n",
      "Iteration 10307 => Loss: 6.74974784946803652019\n",
      "Iteration 10308 => Loss: 6.74974090041064123113\n",
      "Iteration 10309 => Loss: 6.74973395228964268000\n",
      "Iteration 10310 => Loss: 6.74972700510488632375\n",
      "Iteration 10311 => Loss: 6.74972005885627801547\n",
      "Iteration 10312 => Loss: 6.74971311354366676483\n",
      "Iteration 10313 => Loss: 6.74970616916693799681\n",
      "Iteration 10314 => Loss: 6.74969922572597003096\n",
      "Iteration 10315 => Loss: 6.74969228322062875236\n",
      "Iteration 10316 => Loss: 6.74968534165078981601\n",
      "Iteration 10317 => Loss: 6.74967840101633065331\n",
      "Iteration 10318 => Loss: 6.74967146131711892565\n",
      "Iteration 10319 => Loss: 6.74966452255303916985\n",
      "Iteration 10320 => Loss: 6.74965758472395016554\n",
      "Iteration 10321 => Loss: 6.74965064782974000224\n",
      "Iteration 10322 => Loss: 6.74964371187027190047\n",
      "Iteration 10323 => Loss: 6.74963677684542417978\n",
      "Iteration 10324 => Loss: 6.74962984275507782428\n",
      "Iteration 10325 => Loss: 6.74962290959909694266\n",
      "Iteration 10326 => Loss: 6.74961597737736074265\n",
      "Iteration 10327 => Loss: 6.74960904608974487928\n",
      "Iteration 10328 => Loss: 6.74960211573611967850\n",
      "Iteration 10329 => Loss: 6.74959518631635013719\n",
      "Iteration 10330 => Loss: 6.74958825783032878576\n",
      "Iteration 10331 => Loss: 6.74958133027792239744\n",
      "Iteration 10332 => Loss: 6.74957440365899508095\n",
      "Iteration 10333 => Loss: 6.74956747797344291939\n",
      "Iteration 10334 => Loss: 6.74956055322111847516\n",
      "Iteration 10335 => Loss: 6.74955362940191339050\n",
      "Iteration 10336 => Loss: 6.74954670651568644502\n",
      "Iteration 10337 => Loss: 6.74953978456231951100\n",
      "Iteration 10338 => Loss: 6.74953286354168735528\n",
      "Iteration 10339 => Loss: 6.74952594345366829742\n",
      "Iteration 10340 => Loss: 6.74951902429812466977\n",
      "Iteration 10341 => Loss: 6.74951210607494545002\n",
      "Iteration 10342 => Loss: 6.74950518878398764144\n",
      "Iteration 10343 => Loss: 6.74949827242513933356\n",
      "Iteration 10344 => Loss: 6.74949135699827174051\n",
      "Iteration 10345 => Loss: 6.74948444250325074734\n",
      "Iteration 10346 => Loss: 6.74947752893996799628\n",
      "Iteration 10347 => Loss: 6.74947061630829292511\n",
      "Iteration 10348 => Loss: 6.74946370460808520164\n",
      "Iteration 10349 => Loss: 6.74945679383923557992\n",
      "Iteration 10350 => Loss: 6.74944988400161172137\n",
      "Iteration 10351 => Loss: 6.74944297509508306376\n",
      "Iteration 10352 => Loss: 6.74943606711953147936\n",
      "Iteration 10353 => Loss: 6.74942916007483884044\n",
      "Iteration 10354 => Loss: 6.74942225396086392664\n",
      "Iteration 10355 => Loss: 6.74941534877748772203\n",
      "Iteration 10356 => Loss: 6.74940844452459032254\n",
      "Iteration 10357 => Loss: 6.74940154120204116595\n",
      "Iteration 10358 => Loss: 6.74939463880970969001\n",
      "Iteration 10359 => Loss: 6.74938773734747599065\n",
      "Iteration 10360 => Loss: 6.74938083681522549284\n",
      "Iteration 10361 => Loss: 6.74937393721281075898\n",
      "Iteration 10362 => Loss: 6.74936703854012076675\n",
      "Iteration 10363 => Loss: 6.74936014079702673030\n",
      "Iteration 10364 => Loss: 6.74935324398341229823\n",
      "Iteration 10365 => Loss: 6.74934634809913536202\n",
      "Iteration 10366 => Loss: 6.74933945314408667571\n",
      "Iteration 10367 => Loss: 6.74933255911812590710\n",
      "Iteration 10368 => Loss: 6.74932566602113670484\n",
      "Iteration 10369 => Loss: 6.74931877385299738847\n",
      "Iteration 10370 => Loss: 6.74931188261357384306\n",
      "Iteration 10371 => Loss: 6.74930499230274438816\n",
      "Iteration 10372 => Loss: 6.74929810292039178421\n",
      "Iteration 10373 => Loss: 6.74929121446638280446\n",
      "Iteration 10374 => Loss: 6.74928432694059043939\n",
      "Iteration 10375 => Loss: 6.74927744034289478492\n",
      "Iteration 10376 => Loss: 6.74927055467315906157\n",
      "Iteration 10377 => Loss: 6.74926366993127935245\n",
      "Iteration 10378 => Loss: 6.74925678611712065447\n",
      "Iteration 10379 => Loss: 6.74924990323055329355\n",
      "Iteration 10380 => Loss: 6.74924302127145292474\n",
      "Iteration 10381 => Loss: 6.74923614023969253850\n",
      "Iteration 10382 => Loss: 6.74922926013515844801\n",
      "Iteration 10383 => Loss: 6.74922238095771742650\n",
      "Iteration 10384 => Loss: 6.74921550270724868170\n",
      "Iteration 10385 => Loss: 6.74920862538361898686\n",
      "Iteration 10386 => Loss: 6.74920174898671465513\n",
      "Iteration 10387 => Loss: 6.74919487351639801886\n",
      "Iteration 10388 => Loss: 6.74918799897255539122\n",
      "Iteration 10389 => Loss: 6.74918112535506242722\n",
      "Iteration 10390 => Loss: 6.74917425266378767645\n",
      "Iteration 10391 => Loss: 6.74916738089860768213\n",
      "Iteration 10392 => Loss: 6.74916051005939898744\n",
      "Iteration 10393 => Loss: 6.74915364014603369469\n",
      "Iteration 10394 => Loss: 6.74914677115839278798\n",
      "Iteration 10395 => Loss: 6.74913990309634836962\n",
      "Iteration 10396 => Loss: 6.74913303595977609461\n",
      "Iteration 10397 => Loss: 6.74912616974854984164\n",
      "Iteration 10398 => Loss: 6.74911930446255059479\n",
      "Iteration 10399 => Loss: 6.74911244010163891005\n",
      "Iteration 10400 => Loss: 6.74910557666570909419\n",
      "Iteration 10401 => Loss: 6.74909871415462792044\n",
      "Iteration 10402 => Loss: 6.74909185256826749111\n",
      "Iteration 10403 => Loss: 6.74908499190650967847\n",
      "Iteration 10404 => Loss: 6.74907813216921770305\n",
      "Iteration 10405 => Loss: 6.74907127335628231890\n",
      "Iteration 10406 => Loss: 6.74906441546756941108\n",
      "Iteration 10407 => Loss: 6.74905755850296440457\n",
      "Iteration 10408 => Loss: 6.74905070246232963171\n",
      "Iteration 10409 => Loss: 6.74904384734554607661\n",
      "Iteration 10410 => Loss: 6.74903699315249738788\n",
      "Iteration 10411 => Loss: 6.74903013988304945059\n",
      "Iteration 10412 => Loss: 6.74902328753707969611\n",
      "Iteration 10413 => Loss: 6.74901643611445756221\n",
      "Iteration 10414 => Loss: 6.74900958561506669753\n",
      "Iteration 10415 => Loss: 6.74900273603878986250\n",
      "Iteration 10416 => Loss: 6.74899588738548938949\n",
      "Iteration 10417 => Loss: 6.74898903965504626257\n",
      "Iteration 10418 => Loss: 6.74898219284732814316\n",
      "Iteration 10419 => Loss: 6.74897534696222667350\n",
      "Iteration 10420 => Loss: 6.74896850199960240957\n",
      "Iteration 10421 => Loss: 6.74896165795934521725\n",
      "Iteration 10422 => Loss: 6.74895481484131654071\n",
      "Iteration 10423 => Loss: 6.74894797264540624582\n",
      "Iteration 10424 => Loss: 6.74894113137146778314\n",
      "Iteration 10425 => Loss: 6.74893429101939812398\n",
      "Iteration 10426 => Loss: 6.74892745158907114700\n",
      "Iteration 10427 => Loss: 6.74892061308035540179\n",
      "Iteration 10428 => Loss: 6.74891377549313009609\n",
      "Iteration 10429 => Loss: 6.74890693882726999675\n",
      "Iteration 10430 => Loss: 6.74890010308264809424\n",
      "Iteration 10431 => Loss: 6.74889326825914270813\n",
      "Iteration 10432 => Loss: 6.74888643435663659886\n",
      "Iteration 10433 => Loss: 6.74887960137499742785\n",
      "Iteration 10434 => Loss: 6.74887276931410262648\n",
      "Iteration 10435 => Loss: 6.74886593817382962612\n",
      "Iteration 10436 => Loss: 6.74885910795405674634\n",
      "Iteration 10437 => Loss: 6.74885227865464365493\n",
      "Iteration 10438 => Loss: 6.74884545027548465868\n",
      "Iteration 10439 => Loss: 6.74883862281645363623\n",
      "Iteration 10440 => Loss: 6.74883179627742002538\n",
      "Iteration 10441 => Loss: 6.74882497065826658655\n",
      "Iteration 10442 => Loss: 6.74881814595886631025\n",
      "Iteration 10443 => Loss: 6.74881132217909218696\n",
      "Iteration 10444 => Loss: 6.74880449931882342440\n",
      "Iteration 10445 => Loss: 6.74879767737793656579\n",
      "Iteration 10446 => Loss: 6.74879085635630371343\n",
      "Iteration 10447 => Loss: 6.74878403625380762776\n",
      "Iteration 10448 => Loss: 6.74877721707031685838\n",
      "Iteration 10449 => Loss: 6.74877039880571683028\n",
      "Iteration 10450 => Loss: 6.74876358145987165216\n",
      "Iteration 10451 => Loss: 6.74875676503267207806\n",
      "Iteration 10452 => Loss: 6.74874994952398221670\n",
      "Iteration 10453 => Loss: 6.74874313493368571670\n",
      "Iteration 10454 => Loss: 6.74873632126165112766\n",
      "Iteration 10455 => Loss: 6.74872950850775943366\n",
      "Iteration 10456 => Loss: 6.74872269667189250697\n",
      "Iteration 10457 => Loss: 6.74871588575391356812\n",
      "Iteration 10458 => Loss: 6.74870907575371781206\n",
      "Iteration 10459 => Loss: 6.74870226667115868935\n",
      "Iteration 10460 => Loss: 6.74869545850613139493\n",
      "Iteration 10461 => Loss: 6.74868865125849826114\n",
      "Iteration 10462 => Loss: 6.74868184492814382480\n",
      "Iteration 10463 => Loss: 6.74867503951494551728\n",
      "Iteration 10464 => Loss: 6.74866823501877899361\n",
      "Iteration 10465 => Loss: 6.74866143143951369154\n",
      "Iteration 10466 => Loss: 6.74865462877703592426\n",
      "Iteration 10467 => Loss: 6.74864782703121157681\n",
      "Iteration 10468 => Loss: 6.74864102620193229143\n",
      "Iteration 10469 => Loss: 6.74863422628905418321\n",
      "Iteration 10470 => Loss: 6.74862742729247422346\n",
      "Iteration 10471 => Loss: 6.74862062921205740906\n",
      "Iteration 10472 => Loss: 6.74861383204768117139\n",
      "Iteration 10473 => Loss: 6.74860703579921938911\n",
      "Iteration 10474 => Loss: 6.74860024046655926355\n",
      "Iteration 10475 => Loss: 6.74859344604956490343\n",
      "Iteration 10476 => Loss: 6.74858665254812173373\n",
      "Iteration 10477 => Loss: 6.74857985996210363311\n",
      "Iteration 10478 => Loss: 6.74857306829138448023\n",
      "Iteration 10479 => Loss: 6.74856627753584437102\n",
      "Iteration 10480 => Loss: 6.74855948769535984866\n",
      "Iteration 10481 => Loss: 6.74855269876980923272\n",
      "Iteration 10482 => Loss: 6.74854591075906551367\n",
      "Iteration 10483 => Loss: 6.74853912366300789927\n",
      "Iteration 10484 => Loss: 6.74853233748150493909\n",
      "Iteration 10485 => Loss: 6.74852555221445182809\n",
      "Iteration 10486 => Loss: 6.74851876786170734590\n",
      "Iteration 10487 => Loss: 6.74851198442315691750\n",
      "Iteration 10488 => Loss: 6.74850520189867442156\n",
      "Iteration 10489 => Loss: 6.74849842028813817763\n",
      "Iteration 10490 => Loss: 6.74849163959142295255\n",
      "Iteration 10491 => Loss: 6.74848485980840973042\n",
      "Iteration 10492 => Loss: 6.74847808093897594262\n",
      "Iteration 10493 => Loss: 6.74847130298298303330\n",
      "Iteration 10494 => Loss: 6.74846452594032975014\n",
      "Iteration 10495 => Loss: 6.74845774981088197819\n",
      "Iteration 10496 => Loss: 6.74845097459451803701\n",
      "Iteration 10497 => Loss: 6.74844420029110825254\n",
      "Iteration 10498 => Loss: 6.74843742690054337885\n",
      "Iteration 10499 => Loss: 6.74843065442269285370\n",
      "Iteration 10500 => Loss: 6.74842388285744121390\n",
      "Iteration 10501 => Loss: 6.74841711220464635090\n",
      "Iteration 10502 => Loss: 6.74841034246420079512\n",
      "Iteration 10503 => Loss: 6.74840357363598108975\n",
      "Iteration 10504 => Loss: 6.74839680571986288982\n",
      "Iteration 10505 => Loss: 6.74839003871571652127\n",
      "Iteration 10506 => Loss: 6.74838327262342296819\n",
      "Iteration 10507 => Loss: 6.74837650744286854376\n",
      "Iteration 10508 => Loss: 6.74836974317392268574\n",
      "Iteration 10509 => Loss: 6.74836297981645749644\n",
      "Iteration 10510 => Loss: 6.74835621737035573631\n",
      "Iteration 10511 => Loss: 6.74834945583549750125\n",
      "Iteration 10512 => Loss: 6.74834269521175134088\n",
      "Iteration 10513 => Loss: 6.74833593549900534470\n",
      "Iteration 10514 => Loss: 6.74832917669712895048\n",
      "Iteration 10515 => Loss: 6.74832241880599870143\n",
      "Iteration 10516 => Loss: 6.74831566182550091071\n",
      "Iteration 10517 => Loss: 6.74830890575550146337\n",
      "Iteration 10518 => Loss: 6.74830215059588844895\n",
      "Iteration 10519 => Loss: 6.74829539634652686431\n",
      "Iteration 10520 => Loss: 6.74828864300730391079\n",
      "Iteration 10521 => Loss: 6.74828189057809701978\n",
      "Iteration 10522 => Loss: 6.74827513905877740541\n",
      "Iteration 10523 => Loss: 6.74826838844922693994\n",
      "Iteration 10524 => Loss: 6.74826163874932305475\n",
      "Iteration 10525 => Loss: 6.74825488995893962851\n",
      "Iteration 10526 => Loss: 6.74824814207795231624\n",
      "Iteration 10527 => Loss: 6.74824139510624387839\n",
      "Iteration 10528 => Loss: 6.74823464904368996997\n",
      "Iteration 10529 => Loss: 6.74822790389017779233\n",
      "Iteration 10530 => Loss: 6.74822115964556790146\n",
      "Iteration 10531 => Loss: 6.74821441630974927506\n",
      "Iteration 10532 => Loss: 6.74820767388258602182\n",
      "Iteration 10533 => Loss: 6.74820093236398044212\n",
      "Iteration 10534 => Loss: 6.74819419175378243381\n",
      "Iteration 10535 => Loss: 6.74818745205189163272\n",
      "Iteration 10536 => Loss: 6.74818071325816859485\n",
      "Iteration 10537 => Loss: 6.74817397537249430428\n",
      "Iteration 10538 => Loss: 6.74816723839476217961\n",
      "Iteration 10539 => Loss: 6.74816050232483721771\n",
      "Iteration 10540 => Loss: 6.74815376716259329726\n",
      "Iteration 10541 => Loss: 6.74814703290791229051\n",
      "Iteration 10542 => Loss: 6.74814029956067340521\n",
      "Iteration 10543 => Loss: 6.74813356712075851362\n",
      "Iteration 10544 => Loss: 6.74812683558803350081\n",
      "Iteration 10545 => Loss: 6.74812010496238556811\n",
      "Iteration 10546 => Loss: 6.74811337524369125873\n",
      "Iteration 10547 => Loss: 6.74810664643182533950\n",
      "Iteration 10548 => Loss: 6.74809991852667145906\n",
      "Iteration 10549 => Loss: 6.74809319152810171971\n",
      "Iteration 10550 => Loss: 6.74808646543599266465\n",
      "Iteration 10551 => Loss: 6.74807974025022261344\n",
      "Iteration 10552 => Loss: 6.74807301597067787924\n",
      "Iteration 10553 => Loss: 6.74806629259722701164\n",
      "Iteration 10554 => Loss: 6.74805957012975188292\n",
      "Iteration 10555 => Loss: 6.74805284856813525352\n",
      "Iteration 10556 => Loss: 6.74804612791224300850\n",
      "Iteration 10557 => Loss: 6.74803940816196234920\n",
      "Iteration 10558 => Loss: 6.74803268931716893064\n",
      "Iteration 10559 => Loss: 6.74802597137773307878\n",
      "Iteration 10560 => Loss: 6.74801925434354910038\n",
      "Iteration 10561 => Loss: 6.74801253821448288051\n",
      "Iteration 10562 => Loss: 6.74800582299041007417\n",
      "Iteration 10563 => Loss: 6.74799910867123386993\n",
      "Iteration 10564 => Loss: 6.74799239525679261931\n",
      "Iteration 10565 => Loss: 6.74798568274699217540\n",
      "Iteration 10566 => Loss: 6.74797897114170019961\n",
      "Iteration 10567 => Loss: 6.74797226044079501150\n",
      "Iteration 10568 => Loss: 6.74796555064416203606\n",
      "Iteration 10569 => Loss: 6.74795884175167426378\n",
      "Iteration 10570 => Loss: 6.74795213376320823784\n",
      "Iteration 10571 => Loss: 6.74794542667864849506\n",
      "Iteration 10572 => Loss: 6.74793872049786269685\n",
      "Iteration 10573 => Loss: 6.74793201522073982090\n",
      "Iteration 10574 => Loss: 6.74792531084715463408\n",
      "Iteration 10575 => Loss: 6.74791860737698012684\n",
      "Iteration 10576 => Loss: 6.74791190481010083602\n",
      "Iteration 10577 => Loss: 6.74790520314639241661\n",
      "Iteration 10578 => Loss: 6.74789850238572874730\n",
      "Iteration 10579 => Loss: 6.74789180252799969395\n",
      "Iteration 10580 => Loss: 6.74788510357308357612\n",
      "Iteration 10581 => Loss: 6.74787840552083917345\n",
      "Iteration 10582 => Loss: 6.74787170837116434541\n",
      "Iteration 10583 => Loss: 6.74786501212392852977\n",
      "Iteration 10584 => Loss: 6.74785831677901004610\n",
      "Iteration 10585 => Loss: 6.74785162233629787210\n",
      "Iteration 10586 => Loss: 6.74784492879565700463\n",
      "Iteration 10587 => Loss: 6.74783823615697553322\n",
      "Iteration 10588 => Loss: 6.74783154442012023111\n",
      "Iteration 10589 => Loss: 6.74782485358498185235\n",
      "Iteration 10590 => Loss: 6.74781816365143338743\n",
      "Iteration 10591 => Loss: 6.74781147461935404408\n",
      "Iteration 10592 => Loss: 6.74780478648862391822\n",
      "Iteration 10593 => Loss: 6.74779809925911600033\n",
      "Iteration 10594 => Loss: 6.74779141293071926810\n",
      "Iteration 10595 => Loss: 6.74778472750329871843\n",
      "Iteration 10596 => Loss: 6.74777804297674599354\n",
      "Iteration 10597 => Loss: 6.74777135935092697849\n",
      "Iteration 10598 => Loss: 6.74776467662572621009\n",
      "Iteration 10599 => Loss: 6.74775799480103000150\n",
      "Iteration 10600 => Loss: 6.74775131387670779048\n",
      "Iteration 10601 => Loss: 6.74774463385264589022\n",
      "Iteration 10602 => Loss: 6.74773795472870663303\n",
      "Iteration 10603 => Loss: 6.74773127650478521389\n",
      "Iteration 10604 => Loss: 6.74772459918075639962\n",
      "Iteration 10605 => Loss: 6.74771792275649584525\n",
      "Iteration 10606 => Loss: 6.74771124723188453487\n",
      "Iteration 10607 => Loss: 6.74770457260680167622\n",
      "Iteration 10608 => Loss: 6.74769789888112292431\n",
      "Iteration 10609 => Loss: 6.74769122605472926324\n",
      "Iteration 10610 => Loss: 6.74768455412750522981\n",
      "Iteration 10611 => Loss: 6.74767788309931315638\n",
      "Iteration 10612 => Loss: 6.74767121297005711966\n",
      "Iteration 10613 => Loss: 6.74766454373958524116\n",
      "Iteration 10614 => Loss: 6.74765787540780426212\n",
      "Iteration 10615 => Loss: 6.74765120797457829127\n",
      "Iteration 10616 => Loss: 6.74764454143978209544\n",
      "Iteration 10617 => Loss: 6.74763787580331264593\n",
      "Iteration 10618 => Loss: 6.74763121106503582780\n",
      "Iteration 10619 => Loss: 6.74762454722483262515\n",
      "Iteration 10620 => Loss: 6.74761788428258491024\n",
      "Iteration 10621 => Loss: 6.74761122223816123267\n",
      "Iteration 10622 => Loss: 6.74760456109145323467\n",
      "Iteration 10623 => Loss: 6.74759790084233834762\n",
      "Iteration 10624 => Loss: 6.74759124149069045018\n",
      "Iteration 10625 => Loss: 6.74758458303638963827\n",
      "Iteration 10626 => Loss: 6.74757792547931778415\n",
      "Iteration 10627 => Loss: 6.74757126881935143103\n",
      "Iteration 10628 => Loss: 6.74756461305636445758\n",
      "Iteration 10629 => Loss: 6.74755795819025205873\n",
      "Iteration 10630 => Loss: 6.74755130422087479047\n",
      "Iteration 10631 => Loss: 6.74754465114812340687\n",
      "Iteration 10632 => Loss: 6.74753799897187711565\n",
      "Iteration 10633 => Loss: 6.74753134769201423637\n",
      "Iteration 10634 => Loss: 6.74752469730839798956\n",
      "Iteration 10635 => Loss: 6.74751804782093600465\n",
      "Iteration 10636 => Loss: 6.74751139922948439676\n",
      "Iteration 10637 => Loss: 6.74750475153394191352\n",
      "Iteration 10638 => Loss: 6.74749810473416467005\n",
      "Iteration 10639 => Loss: 6.74749145883004697311\n",
      "Iteration 10640 => Loss: 6.74748481382147247132\n",
      "Iteration 10641 => Loss: 6.74747816970830349703\n",
      "Iteration 10642 => Loss: 6.74747152649043968609\n",
      "Iteration 10643 => Loss: 6.74746488416774070629\n",
      "Iteration 10644 => Loss: 6.74745824274010086441\n",
      "Iteration 10645 => Loss: 6.74745160220739137458\n",
      "Iteration 10646 => Loss: 6.74744496256949322088\n",
      "Iteration 10647 => Loss: 6.74743832382629360467\n",
      "Iteration 10648 => Loss: 6.74743168597765929917\n",
      "Iteration 10649 => Loss: 6.74742504902347217666\n",
      "Iteration 10650 => Loss: 6.74741841296362210301\n",
      "Iteration 10651 => Loss: 6.74741177779797318692\n",
      "Iteration 10652 => Loss: 6.74740514352642328788\n",
      "Iteration 10653 => Loss: 6.74739851014883651459\n",
      "Iteration 10654 => Loss: 6.74739187766509740385\n",
      "Iteration 10655 => Loss: 6.74738524607508782793\n",
      "Iteration 10656 => Loss: 6.74737861537868788275\n",
      "Iteration 10657 => Loss: 6.74737198557577411151\n",
      "Iteration 10658 => Loss: 6.74736535666622039287\n",
      "Iteration 10659 => Loss: 6.74735872864991925724\n",
      "Iteration 10660 => Loss: 6.74735210152674280693\n",
      "Iteration 10661 => Loss: 6.74734547529657024967\n",
      "Iteration 10662 => Loss: 6.74733884995928256956\n",
      "Iteration 10663 => Loss: 6.74733222551476163886\n",
      "Iteration 10664 => Loss: 6.74732560196289021803\n",
      "Iteration 10665 => Loss: 6.74731897930353152759\n",
      "Iteration 10666 => Loss: 6.74731235753658165066\n",
      "Iteration 10667 => Loss: 6.74730573666191713045\n",
      "Iteration 10668 => Loss: 6.74729911667941539832\n",
      "Iteration 10669 => Loss: 6.74729249758895299749\n",
      "Iteration 10670 => Loss: 6.74728587939041801746\n",
      "Iteration 10671 => Loss: 6.74727926208368877781\n",
      "Iteration 10672 => Loss: 6.74727264566863116357\n",
      "Iteration 10673 => Loss: 6.74726603014513948153\n",
      "Iteration 10674 => Loss: 6.74725941551309560396\n",
      "Iteration 10675 => Loss: 6.74725280177236808044\n",
      "Iteration 10676 => Loss: 6.74724618892284677685\n",
      "Iteration 10677 => Loss: 6.74723957696441001275\n",
      "Iteration 10678 => Loss: 6.74723296589693077863\n",
      "Iteration 10679 => Loss: 6.74722635572029272311\n",
      "Iteration 10680 => Loss: 6.74721974643437949481\n",
      "Iteration 10681 => Loss: 6.74721313803906852513\n",
      "Iteration 10682 => Loss: 6.74720653053423458090\n",
      "Iteration 10683 => Loss: 6.74719992391976752799\n",
      "Iteration 10684 => Loss: 6.74719331819553591600\n",
      "Iteration 10685 => Loss: 6.74718671336143316353\n",
      "Iteration 10686 => Loss: 6.74718010941733048469\n",
      "Iteration 10687 => Loss: 6.74717350636310353451\n",
      "Iteration 10688 => Loss: 6.74716690419864928430\n",
      "Iteration 10689 => Loss: 6.74716030292382917821\n",
      "Iteration 10690 => Loss: 6.74715370253853485849\n",
      "Iteration 10691 => Loss: 6.74714710304263931562\n",
      "Iteration 10692 => Loss: 6.74714050443602975093\n",
      "Iteration 10693 => Loss: 6.74713390671858626035\n",
      "Iteration 10694 => Loss: 6.74712730989018272254\n",
      "Iteration 10695 => Loss: 6.74712071395069745705\n",
      "Iteration 10696 => Loss: 6.74711411890001588887\n",
      "Iteration 10697 => Loss: 6.74710752473802521934\n",
      "Iteration 10698 => Loss: 6.74710093146459755076\n",
      "Iteration 10699 => Loss: 6.74709433907960320909\n",
      "Iteration 10700 => Loss: 6.74708774758294449470\n",
      "Iteration 10701 => Loss: 6.74708115697448906900\n",
      "Iteration 10702 => Loss: 6.74707456725411347520\n",
      "Iteration 10703 => Loss: 6.74706797842170935553\n",
      "Iteration 10704 => Loss: 6.74706139047714437140\n",
      "Iteration 10705 => Loss: 6.74705480342031105323\n",
      "Iteration 10706 => Loss: 6.74704821725108150332\n",
      "Iteration 10707 => Loss: 6.74704163196933937030\n",
      "Iteration 10708 => Loss: 6.74703504757496741462\n",
      "Iteration 10709 => Loss: 6.74702846406784306765\n",
      "Iteration 10710 => Loss: 6.74702188144784020807\n",
      "Iteration 10711 => Loss: 6.74701529971485403081\n",
      "Iteration 10712 => Loss: 6.74700871886874775640\n",
      "Iteration 10713 => Loss: 6.74700213890941657979\n",
      "Iteration 10714 => Loss: 6.74699555983673260329\n",
      "Iteration 10715 => Loss: 6.74698898165058302823\n",
      "Iteration 10716 => Loss: 6.74698240435084173328\n",
      "Iteration 10717 => Loss: 6.74697582793739414342\n",
      "Iteration 10718 => Loss: 6.74696925241011591368\n",
      "Iteration 10719 => Loss: 6.74696267776890046264\n",
      "Iteration 10720 => Loss: 6.74695610401360301722\n",
      "Iteration 10721 => Loss: 6.74694953114412854234\n",
      "Iteration 10722 => Loss: 6.74694295916034914029\n",
      "Iteration 10723 => Loss: 6.74693638806214224246\n",
      "Iteration 10724 => Loss: 6.74692981784938883294\n",
      "Iteration 10725 => Loss: 6.74692324852198321850\n",
      "Iteration 10726 => Loss: 6.74691668007978240240\n",
      "Iteration 10727 => Loss: 6.74691011252268246778\n",
      "Iteration 10728 => Loss: 6.74690354585056262238\n",
      "Iteration 10729 => Loss: 6.74689698006330385027\n",
      "Iteration 10730 => Loss: 6.74689041516078535921\n",
      "Iteration 10731 => Loss: 6.74688385114288546873\n",
      "Iteration 10732 => Loss: 6.74687728800948871566\n",
      "Iteration 10733 => Loss: 6.74687072576047963679\n",
      "Iteration 10734 => Loss: 6.74686416439572944626\n",
      "Iteration 10735 => Loss: 6.74685760391512090450\n",
      "Iteration 10736 => Loss: 6.74685104431854387741\n",
      "Iteration 10737 => Loss: 6.74684448560586513821\n",
      "Iteration 10738 => Loss: 6.74683792777698254639\n",
      "Iteration 10739 => Loss: 6.74683137083176553972\n",
      "Iteration 10740 => Loss: 6.74682481477009154958\n",
      "Iteration 10741 => Loss: 6.74681825959185221819\n",
      "Iteration 10742 => Loss: 6.74681170529692852966\n",
      "Iteration 10743 => Loss: 6.74680515188518636904\n",
      "Iteration 10744 => Loss: 6.74679859935652448399\n",
      "Iteration 10745 => Loss: 6.74679204771080431868\n",
      "Iteration 10746 => Loss: 6.74678549694793261438\n",
      "Iteration 10747 => Loss: 6.74677894706776992706\n",
      "Iteration 10748 => Loss: 6.74677239807020967532\n",
      "Iteration 10749 => Loss: 6.74676584995511952059\n",
      "Iteration 10750 => Loss: 6.74675930272239465779\n",
      "Iteration 10751 => Loss: 6.74675275637190896560\n",
      "Iteration 10752 => Loss: 6.74674621090353987540\n",
      "Iteration 10753 => Loss: 6.74673966631717991760\n",
      "Iteration 10754 => Loss: 6.74673312261269320089\n",
      "Iteration 10755 => Loss: 6.74672657978998735473\n",
      "Iteration 10756 => Loss: 6.74672003784891582967\n",
      "Iteration 10757 => Loss: 6.74671349678937382066\n",
      "Iteration 10758 => Loss: 6.74670695661123609455\n",
      "Iteration 10759 => Loss: 6.74670041731439074084\n",
      "Iteration 10760 => Loss: 6.74669387889871519093\n",
      "Iteration 10761 => Loss: 6.74668734136409931068\n",
      "Iteration 10762 => Loss: 6.74668080471041431423\n",
      "Iteration 10763 => Loss: 6.74667426893753763295\n",
      "Iteration 10764 => Loss: 6.74666773404536090908\n",
      "Iteration 10765 => Loss: 6.74666120003375624492\n",
      "Iteration 10766 => Loss: 6.74665466690261794724\n",
      "Iteration 10767 => Loss: 6.74664813465181190111\n",
      "Iteration 10768 => Loss: 6.74664160328123596599\n",
      "Iteration 10769 => Loss: 6.74663507279075691514\n",
      "Iteration 10770 => Loss: 6.74662854318026639078\n",
      "Iteration 10771 => Loss: 6.74662201444963649521\n",
      "Iteration 10772 => Loss: 6.74661548659875442979\n",
      "Iteration 10773 => Loss: 6.74660895962749496135\n",
      "Iteration 10774 => Loss: 6.74660243353575861391\n",
      "Iteration 10775 => Loss: 6.74659590832340771982\n",
      "Iteration 10776 => Loss: 6.74658938399032770405\n",
      "Iteration 10777 => Loss: 6.74658286053640310342\n",
      "Iteration 10778 => Loss: 6.74657633796151845473\n",
      "Iteration 10779 => Loss: 6.74656981626554674847\n",
      "Iteration 10780 => Loss: 6.74656329544836985690\n",
      "Iteration 10781 => Loss: 6.74655677550988386315\n",
      "Iteration 10782 => Loss: 6.74655025644995198775\n",
      "Iteration 10783 => Loss: 6.74654373826846498474\n",
      "Iteration 10784 => Loss: 6.74653722096530650276\n",
      "Iteration 10785 => Loss: 6.74653070454035308501\n",
      "Iteration 10786 => Loss: 6.74652418899349193282\n",
      "Iteration 10787 => Loss: 6.74651767432459692486\n",
      "Iteration 10788 => Loss: 6.74651116053355526248\n",
      "Iteration 10789 => Loss: 6.74650464762025148246\n",
      "Iteration 10790 => Loss: 6.74649813558455591078\n",
      "Iteration 10791 => Loss: 6.74649162442636018966\n",
      "Iteration 10792 => Loss: 6.74648511414554352683\n",
      "Iteration 10793 => Loss: 6.74647860474199045910\n",
      "Iteration 10794 => Loss: 6.74647209621558019421\n",
      "Iteration 10795 => Loss: 6.74646558856619460443\n",
      "Iteration 10796 => Loss: 6.74645908179370756841\n",
      "Iteration 10797 => Loss: 6.74645257589801516929\n",
      "Iteration 10798 => Loss: 6.74644607087899483844\n",
      "Iteration 10799 => Loss: 6.74643956673652311906\n",
      "Iteration 10800 => Loss: 6.74643306347048632432\n",
      "Iteration 10801 => Loss: 6.74642656108076099741\n",
      "Iteration 10802 => Loss: 6.74642005956723878057\n",
      "Iteration 10803 => Loss: 6.74641355892979444064\n",
      "Iteration 10804 => Loss: 6.74640705916830807354\n",
      "Iteration 10805 => Loss: 6.74640056028266688060\n",
      "Iteration 10806 => Loss: 6.74639406227275539862\n",
      "Iteration 10807 => Loss: 6.74638756513844750629\n",
      "Iteration 10808 => Loss: 6.74638106887963040492\n",
      "Iteration 10809 => Loss: 6.74637457349617708502\n",
      "Iteration 10810 => Loss: 6.74636807898798629424\n",
      "Iteration 10811 => Loss: 6.74636158535493546395\n",
      "Iteration 10812 => Loss: 6.74635509259689136741\n",
      "Iteration 10813 => Loss: 6.74634860071374831136\n",
      "Iteration 10814 => Loss: 6.74634210970538461538\n",
      "Iteration 10815 => Loss: 6.74633561957169369805\n",
      "Iteration 10816 => Loss: 6.74632913031254854985\n",
      "Iteration 10817 => Loss: 6.74632264192782660217\n",
      "Iteration 10818 => Loss: 6.74631615441741505634\n",
      "Iteration 10819 => Loss: 6.74630966778119400828\n",
      "Iteration 10820 => Loss: 6.74630318201905421205\n",
      "Iteration 10821 => Loss: 6.74629669713086155269\n",
      "Iteration 10822 => Loss: 6.74629021311651300152\n",
      "Iteration 10823 => Loss: 6.74628372997588687809\n",
      "Iteration 10824 => Loss: 6.74627724770887216010\n",
      "Iteration 10825 => Loss: 6.74627076631533117990\n",
      "Iteration 10826 => Loss: 6.74626428579516090878\n",
      "Iteration 10827 => Loss: 6.74625780614825032444\n",
      "Iteration 10828 => Loss: 6.74625132737446264741\n",
      "Iteration 10829 => Loss: 6.74624484947369840171\n",
      "Iteration 10830 => Loss: 6.74623837244582524875\n",
      "Iteration 10831 => Loss: 6.74623189629073127804\n",
      "Iteration 10832 => Loss: 6.74622542100830724365\n",
      "Iteration 10833 => Loss: 6.74621894659842169517\n",
      "Iteration 10834 => Loss: 6.74621247306096361029\n",
      "Iteration 10835 => Loss: 6.74620600039581574947\n",
      "Iteration 10836 => Loss: 6.74619952860286176133\n",
      "Iteration 10837 => Loss: 6.74619305768198618267\n",
      "Iteration 10838 => Loss: 6.74618658763305933945\n",
      "Iteration 10839 => Loss: 6.74618011845596932119\n",
      "Iteration 10840 => Loss: 6.74617365015060954647\n",
      "Iteration 10841 => Loss: 6.74616718271685034125\n",
      "Iteration 10842 => Loss: 6.74616071615458245958\n",
      "Iteration 10843 => Loss: 6.74615425046368066830\n",
      "Iteration 10844 => Loss: 6.74614778564403572148\n",
      "Iteration 10845 => Loss: 6.74614132169552327412\n",
      "Iteration 10846 => Loss: 6.74613485861802875121\n",
      "Iteration 10847 => Loss: 6.74612839641143757774\n",
      "Iteration 10848 => Loss: 6.74612193507562363237\n",
      "Iteration 10849 => Loss: 6.74611547461047589280\n",
      "Iteration 10850 => Loss: 6.74610901501587623130\n",
      "Iteration 10851 => Loss: 6.74610255629170474378\n",
      "Iteration 10852 => Loss: 6.74609609843785396066\n",
      "Iteration 10853 => Loss: 6.74608964145419687242\n",
      "Iteration 10854 => Loss: 6.74608318534062068039\n",
      "Iteration 10855 => Loss: 6.74607673009700548050\n",
      "Iteration 10856 => Loss: 6.74607027572322781594\n",
      "Iteration 10857 => Loss: 6.74606382221918909892\n",
      "Iteration 10858 => Loss: 6.74605736958475077358\n",
      "Iteration 10859 => Loss: 6.74605091781981069943\n",
      "Iteration 10860 => Loss: 6.74604446692424541965\n",
      "Iteration 10861 => Loss: 6.74603801689793325380\n",
      "Iteration 10862 => Loss: 6.74603156774077117319\n",
      "Iteration 10863 => Loss: 6.74602511945262772741\n",
      "Iteration 10864 => Loss: 6.74601867203339278234\n",
      "Iteration 10865 => Loss: 6.74601222548294732206\n",
      "Iteration 10866 => Loss: 6.74600577980118121246\n",
      "Iteration 10867 => Loss: 6.74599933498796477949\n",
      "Iteration 10868 => Loss: 6.74599289104318877719\n",
      "Iteration 10869 => Loss: 6.74598644796673863056\n",
      "Iteration 10870 => Loss: 6.74598000575849177096\n",
      "Iteration 10871 => Loss: 6.74597356441833273522\n",
      "Iteration 10872 => Loss: 6.74596712394614428376\n",
      "Iteration 10873 => Loss: 6.74596068434181184159\n",
      "Iteration 10874 => Loss: 6.74595424560521017554\n",
      "Iteration 10875 => Loss: 6.74594780773623270420\n",
      "Iteration 10876 => Loss: 6.74594137073475863531\n",
      "Iteration 10877 => Loss: 6.74593493460067072931\n",
      "Iteration 10878 => Loss: 6.74592849933385263483\n",
      "Iteration 10879 => Loss: 6.74592206493418267144\n",
      "Iteration 10880 => Loss: 6.74591563140155070499\n",
      "Iteration 10881 => Loss: 6.74590919873584216049\n",
      "Iteration 10882 => Loss: 6.74590276693693979837\n",
      "Iteration 10883 => Loss: 6.74589633600471305641\n",
      "Iteration 10884 => Loss: 6.74588990593906245863\n",
      "Iteration 10885 => Loss: 6.74588347673985744279\n",
      "Iteration 10886 => Loss: 6.74587704840699053932\n",
      "Iteration 10887 => Loss: 6.74587062094034006776\n",
      "Iteration 10888 => Loss: 6.74586419433979855853\n",
      "Iteration 10889 => Loss: 6.74585776860523811393\n",
      "Iteration 10890 => Loss: 6.74585134373654149442\n",
      "Iteration 10891 => Loss: 6.74584491973359412498\n",
      "Iteration 10892 => Loss: 6.74583849659628853601\n",
      "Iteration 10893 => Loss: 6.74583207432450127072\n",
      "Iteration 10894 => Loss: 6.74582565291811153685\n",
      "Iteration 10895 => Loss: 6.74581923237700653573\n",
      "Iteration 10896 => Loss: 6.74581281270107524506\n",
      "Iteration 10897 => Loss: 6.74580639389019420804\n",
      "Iteration 10898 => Loss: 6.74579997594424352059\n",
      "Iteration 10899 => Loss: 6.74579355886311748947\n",
      "Iteration 10900 => Loss: 6.74578714264669088152\n",
      "Iteration 10901 => Loss: 6.74578072729484645720\n",
      "Iteration 10902 => Loss: 6.74577431280747585873\n",
      "Iteration 10903 => Loss: 6.74576789918445829386\n",
      "Iteration 10904 => Loss: 6.74576148642567829938\n",
      "Iteration 10905 => Loss: 6.74575507453101419486\n",
      "Iteration 10906 => Loss: 6.74574866350035140528\n",
      "Iteration 10907 => Loss: 6.74574225333357979650\n",
      "Iteration 10908 => Loss: 6.74573584403057591175\n",
      "Iteration 10909 => Loss: 6.74572943559123405777\n",
      "Iteration 10910 => Loss: 6.74572302801541923145\n",
      "Iteration 10911 => Loss: 6.74571662130303284499\n",
      "Iteration 10912 => Loss: 6.74571021545394433616\n",
      "Iteration 10913 => Loss: 6.74570381046805156444\n",
      "Iteration 10914 => Loss: 6.74569740634523284939\n",
      "Iteration 10915 => Loss: 6.74569100308536118149\n",
      "Iteration 10916 => Loss: 6.74568460068833353205\n",
      "Iteration 10917 => Loss: 6.74567819915402910880\n",
      "Iteration 10918 => Loss: 6.74567179848233688944\n",
      "Iteration 10919 => Loss: 6.74566539867313075263\n",
      "Iteration 10920 => Loss: 6.74565899972629967607\n",
      "Iteration 10921 => Loss: 6.74565260164172464386\n",
      "Iteration 10922 => Loss: 6.74564620441929285732\n",
      "Iteration 10923 => Loss: 6.74563980805889418235\n",
      "Iteration 10924 => Loss: 6.74563341256039894489\n",
      "Iteration 10925 => Loss: 6.74562701792370145171\n",
      "Iteration 10926 => Loss: 6.74562062414867469329\n",
      "Iteration 10927 => Loss: 6.74561423123521919365\n",
      "Iteration 10928 => Loss: 6.74560783918320261421\n",
      "Iteration 10929 => Loss: 6.74560144799251393266\n",
      "Iteration 10930 => Loss: 6.74559505766304745578\n",
      "Iteration 10931 => Loss: 6.74558866819467617404\n",
      "Iteration 10932 => Loss: 6.74558227958727929519\n",
      "Iteration 10933 => Loss: 6.74557589184074757327\n",
      "Iteration 10934 => Loss: 6.74556950495497265052\n",
      "Iteration 10935 => Loss: 6.74556311892982840561\n",
      "Iteration 10936 => Loss: 6.74555673376520470441\n",
      "Iteration 10937 => Loss: 6.74555034946097364923\n",
      "Iteration 10938 => Loss: 6.74554396601703842862\n",
      "Iteration 10939 => Loss: 6.74553758343325959856\n",
      "Iteration 10940 => Loss: 6.74553120170954745305\n",
      "Iteration 10941 => Loss: 6.74552482084576787713\n",
      "Iteration 10942 => Loss: 6.74551844084180629579\n",
      "Iteration 10943 => Loss: 6.74551206169756234488\n",
      "Iteration 10944 => Loss: 6.74550568341289924490\n",
      "Iteration 10945 => Loss: 6.74549930598771307899\n",
      "Iteration 10946 => Loss: 6.74549292942187772582\n",
      "Iteration 10947 => Loss: 6.74548655371529637392\n",
      "Iteration 10948 => Loss: 6.74548017886783313202\n",
      "Iteration 10949 => Loss: 6.74547380487939296501\n",
      "Iteration 10950 => Loss: 6.74546743174983909341\n",
      "Iteration 10951 => Loss: 6.74546105947906493583\n",
      "Iteration 10952 => Loss: 6.74545468806695591724\n",
      "Iteration 10953 => Loss: 6.74544831751339213355\n",
      "Iteration 10954 => Loss: 6.74544194781827322061\n",
      "Iteration 10955 => Loss: 6.74543557898146151075\n",
      "Iteration 10956 => Loss: 6.74542921100284775804\n",
      "Iteration 10957 => Loss: 6.74542284388232360470\n",
      "Iteration 10958 => Loss: 6.74541647761976737030\n",
      "Iteration 10959 => Loss: 6.74541011221506892070\n",
      "Iteration 10960 => Loss: 6.74540374766811279272\n",
      "Iteration 10961 => Loss: 6.74539738397877286502\n",
      "Iteration 10962 => Loss: 6.74539102114694077983\n",
      "Iteration 10963 => Loss: 6.74538465917250551485\n",
      "Iteration 10964 => Loss: 6.74537829805534627781\n",
      "Iteration 10965 => Loss: 6.74537193779534849369\n",
      "Iteration 10966 => Loss: 6.74536557839238959389\n",
      "Iteration 10967 => Loss: 6.74535921984636654969\n",
      "Iteration 10968 => Loss: 6.74535286215715146341\n",
      "Iteration 10969 => Loss: 6.74534650532464485906\n",
      "Iteration 10970 => Loss: 6.74534014934871883895\n",
      "Iteration 10971 => Loss: 6.74533379422926415714\n",
      "Iteration 10972 => Loss: 6.74532743996614936322\n",
      "Iteration 10973 => Loss: 6.74532108655928386298\n",
      "Iteration 10974 => Loss: 6.74531473400853709421\n",
      "Iteration 10975 => Loss: 6.74530838231379892278\n",
      "Iteration 10976 => Loss: 6.74530203147494944460\n",
      "Iteration 10977 => Loss: 6.74529568149187763737\n",
      "Iteration 10978 => Loss: 6.74528933236446626154\n",
      "Iteration 10979 => Loss: 6.74528298409259452484\n",
      "Iteration 10980 => Loss: 6.74527663667616472765\n",
      "Iteration 10981 => Loss: 6.74527029011504453138\n",
      "Iteration 10982 => Loss: 6.74526394440912735462\n",
      "Iteration 10983 => Loss: 6.74525759955828441150\n",
      "Iteration 10984 => Loss: 6.74525125556241444968\n",
      "Iteration 10985 => Loss: 6.74524491242140822322\n",
      "Iteration 10986 => Loss: 6.74523857013512806446\n",
      "Iteration 10987 => Loss: 6.74523222870347272107\n",
      "Iteration 10988 => Loss: 6.74522588812633294708\n",
      "Iteration 10989 => Loss: 6.74521954840357729211\n",
      "Iteration 10990 => Loss: 6.74521320953510628016\n",
      "Iteration 10991 => Loss: 6.74520687152079467808\n",
      "Iteration 10992 => Loss: 6.74520053436054123353\n",
      "Iteration 10993 => Loss: 6.74519419805420916703\n",
      "Iteration 10994 => Loss: 6.74518786260169722624\n",
      "Iteration 10995 => Loss: 6.74518152800288817161\n",
      "Iteration 10996 => Loss: 6.74517519425766920449\n",
      "Iteration 10997 => Loss: 6.74516886136592219714\n",
      "Iteration 10998 => Loss: 6.74516252932753168636\n",
      "Iteration 10999 => Loss: 6.74515619814238398533\n",
      "Iteration 11000 => Loss: 6.74514986781036629537\n",
      "Iteration 11001 => Loss: 6.74514353833136048877\n",
      "Iteration 11002 => Loss: 6.74513720970524754961\n",
      "Iteration 11003 => Loss: 6.74513088193192622555\n",
      "Iteration 11004 => Loss: 6.74512455501126773072\n",
      "Iteration 11005 => Loss: 6.74511822894315837829\n",
      "Iteration 11006 => Loss: 6.74511190372749691591\n",
      "Iteration 11007 => Loss: 6.74510557936415100500\n",
      "Iteration 11008 => Loss: 6.74509925585301672868\n",
      "Iteration 11009 => Loss: 6.74509293319397151834\n",
      "Iteration 11010 => Loss: 6.74508661138691145709\n",
      "Iteration 11011 => Loss: 6.74508029043171042360\n",
      "Iteration 11012 => Loss: 6.74507397032826006011\n",
      "Iteration 11013 => Loss: 6.74506765107644401525\n",
      "Iteration 11014 => Loss: 6.74506133267615748395\n",
      "Iteration 11015 => Loss: 6.74505501512726102220\n",
      "Iteration 11016 => Loss: 6.74504869842965870674\n",
      "Iteration 11017 => Loss: 6.74504238258323773891\n",
      "Iteration 11018 => Loss: 6.74503606758787110920\n",
      "Iteration 11019 => Loss: 6.74502975344345756525\n",
      "Iteration 11020 => Loss: 6.74502344014986743304\n",
      "Iteration 11021 => Loss: 6.74501712770699946020\n",
      "Iteration 11022 => Loss: 6.74501081611473107813\n",
      "Iteration 11023 => Loss: 6.74500450537295304088\n",
      "Iteration 11024 => Loss: 6.74499819548154277982\n",
      "Iteration 11025 => Loss: 6.74499188644039637808\n",
      "Iteration 11026 => Loss: 6.74498557824939215521\n",
      "Iteration 11027 => Loss: 6.74497927090841553621\n",
      "Iteration 11028 => Loss: 6.74497296441735638695\n",
      "Iteration 11029 => Loss: 6.74496665877609657969\n",
      "Iteration 11030 => Loss: 6.74496035398451798670\n",
      "Iteration 11031 => Loss: 6.74495405004251402659\n",
      "Iteration 11032 => Loss: 6.74494774694996568343\n",
      "Iteration 11033 => Loss: 6.74494144470676193492\n",
      "Iteration 11034 => Loss: 6.74493514331278287699\n",
      "Iteration 11035 => Loss: 6.74492884276791837550\n",
      "Iteration 11036 => Loss: 6.74492254307205474362\n",
      "Iteration 11037 => Loss: 6.74491624422507829451\n",
      "Iteration 11038 => Loss: 6.74490994622686557136\n",
      "Iteration 11039 => Loss: 6.74490364907731621003\n",
      "Iteration 11040 => Loss: 6.74489735277629520738\n",
      "Iteration 11041 => Loss: 6.74489105732370752833\n",
      "Iteration 11042 => Loss: 6.74488476271943859786\n",
      "Iteration 11043 => Loss: 6.74487846896336318281\n",
      "Iteration 11044 => Loss: 6.74487217605537114906\n",
      "Iteration 11045 => Loss: 6.74486588399535058613\n",
      "Iteration 11046 => Loss: 6.74485959278318691901\n",
      "Iteration 11047 => Loss: 6.74485330241876379631\n",
      "Iteration 11048 => Loss: 6.74484701290196486667\n",
      "Iteration 11049 => Loss: 6.74484072423267999596\n",
      "Iteration 11050 => Loss: 6.74483443641079638553\n",
      "Iteration 11051 => Loss: 6.74482814943619590764\n",
      "Iteration 11052 => Loss: 6.74482186330876043456\n",
      "Iteration 11053 => Loss: 6.74481557802838516125\n",
      "Iteration 11054 => Loss: 6.74480929359495462450\n",
      "Iteration 11055 => Loss: 6.74480301000835069658\n",
      "Iteration 11056 => Loss: 6.74479672726846057884\n",
      "Iteration 11057 => Loss: 6.74479044537516436719\n",
      "Iteration 11058 => Loss: 6.74478416432836347383\n",
      "Iteration 11059 => Loss: 6.74477788412793177741\n",
      "Iteration 11060 => Loss: 6.74477160477375381475\n",
      "Iteration 11061 => Loss: 6.74476532626571856355\n",
      "Iteration 11062 => Loss: 6.74475904860371766603\n",
      "Iteration 11063 => Loss: 6.74475277178762855357\n",
      "Iteration 11064 => Loss: 6.74474649581734198023\n",
      "Iteration 11065 => Loss: 6.74474022069274248281\n",
      "Iteration 11066 => Loss: 6.74473394641371104541\n",
      "Iteration 11067 => Loss: 6.74472767298015174475\n",
      "Iteration 11068 => Loss: 6.74472140039192602501\n",
      "Iteration 11069 => Loss: 6.74471512864894506833\n",
      "Iteration 11070 => Loss: 6.74470885775107031890\n",
      "Iteration 11071 => Loss: 6.74470258769820230071\n",
      "Iteration 11072 => Loss: 6.74469631849022821513\n",
      "Iteration 11073 => Loss: 6.74469005012702815804\n",
      "Iteration 11074 => Loss: 6.74468378260848577810\n",
      "Iteration 11075 => Loss: 6.74467751593450337566\n",
      "Iteration 11076 => Loss: 6.74467125010494505943\n",
      "Iteration 11077 => Loss: 6.74466498511970602436\n",
      "Iteration 11078 => Loss: 6.74465872097868679447\n",
      "Iteration 11079 => Loss: 6.74465245768175414298\n",
      "Iteration 11080 => Loss: 6.74464619522879704760\n",
      "Iteration 11081 => Loss: 6.74463993361971514418\n",
      "Iteration 11082 => Loss: 6.74463367285437875864\n",
      "Iteration 11083 => Loss: 6.74462741293267686871\n",
      "Iteration 11084 => Loss: 6.74462115385450911020\n",
      "Iteration 11085 => Loss: 6.74461489561975024998\n",
      "Iteration 11086 => Loss: 6.74460863822828926573\n",
      "Iteration 11087 => Loss: 6.74460238168001158243\n",
      "Iteration 11088 => Loss: 6.74459612597480262508\n",
      "Iteration 11089 => Loss: 6.74458987111255581226\n",
      "Iteration 11090 => Loss: 6.74458361709313702903\n",
      "Iteration 11091 => Loss: 6.74457736391646722751\n",
      "Iteration 11092 => Loss: 6.74457111158240163462\n",
      "Iteration 11093 => Loss: 6.74456486009083722166\n",
      "Iteration 11094 => Loss: 6.74455860944167184812\n",
      "Iteration 11095 => Loss: 6.74455235963477317540\n",
      "Iteration 11096 => Loss: 6.74454611067004172753\n",
      "Iteration 11097 => Loss: 6.74453986254735848860\n",
      "Iteration 11098 => Loss: 6.74453361526661243630\n",
      "Iteration 11099 => Loss: 6.74452736882767744930\n",
      "Iteration 11100 => Loss: 6.74452112323045938069\n",
      "Iteration 11101 => Loss: 6.74451487847483299731\n",
      "Iteration 11102 => Loss: 6.74450863456068727686\n",
      "Iteration 11103 => Loss: 6.74450239148791208521\n",
      "Iteration 11104 => Loss: 6.74449614925639462371\n",
      "Iteration 11105 => Loss: 6.74448990786600788283\n",
      "Iteration 11106 => Loss: 6.74448366731665682750\n",
      "Iteration 11107 => Loss: 6.74447742760821444818\n",
      "Iteration 11108 => Loss: 6.74447118874057593985\n",
      "Iteration 11109 => Loss: 6.74446495071363472107\n",
      "Iteration 11110 => Loss: 6.74445871352725934145\n",
      "Iteration 11111 => Loss: 6.74445247718133877868\n",
      "Iteration 11112 => Loss: 6.74444624167577977403\n",
      "Iteration 11113 => Loss: 6.74444000701045087709\n",
      "Iteration 11114 => Loss: 6.74443377318524550645\n",
      "Iteration 11115 => Loss: 6.74442754020004109350\n",
      "Iteration 11116 => Loss: 6.74442130805473372135\n",
      "Iteration 11117 => Loss: 6.74441507674921680859\n",
      "Iteration 11118 => Loss: 6.74440884628335712847\n",
      "Iteration 11119 => Loss: 6.74440261665706142225\n",
      "Iteration 11120 => Loss: 6.74439638787020534494\n",
      "Iteration 11121 => Loss: 6.74439015992268320332\n",
      "Iteration 11122 => Loss: 6.74438393281436887605\n",
      "Iteration 11123 => Loss: 6.74437770654516377533\n",
      "Iteration 11124 => Loss: 6.74437148111494533254\n",
      "Iteration 11125 => Loss: 6.74436525652361051897\n",
      "Iteration 11126 => Loss: 6.74435903277103321329\n",
      "Iteration 11127 => Loss: 6.74435280985711393953\n",
      "Iteration 11128 => Loss: 6.74434658778173012905\n",
      "Iteration 11129 => Loss: 6.74434036654476543049\n",
      "Iteration 11130 => Loss: 6.74433414614611770332\n",
      "Iteration 11131 => Loss: 6.74432792658566615529\n",
      "Iteration 11132 => Loss: 6.74432170786330775769\n",
      "Iteration 11133 => Loss: 6.74431548997891550101\n",
      "Iteration 11134 => Loss: 6.74430927293238635656\n",
      "Iteration 11135 => Loss: 6.74430305672360574931\n",
      "Iteration 11136 => Loss: 6.74429684135245999244\n",
      "Iteration 11137 => Loss: 6.74429062681883806363\n",
      "Iteration 11138 => Loss: 6.74428441312261739427\n",
      "Iteration 11139 => Loss: 6.74427820026369762019\n",
      "Iteration 11140 => Loss: 6.74427198824196416638\n",
      "Iteration 11141 => Loss: 6.74426577705729712875\n",
      "Iteration 11142 => Loss: 6.74425956670958726136\n",
      "Iteration 11143 => Loss: 6.74425335719872620643\n",
      "Iteration 11144 => Loss: 6.74424714852459228354\n",
      "Iteration 11145 => Loss: 6.74424094068707891125\n",
      "Iteration 11146 => Loss: 6.74423473368607595546\n",
      "Iteration 11147 => Loss: 6.74422852752145729482\n",
      "Iteration 11148 => Loss: 6.74422232219312967061\n",
      "Iteration 11149 => Loss: 6.74421611770096696148\n",
      "Iteration 11150 => Loss: 6.74420991404485814513\n",
      "Iteration 11151 => Loss: 6.74420371122469486380\n",
      "Iteration 11152 => Loss: 6.74419750924036254247\n",
      "Iteration 11153 => Loss: 6.74419130809175104702\n",
      "Iteration 11154 => Loss: 6.74418510777874313789\n",
      "Iteration 11155 => Loss: 6.74417890830122335188\n",
      "Iteration 11156 => Loss: 6.74417270965908421942\n",
      "Iteration 11157 => Loss: 6.74416651185221205367\n",
      "Iteration 11158 => Loss: 6.74416031488050560228\n",
      "Iteration 11159 => Loss: 6.74415411874382808577\n",
      "Iteration 11160 => Loss: 6.74414792344208802177\n",
      "Iteration 11161 => Loss: 6.74414172897515928895\n",
      "Iteration 11162 => Loss: 6.74413553534294329950\n",
      "Iteration 11163 => Loss: 6.74412934254531037936\n",
      "Iteration 11164 => Loss: 6.74412315058216815800\n",
      "Iteration 11165 => Loss: 6.74411695945339051406\n",
      "Iteration 11166 => Loss: 6.74411076915886464889\n",
      "Iteration 11167 => Loss: 6.74410457969848042836\n",
      "Iteration 11168 => Loss: 6.74409839107212860654\n",
      "Iteration 11169 => Loss: 6.74409220327969460840\n",
      "Iteration 11170 => Loss: 6.74408601632106208257\n",
      "Iteration 11171 => Loss: 6.74407983019612711217\n",
      "Iteration 11172 => Loss: 6.74407364490477245766\n",
      "Iteration 11173 => Loss: 6.74406746044688976127\n",
      "Iteration 11174 => Loss: 6.74406127682236089527\n",
      "Iteration 11175 => Loss: 6.74405509403107128463\n",
      "Iteration 11176 => Loss: 6.74404891207291701249\n",
      "Iteration 11177 => Loss: 6.74404273094778528019\n",
      "Iteration 11178 => Loss: 6.74403655065555263093\n",
      "Iteration 11179 => Loss: 6.74403037119611958872\n",
      "Iteration 11180 => Loss: 6.74402419256936891401\n",
      "Iteration 11181 => Loss: 6.74401801477519047268\n",
      "Iteration 11182 => Loss: 6.74401183781346080792\n",
      "Iteration 11183 => Loss: 6.74400566168409110190\n",
      "Iteration 11184 => Loss: 6.74399948638694901604\n",
      "Iteration 11185 => Loss: 6.74399331192192885709\n",
      "Iteration 11186 => Loss: 6.74398713828892315547\n",
      "Iteration 11187 => Loss: 6.74398096548780490167\n",
      "Iteration 11188 => Loss: 6.74397479351847994877\n",
      "Iteration 11189 => Loss: 6.74396862238082128727\n",
      "Iteration 11190 => Loss: 6.74396245207473121752\n",
      "Iteration 11191 => Loss: 6.74395628260008717092\n",
      "Iteration 11192 => Loss: 6.74395011395678167787\n",
      "Iteration 11193 => Loss: 6.74394394614470282789\n",
      "Iteration 11194 => Loss: 6.74393777916372894055\n",
      "Iteration 11195 => Loss: 6.74393161301376764527\n",
      "Iteration 11196 => Loss: 6.74392544769468749166\n",
      "Iteration 11197 => Loss: 6.74391928320638545102\n",
      "Iteration 11198 => Loss: 6.74391311954875316559\n",
      "Iteration 11199 => Loss: 6.74390695672166984309\n",
      "Iteration 11200 => Loss: 6.74390079472502623759\n",
      "Iteration 11201 => Loss: 6.74389463355872198491\n",
      "Iteration 11202 => Loss: 6.74388847322262385831\n",
      "Iteration 11203 => Loss: 6.74388231371663771085\n",
      "Iteration 11204 => Loss: 6.74387615504064363847\n",
      "Iteration 11205 => Loss: 6.74386999719453417157\n",
      "Iteration 11206 => Loss: 6.74386384017819207060\n",
      "Iteration 11207 => Loss: 6.74385768399150453689\n",
      "Iteration 11208 => Loss: 6.74385152863437653536\n",
      "Iteration 11209 => Loss: 6.74384537410666862201\n",
      "Iteration 11210 => Loss: 6.74383922040829286715\n",
      "Iteration 11211 => Loss: 6.74383306753912581399\n",
      "Iteration 11212 => Loss: 6.74382691549905821660\n",
      "Iteration 11213 => Loss: 6.74382076428797994083\n",
      "Iteration 11214 => Loss: 6.74381461390577552351\n",
      "Iteration 11215 => Loss: 6.74380846435233571867\n",
      "Iteration 11216 => Loss: 6.74380231562755216856\n",
      "Iteration 11217 => Loss: 6.74379616773130408092\n",
      "Iteration 11218 => Loss: 6.74379002066349197975\n",
      "Iteration 11219 => Loss: 6.74378387442398796736\n",
      "Iteration 11220 => Loss: 6.74377772901270322592\n",
      "Iteration 11221 => Loss: 6.74377158442950008777\n",
      "Iteration 11222 => Loss: 6.74376544067428884688\n",
      "Iteration 11223 => Loss: 6.74375929774694959917\n",
      "Iteration 11224 => Loss: 6.74375315564737043417\n",
      "Iteration 11225 => Loss: 6.74374701437543411231\n",
      "Iteration 11226 => Loss: 6.74374087393103494037\n",
      "Iteration 11227 => Loss: 6.74373473431406100786\n",
      "Iteration 11228 => Loss: 6.74372859552440573339\n",
      "Iteration 11229 => Loss: 6.74372245756195276556\n",
      "Iteration 11230 => Loss: 6.74371632042658575301\n",
      "Iteration 11231 => Loss: 6.74371018411819811433\n",
      "Iteration 11232 => Loss: 6.74370404863668770901\n",
      "Iteration 11233 => Loss: 6.74369791398192397480\n",
      "Iteration 11234 => Loss: 6.74369178015380832392\n",
      "Iteration 11235 => Loss: 6.74368564715222884587\n",
      "Iteration 11236 => Loss: 6.74367951497706652475\n",
      "Iteration 11237 => Loss: 6.74367338362821922004\n",
      "Iteration 11238 => Loss: 6.74366725310557413309\n",
      "Iteration 11239 => Loss: 6.74366112340901668887\n",
      "Iteration 11240 => Loss: 6.74365499453843586508\n",
      "Iteration 11241 => Loss: 6.74364886649371797489\n",
      "Iteration 11242 => Loss: 6.74364273927474844328\n",
      "Iteration 11243 => Loss: 6.74363661288143667605\n",
      "Iteration 11244 => Loss: 6.74363048731364855826\n",
      "Iteration 11245 => Loss: 6.74362436257128283756\n",
      "Iteration 11246 => Loss: 6.74361823865422760349\n",
      "Iteration 11247 => Loss: 6.74361211556237094555\n",
      "Iteration 11248 => Loss: 6.74360599329559295967\n",
      "Iteration 11249 => Loss: 6.74359987185380038710\n",
      "Iteration 11250 => Loss: 6.74359375123686266562\n",
      "Iteration 11251 => Loss: 6.74358763144468831285\n",
      "Iteration 11252 => Loss: 6.74358151247715476018\n",
      "Iteration 11253 => Loss: 6.74357539433415009711\n",
      "Iteration 11254 => Loss: 6.74356927701556685406\n",
      "Iteration 11255 => Loss: 6.74356316052130200234\n",
      "Iteration 11256 => Loss: 6.74355704485122675607\n",
      "Iteration 11257 => Loss: 6.74355093000523098112\n",
      "Iteration 11258 => Loss: 6.74354481598321697788\n",
      "Iteration 11259 => Loss: 6.74353870278507372404\n",
      "Iteration 11260 => Loss: 6.74353259041067776280\n",
      "Iteration 11261 => Loss: 6.74352647885992606547\n",
      "Iteration 11262 => Loss: 6.74352036813270938609\n",
      "Iteration 11263 => Loss: 6.74351425822890959694\n",
      "Iteration 11264 => Loss: 6.74350814914842100478\n",
      "Iteration 11265 => Loss: 6.74350204089113614003\n",
      "Iteration 11266 => Loss: 6.74349593345693332225\n",
      "Iteration 11267 => Loss: 6.74348982684570952273\n",
      "Iteration 11268 => Loss: 6.74348372105734927828\n",
      "Iteration 11269 => Loss: 6.74347761609174956021\n",
      "Iteration 11270 => Loss: 6.74347151194879135261\n",
      "Iteration 11271 => Loss: 6.74346540862837073860\n",
      "Iteration 11272 => Loss: 6.74345930613037047863\n",
      "Iteration 11273 => Loss: 6.74345320445467955039\n",
      "Iteration 11274 => Loss: 6.74344710360119581338\n",
      "Iteration 11275 => Loss: 6.74344100356979581079\n",
      "Iteration 11276 => Loss: 6.74343490436037917846\n",
      "Iteration 11277 => Loss: 6.74342880597283489408\n",
      "Iteration 11278 => Loss: 6.74342270840704305357\n",
      "Iteration 11279 => Loss: 6.74341661166289618734\n",
      "Iteration 11280 => Loss: 6.74341051574029304305\n",
      "Iteration 11281 => Loss: 6.74340442063911460480\n",
      "Iteration 11282 => Loss: 6.74339832635925162663\n",
      "Iteration 11283 => Loss: 6.74339223290059130989\n",
      "Iteration 11284 => Loss: 6.74338614026302796134\n",
      "Iteration 11285 => Loss: 6.74338004844644256508\n",
      "Iteration 11286 => Loss: 6.74337395745073386877\n",
      "Iteration 11287 => Loss: 6.74336786727578996192\n",
      "Iteration 11288 => Loss: 6.74336177792149182864\n",
      "Iteration 11289 => Loss: 6.74335568938773821657\n",
      "Iteration 11290 => Loss: 6.74334960167441366252\n",
      "Iteration 11291 => Loss: 6.74334351478141069691\n",
      "Iteration 11292 => Loss: 6.74333742870861740926\n",
      "Iteration 11293 => Loss: 6.74333134345591744818\n",
      "Iteration 11294 => Loss: 6.74332525902321044953\n",
      "Iteration 11295 => Loss: 6.74331917541038006192\n",
      "Iteration 11296 => Loss: 6.74331309261731615123\n",
      "Iteration 11297 => Loss: 6.74330701064391302424\n",
      "Iteration 11298 => Loss: 6.74330092949005610592\n",
      "Iteration 11299 => Loss: 6.74329484915562549219\n",
      "Iteration 11300 => Loss: 6.74328876964052703613\n",
      "Iteration 11301 => Loss: 6.74328269094464705091\n",
      "Iteration 11302 => Loss: 6.74327661306786474427\n",
      "Iteration 11303 => Loss: 6.74327053601008241657\n",
      "Iteration 11304 => Loss: 6.74326445977118016373\n",
      "Iteration 11305 => Loss: 6.74325838435105229252\n",
      "Iteration 11306 => Loss: 6.74325230974959488606\n",
      "Iteration 11307 => Loss: 6.74324623596668359937\n",
      "Iteration 11308 => Loss: 6.74324016300221806830\n",
      "Iteration 11309 => Loss: 6.74323409085608194147\n",
      "Iteration 11310 => Loss: 6.74322801952816952564\n",
      "Iteration 11311 => Loss: 6.74322194901837157488\n",
      "Iteration 11312 => Loss: 6.74321587932656996145\n",
      "Iteration 11313 => Loss: 6.74320981045266609755\n",
      "Iteration 11314 => Loss: 6.74320374239653652637\n",
      "Iteration 11315 => Loss: 6.74319767515807821923\n",
      "Iteration 11316 => Loss: 6.74319160873718725924\n",
      "Iteration 11317 => Loss: 6.74318554313374818321\n",
      "Iteration 11318 => Loss: 6.74317947834763842252\n",
      "Iteration 11319 => Loss: 6.74317341437877004751\n",
      "Iteration 11320 => Loss: 6.74316735122701604865\n",
      "Iteration 11321 => Loss: 6.74316128889226718002\n",
      "Iteration 11322 => Loss: 6.74315522737442751833\n",
      "Iteration 11323 => Loss: 6.74314916667337893585\n",
      "Iteration 11324 => Loss: 6.74314310678900152851\n",
      "Iteration 11325 => Loss: 6.74313704772119759667\n",
      "Iteration 11326 => Loss: 6.74313098946985967075\n",
      "Iteration 11327 => Loss: 6.74312493203486518212\n",
      "Iteration 11328 => Loss: 6.74311887541611199026\n",
      "Iteration 11329 => Loss: 6.74311281961349084924\n",
      "Iteration 11330 => Loss: 6.74310676462688718402\n",
      "Iteration 11331 => Loss: 6.74310071045619530139\n",
      "Iteration 11332 => Loss: 6.74309465710130062632\n",
      "Iteration 11333 => Loss: 6.74308860456210013012\n",
      "Iteration 11334 => Loss: 6.74308255283847390871\n",
      "Iteration 11335 => Loss: 6.74307650193032603880\n",
      "Iteration 11336 => Loss: 6.74307045183753039908\n",
      "Iteration 11337 => Loss: 6.74306440255999017808\n",
      "Iteration 11338 => Loss: 6.74305835409758369536\n",
      "Iteration 11339 => Loss: 6.74305230645021413949\n",
      "Iteration 11340 => Loss: 6.74304625961776871179\n",
      "Iteration 11341 => Loss: 6.74304021360012750819\n",
      "Iteration 11342 => Loss: 6.74303416839718927633\n",
      "Iteration 11343 => Loss: 6.74302812400884299393\n",
      "Iteration 11344 => Loss: 6.74302208043498296774\n",
      "Iteration 11345 => Loss: 6.74301603767549107005\n",
      "Iteration 11346 => Loss: 6.74300999573025805489\n",
      "Iteration 11347 => Loss: 6.74300395459918355812\n",
      "Iteration 11348 => Loss: 6.74299791428214678746\n",
      "Iteration 11349 => Loss: 6.74299187477904649057\n",
      "Iteration 11350 => Loss: 6.74298583608977253334\n",
      "Iteration 11351 => Loss: 6.74297979821420589985\n",
      "Iteration 11352 => Loss: 6.74297376115224622595\n",
      "Iteration 11353 => Loss: 6.74296772490378426568\n",
      "Iteration 11354 => Loss: 6.74296168946870366767\n",
      "Iteration 11355 => Loss: 6.74295565484689607416\n",
      "Iteration 11356 => Loss: 6.74294962103825756827\n",
      "Iteration 11357 => Loss: 6.74294358804267801588\n",
      "Iteration 11358 => Loss: 6.74293755586004461833\n",
      "Iteration 11359 => Loss: 6.74293152449024457695\n",
      "Iteration 11360 => Loss: 6.74292549393316953399\n",
      "Iteration 11361 => Loss: 6.74291946418872090163\n",
      "Iteration 11362 => Loss: 6.74291343525677699944\n",
      "Iteration 11363 => Loss: 6.74290740713722680511\n",
      "Iteration 11364 => Loss: 6.74290137982997439536\n",
      "Iteration 11365 => Loss: 6.74289535333489631341\n",
      "Iteration 11366 => Loss: 6.74288932765188953056\n",
      "Iteration 11367 => Loss: 6.74288330278085012992\n",
      "Iteration 11368 => Loss: 6.74287727872165554288\n",
      "Iteration 11369 => Loss: 6.74287125547420185256\n",
      "Iteration 11370 => Loss: 6.74286523303838603027\n",
      "Iteration 11371 => Loss: 6.74285921141409350099\n",
      "Iteration 11372 => Loss: 6.74285319060122123602\n",
      "Iteration 11373 => Loss: 6.74284717059964400221\n",
      "Iteration 11374 => Loss: 6.74284115140926409993\n",
      "Iteration 11375 => Loss: 6.74283513302997494776\n",
      "Iteration 11376 => Loss: 6.74282911546165930616\n",
      "Iteration 11377 => Loss: 6.74282309870420970555\n",
      "Iteration 11378 => Loss: 6.74281708275752489357\n",
      "Iteration 11379 => Loss: 6.74281106762148318978\n",
      "Iteration 11380 => Loss: 6.74280505329598689457\n",
      "Iteration 11381 => Loss: 6.74279903978092143291\n",
      "Iteration 11382 => Loss: 6.74279302707617134161\n",
      "Iteration 11383 => Loss: 6.74278701518163980921\n",
      "Iteration 11384 => Loss: 6.74278100409720781983\n",
      "Iteration 11385 => Loss: 6.74277499382276879203\n",
      "Iteration 11386 => Loss: 6.74276898435822324984\n",
      "Iteration 11387 => Loss: 6.74276297570344063104\n",
      "Iteration 11388 => Loss: 6.74275696785833122959\n",
      "Iteration 11389 => Loss: 6.74275096082278579956\n",
      "Iteration 11390 => Loss: 6.74274495459668266051\n",
      "Iteration 11391 => Loss: 6.74273894917992144826\n",
      "Iteration 11392 => Loss: 6.74273294457238581145\n",
      "Iteration 11393 => Loss: 6.74272694077397538592\n",
      "Iteration 11394 => Loss: 6.74272093778457470847\n",
      "Iteration 11395 => Loss: 6.74271493560407453316\n",
      "Iteration 11396 => Loss: 6.74270893423237627218\n",
      "Iteration 11397 => Loss: 6.74270293366935824508\n",
      "Iteration 11398 => Loss: 6.74269693391492452861\n",
      "Iteration 11399 => Loss: 6.74269093496894722506\n",
      "Iteration 11400 => Loss: 6.74268493683133307570\n",
      "Iteration 11401 => Loss: 6.74267893950196839370\n",
      "Iteration 11402 => Loss: 6.74267294298074570946\n",
      "Iteration 11403 => Loss: 6.74266694726755044798\n",
      "Iteration 11404 => Loss: 6.74266095236228135690\n",
      "Iteration 11405 => Loss: 6.74265495826482386121\n",
      "Iteration 11406 => Loss: 6.74264896497507137951\n",
      "Iteration 11407 => Loss: 6.74264297249291821856\n",
      "Iteration 11408 => Loss: 6.74263698081825779695\n",
      "Iteration 11409 => Loss: 6.74263098995096576971\n",
      "Iteration 11410 => Loss: 6.74262499989094887809\n",
      "Iteration 11411 => Loss: 6.74261901063808988255\n",
      "Iteration 11412 => Loss: 6.74261302219227953714\n",
      "Iteration 11413 => Loss: 6.74260703455342103041\n",
      "Iteration 11414 => Loss: 6.74260104772139357010\n",
      "Iteration 11415 => Loss: 6.74259506169609323933\n",
      "Iteration 11416 => Loss: 6.74258907647741434488\n",
      "Iteration 11417 => Loss: 6.74258309206523076540\n",
      "Iteration 11418 => Loss: 6.74257710845946345302\n",
      "Iteration 11419 => Loss: 6.74257112565997918097\n",
      "Iteration 11420 => Loss: 6.74256514366667669691\n",
      "Iteration 11421 => Loss: 6.74255916247944941944\n",
      "Iteration 11422 => Loss: 6.74255318209818987896\n",
      "Iteration 11423 => Loss: 6.74254720252278616499\n",
      "Iteration 11424 => Loss: 6.74254122375312547888\n",
      "Iteration 11425 => Loss: 6.74253524578911189735\n",
      "Iteration 11426 => Loss: 6.74252926863062906904\n",
      "Iteration 11427 => Loss: 6.74252329227756508345\n",
      "Iteration 11428 => Loss: 6.74251731672981602372\n",
      "Iteration 11429 => Loss: 6.74251134198726997937\n",
      "Iteration 11430 => Loss: 6.74250536804982214534\n",
      "Iteration 11431 => Loss: 6.74249939491736594022\n",
      "Iteration 11432 => Loss: 6.74249342258979122988\n",
      "Iteration 11433 => Loss: 6.74248745106698432750\n",
      "Iteration 11434 => Loss: 6.74248148034884486890\n",
      "Iteration 11435 => Loss: 6.74247551043525827907\n",
      "Iteration 11436 => Loss: 6.74246954132611264754\n",
      "Iteration 11437 => Loss: 6.74246357302130938649\n",
      "Iteration 11438 => Loss: 6.74245760552073924998\n",
      "Iteration 11439 => Loss: 6.74245163882428233393\n",
      "Iteration 11440 => Loss: 6.74244567293184449142\n",
      "Iteration 11441 => Loss: 6.74243970784330493018\n",
      "Iteration 11442 => Loss: 6.74243374355856772695\n",
      "Iteration 11443 => Loss: 6.74242778007751564218\n",
      "Iteration 11444 => Loss: 6.74242181740004120627\n",
      "Iteration 11445 => Loss: 6.74241585552604139053\n",
      "Iteration 11446 => Loss: 6.74240989445540339631\n",
      "Iteration 11447 => Loss: 6.74240393418801708947\n",
      "Iteration 11448 => Loss: 6.74239797472378210585\n",
      "Iteration 11449 => Loss: 6.74239201606258298227\n",
      "Iteration 11450 => Loss: 6.74238605820431580185\n",
      "Iteration 11451 => Loss: 6.74238010114886954227\n",
      "Iteration 11452 => Loss: 6.74237414489613584578\n",
      "Iteration 11453 => Loss: 6.74236818944600369008\n",
      "Iteration 11454 => Loss: 6.74236223479837892825\n",
      "Iteration 11455 => Loss: 6.74235628095313987984\n",
      "Iteration 11456 => Loss: 6.74235032791017729892\n",
      "Iteration 11457 => Loss: 6.74234437566939615039\n",
      "Iteration 11458 => Loss: 6.74233842423066853655\n",
      "Iteration 11459 => Loss: 6.74233247359390652775\n",
      "Iteration 11460 => Loss: 6.74232652375899110808\n",
      "Iteration 11461 => Loss: 6.74232057472581214341\n",
      "Iteration 11462 => Loss: 6.74231462649426482869\n",
      "Iteration 11463 => Loss: 6.74230867906425057612\n",
      "Iteration 11464 => Loss: 6.74230273243564770524\n",
      "Iteration 11465 => Loss: 6.74229678660836295734\n",
      "Iteration 11466 => Loss: 6.74229084158226665835\n",
      "Iteration 11467 => Loss: 6.74228489735726288501\n",
      "Iteration 11468 => Loss: 6.74227895393325393769\n",
      "Iteration 11469 => Loss: 6.74227301131011280688\n",
      "Iteration 11470 => Loss: 6.74226706948774623385\n",
      "Iteration 11471 => Loss: 6.74226112846603697903\n",
      "Iteration 11472 => Loss: 6.74225518824488201375\n",
      "Iteration 11473 => Loss: 6.74224924882417564476\n",
      "Iteration 11474 => Loss: 6.74224331020380240886\n",
      "Iteration 11475 => Loss: 6.74223737238366549462\n",
      "Iteration 11476 => Loss: 6.74223143536363878070\n",
      "Iteration 11477 => Loss: 6.74222549914363256107\n",
      "Iteration 11478 => Loss: 6.74221956372353492526\n",
      "Iteration 11479 => Loss: 6.74221362910322774553\n",
      "Iteration 11480 => Loss: 6.74220769528261776316\n",
      "Iteration 11481 => Loss: 6.74220176226158773858\n",
      "Iteration 11482 => Loss: 6.74219583004003464310\n",
      "Iteration 11483 => Loss: 6.74218989861785189532\n",
      "Iteration 11484 => Loss: 6.74218396799492136751\n",
      "Iteration 11485 => Loss: 6.74217803817114713638\n",
      "Iteration 11486 => Loss: 6.74217210914641729147\n",
      "Iteration 11487 => Loss: 6.74216618092061903411\n",
      "Iteration 11488 => Loss: 6.74216025349365644104\n",
      "Iteration 11489 => Loss: 6.74215432686540960816\n",
      "Iteration 11490 => Loss: 6.74214840103577905950\n",
      "Iteration 11491 => Loss: 6.74214247600465821364\n",
      "Iteration 11492 => Loss: 6.74213655177193071921\n",
      "Iteration 11493 => Loss: 6.74213062833749710023\n",
      "Iteration 11494 => Loss: 6.74212470570124011715\n",
      "Iteration 11495 => Loss: 6.74211878386307095212\n",
      "Iteration 11496 => Loss: 6.74211286282285637839\n",
      "Iteration 11497 => Loss: 6.74210694258050935446\n",
      "Iteration 11498 => Loss: 6.74210102313591530532\n",
      "Iteration 11499 => Loss: 6.74209510448896676138\n",
      "Iteration 11500 => Loss: 6.74208918663955536488\n",
      "Iteration 11501 => Loss: 6.74208326958757808711\n",
      "Iteration 11502 => Loss: 6.74207735333291768853\n",
      "Iteration 11503 => Loss: 6.74207143787548091041\n",
      "Iteration 11504 => Loss: 6.74206552321514873682\n",
      "Iteration 11505 => Loss: 6.74205960935181458638\n",
      "Iteration 11506 => Loss: 6.74205369628537720672\n",
      "Iteration 11507 => Loss: 6.74204778401572379920\n",
      "Iteration 11508 => Loss: 6.74204187254275133512\n",
      "Iteration 11509 => Loss: 6.74203596186634790399\n",
      "Iteration 11510 => Loss: 6.74203005198640958895\n",
      "Iteration 11511 => Loss: 6.74202414290282980858\n",
      "Iteration 11512 => Loss: 6.74201823461549931693\n",
      "Iteration 11513 => Loss: 6.74201232712430886806\n",
      "Iteration 11514 => Loss: 6.74200642042915454510\n",
      "Iteration 11515 => Loss: 6.74200051452992887846\n",
      "Iteration 11516 => Loss: 6.74199460942652262219\n",
      "Iteration 11517 => Loss: 6.74198870511883185941\n",
      "Iteration 11518 => Loss: 6.74198280160674201511\n",
      "Iteration 11519 => Loss: 6.74197689889015361331\n",
      "Iteration 11520 => Loss: 6.74197099696895296717\n",
      "Iteration 11521 => Loss: 6.74196509584304592977\n",
      "Iteration 11522 => Loss: 6.74195919551230993250\n",
      "Iteration 11523 => Loss: 6.74195329597664549937\n",
      "Iteration 11524 => Loss: 6.74194739723593894354\n",
      "Iteration 11525 => Loss: 6.74194149929009078903\n",
      "Iteration 11526 => Loss: 6.74193560213899356626\n",
      "Iteration 11527 => Loss: 6.74192970578253536473\n",
      "Iteration 11528 => Loss: 6.74192381022061848483\n",
      "Iteration 11529 => Loss: 6.74191791545312124612\n",
      "Iteration 11530 => Loss: 6.74191202147994328442\n",
      "Iteration 11531 => Loss: 6.74190612830098601194\n",
      "Iteration 11532 => Loss: 6.74190023591613396547\n",
      "Iteration 11533 => Loss: 6.74189434432527257002\n",
      "Iteration 11534 => Loss: 6.74188845352831034319\n",
      "Iteration 11535 => Loss: 6.74188256352513715086\n",
      "Iteration 11536 => Loss: 6.74187667431563752984\n",
      "Iteration 11537 => Loss: 6.74187078589970845144\n",
      "Iteration 11538 => Loss: 6.74186489827724422241\n",
      "Iteration 11539 => Loss: 6.74185901144813914954\n",
      "Iteration 11540 => Loss: 6.74185312541228221050\n",
      "Iteration 11541 => Loss: 6.74184724016957215298\n",
      "Iteration 11542 => Loss: 6.74184135571990061919\n",
      "Iteration 11543 => Loss: 6.74183547206315036959\n",
      "Iteration 11544 => Loss: 6.74182958919923702723\n",
      "Iteration 11545 => Loss: 6.74182370712802558899\n",
      "Iteration 11546 => Loss: 6.74181782584942901337\n",
      "Iteration 11547 => Loss: 6.74181194536334249534\n",
      "Iteration 11548 => Loss: 6.74180606566964524262\n",
      "Iteration 11549 => Loss: 6.74180018676824044377\n",
      "Iteration 11550 => Loss: 6.74179430865901352377\n",
      "Iteration 11551 => Loss: 6.74178843134186056574\n",
      "Iteration 11552 => Loss: 6.74178255481668564641\n",
      "Iteration 11553 => Loss: 6.74177667908336442082\n",
      "Iteration 11554 => Loss: 6.74177080414180007750\n",
      "Iteration 11555 => Loss: 6.74176492999188958777\n",
      "Iteration 11556 => Loss: 6.74175905663351837660\n",
      "Iteration 11557 => Loss: 6.74175318406657897441\n",
      "Iteration 11558 => Loss: 6.74174731229097456975\n",
      "Iteration 11559 => Loss: 6.74174144130658881124\n",
      "Iteration 11560 => Loss: 6.74173557111331867020\n",
      "Iteration 11561 => Loss: 6.74172970171106022974\n",
      "Iteration 11562 => Loss: 6.74172383309970602028\n",
      "Iteration 11563 => Loss: 6.74171796527913791408\n",
      "Iteration 11564 => Loss: 6.74171209824926709331\n",
      "Iteration 11565 => Loss: 6.74170623200997543023\n",
      "Iteration 11566 => Loss: 6.74170036656116433704\n",
      "Iteration 11567 => Loss: 6.74169450190272012691\n",
      "Iteration 11568 => Loss: 6.74168863803453799477\n",
      "Iteration 11569 => Loss: 6.74168277495651135922\n",
      "Iteration 11570 => Loss: 6.74167691266854340881\n",
      "Iteration 11571 => Loss: 6.74167105117051068675\n",
      "Iteration 11572 => Loss: 6.74166519046231638157\n",
      "Iteration 11573 => Loss: 6.74165933054385568823\n",
      "Iteration 11574 => Loss: 6.74165347141502291350\n",
      "Iteration 11575 => Loss: 6.74164761307569637694\n",
      "Iteration 11576 => Loss: 6.74164175552579258976\n",
      "Iteration 11577 => Loss: 6.74163589876518543065\n",
      "Iteration 11578 => Loss: 6.74163004279378341721\n",
      "Iteration 11579 => Loss: 6.74162418761147019808\n",
      "Iteration 11580 => Loss: 6.74161833321814807363\n",
      "Iteration 11581 => Loss: 6.74161247961369891613\n",
      "Iteration 11582 => Loss: 6.74160662679803301955\n",
      "Iteration 11583 => Loss: 6.74160077477102959165\n",
      "Iteration 11584 => Loss: 6.74159492353258471553\n",
      "Iteration 11585 => Loss: 6.74158907308259536251\n",
      "Iteration 11586 => Loss: 6.74158322342095672752\n",
      "Iteration 11587 => Loss: 6.74157737454756045281\n",
      "Iteration 11588 => Loss: 6.74157152646229995696\n",
      "Iteration 11589 => Loss: 6.74156567916506421767\n",
      "Iteration 11590 => Loss: 6.74155983265576441710\n",
      "Iteration 11591 => Loss: 6.74155398693427176937\n",
      "Iteration 11592 => Loss: 6.74154814200049390394\n",
      "Iteration 11593 => Loss: 6.74154229785432157485\n",
      "Iteration 11594 => Loss: 6.74153645449564553616\n",
      "Iteration 11595 => Loss: 6.74153061192436720006\n",
      "Iteration 11596 => Loss: 6.74152477014037287972\n",
      "Iteration 11597 => Loss: 6.74151892914356754005\n",
      "Iteration 11598 => Loss: 6.74151308893382417153\n",
      "Iteration 11599 => Loss: 6.74150724951105395633\n",
      "Iteration 11600 => Loss: 6.74150141087515386573\n",
      "Iteration 11601 => Loss: 6.74149557302600310749\n",
      "Iteration 11602 => Loss: 6.74148973596349865289\n",
      "Iteration 11603 => Loss: 6.74148389968754369050\n",
      "Iteration 11604 => Loss: 6.74147806419803252709\n",
      "Iteration 11605 => Loss: 6.74147222949484703491\n",
      "Iteration 11606 => Loss: 6.74146639557789395525\n",
      "Iteration 11607 => Loss: 6.74146056244705160765\n",
      "Iteration 11608 => Loss: 6.74145473010223472698\n",
      "Iteration 11609 => Loss: 6.74144889854332607371\n",
      "Iteration 11610 => Loss: 6.74144306777021018462\n",
      "Iteration 11611 => Loss: 6.74143723778279913006\n",
      "Iteration 11612 => Loss: 6.74143140858098011137\n",
      "Iteration 11613 => Loss: 6.74142558016464477078\n",
      "Iteration 11614 => Loss: 6.74141975253368652687\n",
      "Iteration 11615 => Loss: 6.74141392568800235097\n",
      "Iteration 11616 => Loss: 6.74140809962749010253\n",
      "Iteration 11617 => Loss: 6.74140227435204231199\n",
      "Iteration 11618 => Loss: 6.74139644986154351614\n",
      "Iteration 11619 => Loss: 6.74139062615589246263\n",
      "Iteration 11620 => Loss: 6.74138480323499411639\n",
      "Iteration 11621 => Loss: 6.74137898109873745511\n",
      "Iteration 11622 => Loss: 6.74137315974701145649\n",
      "Iteration 11623 => Loss: 6.74136733917970598640\n",
      "Iteration 11624 => Loss: 6.74136151939672956246\n",
      "Iteration 11625 => Loss: 6.74135570039796494513\n",
      "Iteration 11626 => Loss: 6.74134988218331532295\n",
      "Iteration 11627 => Loss: 6.74134406475267233816\n",
      "Iteration 11628 => Loss: 6.74133824810592052756\n",
      "Iteration 11629 => Loss: 6.74133243224296663243\n",
      "Iteration 11630 => Loss: 6.74132661716370584770\n",
      "Iteration 11631 => Loss: 6.74132080286801560476\n",
      "Iteration 11632 => Loss: 6.74131498935580797394\n",
      "Iteration 11633 => Loss: 6.74130917662697370929\n",
      "Iteration 11634 => Loss: 6.74130336468140800577\n",
      "Iteration 11635 => Loss: 6.74129755351899540017\n",
      "Iteration 11636 => Loss: 6.74129174313963819287\n",
      "Iteration 11637 => Loss: 6.74128593354322980247\n",
      "Iteration 11638 => Loss: 6.74128012472967075297\n",
      "Iteration 11639 => Loss: 6.74127431669884824572\n",
      "Iteration 11640 => Loss: 6.74126850945065125842\n",
      "Iteration 11641 => Loss: 6.74126270298498830869\n",
      "Iteration 11642 => Loss: 6.74125689730174926240\n",
      "Iteration 11643 => Loss: 6.74125109240081865636\n",
      "Iteration 11644 => Loss: 6.74124528828210678455\n",
      "Iteration 11645 => Loss: 6.74123948494549374288\n",
      "Iteration 11646 => Loss: 6.74123368239088538445\n",
      "Iteration 11647 => Loss: 6.74122788061816713423\n",
      "Iteration 11648 => Loss: 6.74122207962724573349\n",
      "Iteration 11649 => Loss: 6.74121627941799861361\n",
      "Iteration 11650 => Loss: 6.74121047999033251585\n",
      "Iteration 11651 => Loss: 6.74120468134414352335\n",
      "Iteration 11652 => Loss: 6.74119888347932416650\n",
      "Iteration 11653 => Loss: 6.74119308639576342301\n",
      "Iteration 11654 => Loss: 6.74118729009336359326\n",
      "Iteration 11655 => Loss: 6.74118149457200921404\n",
      "Iteration 11656 => Loss: 6.74117569983160969116\n",
      "Iteration 11657 => Loss: 6.74116990587204423235\n",
      "Iteration 11658 => Loss: 6.74116411269322313160\n",
      "Iteration 11659 => Loss: 6.74115832029503092571\n",
      "Iteration 11660 => Loss: 6.74115252867735836872\n",
      "Iteration 11661 => Loss: 6.74114673784011309010\n",
      "Iteration 11662 => Loss: 6.74114094778318140300\n",
      "Iteration 11663 => Loss: 6.74113515850646471961\n",
      "Iteration 11664 => Loss: 6.74112937000984757674\n",
      "Iteration 11665 => Loss: 6.74112358229323760384\n",
      "Iteration 11666 => Loss: 6.74111779535651312045\n",
      "Iteration 11667 => Loss: 6.74111200919958619693\n",
      "Iteration 11668 => Loss: 6.74110622382234048189\n",
      "Iteration 11669 => Loss: 6.74110043922467561117\n",
      "Iteration 11670 => Loss: 6.74109465540649033244\n",
      "Iteration 11671 => Loss: 6.74108887236767184703\n",
      "Iteration 11672 => Loss: 6.74108309010811979078\n",
      "Iteration 11673 => Loss: 6.74107730862772314140\n",
      "Iteration 11674 => Loss: 6.74107152792638242289\n",
      "Iteration 11675 => Loss: 6.74106574800399638292\n",
      "Iteration 11676 => Loss: 6.74105996886045311101\n",
      "Iteration 11677 => Loss: 6.74105419049564780209\n",
      "Iteration 11678 => Loss: 6.74104841290947831567\n",
      "Iteration 11679 => Loss: 6.74104263610183807032\n",
      "Iteration 11680 => Loss: 6.74103686007262581370\n",
      "Iteration 11681 => Loss: 6.74103108482173674076\n",
      "Iteration 11682 => Loss: 6.74102531034905538831\n",
      "Iteration 11683 => Loss: 6.74101953665449027397\n",
      "Iteration 11684 => Loss: 6.74101376373792504637\n",
      "Iteration 11685 => Loss: 6.74100799159926911130\n",
      "Iteration 11686 => Loss: 6.74100222023840078833\n",
      "Iteration 11687 => Loss: 6.74099644965522770690\n",
      "Iteration 11688 => Loss: 6.74099067984964239741\n",
      "Iteration 11689 => Loss: 6.74098491082153827847\n",
      "Iteration 11690 => Loss: 6.74097914257081232137\n",
      "Iteration 11691 => Loss: 6.74097337509735261563\n",
      "Iteration 11692 => Loss: 6.74096760840107478430\n",
      "Iteration 11693 => Loss: 6.74096184248184648879\n",
      "Iteration 11694 => Loss: 6.74095607733957713492\n",
      "Iteration 11695 => Loss: 6.74095031297416014127\n",
      "Iteration 11696 => Loss: 6.74094454938549780820\n",
      "Iteration 11697 => Loss: 6.74093878657347911343\n",
      "Iteration 11698 => Loss: 6.74093302453799658736\n",
      "Iteration 11699 => Loss: 6.74092726327895075400\n",
      "Iteration 11700 => Loss: 6.74092150279623503195\n",
      "Iteration 11701 => Loss: 6.74091574308974372798\n",
      "Iteration 11702 => Loss: 6.74090998415937026067\n",
      "Iteration 11703 => Loss: 6.74090422600502492401\n",
      "Iteration 11704 => Loss: 6.74089846862657449122\n",
      "Iteration 11705 => Loss: 6.74089271202394346716\n",
      "Iteration 11706 => Loss: 6.74088695619701017137\n",
      "Iteration 11707 => Loss: 6.74088120114567956875\n",
      "Iteration 11708 => Loss: 6.74087544686983886066\n",
      "Iteration 11709 => Loss: 6.74086969336939034747\n",
      "Iteration 11710 => Loss: 6.74086394064422034234\n",
      "Iteration 11711 => Loss: 6.74085818869423558652\n",
      "Iteration 11712 => Loss: 6.74085243751932328138\n",
      "Iteration 11713 => Loss: 6.74084668711938306274\n",
      "Iteration 11714 => Loss: 6.74084093749431190190\n",
      "Iteration 11715 => Loss: 6.74083518864400144111\n",
      "Iteration 11716 => Loss: 6.74082944056834865165\n",
      "Iteration 11717 => Loss: 6.74082369326724517578\n",
      "Iteration 11718 => Loss: 6.74081794674060041928\n",
      "Iteration 11719 => Loss: 6.74081220098829092535\n",
      "Iteration 11720 => Loss: 6.74080645601023054070\n",
      "Iteration 11721 => Loss: 6.74080071180629758487\n",
      "Iteration 11722 => Loss: 6.74079496837640590456\n",
      "Iteration 11723 => Loss: 6.74078922572043737205\n",
      "Iteration 11724 => Loss: 6.74078348383829339951\n",
      "Iteration 11725 => Loss: 6.74077774272986740556\n",
      "Iteration 11726 => Loss: 6.74077200239505902601\n",
      "Iteration 11727 => Loss: 6.74076626283375457405\n",
      "Iteration 11728 => Loss: 6.74076052404586345546\n",
      "Iteration 11729 => Loss: 6.74075478603127464794\n",
      "Iteration 11730 => Loss: 6.74074904878987180012\n",
      "Iteration 11731 => Loss: 6.74074331232158208138\n",
      "Iteration 11732 => Loss: 6.74073757662626871223\n",
      "Iteration 11733 => Loss: 6.74073184170383932212\n",
      "Iteration 11734 => Loss: 6.74072610755419621142\n",
      "Iteration 11735 => Loss: 6.74072037417723102237\n",
      "Iteration 11736 => Loss: 6.74071464157283450902\n",
      "Iteration 11737 => Loss: 6.74070890974091430081\n",
      "Iteration 11738 => Loss: 6.74070317868134960548\n",
      "Iteration 11739 => Loss: 6.74069744839405249337\n",
      "Iteration 11740 => Loss: 6.74069171887890838946\n",
      "Iteration 11741 => Loss: 6.74068599013581781776\n",
      "Iteration 11742 => Loss: 6.74068026216467597322\n",
      "Iteration 11743 => Loss: 6.74067453496538426805\n",
      "Iteration 11744 => Loss: 6.74066880853782723904\n",
      "Iteration 11745 => Loss: 6.74066308288190629838\n",
      "Iteration 11746 => Loss: 6.74065735799751752921\n",
      "Iteration 11747 => Loss: 6.74065163388455879101\n",
      "Iteration 11748 => Loss: 6.74064591054292261418\n",
      "Iteration 11749 => Loss: 6.74064018797251041093\n",
      "Iteration 11750 => Loss: 6.74063446617321027077\n",
      "Iteration 11751 => Loss: 6.74062874514492982314\n",
      "Iteration 11752 => Loss: 6.74062302488755449303\n",
      "Iteration 11753 => Loss: 6.74061730540098658082\n",
      "Iteration 11754 => Loss: 6.74061158668511772873\n",
      "Iteration 11755 => Loss: 6.74060586873984757261\n",
      "Iteration 11756 => Loss: 6.74060015156507308376\n",
      "Iteration 11757 => Loss: 6.74059443516068501623\n",
      "Iteration 11758 => Loss: 6.74058871952657501225\n",
      "Iteration 11759 => Loss: 6.74058300466265869488\n",
      "Iteration 11760 => Loss: 6.74057729056881616003\n",
      "Iteration 11761 => Loss: 6.74057157724495947804\n",
      "Iteration 11762 => Loss: 6.74056586469095542213\n",
      "Iteration 11763 => Loss: 6.74056015290672583262\n",
      "Iteration 11764 => Loss: 6.74055444189216856898\n",
      "Iteration 11765 => Loss: 6.74054873164716106260\n",
      "Iteration 11766 => Loss: 6.74054302217160827837\n",
      "Iteration 11767 => Loss: 6.74053731346541074032\n",
      "Iteration 11768 => Loss: 6.74053160552846541975\n",
      "Iteration 11769 => Loss: 6.74052589836065596529\n",
      "Iteration 11770 => Loss: 6.74052019196189533545\n",
      "Iteration 11771 => Loss: 6.74051448633206629069\n",
      "Iteration 11772 => Loss: 6.74050878147107912497\n",
      "Iteration 11773 => Loss: 6.74050307737882015147\n",
      "Iteration 11774 => Loss: 6.74049737405518367694\n",
      "Iteration 11775 => Loss: 6.74049167150007555449\n",
      "Iteration 11776 => Loss: 6.74048596971338742634\n",
      "Iteration 11777 => Loss: 6.74048026869500827019\n",
      "Iteration 11778 => Loss: 6.74047456844485193272\n",
      "Iteration 11779 => Loss: 6.74046886896279939805\n",
      "Iteration 11780 => Loss: 6.74046317024875119017\n",
      "Iteration 11781 => Loss: 6.74045747230260605676\n",
      "Iteration 11782 => Loss: 6.74045177512426274546\n",
      "Iteration 11783 => Loss: 6.74044607871361201035\n",
      "Iteration 11784 => Loss: 6.74044038307055437542\n",
      "Iteration 11785 => Loss: 6.74043468819498059474\n",
      "Iteration 11786 => Loss: 6.74042899408679829776\n",
      "Iteration 11787 => Loss: 6.74042330074589735034\n",
      "Iteration 11788 => Loss: 6.74041760817216406565\n",
      "Iteration 11789 => Loss: 6.74041191636551761945\n",
      "Iteration 11790 => Loss: 6.74040622532583189042\n",
      "Iteration 11791 => Loss: 6.74040053505302516612\n",
      "Iteration 11792 => Loss: 6.74039484554697931884\n",
      "Iteration 11793 => Loss: 6.74038915680759398441\n",
      "Iteration 11794 => Loss: 6.74038346883476702232\n",
      "Iteration 11795 => Loss: 6.74037778162839451568\n",
      "Iteration 11796 => Loss: 6.74037209518836455402\n",
      "Iteration 11797 => Loss: 6.74036640951459453674\n",
      "Iteration 11798 => Loss: 6.74036072460696722430\n",
      "Iteration 11799 => Loss: 6.74035504046538225253\n",
      "Iteration 11800 => Loss: 6.74034935708973215185\n",
      "Iteration 11801 => Loss: 6.74034367447992277533\n",
      "Iteration 11802 => Loss: 6.74033799263584221251\n",
      "Iteration 11803 => Loss: 6.74033231155739187557\n",
      "Iteration 11804 => Loss: 6.74032663124446784764\n",
      "Iteration 11805 => Loss: 6.74032095169696354731\n",
      "Iteration 11806 => Loss: 6.74031527291477683406\n",
      "Iteration 11807 => Loss: 6.74030959489781711369\n",
      "Iteration 11808 => Loss: 6.74030391764595826487\n",
      "Iteration 11809 => Loss: 6.74029824115911502247\n",
      "Iteration 11810 => Loss: 6.74029256543717814054\n",
      "Iteration 11811 => Loss: 6.74028689048004459039\n",
      "Iteration 11812 => Loss: 6.74028121628761311968\n",
      "Iteration 11813 => Loss: 6.74027554285977625881\n",
      "Iteration 11814 => Loss: 6.74026987019643986088\n",
      "Iteration 11815 => Loss: 6.74026419829749379176\n",
      "Iteration 11816 => Loss: 6.74025852716283768729\n",
      "Iteration 11817 => Loss: 6.74025285679236407788\n",
      "Iteration 11818 => Loss: 6.74024718718596993483\n",
      "Iteration 11819 => Loss: 6.74024151834356199942\n",
      "Iteration 11820 => Loss: 6.74023585026502836115\n",
      "Iteration 11821 => Loss: 6.74023018295026865587\n",
      "Iteration 11822 => Loss: 6.74022451639918607214\n",
      "Iteration 11823 => Loss: 6.74021885061166692310\n",
      "Iteration 11824 => Loss: 6.74021318558760995643\n",
      "Iteration 11825 => Loss: 6.74020752132691924885\n",
      "Iteration 11826 => Loss: 6.74020185782948200170\n",
      "Iteration 11827 => Loss: 6.74019619509520939715\n",
      "Iteration 11828 => Loss: 6.74019053312398863653\n",
      "Iteration 11829 => Loss: 6.74018487191571313843\n",
      "Iteration 11830 => Loss: 6.74017921147029763773\n",
      "Iteration 11831 => Loss: 6.74017355178762045398\n",
      "Iteration 11832 => Loss: 6.74016789286757855848\n",
      "Iteration 11833 => Loss: 6.74016223471008579793\n",
      "Iteration 11834 => Loss: 6.74015657731503381456\n",
      "Iteration 11835 => Loss: 6.74015092068230980971\n",
      "Iteration 11836 => Loss: 6.74014526481181430739\n",
      "Iteration 11837 => Loss: 6.74013960970345848978\n",
      "Iteration 11838 => Loss: 6.74013395535711978823\n",
      "Iteration 11839 => Loss: 6.74012830177270494403\n",
      "Iteration 11840 => Loss: 6.74012264895011714572\n",
      "Iteration 11841 => Loss: 6.74011699688924537099\n",
      "Iteration 11842 => Loss: 6.74011134558998836752\n",
      "Iteration 11843 => Loss: 6.74010569505223955389\n",
      "Iteration 11844 => Loss: 6.74010004527591011225\n",
      "Iteration 11845 => Loss: 6.74009439626088635578\n",
      "Iteration 11846 => Loss: 6.74008874800705992669\n",
      "Iteration 11847 => Loss: 6.74008310051434378352\n",
      "Iteration 11848 => Loss: 6.74007745378262157487\n",
      "Iteration 11849 => Loss: 6.74007180781180448292\n",
      "Iteration 11850 => Loss: 6.74006616260178148536\n",
      "Iteration 11851 => Loss: 6.74006051815244244807\n",
      "Iteration 11852 => Loss: 6.74005487446370477045\n",
      "Iteration 11853 => Loss: 6.74004923153545032477\n",
      "Iteration 11854 => Loss: 6.74004358936757874687\n",
      "Iteration 11855 => Loss: 6.74003794795998878442\n",
      "Iteration 11856 => Loss: 6.74003230731257740871\n",
      "Iteration 11857 => Loss: 6.74002666742525047283\n",
      "Iteration 11858 => Loss: 6.74002102829790050720\n",
      "Iteration 11859 => Loss: 6.74001538993041915404\n",
      "Iteration 11860 => Loss: 6.74000975232270249649\n",
      "Iteration 11861 => Loss: 6.74000411547465816398\n",
      "Iteration 11862 => Loss: 6.73999847938618312782\n",
      "Iteration 11863 => Loss: 6.73999284405716458934\n",
      "Iteration 11864 => Loss: 6.73998720948751284254\n",
      "Iteration 11865 => Loss: 6.73998157567711864147\n",
      "Iteration 11866 => Loss: 6.73997594262588162195\n",
      "Iteration 11867 => Loss: 6.73997031033369875530\n",
      "Iteration 11868 => Loss: 6.73996467880046346011\n",
      "Iteration 11869 => Loss: 6.73995904802607714856\n",
      "Iteration 11870 => Loss: 6.73995341801044478558\n",
      "Iteration 11871 => Loss: 6.73994778875345534885\n",
      "Iteration 11872 => Loss: 6.73994216025501025058\n",
      "Iteration 11873 => Loss: 6.73993653251500113299\n",
      "Iteration 11874 => Loss: 6.73993090553333118464\n",
      "Iteration 11875 => Loss: 6.73992527930989737683\n",
      "Iteration 11876 => Loss: 6.73991965384459668087\n",
      "Iteration 11877 => Loss: 6.73991402913732873259\n",
      "Iteration 11878 => Loss: 6.73990840518799938508\n",
      "Iteration 11879 => Loss: 6.73990278199648962243\n",
      "Iteration 11880 => Loss: 6.73989715956270174502\n",
      "Iteration 11881 => Loss: 6.73989153788654693500\n",
      "Iteration 11882 => Loss: 6.73988591696790972918\n",
      "Iteration 11883 => Loss: 6.73988029680668621069\n",
      "Iteration 11884 => Loss: 6.73987467740278045625\n",
      "Iteration 11885 => Loss: 6.73986905875609565442\n",
      "Iteration 11886 => Loss: 6.73986344086652255925\n",
      "Iteration 11887 => Loss: 6.73985782373395725386\n",
      "Iteration 11888 => Loss: 6.73985220735830736771\n",
      "Iteration 11889 => Loss: 6.73984659173945388488\n",
      "Iteration 11890 => Loss: 6.73984097687731686932\n",
      "Iteration 11891 => Loss: 6.73983536277177108786\n",
      "Iteration 11892 => Loss: 6.73982974942273749264\n",
      "Iteration 11893 => Loss: 6.73982413683009706773\n",
      "Iteration 11894 => Loss: 6.73981852499375477805\n",
      "Iteration 11895 => Loss: 6.73981291391361114762\n",
      "Iteration 11896 => Loss: 6.73980730358956048320\n",
      "Iteration 11897 => Loss: 6.73980169402149442703\n",
      "Iteration 11898 => Loss: 6.73979608520932149673\n",
      "Iteration 11899 => Loss: 6.73979047715293955179\n",
      "Iteration 11900 => Loss: 6.73978486985224467531\n",
      "Iteration 11901 => Loss: 6.73977926330712850955\n",
      "Iteration 11902 => Loss: 6.73977365751750046030\n",
      "Iteration 11903 => Loss: 6.73976805248325216979\n",
      "Iteration 11904 => Loss: 6.73976244820427883297\n",
      "Iteration 11905 => Loss: 6.73975684468048097386\n",
      "Iteration 11906 => Loss: 6.73975124191176622190\n",
      "Iteration 11907 => Loss: 6.73974563989801822572\n",
      "Iteration 11908 => Loss: 6.73974003863914994383\n",
      "Iteration 11909 => Loss: 6.73973443813504413669\n",
      "Iteration 11910 => Loss: 6.73972883838560754555\n",
      "Iteration 11911 => Loss: 6.73972323939073181265\n",
      "Iteration 11912 => Loss: 6.73971764115033433740\n",
      "Iteration 11913 => Loss: 6.73971204366429699206\n",
      "Iteration 11914 => Loss: 6.73970644693251941248\n",
      "Iteration 11915 => Loss: 6.73970085095490301086\n",
      "Iteration 11916 => Loss: 6.73969525573133854124\n",
      "Iteration 11917 => Loss: 6.73968966126173807396\n",
      "Iteration 11918 => Loss: 6.73968406754598436947\n",
      "Iteration 11919 => Loss: 6.73967847458399749172\n",
      "Iteration 11920 => Loss: 6.73967288237565220754\n",
      "Iteration 11921 => Loss: 6.73966729092085437003\n",
      "Iteration 11922 => Loss: 6.73966170021951604951\n",
      "Iteration 11923 => Loss: 6.73965611027151911827\n",
      "Iteration 11924 => Loss: 6.73965052107677298210\n",
      "Iteration 11925 => Loss: 6.73964493263516484234\n",
      "Iteration 11926 => Loss: 6.73963934494660055208\n",
      "Iteration 11927 => Loss: 6.73963375801098418805\n",
      "Iteration 11928 => Loss: 6.73962817182820117523\n",
      "Iteration 11929 => Loss: 6.73962258639816091943\n",
      "Iteration 11930 => Loss: 6.73961700172075417470\n",
      "Iteration 11931 => Loss: 6.73961141779588235323\n",
      "Iteration 11932 => Loss: 6.73960583462344953176\n",
      "Iteration 11933 => Loss: 6.73960025220334468798\n",
      "Iteration 11934 => Loss: 6.73959467053547989224\n",
      "Iteration 11935 => Loss: 6.73958908961973612861\n",
      "Iteration 11936 => Loss: 6.73958350945602724380\n",
      "Iteration 11937 => Loss: 6.73957793004424043914\n",
      "Iteration 11938 => Loss: 6.73957235138428600862\n",
      "Iteration 11939 => Loss: 6.73956677347605381811\n",
      "Iteration 11940 => Loss: 6.73956119631944083892\n",
      "Iteration 11941 => Loss: 6.73955561991435736502\n",
      "Iteration 11942 => Loss: 6.73955004426068970957\n",
      "Iteration 11943 => Loss: 6.73954446935834461385\n",
      "Iteration 11944 => Loss: 6.73953889520721372008\n",
      "Iteration 11945 => Loss: 6.73953332180720465772\n",
      "Iteration 11946 => Loss: 6.73952774915820551627\n",
      "Iteration 11947 => Loss: 6.73952217726013280696\n",
      "Iteration 11948 => Loss: 6.73951660611286218483\n",
      "Iteration 11949 => Loss: 6.73951103571631282563\n",
      "Iteration 11950 => Loss: 6.73950546607036926616\n",
      "Iteration 11951 => Loss: 6.73949989717493558317\n",
      "Iteration 11952 => Loss: 6.73949432902991230065\n",
      "Iteration 11953 => Loss: 6.73948876163519905447\n",
      "Iteration 11954 => Loss: 6.73948319499069370409\n",
      "Iteration 11955 => Loss: 6.73947762909629144445\n",
      "Iteration 11956 => Loss: 6.73947206395189457595\n",
      "Iteration 11957 => Loss: 6.73946649955739918170\n",
      "Iteration 11958 => Loss: 6.73946093591271377932\n",
      "Iteration 11959 => Loss: 6.73945537301772024108\n",
      "Iteration 11960 => Loss: 6.73944981087233419004\n",
      "Iteration 11961 => Loss: 6.73944424947643927482\n",
      "Iteration 11962 => Loss: 6.73943868882994845393\n",
      "Iteration 11963 => Loss: 6.73943312893275425779\n",
      "Iteration 11964 => Loss: 6.73942756978476076313\n",
      "Iteration 11965 => Loss: 6.73942201138585961218\n",
      "Iteration 11966 => Loss: 6.73941645373595399349\n",
      "Iteration 11967 => Loss: 6.73941089683493999019\n",
      "Iteration 11968 => Loss: 6.73940534068271812629\n",
      "Iteration 11969 => Loss: 6.73939978527919247853\n",
      "Iteration 11970 => Loss: 6.73939423062425646549\n",
      "Iteration 11971 => Loss: 6.73938867671781238755\n",
      "Iteration 11972 => Loss: 6.73938312355975188694\n",
      "Iteration 11973 => Loss: 6.73937757114998436947\n",
      "Iteration 11974 => Loss: 6.73937201948840769461\n",
      "Iteration 11975 => Loss: 6.73936646857490639917\n",
      "Iteration 11976 => Loss: 6.73936091840939877073\n",
      "Iteration 11977 => Loss: 6.73935536899177822789\n",
      "Iteration 11978 => Loss: 6.73934982032193818924\n",
      "Iteration 11979 => Loss: 6.73934427239978450785\n",
      "Iteration 11980 => Loss: 6.73933872522520882598\n",
      "Iteration 11981 => Loss: 6.73933317879812321394\n",
      "Iteration 11982 => Loss: 6.73932763311841132037\n",
      "Iteration 11983 => Loss: 6.73932208818598343925\n",
      "Iteration 11984 => Loss: 6.73931654400073743005\n",
      "Iteration 11985 => Loss: 6.73931100056257026409\n",
      "Iteration 11986 => Loss: 6.73930545787137713631\n",
      "Iteration 11987 => Loss: 6.73929991592706745251\n",
      "Iteration 11988 => Loss: 6.73929437472953640764\n",
      "Iteration 11989 => Loss: 6.73928883427867830846\n",
      "Iteration 11990 => Loss: 6.73928329457439456718\n",
      "Iteration 11991 => Loss: 6.73927775561659370140\n",
      "Iteration 11992 => Loss: 6.73927221740516113613\n",
      "Iteration 11993 => Loss: 6.73926667994000627715\n",
      "Iteration 11994 => Loss: 6.73926114322102520759\n",
      "Iteration 11995 => Loss: 6.73925560724811667512\n",
      "Iteration 11996 => Loss: 6.73925007202117676286\n",
      "Iteration 11997 => Loss: 6.73924453754011221207\n",
      "Iteration 11998 => Loss: 6.73923900380482621131\n",
      "Iteration 11999 => Loss: 6.73923347081520329738\n",
      "Iteration 12000 => Loss: 6.73922793857115021154\n",
      "Iteration 12001 => Loss: 6.73922240707257191872\n",
      "Iteration 12002 => Loss: 6.73921687631936450202\n",
      "Iteration 12003 => Loss: 6.73921134631142493276\n",
      "Iteration 12004 => Loss: 6.73920581704865639949\n",
      "Iteration 12005 => Loss: 6.73920028853094787991\n",
      "Iteration 12006 => Loss: 6.73919476075821943795\n",
      "Iteration 12007 => Loss: 6.73918923373035383406\n",
      "Iteration 12008 => Loss: 6.73918370744724981591\n",
      "Iteration 12009 => Loss: 6.73917818190881767748\n",
      "Iteration 12010 => Loss: 6.73917265711495350189\n",
      "Iteration 12011 => Loss: 6.73916713306555870133\n",
      "Iteration 12012 => Loss: 6.73916160976052314169\n",
      "Iteration 12013 => Loss: 6.73915608719975711693\n",
      "Iteration 12014 => Loss: 6.73915056538315493384\n",
      "Iteration 12015 => Loss: 6.73914504431061534007\n",
      "Iteration 12016 => Loss: 6.73913952398204507688\n",
      "Iteration 12017 => Loss: 6.73913400439733845104\n",
      "Iteration 12018 => Loss: 6.73912848555639509840\n",
      "Iteration 12019 => Loss: 6.73912296745911554297\n",
      "Iteration 12020 => Loss: 6.73911745010540208511\n",
      "Iteration 12021 => Loss: 6.73911193349515702522\n",
      "Iteration 12022 => Loss: 6.73910641762826667645\n",
      "Iteration 12023 => Loss: 6.73910090250464843820\n",
      "Iteration 12024 => Loss: 6.73909538812418329456\n",
      "Iteration 12025 => Loss: 6.73908987448678509224\n",
      "Iteration 12026 => Loss: 6.73908436159235790797\n",
      "Iteration 12027 => Loss: 6.73907884944078450218\n",
      "Iteration 12028 => Loss: 6.73907333803197516886\n",
      "Iteration 12029 => Loss: 6.73906782736582865567\n",
      "Iteration 12030 => Loss: 6.73906231744224992752\n",
      "Iteration 12031 => Loss: 6.73905680826113151483\n",
      "Iteration 12032 => Loss: 6.73905129982237571795\n",
      "Iteration 12033 => Loss: 6.73904579212587862003\n",
      "Iteration 12034 => Loss: 6.73904028517154873867\n",
      "Iteration 12035 => Loss: 6.73903477895927771613\n",
      "Iteration 12036 => Loss: 6.73902927348897229365\n",
      "Iteration 12037 => Loss: 6.73902376876052588983\n",
      "Iteration 12038 => Loss: 6.73901826477384968683\n",
      "Iteration 12039 => Loss: 6.73901276152882822146\n",
      "Iteration 12040 => Loss: 6.73900725902537089951\n",
      "Iteration 12041 => Loss: 6.73900175726338179771\n",
      "Iteration 12042 => Loss: 6.73899625624275255831\n",
      "Iteration 12043 => Loss: 6.73899075596337571170\n",
      "Iteration 12044 => Loss: 6.73898525642517576273\n",
      "Iteration 12045 => Loss: 6.73897975762803369548\n",
      "Iteration 12046 => Loss: 6.73897425957185802758\n",
      "Iteration 12047 => Loss: 6.73896876225654306580\n",
      "Iteration 12048 => Loss: 6.73896326568199377505\n",
      "Iteration 12049 => Loss: 6.73895776984810890298\n",
      "Iteration 12050 => Loss: 6.73895227475477920365\n",
      "Iteration 12051 => Loss: 6.73894678040192030011\n",
      "Iteration 12052 => Loss: 6.73894128678943182820\n",
      "Iteration 12053 => Loss: 6.73893579391720187743\n",
      "Iteration 12054 => Loss: 6.73893030178513363637\n",
      "Iteration 12055 => Loss: 6.73892481039313206992\n",
      "Iteration 12056 => Loss: 6.73891931974110125481\n",
      "Iteration 12057 => Loss: 6.73891382982893460962\n",
      "Iteration 12058 => Loss: 6.73890834065653354656\n",
      "Iteration 12059 => Loss: 6.73890285222379326058\n",
      "Iteration 12060 => Loss: 6.73889736453062315746\n",
      "Iteration 12061 => Loss: 6.73889187757692198488\n",
      "Iteration 12062 => Loss: 6.73888639136259381957\n",
      "Iteration 12063 => Loss: 6.73888090588752408649\n",
      "Iteration 12064 => Loss: 6.73887542115162752054\n",
      "Iteration 12065 => Loss: 6.73886993715479309941\n",
      "Iteration 12066 => Loss: 6.73886445389693644614\n",
      "Iteration 12067 => Loss: 6.73885897137794920297\n",
      "Iteration 12068 => Loss: 6.73885348959772123578\n",
      "Iteration 12069 => Loss: 6.73884800855616994397\n",
      "Iteration 12070 => Loss: 6.73884252825319052249\n",
      "Iteration 12071 => Loss: 6.73883704868868083082\n",
      "Iteration 12072 => Loss: 6.73883156986254938658\n",
      "Iteration 12073 => Loss: 6.73882609177468161477\n",
      "Iteration 12074 => Loss: 6.73882061442498514481\n",
      "Iteration 12075 => Loss: 6.73881513781336760616\n",
      "Iteration 12076 => Loss: 6.73880966193972508194\n",
      "Iteration 12077 => Loss: 6.73880418680395010256\n",
      "Iteration 12078 => Loss: 6.73879871240595740289\n",
      "Iteration 12079 => Loss: 6.73879323874564040153\n",
      "Iteration 12080 => Loss: 6.73878776582289251706\n",
      "Iteration 12081 => Loss: 6.73878229363762581983\n",
      "Iteration 12082 => Loss: 6.73877682218973639294\n",
      "Iteration 12083 => Loss: 6.73877135147912387225\n",
      "Iteration 12084 => Loss: 6.73876588150569322266\n",
      "Iteration 12085 => Loss: 6.73876041226933697459\n",
      "Iteration 12086 => Loss: 6.73875494376996364565\n",
      "Iteration 12087 => Loss: 6.73874947600747198351\n",
      "Iteration 12088 => Loss: 6.73874400898175629493\n",
      "Iteration 12089 => Loss: 6.73873854269273131479\n",
      "Iteration 12090 => Loss: 6.73873307714028069171\n",
      "Iteration 12091 => Loss: 6.73872761232431560785\n",
      "Iteration 12092 => Loss: 6.73872214824473747541\n",
      "Iteration 12093 => Loss: 6.73871668490144859476\n",
      "Iteration 12094 => Loss: 6.73871122229433705542\n",
      "Iteration 12095 => Loss: 6.73870576042331759226\n",
      "Iteration 12096 => Loss: 6.73870029928828540022\n",
      "Iteration 12097 => Loss: 6.73869483888913922698\n",
      "Iteration 12098 => Loss: 6.73868937922578492561\n",
      "Iteration 12099 => Loss: 6.73868392029811502653\n",
      "Iteration 12100 => Loss: 6.73867846210604160007\n",
      "Iteration 12101 => Loss: 6.73867300464946072935\n",
      "Iteration 12102 => Loss: 6.73866754792827205023\n",
      "Iteration 12103 => Loss: 6.73866209194237608671\n",
      "Iteration 12104 => Loss: 6.73865663669167513916\n",
      "Iteration 12105 => Loss: 6.73865118217606884343\n",
      "Iteration 12106 => Loss: 6.73864572839545505900\n",
      "Iteration 12107 => Loss: 6.73864027534974230349\n",
      "Iteration 12108 => Loss: 6.73863482303882754820\n",
      "Iteration 12109 => Loss: 6.73862937146261042898\n",
      "Iteration 12110 => Loss: 6.73862392062099591072\n",
      "Iteration 12111 => Loss: 6.73861847051388185292\n",
      "Iteration 12112 => Loss: 6.73861302114117766138\n",
      "Iteration 12113 => Loss: 6.73860757250276432018\n",
      "Iteration 12114 => Loss: 6.73860212459856366962\n",
      "Iteration 12115 => Loss: 6.73859667742846735194\n",
      "Iteration 12116 => Loss: 6.73859123099236878573\n",
      "Iteration 12117 => Loss: 6.73858578529018892311\n",
      "Iteration 12118 => Loss: 6.73858034032181496542\n",
      "Iteration 12119 => Loss: 6.73857489608715010121\n",
      "Iteration 12120 => Loss: 6.73856945258609396632\n",
      "Iteration 12121 => Loss: 6.73856400981855507837\n",
      "Iteration 12122 => Loss: 6.73855856778442685595\n",
      "Iteration 12123 => Loss: 6.73855312648361337580\n",
      "Iteration 12124 => Loss: 6.73854768591602049099\n",
      "Iteration 12125 => Loss: 6.73854224608153540288\n",
      "Iteration 12126 => Loss: 6.73853680698006929362\n",
      "Iteration 12127 => Loss: 6.73853136861152712811\n",
      "Iteration 12128 => Loss: 6.73852593097580498949\n",
      "Iteration 12129 => Loss: 6.73852049407280162541\n",
      "Iteration 12130 => Loss: 6.73851505790242999439\n",
      "Iteration 12131 => Loss: 6.73850962246456841598\n",
      "Iteration 12132 => Loss: 6.73850418775914494773\n",
      "Iteration 12133 => Loss: 6.73849875378603879739\n",
      "Iteration 12134 => Loss: 6.73849332054517180524\n",
      "Iteration 12135 => Loss: 6.73848788803642673173\n",
      "Iteration 12136 => Loss: 6.73848245625972186446\n",
      "Iteration 12137 => Loss: 6.73847702521493729932\n",
      "Iteration 12138 => Loss: 6.73847159490198865939\n",
      "Iteration 12139 => Loss: 6.73846616532077913320\n",
      "Iteration 12140 => Loss: 6.73846073647120213934\n",
      "Iteration 12141 => Loss: 6.73845530835316885998\n",
      "Iteration 12142 => Loss: 6.73844988096657004917\n",
      "Iteration 12143 => Loss: 6.73844445431131688906\n",
      "Iteration 12144 => Loss: 6.73843902838730013372\n",
      "Iteration 12145 => Loss: 6.73843360319442652440\n",
      "Iteration 12146 => Loss: 6.73842817873259836148\n",
      "Iteration 12147 => Loss: 6.73842275500172416258\n",
      "Iteration 12148 => Loss: 6.73841733200169290541\n",
      "Iteration 12149 => Loss: 6.73841190973240866668\n",
      "Iteration 12150 => Loss: 6.73840648819377818768\n",
      "Iteration 12151 => Loss: 6.73840106738570732148\n",
      "Iteration 12152 => Loss: 6.73839564730808415760\n",
      "Iteration 12153 => Loss: 6.73839022796081543731\n",
      "Iteration 12154 => Loss: 6.73838480934380790188\n",
      "Iteration 12155 => Loss: 6.73837939145696029897\n",
      "Iteration 12156 => Loss: 6.73837397430016604716\n",
      "Iteration 12157 => Loss: 6.73836855787333721679\n",
      "Iteration 12158 => Loss: 6.73836314217637966095\n",
      "Iteration 12159 => Loss: 6.73835772720917614009\n",
      "Iteration 12160 => Loss: 6.73835231297164405362\n",
      "Iteration 12161 => Loss: 6.73834689946368303737\n",
      "Iteration 12162 => Loss: 6.73834148668519539171\n",
      "Iteration 12163 => Loss: 6.73833607463607187071\n",
      "Iteration 12164 => Loss: 6.73833066331623076195\n",
      "Iteration 12165 => Loss: 6.73832525272555926676\n",
      "Iteration 12166 => Loss: 6.73831984286397123185\n",
      "Iteration 12167 => Loss: 6.73831443373135829944\n",
      "Iteration 12168 => Loss: 6.73830902532762721080\n",
      "Iteration 12169 => Loss: 6.73830361765268115448\n",
      "Iteration 12170 => Loss: 6.73829821070641532543\n",
      "Iteration 12171 => Loss: 6.73829280448873912945\n",
      "Iteration 12172 => Loss: 6.73828739899954953785\n",
      "Iteration 12173 => Loss: 6.73828199423875240370\n",
      "Iteration 12174 => Loss: 6.73827659020624558650\n",
      "Iteration 12175 => Loss: 6.73827118690193138661\n",
      "Iteration 12176 => Loss: 6.73826578432571565713\n",
      "Iteration 12177 => Loss: 6.73826038247749536936\n",
      "Iteration 12178 => Loss: 6.73825498135717282366\n",
      "Iteration 12179 => Loss: 6.73824958096465387314\n",
      "Iteration 12180 => Loss: 6.73824418129983460091\n",
      "Iteration 12181 => Loss: 6.73823878236263151820\n",
      "Iteration 12182 => Loss: 6.73823338415292827364\n",
      "Iteration 12183 => Loss: 6.73822798667063427303\n",
      "Iteration 12184 => Loss: 6.73822258991564915220\n",
      "Iteration 12185 => Loss: 6.73821719388788054061\n",
      "Iteration 12186 => Loss: 6.73821179858722718592\n",
      "Iteration 12187 => Loss: 6.73820640401359138849\n",
      "Iteration 12188 => Loss: 6.73820101016687100781\n",
      "Iteration 12189 => Loss: 6.73819561704697189697\n",
      "Iteration 12190 => Loss: 6.73819022465379813269\n",
      "Iteration 12191 => Loss: 6.73818483298725023900\n",
      "Iteration 12192 => Loss: 6.73817944204722785173\n",
      "Iteration 12193 => Loss: 6.73817405183363948851\n",
      "Iteration 12194 => Loss: 6.73816866234638123245\n",
      "Iteration 12195 => Loss: 6.73816327358535449576\n",
      "Iteration 12196 => Loss: 6.73815788555046335517\n",
      "Iteration 12197 => Loss: 6.73815249824161366377\n",
      "Iteration 12198 => Loss: 6.73814711165870150467\n",
      "Iteration 12199 => Loss: 6.73814172580163450732\n",
      "Iteration 12200 => Loss: 6.73813634067031408392\n",
      "Iteration 12201 => Loss: 6.73813095626463187671\n",
      "Iteration 12202 => Loss: 6.73812557258450528508\n",
      "Iteration 12203 => Loss: 6.73812018962982950399\n",
      "Iteration 12204 => Loss: 6.73811480740050150473\n",
      "Iteration 12205 => Loss: 6.73810942589643779854\n",
      "Iteration 12206 => Loss: 6.73810404511752913947\n",
      "Iteration 12207 => Loss: 6.73809866506367782790\n",
      "Iteration 12208 => Loss: 6.73809328573479593416\n",
      "Iteration 12209 => Loss: 6.73808790713077332413\n",
      "Iteration 12210 => Loss: 6.73808252925151585089\n",
      "Iteration 12211 => Loss: 6.73807715209693558478\n",
      "Iteration 12212 => Loss: 6.73807177566692150350\n",
      "Iteration 12213 => Loss: 6.73806639996138034832\n",
      "Iteration 12214 => Loss: 6.73806102498021708413\n",
      "Iteration 12215 => Loss: 6.73805565072334022858\n",
      "Iteration 12216 => Loss: 6.73805027719063698299\n",
      "Iteration 12217 => Loss: 6.73804490438202385860\n",
      "Iteration 12218 => Loss: 6.73803953229739693853\n",
      "Iteration 12219 => Loss: 6.73803416093665230591\n",
      "Iteration 12220 => Loss: 6.73802879029971002467\n",
      "Iteration 12221 => Loss: 6.73802342038644486166\n",
      "Iteration 12222 => Loss: 6.73801805119678665079\n",
      "Iteration 12223 => Loss: 6.73801268273062703429\n",
      "Iteration 12224 => Loss: 6.73800731498786475981\n",
      "Iteration 12225 => Loss: 6.73800194796840834499\n",
      "Iteration 12226 => Loss: 6.73799658167215831384\n",
      "Iteration 12227 => Loss: 6.73799121609901874308\n",
      "Iteration 12228 => Loss: 6.73798585124889015674\n",
      "Iteration 12229 => Loss: 6.73798048712167041430\n",
      "Iteration 12230 => Loss: 6.73797512371727069791\n",
      "Iteration 12231 => Loss: 6.73796976103559153159\n",
      "Iteration 12232 => Loss: 6.73796439907653077483\n",
      "Iteration 12233 => Loss: 6.73795903783999872161\n",
      "Iteration 12234 => Loss: 6.73795367732588879051\n",
      "Iteration 12235 => Loss: 6.73794831753411482822\n",
      "Iteration 12236 => Loss: 6.73794295846457291788\n",
      "Iteration 12237 => Loss: 6.73793760011716003078\n",
      "Iteration 12238 => Loss: 6.73793224249178646090\n",
      "Iteration 12239 => Loss: 6.73792688558836339041\n",
      "Iteration 12240 => Loss: 6.73792152940677002704\n",
      "Iteration 12241 => Loss: 6.73791617394692643472\n",
      "Iteration 12242 => Loss: 6.73791081920873224931\n",
      "Iteration 12243 => Loss: 6.73790546519209065934\n",
      "Iteration 12244 => Loss: 6.73790011189690218885\n",
      "Iteration 12245 => Loss: 6.73789475932307091455\n",
      "Iteration 12246 => Loss: 6.73788940747050180136\n",
      "Iteration 12247 => Loss: 6.73788405633909004422\n",
      "Iteration 12248 => Loss: 6.73787870592874416076\n",
      "Iteration 12249 => Loss: 6.73787335623937000406\n",
      "Iteration 12250 => Loss: 6.73786800727086720997\n",
      "Iteration 12251 => Loss: 6.73786265902313719067\n",
      "Iteration 12252 => Loss: 6.73785731149608047019\n",
      "Iteration 12253 => Loss: 6.73785196468960645433\n",
      "Iteration 12254 => Loss: 6.73784661860361566710\n",
      "Iteration 12255 => Loss: 6.73784127323800774434\n",
      "Iteration 12256 => Loss: 6.73783592859268853914\n",
      "Iteration 12257 => Loss: 6.73783058466756301641\n",
      "Iteration 12258 => Loss: 6.73782524146253436470\n",
      "Iteration 12259 => Loss: 6.73781989897749689078\n",
      "Iteration 12260 => Loss: 6.73781455721236000045\n",
      "Iteration 12261 => Loss: 6.73780921616703398769\n",
      "Iteration 12262 => Loss: 6.73780387584140694202\n",
      "Iteration 12263 => Loss: 6.73779853623539093377\n",
      "Iteration 12264 => Loss: 6.73779319734888915150\n",
      "Iteration 12265 => Loss: 6.73778785918180300740\n",
      "Iteration 12266 => Loss: 6.73778252173403213732\n",
      "Iteration 12267 => Loss: 6.73777718500548417069\n",
      "Iteration 12268 => Loss: 6.73777184899605963153\n",
      "Iteration 12269 => Loss: 6.73776651370566703747\n",
      "Iteration 12270 => Loss: 6.73776117913420158345\n",
      "Iteration 12271 => Loss: 6.73775584528157267528\n",
      "Iteration 12272 => Loss: 6.73775051214767639607\n",
      "Iteration 12273 => Loss: 6.73774517973242392799\n",
      "Iteration 12274 => Loss: 6.73773984803571401869\n",
      "Iteration 12275 => Loss: 6.73773451705744808038\n",
      "Iteration 12276 => Loss: 6.73772918679753907156\n",
      "Iteration 12277 => Loss: 6.73772385725587508176\n",
      "Iteration 12278 => Loss: 6.73771852843236995767\n",
      "Iteration 12279 => Loss: 6.73771320032692599966\n",
      "Iteration 12280 => Loss: 6.73770787293943929086\n",
      "Iteration 12281 => Loss: 6.73770254626982367796\n",
      "Iteration 12282 => Loss: 6.73769722031797702044\n",
      "Iteration 12283 => Loss: 6.73769189508380250686\n",
      "Iteration 12284 => Loss: 6.73768657056720154941\n",
      "Iteration 12285 => Loss: 6.73768124676807822482\n",
      "Iteration 12286 => Loss: 6.73767592368634016253\n",
      "Iteration 12287 => Loss: 6.73767060132188966293\n",
      "Iteration 12288 => Loss: 6.73766527967462636184\n",
      "Iteration 12289 => Loss: 6.73765995874445255964\n",
      "Iteration 12290 => Loss: 6.73765463853128032667\n",
      "Iteration 12291 => Loss: 6.73764931903500396970\n",
      "Iteration 12292 => Loss: 6.73764400025553200635\n",
      "Iteration 12293 => Loss: 6.73763868219275874338\n",
      "Iteration 12294 => Loss: 6.73763336484659980385\n",
      "Iteration 12295 => Loss: 6.73762804821695837632\n",
      "Iteration 12296 => Loss: 6.73762273230372876753\n",
      "Iteration 12297 => Loss: 6.73761741710681594242\n",
      "Iteration 12298 => Loss: 6.73761210262612930677\n",
      "Iteration 12299 => Loss: 6.73760678886157293732\n",
      "Iteration 12300 => Loss: 6.73760147581304735809\n",
      "Iteration 12301 => Loss: 6.73759616348044954037\n",
      "Iteration 12302 => Loss: 6.73759085186368977816\n",
      "Iteration 12303 => Loss: 6.73758554096267214817\n",
      "Iteration 12304 => Loss: 6.73758023077729895078\n",
      "Iteration 12305 => Loss: 6.73757492130746893366\n",
      "Iteration 12306 => Loss: 6.73756961255309683168\n",
      "Iteration 12307 => Loss: 6.73756430451407783977\n",
      "Iteration 12308 => Loss: 6.73755899719031692285\n",
      "Iteration 12309 => Loss: 6.73755369058172082219\n",
      "Iteration 12310 => Loss: 6.73754838468818650909\n",
      "Iteration 12311 => Loss: 6.73754307950962605389\n",
      "Iteration 12312 => Loss: 6.73753777504593731607\n",
      "Iteration 12313 => Loss: 6.73753247129702526053\n",
      "Iteration 12314 => Loss: 6.73752716826279041129\n",
      "Iteration 12315 => Loss: 6.73752186594314750323\n",
      "Iteration 12316 => Loss: 6.73751656433798373769\n",
      "Iteration 12317 => Loss: 6.73751126344721651407\n",
      "Iteration 12318 => Loss: 6.73750596327074458003\n",
      "Iteration 12319 => Loss: 6.73750066380847023595\n",
      "Iteration 12320 => Loss: 6.73749536506030377581\n",
      "Iteration 12321 => Loss: 6.73749006702613417730\n",
      "Iteration 12322 => Loss: 6.73748476970587883983\n",
      "Iteration 12323 => Loss: 6.73747947309944894556\n",
      "Iteration 12324 => Loss: 6.73747417720672281405\n",
      "Iteration 12325 => Loss: 6.73746888202762406195\n",
      "Iteration 12326 => Loss: 6.73746358756205498963\n",
      "Iteration 12327 => Loss: 6.73745829380990812751\n",
      "Iteration 12328 => Loss: 6.73745300077109998682\n",
      "Iteration 12329 => Loss: 6.73744770844552842703\n",
      "Iteration 12330 => Loss: 6.73744241683309130764\n",
      "Iteration 12331 => Loss: 6.73743712593370958075\n",
      "Iteration 12332 => Loss: 6.73743183574727311225\n",
      "Iteration 12333 => Loss: 6.73742654627368686704\n",
      "Iteration 12334 => Loss: 6.73742125751286113911\n",
      "Iteration 12335 => Loss: 6.73741596946469112339\n",
      "Iteration 12336 => Loss: 6.73741068212909777202\n",
      "Iteration 12337 => Loss: 6.73740539550596206908\n",
      "Iteration 12338 => Loss: 6.73740010959520319034\n",
      "Iteration 12339 => Loss: 6.73739482439672254799\n",
      "Iteration 12340 => Loss: 6.73738953991041977787\n",
      "Iteration 12341 => Loss: 6.73738425613620428578\n",
      "Iteration 12342 => Loss: 6.73737897307397481939\n",
      "Iteration 12343 => Loss: 6.73737369072364344902\n",
      "Iteration 12344 => Loss: 6.73736840908510359327\n",
      "Iteration 12345 => Loss: 6.73736312815826465794\n",
      "Iteration 12346 => Loss: 6.73735784794304137790\n",
      "Iteration 12347 => Loss: 6.73735256843931740178\n",
      "Iteration 12348 => Loss: 6.73734728964700746445\n",
      "Iteration 12349 => Loss: 6.73734201156602452443\n",
      "Iteration 12350 => Loss: 6.73733673419624867762\n",
      "Iteration 12351 => Loss: 6.73733145753761153429\n",
      "Iteration 12352 => Loss: 6.73732618158999674307\n",
      "Iteration 12353 => Loss: 6.73732090635331726247\n",
      "Iteration 12354 => Loss: 6.73731563182748072194\n",
      "Iteration 12355 => Loss: 6.73731035801238054006\n",
      "Iteration 12356 => Loss: 6.73730508490792878717\n",
      "Iteration 12357 => Loss: 6.73729981251403309273\n",
      "Iteration 12358 => Loss: 6.73729454083059486891\n",
      "Iteration 12359 => Loss: 6.73728926985750398160\n",
      "Iteration 12360 => Loss: 6.73728399959468937652\n",
      "Iteration 12361 => Loss: 6.73727873004204091956\n",
      "Iteration 12362 => Loss: 6.73727346119946535197\n",
      "Iteration 12363 => Loss: 6.73726819306685964506\n",
      "Iteration 12364 => Loss: 6.73726292564414031006\n",
      "Iteration 12365 => Loss: 6.73725765893120698280\n",
      "Iteration 12366 => Loss: 6.73725239292796018731\n",
      "Iteration 12367 => Loss: 6.73724712763431021756\n",
      "Iteration 12368 => Loss: 6.73724186305016381482\n",
      "Iteration 12369 => Loss: 6.73723659917540906861\n",
      "Iteration 12370 => Loss: 6.73723133600997403647\n",
      "Iteration 12371 => Loss: 6.73722607355374591975\n",
      "Iteration 12372 => Loss: 6.73722081180662879518\n",
      "Iteration 12373 => Loss: 6.73721555076854006217\n",
      "Iteration 12374 => Loss: 6.73721029043937402747\n",
      "Iteration 12375 => Loss: 6.73720503081903210330\n",
      "Iteration 12376 => Loss: 6.73719977190743701811\n",
      "Iteration 12377 => Loss: 6.73719451370447242056\n",
      "Iteration 12378 => Loss: 6.73718925621005571003\n",
      "Iteration 12379 => Loss: 6.73718399942408563419\n",
      "Iteration 12380 => Loss: 6.73717874334646182888\n",
      "Iteration 12381 => Loss: 6.73717348797709902897\n",
      "Iteration 12382 => Loss: 6.73716823331589598212\n",
      "Iteration 12383 => Loss: 6.73716297936276387048\n",
      "Iteration 12384 => Loss: 6.73715772611759522448\n",
      "Iteration 12385 => Loss: 6.73715247358030477898\n",
      "Iteration 12386 => Loss: 6.73714722175079483435\n",
      "Iteration 12387 => Loss: 6.73714197062897035551\n",
      "Iteration 12388 => Loss: 6.73713672021473630735\n",
      "Iteration 12389 => Loss: 6.73713147050799232574\n",
      "Iteration 12390 => Loss: 6.73712622150865048098\n",
      "Iteration 12391 => Loss: 6.73712097321660685623\n",
      "Iteration 12392 => Loss: 6.73711572563177352180\n",
      "Iteration 12393 => Loss: 6.73711047875405277807\n",
      "Iteration 12394 => Loss: 6.73710523258334870178\n",
      "Iteration 12395 => Loss: 6.73709998711956359330\n",
      "Iteration 12396 => Loss: 6.73709474236261041113\n",
      "Iteration 12397 => Loss: 6.73708949831238790296\n",
      "Iteration 12398 => Loss: 6.73708425496879748096\n",
      "Iteration 12399 => Loss: 6.73707901233175743272\n",
      "Iteration 12400 => Loss: 6.73707377040114963052\n",
      "Iteration 12401 => Loss: 6.73706852917690035554\n",
      "Iteration 12402 => Loss: 6.73706328865890657909\n",
      "Iteration 12403 => Loss: 6.73705804884707148972\n",
      "Iteration 12404 => Loss: 6.73705280974130182869\n",
      "Iteration 12405 => Loss: 6.73704757134150522546\n",
      "Iteration 12406 => Loss: 6.73704233364757687497\n",
      "Iteration 12407 => Loss: 6.73703709665942973572\n",
      "Iteration 12408 => Loss: 6.73703186037697232535\n",
      "Iteration 12409 => Loss: 6.73702662480010339152\n",
      "Iteration 12410 => Loss: 6.73702138992872168188\n",
      "Iteration 12411 => Loss: 6.73701615576274370767\n",
      "Iteration 12412 => Loss: 6.73701092230207265743\n",
      "Iteration 12413 => Loss: 6.73700568954660550247\n",
      "Iteration 12414 => Loss: 6.73700045749626053038\n",
      "Iteration 12415 => Loss: 6.73699522615092938338\n",
      "Iteration 12416 => Loss: 6.73698999551051702639\n",
      "Iteration 12417 => Loss: 6.73698476557493997063\n",
      "Iteration 12418 => Loss: 6.73697953634410406920\n",
      "Iteration 12419 => Loss: 6.73697430781789741161\n",
      "Iteration 12420 => Loss: 6.73696907999623828545\n",
      "Iteration 12421 => Loss: 6.73696385287902543837\n",
      "Iteration 12422 => Loss: 6.73695862646617094072\n",
      "Iteration 12423 => Loss: 6.73695340075757442833\n",
      "Iteration 12424 => Loss: 6.73694817575313997793\n",
      "Iteration 12425 => Loss: 6.73694295145278143622\n",
      "Iteration 12426 => Loss: 6.73693772785639399814\n",
      "Iteration 12427 => Loss: 6.73693250496388706949\n",
      "Iteration 12428 => Loss: 6.73692728277516472701\n",
      "Iteration 12429 => Loss: 6.73692206129013460014\n",
      "Iteration 12430 => Loss: 6.73691684050869810108\n",
      "Iteration 12431 => Loss: 6.73691162043076197108\n",
      "Iteration 12432 => Loss: 6.73690640105622673417\n",
      "Iteration 12433 => Loss: 6.73690118238501689518\n",
      "Iteration 12434 => Loss: 6.73689596441701077367\n",
      "Iteration 12435 => Loss: 6.73689074715213376265\n",
      "Iteration 12436 => Loss: 6.73688553059027483982\n",
      "Iteration 12437 => Loss: 6.73688031473135673366\n",
      "Iteration 12438 => Loss: 6.73687509957527463911\n",
      "Iteration 12439 => Loss: 6.73686988512193618561\n",
      "Iteration 12440 => Loss: 6.73686467137123567994\n",
      "Iteration 12441 => Loss: 6.73685945832309851511\n",
      "Iteration 12442 => Loss: 6.73685424597741988606\n",
      "Iteration 12443 => Loss: 6.73684903433410120499\n",
      "Iteration 12444 => Loss: 6.73684382339305365406\n",
      "Iteration 12445 => Loss: 6.73683861315418219817\n",
      "Iteration 12446 => Loss: 6.73683340361738647317\n",
      "Iteration 12447 => Loss: 6.73682819478257943757\n",
      "Iteration 12448 => Loss: 6.73682298664966516810\n",
      "Iteration 12449 => Loss: 6.73681777921854152424\n",
      "Iteration 12450 => Loss: 6.73681257248912324087\n",
      "Iteration 12451 => Loss: 6.73680736646132061196\n",
      "Iteration 12452 => Loss: 6.73680216113501817432\n",
      "Iteration 12453 => Loss: 6.73679695651013510371\n",
      "Iteration 12454 => Loss: 6.73679175258657902958\n",
      "Iteration 12455 => Loss: 6.73678654936425225230\n",
      "Iteration 12456 => Loss: 6.73678134684305973678\n",
      "Iteration 12457 => Loss: 6.73677614502289934251\n",
      "Iteration 12458 => Loss: 6.73677094390369557431\n",
      "Iteration 12459 => Loss: 6.73676574348534451531\n",
      "Iteration 12460 => Loss: 6.73676054376773869592\n",
      "Iteration 12461 => Loss: 6.73675534475080084462\n",
      "Iteration 12462 => Loss: 6.73675014643443592632\n",
      "Iteration 12463 => Loss: 6.73674494881854091233\n",
      "Iteration 12464 => Loss: 6.73673975190302254390\n",
      "Iteration 12465 => Loss: 6.73673455568778933866\n",
      "Iteration 12466 => Loss: 6.73672936017274803788\n",
      "Iteration 12467 => Loss: 6.73672416535780538283\n",
      "Iteration 12468 => Loss: 6.73671897124286100933\n",
      "Iteration 12469 => Loss: 6.73671377782782521138\n",
      "Iteration 12470 => Loss: 6.73670858511260117751\n",
      "Iteration 12471 => Loss: 6.73670339309709387265\n",
      "Iteration 12472 => Loss: 6.73669820178122069620\n",
      "Iteration 12473 => Loss: 6.73669301116486884951\n",
      "Iteration 12474 => Loss: 6.73668782124795217925\n",
      "Iteration 12475 => Loss: 6.73668263203038808484\n",
      "Iteration 12476 => Loss: 6.73667744351205666220\n",
      "Iteration 12477 => Loss: 6.73667225569288952158\n",
      "Iteration 12478 => Loss: 6.73666706857277297615\n",
      "Iteration 12479 => Loss: 6.73666188215162797803\n",
      "Iteration 12480 => Loss: 6.73665669642935149852\n",
      "Iteration 12481 => Loss: 6.73665151140585560796\n",
      "Iteration 12482 => Loss: 6.73664632708102928405\n",
      "Iteration 12483 => Loss: 6.73664114345480324886\n",
      "Iteration 12484 => Loss: 6.73663596052706914463\n",
      "Iteration 12485 => Loss: 6.73663077829773371263\n",
      "Iteration 12486 => Loss: 6.73662559676669925324\n",
      "Iteration 12487 => Loss: 6.73662041593388494221\n",
      "Iteration 12488 => Loss: 6.73661523579918597449\n",
      "Iteration 12489 => Loss: 6.73661005636250909134\n",
      "Iteration 12490 => Loss: 6.73660487762375925769\n",
      "Iteration 12491 => Loss: 6.73659969958284410296\n",
      "Iteration 12492 => Loss: 6.73659452223967658568\n",
      "Iteration 12493 => Loss: 6.73658934559415278898\n",
      "Iteration 12494 => Loss: 6.73658416964618300682\n",
      "Iteration 12495 => Loss: 6.73657899439567575683\n",
      "Iteration 12496 => Loss: 6.73657381984252268126\n",
      "Iteration 12497 => Loss: 6.73656864598665716670\n",
      "Iteration 12498 => Loss: 6.73656347282795664455\n",
      "Iteration 12499 => Loss: 6.73655830036634473146\n",
      "Iteration 12500 => Loss: 6.73655312860172372780\n",
      "Iteration 12501 => Loss: 6.73654795753399415759\n",
      "Iteration 12502 => Loss: 6.73654278716307075570\n",
      "Iteration 12503 => Loss: 6.73653761748885049343\n",
      "Iteration 12504 => Loss: 6.73653244851124632930\n",
      "Iteration 12505 => Loss: 6.73652728023016411640\n",
      "Iteration 12506 => Loss: 6.73652211264551059600\n",
      "Iteration 12507 => Loss: 6.73651694575718362756\n",
      "Iteration 12508 => Loss: 6.73651177956510149869\n",
      "Iteration 12509 => Loss: 6.73650661406914874618\n",
      "Iteration 12510 => Loss: 6.73650144926926675026\n",
      "Iteration 12511 => Loss: 6.73649628516532938960\n",
      "Iteration 12512 => Loss: 6.73649112175726205720\n",
      "Iteration 12513 => Loss: 6.73648595904495639530\n",
      "Iteration 12514 => Loss: 6.73648079702833424420\n",
      "Iteration 12515 => Loss: 6.73647563570729968063\n",
      "Iteration 12516 => Loss: 6.73647047508174168229\n",
      "Iteration 12517 => Loss: 6.73646531515158031311\n",
      "Iteration 12518 => Loss: 6.73646015591672231437\n",
      "Iteration 12519 => Loss: 6.73645499737706643373\n",
      "Iteration 12520 => Loss: 6.73644983953253451148\n",
      "Iteration 12521 => Loss: 6.73644468238301907803\n",
      "Iteration 12522 => Loss: 6.73643952592842865101\n",
      "Iteration 12523 => Loss: 6.73643437016866553080\n",
      "Iteration 12524 => Loss: 6.73642921510364978133\n",
      "Iteration 12525 => Loss: 6.73642406073328015026\n",
      "Iteration 12526 => Loss: 6.73641890705745449708\n",
      "Iteration 12527 => Loss: 6.73641375407609110937\n",
      "Iteration 12528 => Loss: 6.73640860178909406386\n",
      "Iteration 12529 => Loss: 6.73640345019636477275\n",
      "Iteration 12530 => Loss: 6.73639829929781619455\n",
      "Iteration 12531 => Loss: 6.73639314909334974146\n",
      "Iteration 12532 => Loss: 6.73638799958286949021\n",
      "Iteration 12533 => Loss: 6.73638285076629284021\n",
      "Iteration 12534 => Loss: 6.73637770264351498639\n",
      "Iteration 12535 => Loss: 6.73637255521444888728\n",
      "Iteration 12536 => Loss: 6.73636740847899950779\n",
      "Iteration 12537 => Loss: 6.73636226243707536554\n",
      "Iteration 12538 => Loss: 6.73635711708857964908\n",
      "Iteration 12539 => Loss: 6.73635197243341732332\n",
      "Iteration 12540 => Loss: 6.73634682847149868223\n",
      "Iteration 12541 => Loss: 6.73634168520273224345\n",
      "Iteration 12542 => Loss: 6.73633654262701586646\n",
      "Iteration 12543 => Loss: 6.73633140074426783883\n",
      "Iteration 12544 => Loss: 6.73632625955438957277\n",
      "Iteration 12545 => Loss: 6.73632111905728159229\n",
      "Iteration 12546 => Loss: 6.73631597925286040862\n",
      "Iteration 12547 => Loss: 6.73631084014102476942\n",
      "Iteration 12548 => Loss: 6.73630570172168763321\n",
      "Iteration 12549 => Loss: 6.73630056399474863582\n",
      "Iteration 12550 => Loss: 6.73629542696012073577\n",
      "Iteration 12551 => Loss: 6.73629029061771511522\n",
      "Iteration 12552 => Loss: 6.73628515496742075186\n",
      "Iteration 12553 => Loss: 6.73628002000916481506\n",
      "Iteration 12554 => Loss: 6.73627488574284072342\n",
      "Iteration 12555 => Loss: 6.73626975216835965909\n",
      "Iteration 12556 => Loss: 6.73626461928562569881\n",
      "Iteration 12557 => Loss: 6.73625948709455357744\n",
      "Iteration 12558 => Loss: 6.73625435559504293082\n",
      "Iteration 12559 => Loss: 6.73624922478700582928\n",
      "Iteration 12560 => Loss: 6.73624409467033835597\n",
      "Iteration 12561 => Loss: 6.73623896524496235116\n",
      "Iteration 12562 => Loss: 6.73623383651077123346\n",
      "Iteration 12563 => Loss: 6.73622870846767440867\n",
      "Iteration 12564 => Loss: 6.73622358111558483529\n",
      "Iteration 12565 => Loss: 6.73621845445440747824\n",
      "Iteration 12566 => Loss: 6.73621332848404552607\n",
      "Iteration 12567 => Loss: 6.73620820320441637818\n",
      "Iteration 12568 => Loss: 6.73620307861541434136\n",
      "Iteration 12569 => Loss: 6.73619795471694526867\n",
      "Iteration 12570 => Loss: 6.73619283150892567136\n",
      "Iteration 12571 => Loss: 6.73618770899125873797\n",
      "Iteration 12572 => Loss: 6.73618258716385387430\n",
      "Iteration 12573 => Loss: 6.73617746602661870980\n",
      "Iteration 12574 => Loss: 6.73617234557944932760\n",
      "Iteration 12575 => Loss: 6.73616722582226223892\n",
      "Iteration 12576 => Loss: 6.73616210675496240867\n",
      "Iteration 12577 => Loss: 6.73615698837746279537\n",
      "Iteration 12578 => Loss: 6.73615187068966481121\n",
      "Iteration 12579 => Loss: 6.73614675369146898021\n",
      "Iteration 12580 => Loss: 6.73614163738279625449\n",
      "Iteration 12581 => Loss: 6.73613652176353916445\n",
      "Iteration 12582 => Loss: 6.73613140683361777405\n",
      "Iteration 12583 => Loss: 6.73612629259293083095\n",
      "Iteration 12584 => Loss: 6.73612117904139395819\n",
      "Iteration 12585 => Loss: 6.73611606617889790982\n",
      "Iteration 12586 => Loss: 6.73611095400536541433\n",
      "Iteration 12587 => Loss: 6.73610584252070143663\n",
      "Iteration 12588 => Loss: 6.73610073172481271797\n",
      "Iteration 12589 => Loss: 6.73609562161759800603\n",
      "Iteration 12590 => Loss: 6.73609051219897292384\n",
      "Iteration 12591 => Loss: 6.73608540346884243633\n",
      "Iteration 12592 => Loss: 6.73608029542710884385\n",
      "Iteration 12593 => Loss: 6.73607518807368865765\n",
      "Iteration 12594 => Loss: 6.73607008140848861899\n",
      "Iteration 12595 => Loss: 6.73606497543140481099\n",
      "Iteration 12596 => Loss: 6.73605987014235196852\n",
      "Iteration 12597 => Loss: 6.73605476554124482647\n",
      "Iteration 12598 => Loss: 6.73604966162798213247\n",
      "Iteration 12599 => Loss: 6.73604455840246618692\n",
      "Iteration 12600 => Loss: 6.73603945586461172468\n",
      "Iteration 12601 => Loss: 6.73603435401432992791\n",
      "Iteration 12602 => Loss: 6.73602925285151865609\n",
      "Iteration 12603 => Loss: 6.73602415237608642684\n",
      "Iteration 12604 => Loss: 6.73601905258795419229\n",
      "Iteration 12605 => Loss: 6.73601395348700293653\n",
      "Iteration 12606 => Loss: 6.73600885507317048706\n",
      "Iteration 12607 => Loss: 6.73600375734633782798\n",
      "Iteration 12608 => Loss: 6.73599866030643301684\n",
      "Iteration 12609 => Loss: 6.73599356395335213676\n",
      "Iteration 12610 => Loss: 6.73598846828700370537\n",
      "Iteration 12611 => Loss: 6.73598337330729624028\n",
      "Iteration 12612 => Loss: 6.73597827901413381824\n",
      "Iteration 12613 => Loss: 6.73597318540744272042\n",
      "Iteration 12614 => Loss: 6.73596809248710126639\n",
      "Iteration 12615 => Loss: 6.73596300025303929004\n",
      "Iteration 12616 => Loss: 6.73595790870514665727\n",
      "Iteration 12617 => Loss: 6.73595281784335053743\n",
      "Iteration 12618 => Loss: 6.73594772766754790183\n",
      "Iteration 12619 => Loss: 6.73594263817763838631\n",
      "Iteration 12620 => Loss: 6.73593754937354294299\n",
      "Iteration 12621 => Loss: 6.73593246125516920131\n",
      "Iteration 12622 => Loss: 6.73592737382241590893\n",
      "Iteration 12623 => Loss: 6.73592228707518980713\n",
      "Iteration 12624 => Loss: 6.73591720101341095983\n",
      "Iteration 12625 => Loss: 6.73591211563697722653\n",
      "Iteration 12626 => Loss: 6.73590703094579712484\n",
      "Iteration 12627 => Loss: 6.73590194693977739604\n",
      "Iteration 12628 => Loss: 6.73589686361883099863\n",
      "Iteration 12629 => Loss: 6.73589178098286200935\n",
      "Iteration 12630 => Loss: 6.73588669903177539311\n",
      "Iteration 12631 => Loss: 6.73588161776548144388\n",
      "Iteration 12632 => Loss: 6.73587653718389400836\n",
      "Iteration 12633 => Loss: 6.73587145728691005786\n",
      "Iteration 12634 => Loss: 6.73586637807445143267\n",
      "Iteration 12635 => Loss: 6.73586129954641243955\n",
      "Iteration 12636 => Loss: 6.73585622170270248432\n",
      "Iteration 12637 => Loss: 6.73585114454323807820\n",
      "Iteration 12638 => Loss: 6.73584606806791086342\n",
      "Iteration 12639 => Loss: 6.73584099227664534482\n",
      "Iteration 12640 => Loss: 6.73583591716935270455\n",
      "Iteration 12641 => Loss: 6.73583084274590948581\n",
      "Iteration 12642 => Loss: 6.73582576900626239791\n",
      "Iteration 12643 => Loss: 6.73582069595029864217\n",
      "Iteration 12644 => Loss: 6.73581562357792851259\n",
      "Iteration 12645 => Loss: 6.73581055188905875042\n",
      "Iteration 12646 => Loss: 6.73580548088360941961\n",
      "Iteration 12647 => Loss: 6.73580041056146594514\n",
      "Iteration 12648 => Loss: 6.73579534092255016731\n",
      "Iteration 12649 => Loss: 6.73579027196677593281\n",
      "Iteration 12650 => Loss: 6.73578520369403488388\n",
      "Iteration 12651 => Loss: 6.73578013610424708446\n",
      "Iteration 12652 => Loss: 6.73577506919732993396\n",
      "Iteration 12653 => Loss: 6.73577000297315997557\n",
      "Iteration 12654 => Loss: 6.73576493743167237227\n",
      "Iteration 12655 => Loss: 6.73575987257277208897\n",
      "Iteration 12656 => Loss: 6.73575480839635165609\n",
      "Iteration 12657 => Loss: 6.73574974490233646662\n",
      "Iteration 12658 => Loss: 6.73574468209062171553\n",
      "Iteration 12659 => Loss: 6.73573961996112746675\n",
      "Iteration 12660 => Loss: 6.73573455851375335612\n",
      "Iteration 12661 => Loss: 6.73572949774840878945\n",
      "Iteration 12662 => Loss: 6.73572443766500228435\n",
      "Iteration 12663 => Loss: 6.73571937826344413480\n",
      "Iteration 12664 => Loss: 6.73571431954363841754\n",
      "Iteration 12665 => Loss: 6.73570926150549453837\n",
      "Iteration 12666 => Loss: 6.73570420414892812033\n",
      "Iteration 12667 => Loss: 6.73569914747383524656\n",
      "Iteration 12668 => Loss: 6.73569409148013242827\n",
      "Iteration 12669 => Loss: 6.73568903616772907128\n",
      "Iteration 12670 => Loss: 6.73568398153652214688\n",
      "Iteration 12671 => Loss: 6.73567892758642550177\n",
      "Iteration 12672 => Loss: 6.73567387431735564718\n",
      "Iteration 12673 => Loss: 6.73566882172921577165\n",
      "Iteration 12674 => Loss: 6.73566376982190373468\n",
      "Iteration 12675 => Loss: 6.73565871859534315291\n",
      "Iteration 12676 => Loss: 6.73565366804943543855\n",
      "Iteration 12677 => Loss: 6.73564861818408378014\n",
      "Iteration 12678 => Loss: 6.73564356899920824162\n",
      "Iteration 12679 => Loss: 6.73563852049470934702\n",
      "Iteration 12680 => Loss: 6.73563347267049739031\n",
      "Iteration 12681 => Loss: 6.73562842552648088912\n",
      "Iteration 12682 => Loss: 6.73562337906256036746\n",
      "Iteration 12683 => Loss: 6.73561833327865766563\n",
      "Iteration 12684 => Loss: 6.73561328817467419583\n",
      "Iteration 12685 => Loss: 6.73560824375051581114\n",
      "Iteration 12686 => Loss: 6.73560320000609635827\n",
      "Iteration 12687 => Loss: 6.73559815694131724939\n",
      "Iteration 12688 => Loss: 6.73559311455610121300\n",
      "Iteration 12689 => Loss: 6.73558807285033811496\n",
      "Iteration 12690 => Loss: 6.73558303182395334829\n",
      "Iteration 12691 => Loss: 6.73557799147683944341\n",
      "Iteration 12692 => Loss: 6.73557295180891557607\n",
      "Iteration 12693 => Loss: 6.73556791282008404664\n",
      "Iteration 12694 => Loss: 6.73556287451026580726\n",
      "Iteration 12695 => Loss: 6.73555783687935605286\n",
      "Iteration 12696 => Loss: 6.73555279992726418925\n",
      "Iteration 12697 => Loss: 6.73554776365390583948\n",
      "Iteration 12698 => Loss: 6.73554272805917708666\n",
      "Iteration 12699 => Loss: 6.73553769314300776472\n",
      "Iteration 12700 => Loss: 6.73553265890528773951\n",
      "Iteration 12701 => Loss: 6.73552762534593263410\n",
      "Iteration 12702 => Loss: 6.73552259246484208433\n",
      "Iteration 12703 => Loss: 6.73551756026194148319\n",
      "Iteration 12704 => Loss: 6.73551252873712780200\n",
      "Iteration 12705 => Loss: 6.73550749789031666381\n",
      "Iteration 12706 => Loss: 6.73550246772140770446\n",
      "Iteration 12707 => Loss: 6.73549743823032009971\n",
      "Iteration 12708 => Loss: 6.73549240941694904450\n",
      "Iteration 12709 => Loss: 6.73548738128121993185\n",
      "Iteration 12710 => Loss: 6.73548235382302440399\n",
      "Iteration 12711 => Loss: 6.73547732704228341305\n",
      "Iteration 12712 => Loss: 6.73547230093889925939\n",
      "Iteration 12713 => Loss: 6.73546727551278134882\n",
      "Iteration 12714 => Loss: 6.73546225076384263986\n",
      "Iteration 12715 => Loss: 6.73545722669199165011\n",
      "Iteration 12716 => Loss: 6.73545220329713156815\n",
      "Iteration 12717 => Loss: 6.73544718057917357612\n",
      "Iteration 12718 => Loss: 6.73544215853802974436\n",
      "Iteration 12719 => Loss: 6.73543713717360414961\n",
      "Iteration 12720 => Loss: 6.73543211648581063855\n",
      "Iteration 12721 => Loss: 6.73542709647455062338\n",
      "Iteration 12722 => Loss: 6.73542207713973972716\n",
      "Iteration 12723 => Loss: 6.73541705848129002021\n",
      "Iteration 12724 => Loss: 6.73541204049909847384\n",
      "Iteration 12725 => Loss: 6.73540702319307982293\n",
      "Iteration 12726 => Loss: 6.73540200656314791416\n",
      "Iteration 12727 => Loss: 6.73539699060920771245\n",
      "Iteration 12728 => Loss: 6.73539197533115974181\n",
      "Iteration 12729 => Loss: 6.73538696072893028344\n",
      "Iteration 12730 => Loss: 6.73538194680241364409\n",
      "Iteration 12731 => Loss: 6.73537693355152544683\n",
      "Iteration 12732 => Loss: 6.73537192097617332109\n",
      "Iteration 12733 => Loss: 6.73536690907626667268\n",
      "Iteration 12734 => Loss: 6.73536189785171135469\n",
      "Iteration 12735 => Loss: 6.73535688730242121380\n",
      "Iteration 12736 => Loss: 6.73535187742830032676\n",
      "Iteration 12737 => Loss: 6.73534686822926342842\n",
      "Iteration 12738 => Loss: 6.73534185970521637188\n",
      "Iteration 12739 => Loss: 6.73533685185606945112\n",
      "Iteration 12740 => Loss: 6.73533184468172763104\n",
      "Iteration 12741 => Loss: 6.73532683818209942928\n",
      "Iteration 12742 => Loss: 6.73532183235710757430\n",
      "Iteration 12743 => Loss: 6.73531682720664370834\n",
      "Iteration 12744 => Loss: 6.73531182273062700716\n",
      "Iteration 12745 => Loss: 6.73530681892896243568\n",
      "Iteration 12746 => Loss: 6.73530181580155851151\n",
      "Iteration 12747 => Loss: 6.73529681334832641681\n",
      "Iteration 12748 => Loss: 6.73529181156918088647\n",
      "Iteration 12749 => Loss: 6.73528681046401977994\n",
      "Iteration 12750 => Loss: 6.73528181003276138483\n",
      "Iteration 12751 => Loss: 6.73527681027531244240\n",
      "Iteration 12752 => Loss: 6.73527181119157969391\n",
      "Iteration 12753 => Loss: 6.73526681278147432153\n",
      "Iteration 12754 => Loss: 6.73526181504490395469\n",
      "Iteration 12755 => Loss: 6.73525681798177799919\n",
      "Iteration 12756 => Loss: 6.73525182159200497267\n",
      "Iteration 12757 => Loss: 6.73524682587550138635\n",
      "Iteration 12758 => Loss: 6.73524183083216865242\n",
      "Iteration 12759 => Loss: 6.73523683646191972940\n",
      "Iteration 12760 => Loss: 6.73523184276465869402\n",
      "Iteration 12761 => Loss: 6.73522684974030383387\n",
      "Iteration 12762 => Loss: 6.73522185738875123207\n",
      "Iteration 12763 => Loss: 6.73521686570992361709\n",
      "Iteration 12764 => Loss: 6.73521187470372773021\n",
      "Iteration 12765 => Loss: 6.73520688437007031268\n",
      "Iteration 12766 => Loss: 6.73520189470885544125\n",
      "Iteration 12767 => Loss: 6.73519690572000229167\n",
      "Iteration 12768 => Loss: 6.73519191740341138797\n",
      "Iteration 12769 => Loss: 6.73518692975899835318\n",
      "Iteration 12770 => Loss: 6.73518194278667614583\n",
      "Iteration 12771 => Loss: 6.73517695648634440175\n",
      "Iteration 12772 => Loss: 6.73517197085791785582\n",
      "Iteration 12773 => Loss: 6.73516698590130147295\n",
      "Iteration 12774 => Loss: 6.73516200161641265254\n",
      "Iteration 12775 => Loss: 6.73515701800315369496\n",
      "Iteration 12776 => Loss: 6.73515203506143844692\n",
      "Iteration 12777 => Loss: 6.73514705279117809056\n",
      "Iteration 12778 => Loss: 6.73514207119227137355\n",
      "Iteration 12779 => Loss: 6.73513709026464546525\n",
      "Iteration 12780 => Loss: 6.73513211000819289609\n",
      "Iteration 12781 => Loss: 6.73512713042282840092\n",
      "Iteration 12782 => Loss: 6.73512215150847115552\n",
      "Iteration 12783 => Loss: 6.73511717326501635483\n",
      "Iteration 12784 => Loss: 6.73511219569238672733\n",
      "Iteration 12785 => Loss: 6.73510721879047569161\n",
      "Iteration 12786 => Loss: 6.73510224255921219338\n",
      "Iteration 12787 => Loss: 6.73509726699849053944\n",
      "Iteration 12788 => Loss: 6.73509229210822635281\n",
      "Iteration 12789 => Loss: 6.73508731788833880927\n",
      "Iteration 12790 => Loss: 6.73508234433871688651\n",
      "Iteration 12791 => Loss: 6.73507737145928331302\n",
      "Iteration 12792 => Loss: 6.73507239924994305369\n",
      "Iteration 12793 => Loss: 6.73506742771061528430\n",
      "Iteration 12794 => Loss: 6.73506245684119608796\n",
      "Iteration 12795 => Loss: 6.73505748664160641681\n",
      "Iteration 12796 => Loss: 6.73505251711174857121\n",
      "Iteration 12797 => Loss: 6.73504754825153728603\n",
      "Iteration 12798 => Loss: 6.73504258006087841437\n",
      "Iteration 12799 => Loss: 6.73503761253968757927\n",
      "Iteration 12800 => Loss: 6.73503264568786885746\n",
      "Iteration 12801 => Loss: 6.73502767950532987840\n",
      "Iteration 12802 => Loss: 6.73502271399198981783\n",
      "Iteration 12803 => Loss: 6.73501774914774742342\n",
      "Iteration 12804 => Loss: 6.73501278497252275912\n",
      "Iteration 12805 => Loss: 6.73500782146621546076\n",
      "Iteration 12806 => Loss: 6.73500285862874648046\n",
      "Iteration 12807 => Loss: 6.73499789646002167132\n",
      "Iteration 12808 => Loss: 6.73499293495994955094\n",
      "Iteration 12809 => Loss: 6.73498797412843241972\n",
      "Iteration 12810 => Loss: 6.73498301396539567065\n",
      "Iteration 12811 => Loss: 6.73497805447073627505\n",
      "Iteration 12812 => Loss: 6.73497309564436985596\n",
      "Iteration 12813 => Loss: 6.73496813748621026008\n",
      "Iteration 12814 => Loss: 6.73496317999615712324\n",
      "Iteration 12815 => Loss: 6.73495822317413406211\n",
      "Iteration 12816 => Loss: 6.73495326702003538344\n",
      "Iteration 12817 => Loss: 6.73494831153378026301\n",
      "Iteration 12818 => Loss: 6.73494335671527455389\n",
      "Iteration 12819 => Loss: 6.73493840256443920822\n",
      "Iteration 12820 => Loss: 6.73493344908117119729\n",
      "Iteration 12821 => Loss: 6.73492849626538614416\n",
      "Iteration 12822 => Loss: 6.73492354411699434280\n",
      "Iteration 12823 => Loss: 6.73491859263590164630\n",
      "Iteration 12824 => Loss: 6.73491364182202012501\n",
      "Iteration 12825 => Loss: 6.73490869167526806649\n",
      "Iteration 12826 => Loss: 6.73490374219554688295\n",
      "Iteration 12827 => Loss: 6.73489879338276065113\n",
      "Iteration 12828 => Loss: 6.73489384523683476402\n",
      "Iteration 12829 => Loss: 6.73488889775767596291\n",
      "Iteration 12830 => Loss: 6.73488395094518299544\n",
      "Iteration 12831 => Loss: 6.73487900479927414921\n",
      "Iteration 12832 => Loss: 6.73487405931986149454\n",
      "Iteration 12833 => Loss: 6.73486911450685354907\n",
      "Iteration 12834 => Loss: 6.73486417036015083681\n",
      "Iteration 12835 => Loss: 6.73485922687967875078\n",
      "Iteration 12836 => Loss: 6.73485428406533870316\n",
      "Iteration 12837 => Loss: 6.73484934191704365247\n",
      "Iteration 12838 => Loss: 6.73484440043470922177\n",
      "Iteration 12839 => Loss: 6.73483945961823504689\n",
      "Iteration 12840 => Loss: 6.73483451946753497452\n",
      "Iteration 12841 => Loss: 6.73482957998252640408\n",
      "Iteration 12842 => Loss: 6.73482464116310985958\n",
      "Iteration 12843 => Loss: 6.73481970300919652317\n",
      "Iteration 12844 => Loss: 6.73481476552070201791\n",
      "Iteration 12845 => Loss: 6.73480982869754196685\n",
      "Iteration 12846 => Loss: 6.73480489253961156493\n",
      "Iteration 12847 => Loss: 6.73479995704682909974\n",
      "Iteration 12848 => Loss: 6.73479502221910220072\n",
      "Iteration 12849 => Loss: 6.73479008805635093182\n",
      "Iteration 12850 => Loss: 6.73478515455846871163\n",
      "Iteration 12851 => Loss: 6.73478022172538803858\n",
      "Iteration 12852 => Loss: 6.73477528955700321944\n",
      "Iteration 12853 => Loss: 6.73477035805322366002\n",
      "Iteration 12854 => Loss: 6.73476542721396320701\n",
      "Iteration 12855 => Loss: 6.73476049703913837163\n",
      "Iteration 12856 => Loss: 6.73475556752865145427\n",
      "Iteration 12857 => Loss: 6.73475063868242251885\n",
      "Iteration 12858 => Loss: 6.73474571050035475395\n",
      "Iteration 12859 => Loss: 6.73474078298235845352\n",
      "Iteration 12860 => Loss: 6.73473585612834479974\n",
      "Iteration 12861 => Loss: 6.73473092993822586294\n",
      "Iteration 12862 => Loss: 6.73472600441190660803\n",
      "Iteration 12863 => Loss: 6.73472107954931420437\n",
      "Iteration 12864 => Loss: 6.73471615535033318878\n",
      "Iteration 12865 => Loss: 6.73471123181489517151\n",
      "Iteration 12866 => Loss: 6.73470630894290245294\n",
      "Iteration 12867 => Loss: 6.73470138673427420883\n",
      "Iteration 12868 => Loss: 6.73469646518890030507\n",
      "Iteration 12869 => Loss: 6.73469154430671856915\n",
      "Iteration 12870 => Loss: 6.73468662408761709059\n",
      "Iteration 12871 => Loss: 6.73468170453151504518\n",
      "Iteration 12872 => Loss: 6.73467678563833249683\n",
      "Iteration 12873 => Loss: 6.73467186740796730504\n",
      "Iteration 12874 => Loss: 6.73466694984033331650\n",
      "Iteration 12875 => Loss: 6.73466203293533727248\n",
      "Iteration 12876 => Loss: 6.73465711669289923691\n",
      "Iteration 12877 => Loss: 6.73465220111292417471\n",
      "Iteration 12878 => Loss: 6.73464728619532948528\n",
      "Iteration 12879 => Loss: 6.73464237194001213993\n",
      "Iteration 12880 => Loss: 6.73463745834689131442\n",
      "Iteration 12881 => Loss: 6.73463254541588263180\n",
      "Iteration 12882 => Loss: 6.73462763314689194516\n",
      "Iteration 12883 => Loss: 6.73462272153982421941\n",
      "Iteration 12884 => Loss: 6.73461781059460129484\n",
      "Iteration 12885 => Loss: 6.73461290031112991272\n",
      "Iteration 12886 => Loss: 6.73460799068931059708\n",
      "Iteration 12887 => Loss: 6.73460308172907229363\n",
      "Iteration 12888 => Loss: 6.73459817343030753278\n",
      "Iteration 12889 => Loss: 6.73459326579293993120\n",
      "Iteration 12890 => Loss: 6.73458835881688422376\n",
      "Iteration 12891 => Loss: 6.73458345250203738175\n",
      "Iteration 12892 => Loss: 6.73457854684832035730\n",
      "Iteration 12893 => Loss: 6.73457364185563722714\n",
      "Iteration 12894 => Loss: 6.73456873752390361432\n",
      "Iteration 12895 => Loss: 6.73456383385303336553\n",
      "Iteration 12896 => Loss: 6.73455893084292700479\n",
      "Iteration 12897 => Loss: 6.73455402849350104333\n",
      "Iteration 12898 => Loss: 6.73454912680466932784\n",
      "Iteration 12899 => Loss: 6.73454422577634748137\n",
      "Iteration 12900 => Loss: 6.73453932540842625798\n",
      "Iteration 12901 => Loss: 6.73453442570083549157\n",
      "Iteration 12902 => Loss: 6.73452952665348103523\n",
      "Iteration 12903 => Loss: 6.73452462826627940018\n",
      "Iteration 12904 => Loss: 6.73451973053912400502\n",
      "Iteration 12905 => Loss: 6.73451483347194468365\n",
      "Iteration 12906 => Loss: 6.73450993706464551281\n",
      "Iteration 12907 => Loss: 6.73450504131714122735\n",
      "Iteration 12908 => Loss: 6.73450014622933323949\n",
      "Iteration 12909 => Loss: 6.73449525180114427769\n",
      "Iteration 12910 => Loss: 6.73449035803247486598\n",
      "Iteration 12911 => Loss: 6.73448546492323973922\n",
      "Iteration 12912 => Loss: 6.73448057247335718500\n",
      "Iteration 12913 => Loss: 6.73447568068273128006\n",
      "Iteration 12914 => Loss: 6.73447078955126965383\n",
      "Iteration 12915 => Loss: 6.73446589907889769933\n",
      "Iteration 12916 => Loss: 6.73446100926550794696\n",
      "Iteration 12917 => Loss: 6.73445612011103111882\n",
      "Iteration 12918 => Loss: 6.73445123161535263989\n",
      "Iteration 12919 => Loss: 6.73444634377841122586\n",
      "Iteration 12920 => Loss: 6.73444145660010295984\n",
      "Iteration 12921 => Loss: 6.73443657008034168854\n",
      "Iteration 12922 => Loss: 6.73443168421904303500\n",
      "Iteration 12923 => Loss: 6.73442679901611018778\n",
      "Iteration 12924 => Loss: 6.73442191447146409899\n",
      "Iteration 12925 => Loss: 6.73441703058500440449\n",
      "Iteration 12926 => Loss: 6.73441214735665383273\n",
      "Iteration 12927 => Loss: 6.73440726478631290775\n",
      "Iteration 12928 => Loss: 6.73440238287390524619\n",
      "Iteration 12929 => Loss: 6.73439750161933048389\n",
      "Iteration 12930 => Loss: 6.73439262102250602027\n",
      "Iteration 12931 => Loss: 6.73438774108334925472\n",
      "Iteration 12932 => Loss: 6.73438286180175715856\n",
      "Iteration 12933 => Loss: 6.73437798317764713119\n",
      "Iteration 12934 => Loss: 6.73437310521093568383\n",
      "Iteration 12935 => Loss: 6.73436822790153222229\n",
      "Iteration 12936 => Loss: 6.73436335124934348784\n",
      "Iteration 12937 => Loss: 6.73435847525428421534\n",
      "Iteration 12938 => Loss: 6.73435359991627180420\n",
      "Iteration 12939 => Loss: 6.73434872523520233756\n",
      "Iteration 12940 => Loss: 6.73434385121100653748\n",
      "Iteration 12941 => Loss: 6.73433897784357604621\n",
      "Iteration 12942 => Loss: 6.73433410513283092769\n",
      "Iteration 12943 => Loss: 6.73432923307869479856\n",
      "Iteration 12944 => Loss: 6.73432436168106018926\n",
      "Iteration 12945 => Loss: 6.73431949093985160459\n",
      "Iteration 12946 => Loss: 6.73431462085497578585\n",
      "Iteration 12947 => Loss: 6.73430975142633769792\n",
      "Iteration 12948 => Loss: 6.73430488265386273383\n",
      "Iteration 12949 => Loss: 6.73430001453745408213\n",
      "Iteration 12950 => Loss: 6.73429514707702292498\n",
      "Iteration 12951 => Loss: 6.73429028027247777999\n",
      "Iteration 12952 => Loss: 6.73428541412374492836\n",
      "Iteration 12953 => Loss: 6.73428054863071778868\n",
      "Iteration 12954 => Loss: 6.73427568379331642490\n",
      "Iteration 12955 => Loss: 6.73427081961145646005\n",
      "Iteration 12956 => Loss: 6.73426595608504285906\n",
      "Iteration 12957 => Loss: 6.73426109321398946861\n",
      "Iteration 12958 => Loss: 6.73425623099821457629\n",
      "Iteration 12959 => Loss: 6.73425136943761426522\n",
      "Iteration 12960 => Loss: 6.73424650853211481660\n",
      "Iteration 12961 => Loss: 6.73424164828162119534\n",
      "Iteration 12962 => Loss: 6.73423678868604991266\n",
      "Iteration 12963 => Loss: 6.73423192974531037436\n",
      "Iteration 12964 => Loss: 6.73422707145930576900\n",
      "Iteration 12965 => Loss: 6.73422221382796237776\n",
      "Iteration 12966 => Loss: 6.73421735685117983650\n",
      "Iteration 12967 => Loss: 6.73421250052888531457\n",
      "Iteration 12968 => Loss: 6.73420764486096778967\n",
      "Iteration 12969 => Loss: 6.73420278984736331296\n",
      "Iteration 12970 => Loss: 6.73419793548795464488\n",
      "Iteration 12971 => Loss: 6.73419308178269027110\n",
      "Iteration 12972 => Loss: 6.73418822873146005747\n",
      "Iteration 12973 => Loss: 6.73418337633416630439\n",
      "Iteration 12974 => Loss: 6.73417852459074239846\n",
      "Iteration 12975 => Loss: 6.73417367350109064006\n",
      "Iteration 12976 => Loss: 6.73416882306512754042\n",
      "Iteration 12977 => Loss: 6.73416397328275628809\n",
      "Iteration 12978 => Loss: 6.73415912415390138790\n",
      "Iteration 12979 => Loss: 6.73415427567846069934\n",
      "Iteration 12980 => Loss: 6.73414942785634362821\n",
      "Iteration 12981 => Loss: 6.73414458068747645569\n",
      "Iteration 12982 => Loss: 6.73413973417177391667\n",
      "Iteration 12983 => Loss: 6.73413488830913387062\n",
      "Iteration 12984 => Loss: 6.73413004309947726966\n",
      "Iteration 12985 => Loss: 6.73412519854270907871\n",
      "Iteration 12986 => Loss: 6.73412035463874847352\n",
      "Iteration 12987 => Loss: 6.73411551138750130718\n",
      "Iteration 12988 => Loss: 6.73411066878888497911\n",
      "Iteration 12989 => Loss: 6.73410582684280800692\n",
      "Iteration 12990 => Loss: 6.73410098554917713187\n",
      "Iteration 12991 => Loss: 6.73409614490792485242\n",
      "Iteration 12992 => Loss: 6.73409130491893304082\n",
      "Iteration 12993 => Loss: 6.73408646558214929456\n",
      "Iteration 12994 => Loss: 6.73408162689745459772\n",
      "Iteration 12995 => Loss: 6.73407678886477256697\n",
      "Iteration 12996 => Loss: 6.73407195148401971352\n",
      "Iteration 12997 => Loss: 6.73406711475510011411\n",
      "Iteration 12998 => Loss: 6.73406227867793472086\n",
      "Iteration 12999 => Loss: 6.73405744325242761050\n",
      "Iteration 13000 => Loss: 6.73405260847849795880\n",
      "Iteration 13001 => Loss: 6.73404777435605517155\n",
      "Iteration 13002 => Loss: 6.73404294088500510185\n",
      "Iteration 13003 => Loss: 6.73403810806526426092\n",
      "Iteration 13004 => Loss: 6.73403327589675271270\n",
      "Iteration 13005 => Loss: 6.73402844437937364575\n",
      "Iteration 13006 => Loss: 6.73402361351304179493\n",
      "Iteration 13007 => Loss: 6.73401878329767455966\n",
      "Iteration 13008 => Loss: 6.73401395373317779303\n",
      "Iteration 13009 => Loss: 6.73400912481945912447\n",
      "Iteration 13010 => Loss: 6.73400429655643595339\n",
      "Iteration 13011 => Loss: 6.73399946894403012010\n",
      "Iteration 13012 => Loss: 6.73399464198213859589\n",
      "Iteration 13013 => Loss: 6.73398981567068677379\n",
      "Iteration 13014 => Loss: 6.73398499000957517779\n",
      "Iteration 13015 => Loss: 6.73398016499872209550\n",
      "Iteration 13016 => Loss: 6.73397534063804048543\n",
      "Iteration 13017 => Loss: 6.73397051692744330609\n",
      "Iteration 13018 => Loss: 6.73396569386683907510\n",
      "Iteration 13019 => Loss: 6.73396087145614696823\n",
      "Iteration 13020 => Loss: 6.73395604969527106221\n",
      "Iteration 13021 => Loss: 6.73395122858413053279\n",
      "Iteration 13022 => Loss: 6.73394640812263212126\n",
      "Iteration 13023 => Loss: 6.73394158831069056248\n",
      "Iteration 13024 => Loss: 6.73393676914822769675\n",
      "Iteration 13025 => Loss: 6.73393195063514227172\n",
      "Iteration 13026 => Loss: 6.73392713277134724592\n",
      "Iteration 13027 => Loss: 6.73392231555676001875\n",
      "Iteration 13028 => Loss: 6.73391749899129532508\n",
      "Iteration 13029 => Loss: 6.73391268307486789979\n",
      "Iteration 13030 => Loss: 6.73390786780738537232\n",
      "Iteration 13031 => Loss: 6.73390305318875181939\n",
      "Iteration 13032 => Loss: 6.73389823921889441039\n",
      "Iteration 13033 => Loss: 6.73389342589771633385\n",
      "Iteration 13034 => Loss: 6.73388861322513676555\n",
      "Iteration 13035 => Loss: 6.73388380120106333493\n",
      "Iteration 13036 => Loss: 6.73387898982540900050\n",
      "Iteration 13037 => Loss: 6.73387417909809649075\n",
      "Iteration 13038 => Loss: 6.73386936901902277697\n",
      "Iteration 13039 => Loss: 6.73386455958810703493\n",
      "Iteration 13040 => Loss: 6.73385975080526932857\n",
      "Iteration 13041 => Loss: 6.73385494267040662919\n",
      "Iteration 13042 => Loss: 6.73385013518344432981\n",
      "Iteration 13043 => Loss: 6.73384532834429094805\n",
      "Iteration 13044 => Loss: 6.73384052215286033061\n",
      "Iteration 13045 => Loss: 6.73383571660906099510\n",
      "Iteration 13046 => Loss: 6.73383091171281389364\n",
      "Iteration 13047 => Loss: 6.73382610746402665569\n",
      "Iteration 13048 => Loss: 6.73382130386261046340\n",
      "Iteration 13049 => Loss: 6.73381650090848271617\n",
      "Iteration 13050 => Loss: 6.73381169860154393803\n",
      "Iteration 13051 => Loss: 6.73380689694172573923\n",
      "Iteration 13052 => Loss: 6.73380209592892686743\n",
      "Iteration 13053 => Loss: 6.73379729556306738658\n",
      "Iteration 13054 => Loss: 6.73379249584405759066\n",
      "Iteration 13055 => Loss: 6.73378769677181310271\n",
      "Iteration 13056 => Loss: 6.73378289834623711130\n",
      "Iteration 13057 => Loss: 6.73377810056725234489\n",
      "Iteration 13058 => Loss: 6.73377330343476820929\n",
      "Iteration 13059 => Loss: 6.73376850694870032754\n",
      "Iteration 13060 => Loss: 6.73376371110896076999\n",
      "Iteration 13061 => Loss: 6.73375891591545894244\n",
      "Iteration 13062 => Loss: 6.73375412136810869157\n",
      "Iteration 13063 => Loss: 6.73374932746682297591\n",
      "Iteration 13064 => Loss: 6.73374453421151919486\n",
      "Iteration 13065 => Loss: 6.73373974160210142514\n",
      "Iteration 13066 => Loss: 6.73373494963849328343\n",
      "Iteration 13067 => Loss: 6.73373015832060506369\n",
      "Iteration 13068 => Loss: 6.73372536764834350720\n",
      "Iteration 13069 => Loss: 6.73372057762162601335\n",
      "Iteration 13070 => Loss: 6.73371578824036376432\n",
      "Iteration 13071 => Loss: 6.73371099950447060678\n",
      "Iteration 13072 => Loss: 6.73370621141386038744\n",
      "Iteration 13073 => Loss: 6.73370142396844695298\n",
      "Iteration 13074 => Loss: 6.73369663716813970922\n",
      "Iteration 13075 => Loss: 6.73369185101285872008\n",
      "Iteration 13076 => Loss: 6.73368706550250450960\n",
      "Iteration 13077 => Loss: 6.73368228063700779984\n",
      "Iteration 13078 => Loss: 6.73367749641626556212\n",
      "Iteration 13079 => Loss: 6.73367271284020318944\n",
      "Iteration 13080 => Loss: 6.73366792990872475855\n",
      "Iteration 13081 => Loss: 6.73366314762174589248\n",
      "Iteration 13082 => Loss: 6.73365836597918043793\n",
      "Iteration 13083 => Loss: 6.73365358498094046524\n",
      "Iteration 13084 => Loss: 6.73364880462694159746\n",
      "Iteration 13085 => Loss: 6.73364402491709856946\n",
      "Iteration 13086 => Loss: 6.73363924585132256340\n",
      "Iteration 13087 => Loss: 6.73363446742951943236\n",
      "Iteration 13088 => Loss: 6.73362968965161190482\n",
      "Iteration 13089 => Loss: 6.73362491251751382748\n",
      "Iteration 13090 => Loss: 6.73362013602713371796\n",
      "Iteration 13091 => Loss: 6.73361536018038364659\n",
      "Iteration 13092 => Loss: 6.73361058497718278915\n",
      "Iteration 13093 => Loss: 6.73360581041743611053\n",
      "Iteration 13094 => Loss: 6.73360103650106101014\n",
      "Iteration 13095 => Loss: 6.73359626322798110465\n",
      "Iteration 13096 => Loss: 6.73359149059809070081\n",
      "Iteration 13097 => Loss: 6.73358671861131519165\n",
      "Iteration 13098 => Loss: 6.73358194726756931203\n",
      "Iteration 13099 => Loss: 6.73357717656675980322\n",
      "Iteration 13100 => Loss: 6.73357240650879962374\n",
      "Iteration 13101 => Loss: 6.73356763709360084391\n",
      "Iteration 13102 => Loss: 6.73356286832108974494\n",
      "Iteration 13103 => Loss: 6.73355810019117217990\n",
      "Iteration 13104 => Loss: 6.73355333270375755461\n",
      "Iteration 13105 => Loss: 6.73354856585875882757\n",
      "Iteration 13106 => Loss: 6.73354379965609517456\n",
      "Iteration 13107 => Loss: 6.73353903409568133043\n",
      "Iteration 13108 => Loss: 6.73353426917741604285\n",
      "Iteration 13109 => Loss: 6.73352950490123713934\n",
      "Iteration 13110 => Loss: 6.73352474126703892665\n",
      "Iteration 13111 => Loss: 6.73351997827473791602\n",
      "Iteration 13112 => Loss: 6.73351521592425950047\n",
      "Iteration 13113 => Loss: 6.73351045421550242764\n",
      "Iteration 13114 => Loss: 6.73350569314838764967\n",
      "Iteration 13115 => Loss: 6.73350093272281924328\n",
      "Iteration 13116 => Loss: 6.73349617293873059509\n",
      "Iteration 13117 => Loss: 6.73349141379601512369\n",
      "Iteration 13118 => Loss: 6.73348665529460088663\n",
      "Iteration 13119 => Loss: 6.73348189743438574340\n",
      "Iteration 13120 => Loss: 6.73347714021530219242\n",
      "Iteration 13121 => Loss: 6.73347238363724809318\n",
      "Iteration 13122 => Loss: 6.73346762770014262145\n",
      "Iteration 13123 => Loss: 6.73346287240390495299\n",
      "Iteration 13124 => Loss: 6.73345811774844271724\n",
      "Iteration 13125 => Loss: 6.73345336373367331362\n",
      "Iteration 13126 => Loss: 6.73344861035950259520\n",
      "Iteration 13127 => Loss: 6.73344385762584884958\n",
      "Iteration 13128 => Loss: 6.73343910553263302887\n",
      "Iteration 13129 => Loss: 6.73343435407975032803\n",
      "Iteration 13130 => Loss: 6.73342960326713146912\n",
      "Iteration 13131 => Loss: 6.73342485309469207522\n",
      "Iteration 13132 => Loss: 6.73342010356233533486\n",
      "Iteration 13133 => Loss: 6.73341535466997420656\n",
      "Iteration 13134 => Loss: 6.73341060641752964244\n",
      "Iteration 13135 => Loss: 6.73340585880490483106\n",
      "Iteration 13136 => Loss: 6.73340111183203582357\n",
      "Iteration 13137 => Loss: 6.73339636549881515037\n",
      "Iteration 13138 => Loss: 6.73339161980515843453\n",
      "Iteration 13139 => Loss: 6.73338687475099373358\n",
      "Iteration 13140 => Loss: 6.73338213033621091341\n",
      "Iteration 13141 => Loss: 6.73337738656075135424\n",
      "Iteration 13142 => Loss: 6.73337264342451380372\n",
      "Iteration 13143 => Loss: 6.73336790092740855584\n",
      "Iteration 13144 => Loss: 6.73336315906936011544\n",
      "Iteration 13145 => Loss: 6.73335841785027700013\n",
      "Iteration 13146 => Loss: 6.73335367727007216843\n",
      "Iteration 13147 => Loss: 6.73334893732866657245\n",
      "Iteration 13148 => Loss: 6.73334419802596606530\n",
      "Iteration 13149 => Loss: 6.73333945936187827641\n",
      "Iteration 13150 => Loss: 6.73333472133633126333\n",
      "Iteration 13151 => Loss: 6.73332998394923798458\n",
      "Iteration 13152 => Loss: 6.73332524720050074052\n",
      "Iteration 13153 => Loss: 6.73332051109004225964\n",
      "Iteration 13154 => Loss: 6.73331577561777816499\n",
      "Iteration 13155 => Loss: 6.73331104078361875054\n",
      "Iteration 13156 => Loss: 6.73330630658747963935\n",
      "Iteration 13157 => Loss: 6.73330157302927556628\n",
      "Iteration 13158 => Loss: 6.73329684010891593715\n",
      "Iteration 13159 => Loss: 6.73329210782631015775\n",
      "Iteration 13160 => Loss: 6.73328737618139339105\n",
      "Iteration 13161 => Loss: 6.73328264517405905565\n",
      "Iteration 13162 => Loss: 6.73327791480422721548\n",
      "Iteration 13163 => Loss: 6.73327318507180905272\n",
      "Iteration 13164 => Loss: 6.73326845597673084853\n",
      "Iteration 13165 => Loss: 6.73326372751890023238\n",
      "Iteration 13166 => Loss: 6.73325899969822128099\n",
      "Iteration 13167 => Loss: 6.73325427251462738099\n",
      "Iteration 13168 => Loss: 6.73324954596800928641\n",
      "Iteration 13169 => Loss: 6.73324482005830571296\n",
      "Iteration 13170 => Loss: 6.73324009478541185558\n",
      "Iteration 13171 => Loss: 6.73323537014924689004\n",
      "Iteration 13172 => Loss: 6.73323064614973088027\n",
      "Iteration 13173 => Loss: 6.73322592278677056754\n",
      "Iteration 13174 => Loss: 6.73322120006028690398\n",
      "Iteration 13175 => Loss: 6.73321647797019284809\n",
      "Iteration 13176 => Loss: 6.73321175651639514115\n",
      "Iteration 13177 => Loss: 6.73320703569882184070\n",
      "Iteration 13178 => Loss: 6.73320231551736814168\n",
      "Iteration 13179 => Loss: 6.73319759597196920708\n",
      "Iteration 13180 => Loss: 6.73319287706253000181\n",
      "Iteration 13181 => Loss: 6.73318815878895282623\n",
      "Iteration 13182 => Loss: 6.73318344115117373150\n",
      "Iteration 13183 => Loss: 6.73317872414909235346\n",
      "Iteration 13184 => Loss: 6.73317400778263230876\n",
      "Iteration 13185 => Loss: 6.73316929205169856232\n",
      "Iteration 13186 => Loss: 6.73316457695620940171\n",
      "Iteration 13187 => Loss: 6.73315986249607867364\n",
      "Iteration 13188 => Loss: 6.73315514867122555387\n",
      "Iteration 13189 => Loss: 6.73315043548155944819\n",
      "Iteration 13190 => Loss: 6.73314572292699686784\n",
      "Iteration 13191 => Loss: 6.73314101100744810680\n",
      "Iteration 13192 => Loss: 6.73313629972283322900\n",
      "Iteration 13193 => Loss: 6.73313158907307052203\n",
      "Iteration 13194 => Loss: 6.73312687905805606903\n",
      "Iteration 13195 => Loss: 6.73312216967772414478\n",
      "Iteration 13196 => Loss: 6.73311746093197704965\n",
      "Iteration 13197 => Loss: 6.73311275282074106485\n",
      "Iteration 13198 => Loss: 6.73310804534391671439\n",
      "Iteration 13199 => Loss: 6.73310333850142406220\n",
      "Iteration 13200 => Loss: 6.73309863229318317224\n",
      "Iteration 13201 => Loss: 6.73309392671910611483\n",
      "Iteration 13202 => Loss: 6.73308922177909963125\n",
      "Iteration 13203 => Loss: 6.73308451747308911450\n",
      "Iteration 13204 => Loss: 6.73307981380097864132\n",
      "Iteration 13205 => Loss: 6.73307511076269271655\n",
      "Iteration 13206 => Loss: 6.73307040835814074597\n",
      "Iteration 13207 => Loss: 6.73306570658723835265\n",
      "Iteration 13208 => Loss: 6.73306100544989938328\n",
      "Iteration 13209 => Loss: 6.73305630494604123726\n",
      "Iteration 13210 => Loss: 6.73305160507557420857\n",
      "Iteration 13211 => Loss: 6.73304690583841303209\n",
      "Iteration 13212 => Loss: 6.73304220723447688357\n",
      "Iteration 13213 => Loss: 6.73303750926368138607\n",
      "Iteration 13214 => Loss: 6.73303281192593150450\n",
      "Iteration 13215 => Loss: 6.73302811522115440823\n",
      "Iteration 13216 => Loss: 6.73302341914925239763\n",
      "Iteration 13217 => Loss: 6.73301872371014820118\n",
      "Iteration 13218 => Loss: 6.73301402890376099464\n",
      "Iteration 13219 => Loss: 6.73300933472999041385\n",
      "Iteration 13220 => Loss: 6.73300464118876718089\n",
      "Iteration 13221 => Loss: 6.73299994827999270797\n",
      "Iteration 13222 => Loss: 6.73299525600359238808\n",
      "Iteration 13223 => Loss: 6.73299056435947651522\n",
      "Iteration 13224 => Loss: 6.73298587334755893608\n",
      "Iteration 13225 => Loss: 6.73298118296775083280\n",
      "Iteration 13226 => Loss: 6.73297649321997937477\n",
      "Iteration 13227 => Loss: 6.73297180410414686236\n",
      "Iteration 13228 => Loss: 6.73296711562018224129\n",
      "Iteration 13229 => Loss: 6.73296242776797804197\n",
      "Iteration 13230 => Loss: 6.73295774054746765103\n",
      "Iteration 13231 => Loss: 6.73295305395856846786\n",
      "Iteration 13232 => Loss: 6.73294836800116947018\n",
      "Iteration 13233 => Loss: 6.73294368267522358451\n",
      "Iteration 13234 => Loss: 6.73293899798061090678\n",
      "Iteration 13235 => Loss: 6.73293431391726926449\n",
      "Iteration 13236 => Loss: 6.73292963048510006985\n",
      "Iteration 13237 => Loss: 6.73292494768402871586\n",
      "Iteration 13238 => Loss: 6.73292026551396727285\n",
      "Iteration 13239 => Loss: 6.73291558397481981757\n",
      "Iteration 13240 => Loss: 6.73291090306652240116\n",
      "Iteration 13241 => Loss: 6.73290622278896844222\n",
      "Iteration 13242 => Loss: 6.73290154314208066921\n",
      "Iteration 13243 => Loss: 6.73289686412577825791\n",
      "Iteration 13244 => Loss: 6.73289218573997860773\n",
      "Iteration 13245 => Loss: 6.73288750798458313085\n",
      "Iteration 13246 => Loss: 6.73288283085952077300\n",
      "Iteration 13247 => Loss: 6.73287815436469738728\n",
      "Iteration 13248 => Loss: 6.73287347850004014305\n",
      "Iteration 13249 => Loss: 6.73286880326545134068\n",
      "Iteration 13250 => Loss: 6.73286412866085282047\n",
      "Iteration 13251 => Loss: 6.73285945468615398823\n",
      "Iteration 13252 => Loss: 6.73285478134127757244\n",
      "Iteration 13253 => Loss: 6.73285010862612853799\n",
      "Iteration 13254 => Loss: 6.73284543654063227791\n",
      "Iteration 13255 => Loss: 6.73284076508469908617\n",
      "Iteration 13256 => Loss: 6.73283609425824636219\n",
      "Iteration 13257 => Loss: 6.73283142406118706447\n",
      "Iteration 13258 => Loss: 6.73282675449343681606\n",
      "Iteration 13259 => Loss: 6.73282208555490857549\n",
      "Iteration 13260 => Loss: 6.73281741724552684758\n",
      "Iteration 13261 => Loss: 6.73281274956519482089\n",
      "Iteration 13262 => Loss: 6.73280808251383788843\n",
      "Iteration 13263 => Loss: 6.73280341609135923875\n",
      "Iteration 13264 => Loss: 6.73279875029768959394\n",
      "Iteration 13265 => Loss: 6.73279408513272770165\n",
      "Iteration 13266 => Loss: 6.73278942059640073126\n",
      "Iteration 13267 => Loss: 6.73278475668861986492\n",
      "Iteration 13268 => Loss: 6.73278009340930161386\n",
      "Iteration 13269 => Loss: 6.73277543075836781838\n",
      "Iteration 13270 => Loss: 6.73277076873571278526\n",
      "Iteration 13271 => Loss: 6.73276610734127523017\n",
      "Iteration 13272 => Loss: 6.73276144657495834167\n",
      "Iteration 13273 => Loss: 6.73275678643668218371\n",
      "Iteration 13274 => Loss: 6.73275212692635616207\n",
      "Iteration 13275 => Loss: 6.73274746804390478161\n",
      "Iteration 13276 => Loss: 6.73274280978923211904\n",
      "Iteration 13277 => Loss: 6.73273815216225823832\n",
      "Iteration 13278 => Loss: 6.73273349516290586791\n",
      "Iteration 13279 => Loss: 6.73272883879108441363\n",
      "Iteration 13280 => Loss: 6.73272418304670861033\n",
      "Iteration 13281 => Loss: 6.73271952792969674562\n",
      "Iteration 13282 => Loss: 6.73271487343995911345\n",
      "Iteration 13283 => Loss: 6.73271021957741311326\n",
      "Iteration 13284 => Loss: 6.73270556634197614443\n",
      "Iteration 13285 => Loss: 6.73270091373356560638\n",
      "Iteration 13286 => Loss: 6.73269626175208824037\n",
      "Iteration 13287 => Loss: 6.73269161039747476849\n",
      "Iteration 13288 => Loss: 6.73268695966962216204\n",
      "Iteration 13289 => Loss: 6.73268230956846025492\n",
      "Iteration 13290 => Loss: 6.73267766009389845294\n",
      "Iteration 13291 => Loss: 6.73267301124585948457\n",
      "Iteration 13292 => Loss: 6.73266836302425009109\n",
      "Iteration 13293 => Loss: 6.73266371542898323099\n",
      "Iteration 13294 => Loss: 6.73265906845998962638\n",
      "Iteration 13295 => Loss: 6.73265442211716624854\n",
      "Iteration 13296 => Loss: 6.73264977640043937868\n",
      "Iteration 13297 => Loss: 6.73264513130973174526\n",
      "Iteration 13298 => Loss: 6.73264048684494298413\n",
      "Iteration 13299 => Loss: 6.73263584300600292920\n",
      "Iteration 13300 => Loss: 6.73263119979281210448\n",
      "Iteration 13301 => Loss: 6.73262655720529412662\n",
      "Iteration 13302 => Loss: 6.73262191524337527682\n",
      "Iteration 13303 => Loss: 6.73261727390694808548\n",
      "Iteration 13304 => Loss: 6.73261263319595038013\n",
      "Iteration 13305 => Loss: 6.73260799311028712566\n",
      "Iteration 13306 => Loss: 6.73260335364987838602\n",
      "Iteration 13307 => Loss: 6.73259871481463001430\n",
      "Iteration 13308 => Loss: 6.73259407660446651533\n",
      "Iteration 13309 => Loss: 6.73258943901930706488\n",
      "Iteration 13310 => Loss: 6.73258480205906462146\n",
      "Iteration 13311 => Loss: 6.73258016572364326180\n",
      "Iteration 13312 => Loss: 6.73257553001297459616\n",
      "Iteration 13313 => Loss: 6.73257089492696803035\n",
      "Iteration 13314 => Loss: 6.73256626046553741105\n",
      "Iteration 13315 => Loss: 6.73256162662860635493\n",
      "Iteration 13316 => Loss: 6.73255699341608071506\n",
      "Iteration 13317 => Loss: 6.73255236082788410812\n",
      "Iteration 13318 => Loss: 6.73254772886392505171\n",
      "Iteration 13319 => Loss: 6.73254309752412716250\n",
      "Iteration 13320 => Loss: 6.73253846680839718175\n",
      "Iteration 13321 => Loss: 6.73253383671666316701\n",
      "Iteration 13322 => Loss: 6.73252920724882741865\n",
      "Iteration 13323 => Loss: 6.73252457840482065876\n",
      "Iteration 13324 => Loss: 6.73251995018454785225\n",
      "Iteration 13325 => Loss: 6.73251532258792373398\n",
      "Iteration 13326 => Loss: 6.73251069561487458515\n",
      "Iteration 13327 => Loss: 6.73250606926530803520\n",
      "Iteration 13328 => Loss: 6.73250144353913970718\n",
      "Iteration 13329 => Loss: 6.73249681843629321776\n",
      "Iteration 13330 => Loss: 6.73249219395667353183\n",
      "Iteration 13331 => Loss: 6.73248757010020870695\n",
      "Iteration 13332 => Loss: 6.73248294686680726073\n",
      "Iteration 13333 => Loss: 6.73247832425638925713\n",
      "Iteration 13334 => Loss: 6.73247370226886232558\n",
      "Iteration 13335 => Loss: 6.73246908090415274728\n",
      "Iteration 13336 => Loss: 6.73246446016216459896\n",
      "Iteration 13337 => Loss: 6.73245984004283481994\n",
      "Iteration 13338 => Loss: 6.73245522054606393425\n",
      "Iteration 13339 => Loss: 6.73245060167176401222\n",
      "Iteration 13340 => Loss: 6.73244598341985689416\n",
      "Iteration 13341 => Loss: 6.73244136579026264400\n",
      "Iteration 13342 => Loss: 6.73243674878290043750\n",
      "Iteration 13343 => Loss: 6.73243213239767790412\n",
      "Iteration 13344 => Loss: 6.73242751663450977873\n",
      "Iteration 13345 => Loss: 6.73242290149331346072\n",
      "Iteration 13346 => Loss: 6.73241828697401256676\n",
      "Iteration 13347 => Loss: 6.73241367307651206175\n",
      "Iteration 13348 => Loss: 6.73240905980074977322\n",
      "Iteration 13349 => Loss: 6.73240444714661290249\n",
      "Iteration 13350 => Loss: 6.73239983511403572436\n",
      "Iteration 13351 => Loss: 6.73239522370293119735\n",
      "Iteration 13352 => Loss: 6.73239061291321583269\n",
      "Iteration 13353 => Loss: 6.73238600274479814800\n",
      "Iteration 13354 => Loss: 6.73238139319760886536\n",
      "Iteration 13355 => Loss: 6.73237678427154850880\n",
      "Iteration 13356 => Loss: 6.73237217596654691221\n",
      "Iteration 13357 => Loss: 6.73236756828250992868\n",
      "Iteration 13358 => Loss: 6.73236296121936472758\n",
      "Iteration 13359 => Loss: 6.73235835477702337926\n",
      "Iteration 13360 => Loss: 6.73235374895539351314\n",
      "Iteration 13361 => Loss: 6.73234914375440141043\n",
      "Iteration 13362 => Loss: 6.73234453917396269418\n",
      "Iteration 13363 => Loss: 6.73233993521398765836\n",
      "Iteration 13364 => Loss: 6.73233533187439547874\n",
      "Iteration 13365 => Loss: 6.73233072915510888379\n",
      "Iteration 13366 => Loss: 6.73232612705603283842\n",
      "Iteration 13367 => Loss: 6.73232152557709984109\n",
      "Iteration 13368 => Loss: 6.73231692471820331036\n",
      "Iteration 13369 => Loss: 6.73231232447928285012\n",
      "Iteration 13370 => Loss: 6.73230772486024875434\n",
      "Iteration 13371 => Loss: 6.73230312586100065886\n",
      "Iteration 13372 => Loss: 6.73229852748147905572\n",
      "Iteration 13373 => Loss: 6.73229392972157913988\n",
      "Iteration 13374 => Loss: 6.73228933258123696248\n",
      "Iteration 13375 => Loss: 6.73228473606035304755\n",
      "Iteration 13376 => Loss: 6.73228014015885101173\n",
      "Iteration 13377 => Loss: 6.73227554487665447169\n",
      "Iteration 13378 => Loss: 6.73227095021367105687\n",
      "Iteration 13379 => Loss: 6.73226635616981727850\n",
      "Iteration 13380 => Loss: 6.73226176274500698327\n",
      "Iteration 13381 => Loss: 6.73225716993916289965\n",
      "Iteration 13382 => Loss: 6.73225257775219798617\n",
      "Iteration 13383 => Loss: 6.73224798618403585948\n",
      "Iteration 13384 => Loss: 6.73224339523458592538\n",
      "Iteration 13385 => Loss: 6.73223880490376203056\n",
      "Iteration 13386 => Loss: 6.73223421519149489711\n",
      "Iteration 13387 => Loss: 6.73222962609768860176\n",
      "Iteration 13388 => Loss: 6.73222503762226054391\n",
      "Iteration 13389 => Loss: 6.73222044976512457026\n",
      "Iteration 13390 => Loss: 6.73221586252621406743\n",
      "Iteration 13391 => Loss: 6.73221127590542867125\n",
      "Iteration 13392 => Loss: 6.73220668990269022203\n",
      "Iteration 13393 => Loss: 6.73220210451791611916\n",
      "Iteration 13394 => Loss: 6.73219751975102642660\n",
      "Iteration 13395 => Loss: 6.73219293560193499104\n",
      "Iteration 13396 => Loss: 6.73218835207055210645\n",
      "Iteration 13397 => Loss: 6.73218376915679606043\n",
      "Iteration 13398 => Loss: 6.73217918686060112776\n",
      "Iteration 13399 => Loss: 6.73217460518185983886\n",
      "Iteration 13400 => Loss: 6.73217002412050291582\n",
      "Iteration 13401 => Loss: 6.73216544367644864622\n",
      "Iteration 13402 => Loss: 6.73216086384960554767\n",
      "Iteration 13403 => Loss: 6.73215628463989812502\n",
      "Iteration 13404 => Loss: 6.73215170604724466585\n",
      "Iteration 13405 => Loss: 6.73214712807154747054\n",
      "Iteration 13406 => Loss: 6.73214255071273548481\n",
      "Iteration 13407 => Loss: 6.73213797397071900264\n",
      "Iteration 13408 => Loss: 6.73213339784542696975\n",
      "Iteration 13409 => Loss: 6.73212882233676435106\n",
      "Iteration 13410 => Loss: 6.73212424744465032234\n",
      "Iteration 13411 => Loss: 6.73211967316900672387\n",
      "Iteration 13412 => Loss: 6.73211509950974118510\n",
      "Iteration 13413 => Loss: 6.73211052646677909905\n",
      "Iteration 13414 => Loss: 6.73210595404004052966\n",
      "Iteration 13415 => Loss: 6.73210138222943221820\n",
      "Iteration 13416 => Loss: 6.73209681103487511677\n",
      "Iteration 13417 => Loss: 6.73209224045629017752\n",
      "Iteration 13418 => Loss: 6.73208767049358414170\n",
      "Iteration 13419 => Loss: 6.73208310114669128410\n",
      "Iteration 13420 => Loss: 6.73207853241550768786\n",
      "Iteration 13421 => Loss: 6.73207396429996851595\n",
      "Iteration 13422 => Loss: 6.73206939679998228598\n",
      "Iteration 13423 => Loss: 6.73206482991546106831\n",
      "Iteration 13424 => Loss: 6.73206026364633647319\n",
      "Iteration 13425 => Loss: 6.73205569799250902463\n",
      "Iteration 13426 => Loss: 6.73205113295390766837\n",
      "Iteration 13427 => Loss: 6.73204656853044713927\n",
      "Iteration 13428 => Loss: 6.73204200472204394856\n",
      "Iteration 13429 => Loss: 6.73203744152860839023\n",
      "Iteration 13430 => Loss: 6.73203287895006674546\n",
      "Iteration 13431 => Loss: 6.73202831698633552548\n",
      "Iteration 13432 => Loss: 6.73202375563732413610\n",
      "Iteration 13433 => Loss: 6.73201919490295885851\n",
      "Iteration 13434 => Loss: 6.73201463478314376943\n",
      "Iteration 13435 => Loss: 6.73201007527781669637\n",
      "Iteration 13436 => Loss: 6.73200551638688171607\n",
      "Iteration 13437 => Loss: 6.73200095811025800430\n",
      "Iteration 13438 => Loss: 6.73199640044785674320\n",
      "Iteration 13439 => Loss: 6.73199184339960421397\n",
      "Iteration 13440 => Loss: 6.73198728696541248695\n",
      "Iteration 13441 => Loss: 6.73198273114520340243\n",
      "Iteration 13442 => Loss: 6.73197817593888725440\n",
      "Iteration 13443 => Loss: 6.73197362134638854769\n",
      "Iteration 13444 => Loss: 6.73196906736762556989\n",
      "Iteration 13445 => Loss: 6.73196451400251039132\n",
      "Iteration 13446 => Loss: 6.73195996125095863505\n",
      "Iteration 13447 => Loss: 6.73195540911288503594\n",
      "Iteration 13448 => Loss: 6.73195085758821587518\n",
      "Iteration 13449 => Loss: 6.73194630667687032854\n",
      "Iteration 13450 => Loss: 6.73194175637875957818\n",
      "Iteration 13451 => Loss: 6.73193720669379214172\n",
      "Iteration 13452 => Loss: 6.73193265762190229395\n",
      "Iteration 13453 => Loss: 6.73192810916300565793\n",
      "Iteration 13454 => Loss: 6.73192356131700986310\n",
      "Iteration 13455 => Loss: 6.73191901408383497341\n",
      "Iteration 13456 => Loss: 6.73191446746340016460\n",
      "Iteration 13457 => Loss: 6.73190992145562105975\n",
      "Iteration 13458 => Loss: 6.73190537606041505825\n",
      "Iteration 13459 => Loss: 6.73190083127770666493\n",
      "Iteration 13460 => Loss: 6.73189628710740972650\n",
      "Iteration 13461 => Loss: 6.73189174354943631329\n",
      "Iteration 13462 => Loss: 6.73188720060371093012\n",
      "Iteration 13463 => Loss: 6.73188265827014298281\n",
      "Iteration 13464 => Loss: 6.73187811654866141708\n",
      "Iteration 13465 => Loss: 6.73187357543917652691\n",
      "Iteration 13466 => Loss: 6.73186903494159594175\n",
      "Iteration 13467 => Loss: 6.73186449505585660091\n",
      "Iteration 13468 => Loss: 6.73185995578187412747\n",
      "Iteration 13469 => Loss: 6.73185541711954549271\n",
      "Iteration 13470 => Loss: 6.73185087906880674780\n",
      "Iteration 13471 => Loss: 6.73184634162957351577\n",
      "Iteration 13472 => Loss: 6.73184180480175431427\n",
      "Iteration 13473 => Loss: 6.73183726858527542447\n",
      "Iteration 13474 => Loss: 6.73183273298005424579\n",
      "Iteration 13475 => Loss: 6.73182819798600551309\n",
      "Iteration 13476 => Loss: 6.73182366360305106667\n",
      "Iteration 13477 => Loss: 6.73181912983110475324\n",
      "Iteration 13478 => Loss: 6.73181459667007953129\n",
      "Iteration 13479 => Loss: 6.73181006411989457661\n",
      "Iteration 13480 => Loss: 6.73180553218047617037\n",
      "Iteration 13481 => Loss: 6.73180100085173460656\n",
      "Iteration 13482 => Loss: 6.73179647013359439001\n",
      "Iteration 13483 => Loss: 6.73179194002596936741\n",
      "Iteration 13484 => Loss: 6.73178741052877249729\n",
      "Iteration 13485 => Loss: 6.73178288164192828447\n",
      "Iteration 13486 => Loss: 6.73177835336535146382\n",
      "Iteration 13487 => Loss: 6.73177382569896121112\n",
      "Iteration 13488 => Loss: 6.73176929864267137305\n",
      "Iteration 13489 => Loss: 6.73176477219640467808\n",
      "Iteration 13490 => Loss: 6.73176024636007852564\n",
      "Iteration 13491 => Loss: 6.73175572113360320969\n",
      "Iteration 13492 => Loss: 6.73175119651690589961\n",
      "Iteration 13493 => Loss: 6.73174667250990577116\n",
      "Iteration 13494 => Loss: 6.73174214911251223015\n",
      "Iteration 13495 => Loss: 6.73173762632464711686\n",
      "Iteration 13496 => Loss: 6.73173310414622427800\n",
      "Iteration 13497 => Loss: 6.73172858257717443564\n",
      "Iteration 13498 => Loss: 6.73172406161739811381\n",
      "Iteration 13499 => Loss: 6.73171954126682692277\n",
      "Iteration 13500 => Loss: 6.73171502152536938013\n",
      "Iteration 13501 => Loss: 6.73171050239294643802\n",
      "Iteration 13502 => Loss: 6.73170598386947993674\n",
      "Iteration 13503 => Loss: 6.73170146595488461116\n",
      "Iteration 13504 => Loss: 6.73169694864907519616\n",
      "Iteration 13505 => Loss: 6.73169243195197086749\n",
      "Iteration 13506 => Loss: 6.73168791586350678813\n",
      "Iteration 13507 => Loss: 6.73168340038356749488\n",
      "Iteration 13508 => Loss: 6.73167888551210236159\n",
      "Iteration 13509 => Loss: 6.73167437124901013590\n",
      "Iteration 13510 => Loss: 6.73166985759421532265\n",
      "Iteration 13511 => Loss: 6.73166534454763887396\n",
      "Iteration 13512 => Loss: 6.73166083210919730107\n",
      "Iteration 13513 => Loss: 6.73165632027880445065\n",
      "Iteration 13514 => Loss: 6.73165180905637772213\n",
      "Iteration 13515 => Loss: 6.73164729844183984397\n",
      "Iteration 13516 => Loss: 6.73164278843511088013\n",
      "Iteration 13517 => Loss: 6.73163827903610467729\n",
      "Iteration 13518 => Loss: 6.73163377024474396393\n",
      "Iteration 13519 => Loss: 6.73162926206093725767\n",
      "Iteration 13520 => Loss: 6.73162475448461172789\n",
      "Iteration 13521 => Loss: 6.73162024751567944492\n",
      "Iteration 13522 => Loss: 6.73161574115406224905\n",
      "Iteration 13523 => Loss: 6.73161123539968020424\n",
      "Iteration 13524 => Loss: 6.73160673025244982171\n",
      "Iteration 13525 => Loss: 6.73160222571228583632\n",
      "Iteration 13526 => Loss: 6.73159772177910831203\n",
      "Iteration 13527 => Loss: 6.73159321845283287189\n",
      "Iteration 13528 => Loss: 6.73158871573339201433\n",
      "Iteration 13529 => Loss: 6.73158421362068448701\n",
      "Iteration 13530 => Loss: 6.73157971211463479477\n",
      "Iteration 13531 => Loss: 6.73157521121516833063\n",
      "Iteration 13532 => Loss: 6.73157071092219805308\n",
      "Iteration 13533 => Loss: 6.73156621123564047338\n",
      "Iteration 13534 => Loss: 6.73156171215542187269\n",
      "Iteration 13535 => Loss: 6.73155721368144988048\n",
      "Iteration 13536 => Loss: 6.73155271581364544886\n",
      "Iteration 13537 => Loss: 6.73154821855193308267\n",
      "Iteration 13538 => Loss: 6.73154372189622485223\n",
      "Iteration 13539 => Loss: 6.73153922584644082150\n",
      "Iteration 13540 => Loss: 6.73153473040250460713\n",
      "Iteration 13541 => Loss: 6.73153023556432117402\n",
      "Iteration 13542 => Loss: 6.73152574133182746152\n",
      "Iteration 13543 => Loss: 6.73152124770493021089\n",
      "Iteration 13544 => Loss: 6.73151675468354415699\n",
      "Iteration 13545 => Loss: 6.73151226226759558102\n",
      "Iteration 13546 => Loss: 6.73150777045700099421\n",
      "Iteration 13547 => Loss: 6.73150327925167424326\n",
      "Iteration 13548 => Loss: 6.73149878865154427388\n",
      "Iteration 13549 => Loss: 6.73149429865651871552\n",
      "Iteration 13550 => Loss: 6.73148980926652118484\n",
      "Iteration 13551 => Loss: 6.73148532048146464035\n",
      "Iteration 13552 => Loss: 6.73148083230128335686\n",
      "Iteration 13553 => Loss: 6.73147634472587785837\n",
      "Iteration 13554 => Loss: 6.73147185775517531425\n",
      "Iteration 13555 => Loss: 6.73146737138909134757\n",
      "Iteration 13556 => Loss: 6.73146288562754424589\n",
      "Iteration 13557 => Loss: 6.73145840047045052046\n",
      "Iteration 13558 => Loss: 6.73145391591773822881\n",
      "Iteration 13559 => Loss: 6.73144943196931855312\n",
      "Iteration 13560 => Loss: 6.73144494862511422184\n",
      "Iteration 13561 => Loss: 6.73144046588503375261\n",
      "Iteration 13562 => Loss: 6.73143598374901142023\n",
      "Iteration 13563 => Loss: 6.73143150221695307778\n",
      "Iteration 13564 => Loss: 6.73142702128877434831\n",
      "Iteration 13565 => Loss: 6.73142254096440861844\n",
      "Iteration 13566 => Loss: 6.73141806124376440579\n",
      "Iteration 13567 => Loss: 6.73141358212676799155\n",
      "Iteration 13568 => Loss: 6.73140910361333144607\n",
      "Iteration 13569 => Loss: 6.73140462570337039239\n",
      "Iteration 13570 => Loss: 6.73140014839681466441\n",
      "Iteration 13571 => Loss: 6.73139567169356478615\n",
      "Iteration 13572 => Loss: 6.73139119559356213784\n",
      "Iteration 13573 => Loss: 6.73138672009671346075\n",
      "Iteration 13574 => Loss: 6.73138224520293348974\n",
      "Iteration 13575 => Loss: 6.73137777091214584146\n",
      "Iteration 13576 => Loss: 6.73137329722427235623\n",
      "Iteration 13577 => Loss: 6.73136882413922954527\n",
      "Iteration 13578 => Loss: 6.73136435165693214344\n",
      "Iteration 13579 => Loss: 6.73135987977730376741\n",
      "Iteration 13580 => Loss: 6.73135540850026004023\n",
      "Iteration 13581 => Loss: 6.73135093782572013765\n",
      "Iteration 13582 => Loss: 6.73134646775360678816\n",
      "Iteration 13583 => Loss: 6.73134199828383472664\n",
      "Iteration 13584 => Loss: 6.73133752941632756972\n",
      "Iteration 13585 => Loss: 6.73133306115099561140\n",
      "Iteration 13586 => Loss: 6.73132859348776246833\n",
      "Iteration 13587 => Loss: 6.73132412642655264534\n",
      "Iteration 13588 => Loss: 6.73131965996727998913\n",
      "Iteration 13589 => Loss: 6.73131519410985834639\n",
      "Iteration 13590 => Loss: 6.73131072885420955743\n",
      "Iteration 13591 => Loss: 6.73130626420025812706\n",
      "Iteration 13592 => Loss: 6.73130180014791790200\n",
      "Iteration 13593 => Loss: 6.73129733669711516342\n",
      "Iteration 13594 => Loss: 6.73129287384775309988\n",
      "Iteration 13595 => Loss: 6.73128841159976509800\n",
      "Iteration 13596 => Loss: 6.73128394995306589266\n",
      "Iteration 13597 => Loss: 6.73127948890756933054\n",
      "Iteration 13598 => Loss: 6.73127502846320524554\n",
      "Iteration 13599 => Loss: 6.73127056861988126713\n",
      "Iteration 13600 => Loss: 6.73126610937752101194\n",
      "Iteration 13601 => Loss: 6.73126165073605253752\n",
      "Iteration 13602 => Loss: 6.73125719269537725609\n",
      "Iteration 13603 => Loss: 6.73125273525542588970\n",
      "Iteration 13604 => Loss: 6.73124827841612027868\n",
      "Iteration 13605 => Loss: 6.73124382217736449974\n",
      "Iteration 13606 => Loss: 6.73123936653909549221\n",
      "Iteration 13607 => Loss: 6.73123491150122355009\n",
      "Iteration 13608 => Loss: 6.73123045706366252006\n",
      "Iteration 13609 => Loss: 6.73122600322633957148\n",
      "Iteration 13610 => Loss: 6.73122154998917121560\n",
      "Iteration 13611 => Loss: 6.73121709735208106906\n",
      "Iteration 13612 => Loss: 6.73121264531498120220\n",
      "Iteration 13613 => Loss: 6.73120819387779167897\n",
      "Iteration 13614 => Loss: 6.73120374304043700420\n",
      "Iteration 13615 => Loss: 6.73119929280283635364\n",
      "Iteration 13616 => Loss: 6.73119484316489913311\n",
      "Iteration 13617 => Loss: 6.73119039412655340016\n",
      "Iteration 13618 => Loss: 6.73118594568771388964\n",
      "Iteration 13619 => Loss: 6.73118149784829711280\n",
      "Iteration 13620 => Loss: 6.73117705060824178531\n",
      "Iteration 13621 => Loss: 6.73117260396744132578\n",
      "Iteration 13622 => Loss: 6.73116815792582467992\n",
      "Iteration 13623 => Loss: 6.73116371248332168165\n",
      "Iteration 13624 => Loss: 6.73115926763983463132\n",
      "Iteration 13625 => Loss: 6.73115482339529247469\n",
      "Iteration 13626 => Loss: 6.73115037974961527567\n",
      "Iteration 13627 => Loss: 6.73114593670271421644\n",
      "Iteration 13628 => Loss: 6.73114149425451913089\n",
      "Iteration 13629 => Loss: 6.73113705240494297755\n",
      "Iteration 13630 => Loss: 6.73113261115390670852\n",
      "Iteration 13631 => Loss: 6.73112817050132328234\n",
      "Iteration 13632 => Loss: 6.73112373044712253289\n",
      "Iteration 13633 => Loss: 6.73111929099122541231\n",
      "Iteration 13634 => Loss: 6.73111485213354399093\n",
      "Iteration 13635 => Loss: 6.73111041387399211544\n",
      "Iteration 13636 => Loss: 6.73110597621250139611\n",
      "Iteration 13637 => Loss: 6.73110153914898212690\n",
      "Iteration 13638 => Loss: 6.73109710268335881267\n",
      "Iteration 13639 => Loss: 6.73109266681555062917\n",
      "Iteration 13640 => Loss: 6.73108823154547408762\n",
      "Iteration 13641 => Loss: 6.73108379687305458106\n",
      "Iteration 13642 => Loss: 6.73107936279820062708\n",
      "Iteration 13643 => Loss: 6.73107492932084561232\n",
      "Iteration 13644 => Loss: 6.73107049644089805440\n",
      "Iteration 13645 => Loss: 6.73106606415827890544\n",
      "Iteration 13646 => Loss: 6.73106163247291622298\n",
      "Iteration 13647 => Loss: 6.73105720138472474190\n",
      "Iteration 13648 => Loss: 6.73105277089361475618\n",
      "Iteration 13649 => Loss: 6.73104834099951787607\n",
      "Iteration 13650 => Loss: 6.73104391170234706010\n",
      "Iteration 13651 => Loss: 6.73103948300203125399\n",
      "Iteration 13652 => Loss: 6.73103505489848075172\n",
      "Iteration 13653 => Loss: 6.73103062739161384087\n",
      "Iteration 13654 => Loss: 6.73102620048135324993\n",
      "Iteration 13655 => Loss: 6.73102177416762081918\n",
      "Iteration 13656 => Loss: 6.73101734845033838894\n",
      "Iteration 13657 => Loss: 6.73101292332941891772\n",
      "Iteration 13658 => Loss: 6.73100849880478069309\n",
      "Iteration 13659 => Loss: 6.73100407487635266079\n",
      "Iteration 13660 => Loss: 6.73099965154404689116\n",
      "Iteration 13661 => Loss: 6.73099522880778966538\n",
      "Iteration 13662 => Loss: 6.73099080666749305379\n",
      "Iteration 13663 => Loss: 6.73098638512307889670\n",
      "Iteration 13664 => Loss: 6.73098196417447169893\n",
      "Iteration 13665 => Loss: 6.73097754382158708353\n",
      "Iteration 13666 => Loss: 6.73097312406434511445\n",
      "Iteration 13667 => Loss: 6.73096870490266407927\n",
      "Iteration 13668 => Loss: 6.73096428633646048922\n",
      "Iteration 13669 => Loss: 6.73095986836566417821\n",
      "Iteration 13670 => Loss: 6.73095545099019609836\n",
      "Iteration 13671 => Loss: 6.73095103420996121457\n",
      "Iteration 13672 => Loss: 6.73094661802489202529\n",
      "Iteration 13673 => Loss: 6.73094220243490326538\n",
      "Iteration 13674 => Loss: 6.73093778743991499880\n",
      "Iteration 13675 => Loss: 6.73093337303984906583\n",
      "Iteration 13676 => Loss: 6.73092895923461931318\n",
      "Iteration 13677 => Loss: 6.73092454602415735110\n",
      "Iteration 13678 => Loss: 6.73092013340836903268\n",
      "Iteration 13679 => Loss: 6.73091572138718596818\n",
      "Iteration 13680 => Loss: 6.73091130996052022795\n",
      "Iteration 13681 => Loss: 6.73090689912829098773\n",
      "Iteration 13682 => Loss: 6.73090248889043341052\n",
      "Iteration 13683 => Loss: 6.73089807924684357943\n",
      "Iteration 13684 => Loss: 6.73089367019745932197\n",
      "Iteration 13685 => Loss: 6.73088926174219182030\n",
      "Iteration 13686 => Loss: 6.73088485388096824380\n",
      "Iteration 13687 => Loss: 6.73088044661370066279\n",
      "Iteration 13688 => Loss: 6.73087603994031358212\n",
      "Iteration 13689 => Loss: 6.73087163386072351301\n",
      "Iteration 13690 => Loss: 6.73086722837485584847\n",
      "Iteration 13691 => Loss: 6.73086282348263242881\n",
      "Iteration 13692 => Loss: 6.73085841918396354799\n",
      "Iteration 13693 => Loss: 6.73085401547877548722\n",
      "Iteration 13694 => Loss: 6.73084961236698742226\n",
      "Iteration 13695 => Loss: 6.73084520984852296976\n",
      "Iteration 13696 => Loss: 6.73084080792328531828\n",
      "Iteration 13697 => Loss: 6.73083640659121495986\n",
      "Iteration 13698 => Loss: 6.73083200585222929391\n",
      "Iteration 13699 => Loss: 6.73082760570623683805\n",
      "Iteration 13700 => Loss: 6.73082320615316742618\n",
      "Iteration 13701 => Loss: 6.73081880719293756954\n",
      "Iteration 13702 => Loss: 6.73081440882546466753\n",
      "Iteration 13703 => Loss: 6.73081001105067588952\n",
      "Iteration 13704 => Loss: 6.73080561386848863492\n",
      "Iteration 13705 => Loss: 6.73080121727882385585\n",
      "Iteration 13706 => Loss: 6.73079682128159184629\n",
      "Iteration 13707 => Loss: 6.73079242587672688103\n",
      "Iteration 13708 => Loss: 6.73078803106414191859\n",
      "Iteration 13709 => Loss: 6.73078363684375702292\n",
      "Iteration 13710 => Loss: 6.73077924321549492248\n",
      "Iteration 13711 => Loss: 6.73077485017927124034\n",
      "Iteration 13712 => Loss: 6.73077045773501758674\n",
      "Iteration 13713 => Loss: 6.73076606588263803843\n",
      "Iteration 13714 => Loss: 6.73076167462206775838\n",
      "Iteration 13715 => Loss: 6.73075728395320993513\n",
      "Iteration 13716 => Loss: 6.73075289387600417257\n",
      "Iteration 13717 => Loss: 6.73074850439036165284\n",
      "Iteration 13718 => Loss: 6.73074411549619711082\n",
      "Iteration 13719 => Loss: 6.73073972719343416315\n",
      "Iteration 13720 => Loss: 6.73073533948200886101\n",
      "Iteration 13721 => Loss: 6.73073095236181373480\n",
      "Iteration 13722 => Loss: 6.73072656583279194109\n",
      "Iteration 13723 => Loss: 6.73072217989484933298\n",
      "Iteration 13724 => Loss: 6.73071779454791840891\n",
      "Iteration 13725 => Loss: 6.73071340979191212739\n",
      "Iteration 13726 => Loss: 6.73070902562674877601\n",
      "Iteration 13727 => Loss: 6.73070464205235019506\n",
      "Iteration 13728 => Loss: 6.73070025906864355392\n",
      "Iteration 13729 => Loss: 6.73069587667554536381\n",
      "Iteration 13730 => Loss: 6.73069149487297124779\n",
      "Iteration 13731 => Loss: 6.73068711366084748704\n",
      "Iteration 13732 => Loss: 6.73068273303908792826\n",
      "Iteration 13733 => Loss: 6.73067835300762506989\n",
      "Iteration 13734 => Loss: 6.73067397356636654138\n",
      "Iteration 13735 => Loss: 6.73066959471524128844\n",
      "Iteration 13736 => Loss: 6.73066521645416493413\n",
      "Iteration 13737 => Loss: 6.73066083878306020694\n",
      "Iteration 13738 => Loss: 6.73065646170184628261\n",
      "Iteration 13739 => Loss: 6.73065208521043967238\n",
      "Iteration 13740 => Loss: 6.73064770930876843380\n",
      "Iteration 13741 => Loss: 6.73064333399675618352\n",
      "Iteration 13742 => Loss: 6.73063895927430611010\n",
      "Iteration 13743 => Loss: 6.73063458514136314648\n",
      "Iteration 13744 => Loss: 6.73063021159782870484\n",
      "Iteration 13745 => Loss: 6.73062583864362906638\n",
      "Iteration 13746 => Loss: 6.73062146627868340687\n",
      "Iteration 13747 => Loss: 6.73061709450291711931\n",
      "Iteration 13748 => Loss: 6.73061272331625293219\n",
      "Iteration 13749 => Loss: 6.73060835271860291584\n",
      "Iteration 13750 => Loss: 6.73060398270988979874\n",
      "Iteration 13751 => Loss: 6.73059961329003275665\n",
      "Iteration 13752 => Loss: 6.73059524445896517619\n",
      "Iteration 13753 => Loss: 6.73059087621658491685\n",
      "Iteration 13754 => Loss: 6.73058650856283158248\n",
      "Iteration 13755 => Loss: 6.73058214149761724343\n",
      "Iteration 13756 => Loss: 6.73057777502086818089\n",
      "Iteration 13757 => Loss: 6.73057340913250623515\n",
      "Iteration 13758 => Loss: 6.73056904383244347656\n",
      "Iteration 13759 => Loss: 6.73056467912060529812\n",
      "Iteration 13760 => Loss: 6.73056031499691709286\n",
      "Iteration 13761 => Loss: 6.73055595146129004291\n",
      "Iteration 13762 => Loss: 6.73055158851364954131\n",
      "Iteration 13763 => Loss: 6.73054722615391654017\n",
      "Iteration 13764 => Loss: 6.73054286438201554432\n",
      "Iteration 13765 => Loss: 6.73053850319785240686\n",
      "Iteration 13766 => Loss: 6.73053414260137561342\n",
      "Iteration 13767 => Loss: 6.73052978259247947079\n",
      "Iteration 13768 => Loss: 6.73052542317110091830\n",
      "Iteration 13769 => Loss: 6.73052106433715024991\n",
      "Iteration 13770 => Loss: 6.73051670609055374683\n",
      "Iteration 13771 => Loss: 6.73051234843122703211\n",
      "Iteration 13772 => Loss: 6.73050799135910171600\n",
      "Iteration 13773 => Loss: 6.73050363487409342156\n",
      "Iteration 13774 => Loss: 6.73049927897610977823\n",
      "Iteration 13775 => Loss: 6.73049492366509305441\n",
      "Iteration 13776 => Loss: 6.73049056894095265591\n",
      "Iteration 13777 => Loss: 6.73048621480361841662\n",
      "Iteration 13778 => Loss: 6.73048186125299707783\n",
      "Iteration 13779 => Loss: 6.73047750828901936160\n",
      "Iteration 13780 => Loss: 6.73047315591160177917\n",
      "Iteration 13781 => Loss: 6.73046880412067149990\n",
      "Iteration 13782 => Loss: 6.73046445291614325868\n",
      "Iteration 13783 => Loss: 6.73046010229793711943\n",
      "Iteration 13784 => Loss: 6.73045575226598202789\n",
      "Iteration 13785 => Loss: 6.73045140282019271893\n",
      "Iteration 13786 => Loss: 6.73044705396049280921\n",
      "Iteration 13787 => Loss: 6.73044270568679792177\n",
      "Iteration 13788 => Loss: 6.73043835799903611417\n",
      "Iteration 13789 => Loss: 6.73043401089711768037\n",
      "Iteration 13790 => Loss: 6.73042966438098666515\n",
      "Iteration 13791 => Loss: 6.73042531845053915163\n",
      "Iteration 13792 => Loss: 6.73042097310571119095\n",
      "Iteration 13793 => Loss: 6.73041662834641041258\n",
      "Iteration 13794 => Loss: 6.73041228417257109129\n",
      "Iteration 13795 => Loss: 6.73040794058411417922\n",
      "Iteration 13796 => Loss: 6.73040359758095085851\n",
      "Iteration 13797 => Loss: 6.73039925516300474584\n",
      "Iteration 13798 => Loss: 6.73039491333020656327\n",
      "Iteration 13799 => Loss: 6.73039057208247015751\n",
      "Iteration 13800 => Loss: 6.73038623141971381614\n",
      "Iteration 13801 => Loss: 6.73038189134186559670\n",
      "Iteration 13802 => Loss: 6.73037755184883756954\n",
      "Iteration 13803 => Loss: 6.73037321294055779219\n",
      "Iteration 13804 => Loss: 6.73036887461695076951\n",
      "Iteration 13805 => Loss: 6.73036453687793301270\n",
      "Iteration 13806 => Loss: 6.73036019972342014484\n",
      "Iteration 13807 => Loss: 6.73035586315334466434\n",
      "Iteration 13808 => Loss: 6.73035152716761686520\n",
      "Iteration 13809 => Loss: 6.73034719176616746950\n",
      "Iteration 13810 => Loss: 6.73034285694891476481\n",
      "Iteration 13811 => Loss: 6.73033852271577348603\n",
      "Iteration 13812 => Loss: 6.73033418906667524340\n",
      "Iteration 13813 => Loss: 6.73032985600154276540\n",
      "Iteration 13814 => Loss: 6.73032552352028012876\n",
      "Iteration 13815 => Loss: 6.73032119162282693736\n",
      "Iteration 13816 => Loss: 6.73031686030909082064\n",
      "Iteration 13817 => Loss: 6.73031252957900338885\n",
      "Iteration 13818 => Loss: 6.73030819943248026505\n",
      "Iteration 13819 => Loss: 6.73030386986945128314\n",
      "Iteration 13820 => Loss: 6.73029954088982318439\n",
      "Iteration 13821 => Loss: 6.73029521249352669088\n",
      "Iteration 13822 => Loss: 6.73029088468048541927\n",
      "Iteration 13823 => Loss: 6.73028655745060877535\n",
      "Iteration 13824 => Loss: 6.73028223080383991572\n",
      "Iteration 13825 => Loss: 6.73027790474008291710\n",
      "Iteration 13826 => Loss: 6.73027357925925784343\n",
      "Iteration 13827 => Loss: 6.73026925436129541680\n",
      "Iteration 13828 => Loss: 6.73026493004611214843\n",
      "Iteration 13829 => Loss: 6.73026060631362366138\n",
      "Iteration 13830 => Loss: 6.73025628316377311222\n",
      "Iteration 13831 => Loss: 6.73025196059646013680\n",
      "Iteration 13832 => Loss: 6.73024763861161723355\n",
      "Iteration 13833 => Loss: 6.73024331720915469646\n",
      "Iteration 13834 => Loss: 6.73023899638900324760\n",
      "Iteration 13835 => Loss: 6.73023467615108028639\n",
      "Iteration 13836 => Loss: 6.73023035649531742308\n",
      "Iteration 13837 => Loss: 6.73022603742162139895\n",
      "Iteration 13838 => Loss: 6.73022171892992826514\n",
      "Iteration 13839 => Loss: 6.73021740102015098017\n",
      "Iteration 13840 => Loss: 6.73021308369220516710\n",
      "Iteration 13841 => Loss: 6.73020876694602954160\n",
      "Iteration 13842 => Loss: 6.73020445078152818041\n",
      "Iteration 13843 => Loss: 6.73020013519863358198\n",
      "Iteration 13844 => Loss: 6.73019582019726048117\n",
      "Iteration 13845 => Loss: 6.73019150577734048824\n",
      "Iteration 13846 => Loss: 6.73018719193877679174\n",
      "Iteration 13847 => Loss: 6.73018287868151876552\n",
      "Iteration 13848 => Loss: 6.73017856600546604540\n",
      "Iteration 13849 => Loss: 6.73017425391054668893\n",
      "Iteration 13850 => Loss: 6.73016994239668253641\n",
      "Iteration 13851 => Loss: 6.73016563146379009908\n",
      "Iteration 13852 => Loss: 6.73016132111180453990\n",
      "Iteration 13853 => Loss: 6.73015701134063792921\n",
      "Iteration 13854 => Loss: 6.73015270215020855460\n",
      "Iteration 13855 => Loss: 6.73014839354044269726\n",
      "Iteration 13856 => Loss: 6.73014408551126663838\n",
      "Iteration 13857 => Loss: 6.73013977806259422465\n",
      "Iteration 13858 => Loss: 6.73013547119435617816\n",
      "Iteration 13859 => Loss: 6.73013116490646901013\n",
      "Iteration 13860 => Loss: 6.73012685919884745545\n",
      "Iteration 13861 => Loss: 6.73012255407141868346\n",
      "Iteration 13862 => Loss: 6.73011824952411430445\n",
      "Iteration 13863 => Loss: 6.73011394555684372421\n",
      "Iteration 13864 => Loss: 6.73010964216953144756\n",
      "Iteration 13865 => Loss: 6.73010533936210730843\n",
      "Iteration 13866 => Loss: 6.73010103713448426532\n",
      "Iteration 13867 => Loss: 6.73009673548658593489\n",
      "Iteration 13868 => Loss: 6.73009243441833771016\n",
      "Iteration 13869 => Loss: 6.73008813392965521416\n",
      "Iteration 13870 => Loss: 6.73008383402046561628\n",
      "Iteration 13871 => Loss: 6.73007953469069075680\n",
      "Iteration 13872 => Loss: 6.73007523594024270608\n",
      "Iteration 13873 => Loss: 6.73007093776905751525\n",
      "Iteration 13874 => Loss: 6.73006664017704903102\n",
      "Iteration 13875 => Loss: 6.73006234316414619911\n",
      "Iteration 13876 => Loss: 6.73005804673026197804\n",
      "Iteration 13877 => Loss: 6.73005375087532975442\n",
      "Iteration 13878 => Loss: 6.73004945559925449317\n",
      "Iteration 13879 => Loss: 6.73004516090197046907\n",
      "Iteration 13880 => Loss: 6.73004086678339952243\n",
      "Iteration 13881 => Loss: 6.73003657324345816448\n",
      "Iteration 13882 => Loss: 6.73003228028208067002\n",
      "Iteration 13883 => Loss: 6.73002798789916933941\n",
      "Iteration 13884 => Loss: 6.73002369609466377653\n",
      "Iteration 13885 => Loss: 6.73001940486847249900\n",
      "Iteration 13886 => Loss: 6.73001511422053066980\n",
      "Iteration 13887 => Loss: 6.73001082415075213561\n",
      "Iteration 13888 => Loss: 6.73000653465906317763\n",
      "Iteration 13889 => Loss: 6.73000224574537408984\n",
      "Iteration 13890 => Loss: 6.72999795740962447610\n",
      "Iteration 13891 => Loss: 6.72999366965172551858\n",
      "Iteration 13892 => Loss: 6.72998938247160616299\n",
      "Iteration 13893 => Loss: 6.72998509586918292058\n",
      "Iteration 13894 => Loss: 6.72998080984438384888\n",
      "Iteration 13895 => Loss: 6.72997652439711568917\n",
      "Iteration 13896 => Loss: 6.72997223952732337438\n",
      "Iteration 13897 => Loss: 6.72996795523490920488\n",
      "Iteration 13898 => Loss: 6.72996367151980656729\n",
      "Iteration 13899 => Loss: 6.72995938838193730192\n",
      "Iteration 13900 => Loss: 6.72995510582121347909\n",
      "Iteration 13901 => Loss: 6.72995082383757736721\n",
      "Iteration 13902 => Loss: 6.72994654243092949031\n",
      "Iteration 13903 => Loss: 6.72994226160120767588\n",
      "Iteration 13904 => Loss: 6.72993798134832310609\n",
      "Iteration 13905 => Loss: 6.72993370167220561484\n",
      "Iteration 13906 => Loss: 6.72992942257276904883\n",
      "Iteration 13907 => Loss: 6.72992514404994501831\n",
      "Iteration 13908 => Loss: 6.72992086610365891630\n",
      "Iteration 13909 => Loss: 6.72991658873381748407\n",
      "Iteration 13910 => Loss: 6.72991231194035854912\n",
      "Iteration 13911 => Loss: 6.72990803572319595816\n",
      "Iteration 13912 => Loss: 6.72990376008225066329\n",
      "Iteration 13913 => Loss: 6.72989948501745161025\n",
      "Iteration 13914 => Loss: 6.72989521052871086937\n",
      "Iteration 13915 => Loss: 6.72989093661596804452\n",
      "Iteration 13916 => Loss: 6.72988666327912987697\n",
      "Iteration 13917 => Loss: 6.72988239051812353608\n",
      "Iteration 13918 => Loss: 6.72987811833287352670\n",
      "Iteration 13919 => Loss: 6.72987384672330168911\n",
      "Iteration 13920 => Loss: 6.72986957568932631091\n",
      "Iteration 13921 => Loss: 6.72986530523087989053\n",
      "Iteration 13922 => Loss: 6.72986103534787183378\n",
      "Iteration 13923 => Loss: 6.72985676604023907998\n",
      "Iteration 13924 => Loss: 6.72985249730788037681\n",
      "Iteration 13925 => Loss: 6.72984822915074776262\n",
      "Iteration 13926 => Loss: 6.72984396156874797867\n",
      "Iteration 13927 => Loss: 6.72983969456179842439\n",
      "Iteration 13928 => Loss: 6.72983542812983071002\n",
      "Iteration 13929 => Loss: 6.72983116227277022858\n",
      "Iteration 13930 => Loss: 6.72982689699052905041\n",
      "Iteration 13931 => Loss: 6.72982263228303789759\n",
      "Iteration 13932 => Loss: 6.72981836815021061682\n",
      "Iteration 13933 => Loss: 6.72981410459198237106\n",
      "Iteration 13934 => Loss: 6.72980984160827055973\n",
      "Iteration 13935 => Loss: 6.72980557919899258223\n",
      "Iteration 13936 => Loss: 6.72980131736407294341\n",
      "Iteration 13937 => Loss: 6.72979705610343525990\n",
      "Iteration 13938 => Loss: 6.72979279541699870748\n",
      "Iteration 13939 => Loss: 6.72978853530470200184\n",
      "Iteration 13940 => Loss: 6.72978427576645099606\n",
      "Iteration 13941 => Loss: 6.72978001680216930680\n",
      "Iteration 13942 => Loss: 6.72977575841178587979\n",
      "Iteration 13943 => Loss: 6.72977150059521989078\n",
      "Iteration 13944 => Loss: 6.72976724335239495645\n",
      "Iteration 13945 => Loss: 6.72976298668323114072\n",
      "Iteration 13946 => Loss: 6.72975873058766627111\n",
      "Iteration 13947 => Loss: 6.72975447506559909527\n",
      "Iteration 13948 => Loss: 6.72975022011696832891\n",
      "Iteration 13949 => Loss: 6.72974596574169314778\n",
      "Iteration 13950 => Loss: 6.72974171193969272764\n",
      "Iteration 13951 => Loss: 6.72973745871088446791\n",
      "Iteration 13952 => Loss: 6.72973320605520619608\n",
      "Iteration 13953 => Loss: 6.72972895397257886430\n",
      "Iteration 13954 => Loss: 6.72972470246291454288\n",
      "Iteration 13955 => Loss: 6.72972045152614040120\n",
      "Iteration 13956 => Loss: 6.72971620116218005592\n",
      "Iteration 13957 => Loss: 6.72971195137096245276\n",
      "Iteration 13958 => Loss: 6.72970770215240055023\n",
      "Iteration 13959 => Loss: 6.72970345350642151772\n",
      "Iteration 13960 => Loss: 6.72969920543294986004\n",
      "Iteration 13961 => Loss: 6.72969495793190386479\n",
      "Iteration 13962 => Loss: 6.72969071100321158951\n",
      "Iteration 13963 => Loss: 6.72968646464678688091\n",
      "Iteration 13964 => Loss: 6.72968221886256579012\n",
      "Iteration 13965 => Loss: 6.72967797365046482838\n",
      "Iteration 13966 => Loss: 6.72967372901040317146\n",
      "Iteration 13967 => Loss: 6.72966948494230354783\n",
      "Iteration 13968 => Loss: 6.72966524144610289682\n",
      "Iteration 13969 => Loss: 6.72966099852170884787\n",
      "Iteration 13970 => Loss: 6.72965675616904412948\n",
      "Iteration 13971 => Loss: 6.72965251438804212825\n",
      "Iteration 13972 => Loss: 6.72964827317861669087\n",
      "Iteration 13973 => Loss: 6.72964403254069498672\n",
      "Iteration 13974 => Loss: 6.72963979247420507335\n",
      "Iteration 13975 => Loss: 6.72963555297905813291\n",
      "Iteration 13976 => Loss: 6.72963131405518577566\n",
      "Iteration 13977 => Loss: 6.72962707570250984190\n",
      "Iteration 13978 => Loss: 6.72962283792095217194\n",
      "Iteration 13979 => Loss: 6.72961860071043371789\n",
      "Iteration 13980 => Loss: 6.72961436407088076095\n",
      "Iteration 13981 => Loss: 6.72961012800221158869\n",
      "Iteration 13982 => Loss: 6.72960589250436047593\n",
      "Iteration 13983 => Loss: 6.72960165757723949298\n",
      "Iteration 13984 => Loss: 6.72959742322077758558\n",
      "Iteration 13985 => Loss: 6.72959318943489392950\n",
      "Iteration 13986 => Loss: 6.72958895621951036503\n",
      "Iteration 13987 => Loss: 6.72958472357455850243\n",
      "Iteration 13988 => Loss: 6.72958049149995041205\n",
      "Iteration 13989 => Loss: 6.72957625999561237506\n",
      "Iteration 13990 => Loss: 6.72957202906148133081\n",
      "Iteration 13991 => Loss: 6.72956779869745869149\n",
      "Iteration 13992 => Loss: 6.72956356890348139643\n",
      "Iteration 13993 => Loss: 6.72955933967947039775\n",
      "Iteration 13994 => Loss: 6.72955511102534309487\n",
      "Iteration 13995 => Loss: 6.72955088294103287438\n",
      "Iteration 13996 => Loss: 6.72954665542645447118\n",
      "Iteration 13997 => Loss: 6.72954242848153327827\n",
      "Iteration 13998 => Loss: 6.72953820210619557685\n",
      "Iteration 13999 => Loss: 6.72953397630035876631\n",
      "Iteration 14000 => Loss: 6.72952975106395534510\n",
      "Iteration 14001 => Loss: 6.72952552639690626535\n",
      "Iteration 14002 => Loss: 6.72952130229911738013\n",
      "Iteration 14003 => Loss: 6.72951707877053273421\n",
      "Iteration 14004 => Loss: 6.72951285581107150335\n",
      "Iteration 14005 => Loss: 6.72950863342064842243\n",
      "Iteration 14006 => Loss: 6.72950441159920220713\n",
      "Iteration 14007 => Loss: 6.72950019034663871054\n",
      "Iteration 14008 => Loss: 6.72949596966289309563\n",
      "Iteration 14009 => Loss: 6.72949174954788009728\n",
      "Iteration 14010 => Loss: 6.72948753000153754300\n",
      "Iteration 14011 => Loss: 6.72948331102377217405\n",
      "Iteration 14012 => Loss: 6.72947909261451204799\n",
      "Iteration 14013 => Loss: 6.72947487477368699871\n",
      "Iteration 14014 => Loss: 6.72947065750122064287\n",
      "Iteration 14015 => Loss: 6.72946644079702149810\n",
      "Iteration 14016 => Loss: 6.72946222466102561555\n",
      "Iteration 14017 => Loss: 6.72945800909315927640\n",
      "Iteration 14018 => Loss: 6.72945379409334076826\n",
      "Iteration 14019 => Loss: 6.72944957966149193140\n",
      "Iteration 14020 => Loss: 6.72944536579753993522\n",
      "Iteration 14021 => Loss: 6.72944115250140573181\n",
      "Iteration 14022 => Loss: 6.72943693977301293785\n",
      "Iteration 14023 => Loss: 6.72943272761228694634\n",
      "Iteration 14024 => Loss: 6.72942851601914959758\n",
      "Iteration 14025 => Loss: 6.72942430499352362006\n",
      "Iteration 14026 => Loss: 6.72942009453533085406\n",
      "Iteration 14027 => Loss: 6.72941588464450024532\n",
      "Iteration 14028 => Loss: 6.72941167532095807502\n",
      "Iteration 14029 => Loss: 6.72940746656461907804\n",
      "Iteration 14030 => Loss: 6.72940325837540420650\n",
      "Iteration 14031 => Loss: 6.72939905075324773520\n",
      "Iteration 14032 => Loss: 6.72939484369807328079\n",
      "Iteration 14033 => Loss: 6.72939063720979202543\n",
      "Iteration 14034 => Loss: 6.72938643128833824392\n",
      "Iteration 14035 => Loss: 6.72938222593363288837\n",
      "Iteration 14036 => Loss: 6.72937802114560490452\n",
      "Iteration 14037 => Loss: 6.72937381692416103363\n",
      "Iteration 14038 => Loss: 6.72936961326924620863\n",
      "Iteration 14039 => Loss: 6.72936541018076717080\n",
      "Iteration 14040 => Loss: 6.72936120765866174764\n",
      "Iteration 14041 => Loss: 6.72935700570284733857\n",
      "Iteration 14042 => Loss: 6.72935280431323867845\n",
      "Iteration 14043 => Loss: 6.72934860348976915390\n",
      "Iteration 14044 => Loss: 6.72934440323236504611\n",
      "Iteration 14045 => Loss: 6.72934020354094286631\n",
      "Iteration 14046 => Loss: 6.72933600441543244841\n",
      "Iteration 14047 => Loss: 6.72933180585575119181\n",
      "Iteration 14048 => Loss: 6.72932760786182981860\n",
      "Iteration 14049 => Loss: 6.72932341043358484001\n",
      "Iteration 14050 => Loss: 6.72931921357094520175\n",
      "Iteration 14051 => Loss: 6.72931501727383363232\n",
      "Iteration 14052 => Loss: 6.72931082154217374836\n",
      "Iteration 14053 => Loss: 6.72930662637588827835\n",
      "Iteration 14054 => Loss: 6.72930243177489817441\n",
      "Iteration 14055 => Loss: 6.72929823773913149410\n",
      "Iteration 14056 => Loss: 6.72929404426851363041\n",
      "Iteration 14057 => Loss: 6.72928985136297441727\n",
      "Iteration 14058 => Loss: 6.72928565902241793140\n",
      "Iteration 14059 => Loss: 6.72928146724678377666\n",
      "Iteration 14060 => Loss: 6.72927727603599290518\n",
      "Iteration 14061 => Loss: 6.72927308538997248633\n",
      "Iteration 14062 => Loss: 6.72926889530863370226\n",
      "Iteration 14063 => Loss: 6.72926470579190993959\n",
      "Iteration 14064 => Loss: 6.72926051683972570316\n",
      "Iteration 14065 => Loss: 6.72925632845200194510\n",
      "Iteration 14066 => Loss: 6.72925214062866317022\n",
      "Iteration 14067 => Loss: 6.72924795336963565973\n",
      "Iteration 14068 => Loss: 6.72924376667484125392\n",
      "Iteration 14069 => Loss: 6.72923958054420090491\n",
      "Iteration 14070 => Loss: 6.72923539497765244022\n",
      "Iteration 14071 => Loss: 6.72923120997509904839\n",
      "Iteration 14072 => Loss: 6.72922702553647766877\n",
      "Iteration 14073 => Loss: 6.72922284166170836528\n",
      "Iteration 14074 => Loss: 6.72921865835072363637\n",
      "Iteration 14075 => Loss: 6.72921447560343022332\n",
      "Iteration 14076 => Loss: 6.72921029341977039451\n",
      "Iteration 14077 => Loss: 6.72920611179965888482\n",
      "Iteration 14078 => Loss: 6.72920193074301931091\n",
      "Iteration 14079 => Loss: 6.72919775024977528943\n",
      "Iteration 14080 => Loss: 6.72919357031985665429\n",
      "Iteration 14081 => Loss: 6.72918939095317991672\n",
      "Iteration 14082 => Loss: 6.72918521214967846333\n",
      "Iteration 14083 => Loss: 6.72918103390926614082\n",
      "Iteration 14084 => Loss: 6.72917685623187455946\n",
      "Iteration 14085 => Loss: 6.72917267911742555953\n",
      "Iteration 14086 => Loss: 6.72916850256583831680\n",
      "Iteration 14087 => Loss: 6.72916432657705154696\n",
      "Iteration 14088 => Loss: 6.72916015115097376764\n",
      "Iteration 14089 => Loss: 6.72915597628753303638\n",
      "Iteration 14090 => Loss: 6.72915180198665563438\n",
      "Iteration 14091 => Loss: 6.72914762824826784282\n",
      "Iteration 14092 => Loss: 6.72914345507228794929\n",
      "Iteration 14093 => Loss: 6.72913928245865022859\n",
      "Iteration 14094 => Loss: 6.72913511040727563284\n",
      "Iteration 14095 => Loss: 6.72913093891807267966\n",
      "Iteration 14096 => Loss: 6.72912676799098274927\n",
      "Iteration 14097 => Loss: 6.72912259762592768197\n",
      "Iteration 14098 => Loss: 6.72911842782283375897\n",
      "Iteration 14099 => Loss: 6.72911425858161571512\n",
      "Iteration 14100 => Loss: 6.72911008990220871340\n",
      "Iteration 14101 => Loss: 6.72910592178452215961\n",
      "Iteration 14102 => Loss: 6.72910175422849299309\n",
      "Iteration 14103 => Loss: 6.72909758723404838321\n",
      "Iteration 14104 => Loss: 6.72909342080109595940\n",
      "Iteration 14105 => Loss: 6.72908925492958509551\n",
      "Iteration 14106 => Loss: 6.72908508961941365101\n",
      "Iteration 14107 => Loss: 6.72908092487052211794\n",
      "Iteration 14108 => Loss: 6.72907676068282967208\n",
      "Iteration 14109 => Loss: 6.72907259705626525914\n",
      "Iteration 14110 => Loss: 6.72906843399074805490\n",
      "Iteration 14111 => Loss: 6.72906427148620078782\n",
      "Iteration 14112 => Loss: 6.72906010954255773271\n",
      "Iteration 14113 => Loss: 6.72905594815973540079\n",
      "Iteration 14114 => Loss: 6.72905178733765652055\n",
      "Iteration 14115 => Loss: 6.72904762707624914952\n",
      "Iteration 14116 => Loss: 6.72904346737543601620\n",
      "Iteration 14117 => Loss: 6.72903930823514606629\n",
      "Iteration 14118 => Loss: 6.72903514965530291647\n",
      "Iteration 14119 => Loss: 6.72903099163582218978\n",
      "Iteration 14120 => Loss: 6.72902683417663549648\n",
      "Iteration 14121 => Loss: 6.72902267727767089411\n",
      "Iteration 14122 => Loss: 6.72901852093885022299\n",
      "Iteration 14123 => Loss: 6.72901436516008821798\n",
      "Iteration 14124 => Loss: 6.72901020994132981201\n",
      "Iteration 14125 => Loss: 6.72900605528247464093\n",
      "Iteration 14126 => Loss: 6.72900190118347119039\n",
      "Iteration 14127 => Loss: 6.72899774764423330708\n",
      "Iteration 14128 => Loss: 6.72899359466467572588\n",
      "Iteration 14129 => Loss: 6.72898944224473982700\n",
      "Iteration 14130 => Loss: 6.72898529038433323990\n",
      "Iteration 14131 => Loss: 6.72898113908339379208\n",
      "Iteration 14132 => Loss: 6.72897698834184687655\n",
      "Iteration 14133 => Loss: 6.72897283815961699815\n",
      "Iteration 14134 => Loss: 6.72896868853661445087\n",
      "Iteration 14135 => Loss: 6.72896453947277972674\n",
      "Iteration 14136 => Loss: 6.72896039096802489610\n",
      "Iteration 14137 => Loss: 6.72895624302229311553\n",
      "Iteration 14138 => Loss: 6.72895209563548934995\n",
      "Iteration 14139 => Loss: 6.72894794880754965050\n",
      "Iteration 14140 => Loss: 6.72894380253839141659\n",
      "Iteration 14141 => Loss: 6.72893965682794181760\n",
      "Iteration 14142 => Loss: 6.72893551167613601649\n",
      "Iteration 14143 => Loss: 6.72893136708287897818\n",
      "Iteration 14144 => Loss: 6.72892722304811297107\n",
      "Iteration 14145 => Loss: 6.72892307957175184185\n",
      "Iteration 14146 => Loss: 6.72891893665372720079\n",
      "Iteration 14147 => Loss: 6.72891479429396177636\n",
      "Iteration 14148 => Loss: 6.72891065249238007340\n",
      "Iteration 14149 => Loss: 6.72890651124890570856\n",
      "Iteration 14150 => Loss: 6.72890237056346318667\n",
      "Iteration 14151 => Loss: 6.72889823043597345986\n",
      "Iteration 14152 => Loss: 6.72889409086638057289\n",
      "Iteration 14153 => Loss: 6.72888995185458327342\n",
      "Iteration 14154 => Loss: 6.72888581340052294166\n",
      "Iteration 14155 => Loss: 6.72888167550411608886\n",
      "Iteration 14156 => Loss: 6.72887753816528721984\n",
      "Iteration 14157 => Loss: 6.72887340138397327394\n",
      "Iteration 14158 => Loss: 6.72886926516008898602\n",
      "Iteration 14159 => Loss: 6.72886512949356152546\n",
      "Iteration 14160 => Loss: 6.72886099438431362074\n",
      "Iteration 14161 => Loss: 6.72885685983226888851\n",
      "Iteration 14162 => Loss: 6.72885272583735893903\n",
      "Iteration 14163 => Loss: 6.72884859239951005350\n",
      "Iteration 14164 => Loss: 6.72884445951864140767\n",
      "Iteration 14165 => Loss: 6.72884032719467040096\n",
      "Iteration 14166 => Loss: 6.72883619542753308451\n",
      "Iteration 14167 => Loss: 6.72883206421715840406\n",
      "Iteration 14168 => Loss: 6.72882793356345487723\n",
      "Iteration 14169 => Loss: 6.72882380346636121971\n",
      "Iteration 14170 => Loss: 6.72881967392580282450\n",
      "Iteration 14171 => Loss: 6.72881554494169087377\n",
      "Iteration 14172 => Loss: 6.72881141651397118864\n",
      "Iteration 14173 => Loss: 6.72880728864255495125\n",
      "Iteration 14174 => Loss: 6.72880316132736400192\n",
      "Iteration 14175 => Loss: 6.72879903456832373365\n",
      "Iteration 14176 => Loss: 6.72879490836538174392\n",
      "Iteration 14177 => Loss: 6.72879078271843322767\n",
      "Iteration 14178 => Loss: 6.72878665762742134149\n",
      "Iteration 14179 => Loss: 6.72878253309226259660\n",
      "Iteration 14180 => Loss: 6.72877840911288327419\n",
      "Iteration 14181 => Loss: 6.72877428568922297814\n",
      "Iteration 14182 => Loss: 6.72877016282118134427\n",
      "Iteration 14183 => Loss: 6.72876604050870152918\n",
      "Iteration 14184 => Loss: 6.72876191875170182044\n",
      "Iteration 14185 => Loss: 6.72875779755011205197\n",
      "Iteration 14186 => Loss: 6.72875367690384695862\n",
      "Iteration 14187 => Loss: 6.72874955681284525610\n",
      "Iteration 14188 => Loss: 6.72874543727703056106\n",
      "Iteration 14189 => Loss: 6.72874131829631938473\n",
      "Iteration 14190 => Loss: 6.72873719987063800829\n",
      "Iteration 14191 => Loss: 6.72873308199991981837\n",
      "Iteration 14192 => Loss: 6.72872896468407688531\n",
      "Iteration 14193 => Loss: 6.72872484792305058932\n",
      "Iteration 14194 => Loss: 6.72872073171675477710\n",
      "Iteration 14195 => Loss: 6.72871661606511395348\n",
      "Iteration 14196 => Loss: 6.72871250096806949870\n",
      "Iteration 14197 => Loss: 6.72870838642552815401\n",
      "Iteration 14198 => Loss: 6.72870427243742419421\n",
      "Iteration 14199 => Loss: 6.72870015900367768324\n",
      "Iteration 14200 => Loss: 6.72869604612421312595\n",
      "Iteration 14201 => Loss: 6.72869193379896213258\n",
      "Iteration 14202 => Loss: 6.72868782202784920798\n",
      "Iteration 14203 => Loss: 6.72868371081079263973\n",
      "Iteration 14204 => Loss: 6.72867960014773203170\n",
      "Iteration 14205 => Loss: 6.72867549003858034240\n",
      "Iteration 14206 => Loss: 6.72867138048326385302\n",
      "Iteration 14207 => Loss: 6.72866727148170973294\n",
      "Iteration 14208 => Loss: 6.72866316303384159880\n",
      "Iteration 14209 => Loss: 6.72865905513959106088\n",
      "Iteration 14210 => Loss: 6.72865494779888262400\n",
      "Iteration 14211 => Loss: 6.72865084101163279939\n",
      "Iteration 14212 => Loss: 6.72864673477777408550\n",
      "Iteration 14213 => Loss: 6.72864262909723365169\n",
      "Iteration 14214 => Loss: 6.72863852396993333826\n",
      "Iteration 14215 => Loss: 6.72863441939579676188\n",
      "Iteration 14216 => Loss: 6.72863031537475730914\n",
      "Iteration 14217 => Loss: 6.72862621190672527405\n",
      "Iteration 14218 => Loss: 6.72862210899164470135\n",
      "Iteration 14219 => Loss: 6.72861800662942677320\n",
      "Iteration 14220 => Loss: 6.72861390482000398805\n",
      "Iteration 14221 => Loss: 6.72860980356330173890\n",
      "Iteration 14222 => Loss: 6.72860570285924186607\n",
      "Iteration 14223 => Loss: 6.72860160270775331526\n",
      "Iteration 14224 => Loss: 6.72859750310876147950\n",
      "Iteration 14225 => Loss: 6.72859340406219263997\n",
      "Iteration 14226 => Loss: 6.72858930556797041334\n",
      "Iteration 14227 => Loss: 6.72858520762601575171\n",
      "Iteration 14228 => Loss: 6.72858111023626204172\n",
      "Iteration 14229 => Loss: 6.72857701339863290002\n",
      "Iteration 14230 => Loss: 6.72857291711304661419\n",
      "Iteration 14231 => Loss: 6.72856882137944634081\n",
      "Iteration 14232 => Loss: 6.72856472619774237387\n",
      "Iteration 14233 => Loss: 6.72856063156785744184\n",
      "Iteration 14234 => Loss: 6.72855653748973114858\n",
      "Iteration 14235 => Loss: 6.72855244396328178169\n",
      "Iteration 14236 => Loss: 6.72854835098843739871\n",
      "Iteration 14237 => Loss: 6.72854425856511628723\n",
      "Iteration 14238 => Loss: 6.72854016669325183386\n",
      "Iteration 14239 => Loss: 6.72853607537276232620\n",
      "Iteration 14240 => Loss: 6.72853198460358825628\n",
      "Iteration 14241 => Loss: 6.72852789438564080626\n",
      "Iteration 14242 => Loss: 6.72852380471884714552\n",
      "Iteration 14243 => Loss: 6.72851971560313266707\n",
      "Iteration 14244 => Loss: 6.72851562703843519841\n",
      "Iteration 14245 => Loss: 6.72851153902467835621\n",
      "Iteration 14246 => Loss: 6.72850745156177243445\n",
      "Iteration 14247 => Loss: 6.72850336464965703698\n",
      "Iteration 14248 => Loss: 6.72849927828825133957\n",
      "Iteration 14249 => Loss: 6.72849519247747984707\n",
      "Iteration 14250 => Loss: 6.72849110721727683426\n",
      "Iteration 14251 => Loss: 6.72848702250755614784\n",
      "Iteration 14252 => Loss: 6.72848293834825916804\n",
      "Iteration 14253 => Loss: 6.72847885473929707700\n",
      "Iteration 14254 => Loss: 6.72847477168059793229\n",
      "Iteration 14255 => Loss: 6.72847068917210133776\n",
      "Iteration 14256 => Loss: 6.72846660721371847558\n",
      "Iteration 14257 => Loss: 6.72846252580537740329\n",
      "Iteration 14258 => Loss: 6.72845844494700884297\n",
      "Iteration 14259 => Loss: 6.72845436463853374676\n",
      "Iteration 14260 => Loss: 6.72845028487988727761\n",
      "Iteration 14261 => Loss: 6.72844620567098150588\n",
      "Iteration 14262 => Loss: 6.72844212701175159452\n",
      "Iteration 14263 => Loss: 6.72843804890212382475\n",
      "Iteration 14264 => Loss: 6.72843397134201470777\n",
      "Iteration 14265 => Loss: 6.72842989433136473565\n",
      "Iteration 14266 => Loss: 6.72842581787009041960\n",
      "Iteration 14267 => Loss: 6.72842174195811804083\n",
      "Iteration 14268 => Loss: 6.72841766659537388051\n",
      "Iteration 14269 => Loss: 6.72841359178178777256\n",
      "Iteration 14270 => Loss: 6.72840951751728244545\n",
      "Iteration 14271 => Loss: 6.72840544380178684492\n",
      "Iteration 14272 => Loss: 6.72840137063522458760\n",
      "Iteration 14273 => Loss: 6.72839729801751396110\n",
      "Iteration 14274 => Loss: 6.72839322594859634563\n",
      "Iteration 14275 => Loss: 6.72838915442839091696\n",
      "Iteration 14276 => Loss: 6.72838508345682484446\n",
      "Iteration 14277 => Loss: 6.72838101303381463936\n",
      "Iteration 14278 => Loss: 6.72837694315929546462\n",
      "Iteration 14279 => Loss: 6.72837287383319893053\n",
      "Iteration 14280 => Loss: 6.72836880505544154829\n",
      "Iteration 14281 => Loss: 6.72836473682594959911\n",
      "Iteration 14282 => Loss: 6.72836066914465291688\n",
      "Iteration 14283 => Loss: 6.72835660201147689463\n",
      "Iteration 14284 => Loss: 6.72835253542634159629\n",
      "Iteration 14285 => Loss: 6.72834846938918840209\n",
      "Iteration 14286 => Loss: 6.72834440389993293508\n",
      "Iteration 14287 => Loss: 6.72834033895850236462\n",
      "Iteration 14288 => Loss: 6.72833627456481586648\n",
      "Iteration 14289 => Loss: 6.72833221071881482089\n",
      "Iteration 14290 => Loss: 6.72832814742040952183\n",
      "Iteration 14291 => Loss: 6.72832408466953868498\n",
      "Iteration 14292 => Loss: 6.72832002246612770335\n",
      "Iteration 14293 => Loss: 6.72831596081009131183\n",
      "Iteration 14294 => Loss: 6.72831189970136556155\n",
      "Iteration 14295 => Loss: 6.72830783913987762190\n",
      "Iteration 14296 => Loss: 6.72830377912554933317\n",
      "Iteration 14297 => Loss: 6.72829971965830875291\n",
      "Iteration 14298 => Loss: 6.72829566073807949778\n",
      "Iteration 14299 => Loss: 6.72829160236479495438\n",
      "Iteration 14300 => Loss: 6.72828754453837429850\n",
      "Iteration 14301 => Loss: 6.72828348725874114677\n",
      "Iteration 14302 => Loss: 6.72827943052582799766\n",
      "Iteration 14303 => Loss: 6.72827537433956468504\n",
      "Iteration 14304 => Loss: 6.72827131869987038471\n",
      "Iteration 14305 => Loss: 6.72826726360667937143\n",
      "Iteration 14306 => Loss: 6.72826320905990193921\n",
      "Iteration 14307 => Loss: 6.72825915505947680373\n",
      "Iteration 14308 => Loss: 6.72825510160533291071\n",
      "Iteration 14309 => Loss: 6.72825104869739210045\n",
      "Iteration 14310 => Loss: 6.72824699633557887779\n",
      "Iteration 14311 => Loss: 6.72824294451982574117\n",
      "Iteration 14312 => Loss: 6.72823889325004920181\n",
      "Iteration 14313 => Loss: 6.72823484252618619905\n",
      "Iteration 14314 => Loss: 6.72823079234815502048\n",
      "Iteration 14315 => Loss: 6.72822674271589349360\n",
      "Iteration 14316 => Loss: 6.72822269362931368875\n",
      "Iteration 14317 => Loss: 6.72821864508834632801\n",
      "Iteration 14318 => Loss: 6.72821459709292390983\n",
      "Iteration 14319 => Loss: 6.72821054964296560996\n",
      "Iteration 14320 => Loss: 6.72820650273840570321\n",
      "Iteration 14321 => Loss: 6.72820245637916780623\n",
      "Iteration 14322 => Loss: 6.72819841056516931843\n",
      "Iteration 14323 => Loss: 6.72819436529635428457\n",
      "Iteration 14324 => Loss: 6.72819032057263477498\n",
      "Iteration 14325 => Loss: 6.72818627639394239992\n",
      "Iteration 14326 => Loss: 6.72818223276020166423\n",
      "Iteration 14327 => Loss: 6.72817818967133884911\n",
      "Iteration 14328 => Loss: 6.72817414712728645299\n",
      "Iteration 14329 => Loss: 6.72817010512797075705\n",
      "Iteration 14330 => Loss: 6.72816606367330560801\n",
      "Iteration 14331 => Loss: 6.72816202276323771514\n",
      "Iteration 14332 => Loss: 6.72815798239767559608\n",
      "Iteration 14333 => Loss: 6.72815394257655352561\n",
      "Iteration 14334 => Loss: 6.72814990329980222583\n",
      "Iteration 14335 => Loss: 6.72814586456734353703\n",
      "Iteration 14336 => Loss: 6.72814182637909130591\n",
      "Iteration 14337 => Loss: 6.72813778873500112354\n",
      "Iteration 14338 => Loss: 6.72813375163497617848\n",
      "Iteration 14339 => Loss: 6.72812971507895429824\n",
      "Iteration 14340 => Loss: 6.72812567906685821129\n",
      "Iteration 14341 => Loss: 6.72812164359860798157\n",
      "Iteration 14342 => Loss: 6.72811760867414232479\n",
      "Iteration 14343 => Loss: 6.72811357429338752212\n",
      "Iteration 14344 => Loss: 6.72810954045626541387\n",
      "Iteration 14345 => Loss: 6.72810550716269784033\n",
      "Iteration 14346 => Loss: 6.72810147441261374723\n",
      "Iteration 14347 => Loss: 6.72809744220595185027\n",
      "Iteration 14348 => Loss: 6.72809341054262510795\n",
      "Iteration 14349 => Loss: 6.72808937942256513054\n",
      "Iteration 14350 => Loss: 6.72808534884570086376\n",
      "Iteration 14351 => Loss: 6.72808131881196214152\n",
      "Iteration 14352 => Loss: 6.72807728932126458687\n",
      "Iteration 14353 => Loss: 6.72807326037353803372\n",
      "Iteration 14354 => Loss: 6.72806923196871675685\n",
      "Iteration 14355 => Loss: 6.72806520410672526111\n",
      "Iteration 14356 => Loss: 6.72806117678748627497\n",
      "Iteration 14357 => Loss: 6.72805715001092696781\n",
      "Iteration 14358 => Loss: 6.72805312377697894988\n",
      "Iteration 14359 => Loss: 6.72804909808556761419\n",
      "Iteration 14360 => Loss: 6.72804507293661302469\n",
      "Iteration 14361 => Loss: 6.72804104833005034436\n",
      "Iteration 14362 => Loss: 6.72803702426580407803\n",
      "Iteration 14363 => Loss: 6.72803300074380405960\n",
      "Iteration 14364 => Loss: 6.72802897776397035301\n",
      "Iteration 14365 => Loss: 6.72802495532623279217\n",
      "Iteration 14366 => Loss: 6.72802093343052209917\n",
      "Iteration 14367 => Loss: 6.72801691207675744977\n",
      "Iteration 14368 => Loss: 6.72801289126487578329\n",
      "Iteration 14369 => Loss: 6.72800887099479894005\n",
      "Iteration 14370 => Loss: 6.72800485126644520761\n",
      "Iteration 14371 => Loss: 6.72800083207975863075\n",
      "Iteration 14372 => Loss: 6.72799681343465660888\n",
      "Iteration 14373 => Loss: 6.72799279533106719953\n",
      "Iteration 14374 => Loss: 6.72798877776891135483\n",
      "Iteration 14375 => Loss: 6.72798476074812690229\n",
      "Iteration 14376 => Loss: 6.72798074426864012310\n",
      "Iteration 14377 => Loss: 6.72797672833037108120\n",
      "Iteration 14378 => Loss: 6.72797271293324605779\n",
      "Iteration 14379 => Loss: 6.72796869807720465673\n",
      "Iteration 14380 => Loss: 6.72796468376215628382\n",
      "Iteration 14381 => Loss: 6.72796066998804409565\n",
      "Iteration 14382 => Loss: 6.72795665675478549161\n",
      "Iteration 14383 => Loss: 6.72795264406230852927\n",
      "Iteration 14384 => Loss: 6.72794863191054659524\n",
      "Iteration 14385 => Loss: 6.72794462029942152981\n",
      "Iteration 14386 => Loss: 6.72794060922886227871\n",
      "Iteration 14387 => Loss: 6.72793659869879068225\n",
      "Iteration 14388 => Loss: 6.72793258870913835068\n",
      "Iteration 14389 => Loss: 6.72792857925983245337\n",
      "Iteration 14390 => Loss: 6.72792457035080637695\n",
      "Iteration 14391 => Loss: 6.72792056198197219175\n",
      "Iteration 14392 => Loss: 6.72791655415327216616\n",
      "Iteration 14393 => Loss: 6.72791254686462991685\n",
      "Iteration 14394 => Loss: 6.72790854011596017870\n",
      "Iteration 14395 => Loss: 6.72790453390720699645\n",
      "Iteration 14396 => Loss: 6.72790052823829043405\n",
      "Iteration 14397 => Loss: 6.72789652310913854905\n",
      "Iteration 14398 => Loss: 6.72789251851967584628\n",
      "Iteration 14399 => Loss: 6.72788851446983926508\n",
      "Iteration 14400 => Loss: 6.72788451095953821124\n",
      "Iteration 14401 => Loss: 6.72788050798871672953\n",
      "Iteration 14402 => Loss: 6.72787650555729310753\n",
      "Iteration 14403 => Loss: 6.72787250366519984368\n",
      "Iteration 14404 => Loss: 6.72786850231235789010\n",
      "Iteration 14405 => Loss: 6.72786450149870063342\n",
      "Iteration 14406 => Loss: 6.72786050122415257846\n",
      "Iteration 14407 => Loss: 6.72785650148864355913\n",
      "Iteration 14408 => Loss: 6.72785250229210074480\n",
      "Iteration 14409 => Loss: 6.72784850363444331123\n",
      "Iteration 14410 => Loss: 6.72784450551561175047\n",
      "Iteration 14411 => Loss: 6.72784050793552967917\n",
      "Iteration 14412 => Loss: 6.72783651089411538493\n",
      "Iteration 14413 => Loss: 6.72783251439130136617\n",
      "Iteration 14414 => Loss: 6.72782851842702189771\n",
      "Iteration 14415 => Loss: 6.72782452300119881983\n",
      "Iteration 14416 => Loss: 6.72782052811375486101\n",
      "Iteration 14417 => Loss: 6.72781653376463051330\n",
      "Iteration 14418 => Loss: 6.72781253995373518251\n",
      "Iteration 14419 => Loss: 6.72780854668101202520\n",
      "Iteration 14420 => Loss: 6.72780455394637932898\n",
      "Iteration 14421 => Loss: 6.72780056174977136862\n",
      "Iteration 14422 => Loss: 6.72779657009111176080\n",
      "Iteration 14423 => Loss: 6.72779257897032323399\n",
      "Iteration 14424 => Loss: 6.72778858838734983294\n",
      "Iteration 14425 => Loss: 6.72778459834209652257\n",
      "Iteration 14426 => Loss: 6.72778060883450734764\n",
      "Iteration 14427 => Loss: 6.72777661986449970755\n",
      "Iteration 14428 => Loss: 6.72777263143201320617\n",
      "Iteration 14429 => Loss: 6.72776864353696613108\n",
      "Iteration 14430 => Loss: 6.72776465617928298713\n",
      "Iteration 14431 => Loss: 6.72776066935890160181\n",
      "Iteration 14432 => Loss: 6.72775668307574292726\n",
      "Iteration 14433 => Loss: 6.72775269732973768555\n",
      "Iteration 14434 => Loss: 6.72774871212081215788\n",
      "Iteration 14435 => Loss: 6.72774472744889617815\n",
      "Iteration 14436 => Loss: 6.72774074331390714576\n",
      "Iteration 14437 => Loss: 6.72773675971578732913\n",
      "Iteration 14438 => Loss: 6.72773277665445235129\n",
      "Iteration 14439 => Loss: 6.72772879412984270431\n",
      "Iteration 14440 => Loss: 6.72772481214187312304\n",
      "Iteration 14441 => Loss: 6.72772083069047699411\n",
      "Iteration 14442 => Loss: 6.72771684977558415142\n",
      "Iteration 14443 => Loss: 6.72771286939711998798\n",
      "Iteration 14444 => Loss: 6.72770888955500812045\n",
      "Iteration 14445 => Loss: 6.72770491024917927092\n",
      "Iteration 14446 => Loss: 6.72770093147956682600\n",
      "Iteration 14447 => Loss: 6.72769695324609351417\n",
      "Iteration 14448 => Loss: 6.72769297554869005751\n",
      "Iteration 14449 => Loss: 6.72768899838727474361\n",
      "Iteration 14450 => Loss: 6.72768502176179161722\n",
      "Iteration 14451 => Loss: 6.72768104567215097234\n",
      "Iteration 14452 => Loss: 6.72767707011828974828\n",
      "Iteration 14453 => Loss: 6.72767309510013156171\n",
      "Iteration 14454 => Loss: 6.72766912061761601649\n",
      "Iteration 14455 => Loss: 6.72766514667065695932\n",
      "Iteration 14456 => Loss: 6.72766117325918955316\n",
      "Iteration 14457 => Loss: 6.72765720038313652651\n",
      "Iteration 14458 => Loss: 6.72765322804243126598\n",
      "Iteration 14459 => Loss: 6.72764925623700360546\n",
      "Iteration 14460 => Loss: 6.72764528496677183256\n",
      "Iteration 14461 => Loss: 6.72764131423167199841\n",
      "Iteration 14462 => Loss: 6.72763734403162505515\n",
      "Iteration 14463 => Loss: 6.72763337436656172486\n",
      "Iteration 14464 => Loss: 6.72762940523641450596\n",
      "Iteration 14465 => Loss: 6.72762543664110701513\n",
      "Iteration 14466 => Loss: 6.72762146858056819809\n",
      "Iteration 14467 => Loss: 6.72761750105473232964\n",
      "Iteration 14468 => Loss: 6.72761353406350970374\n",
      "Iteration 14469 => Loss: 6.72760956760684702971\n",
      "Iteration 14470 => Loss: 6.72760560168465904241\n",
      "Iteration 14471 => Loss: 6.72760163629687824027\n",
      "Iteration 14472 => Loss: 6.72759767144344333900\n",
      "Iteration 14473 => Loss: 6.72759370712426285621\n",
      "Iteration 14474 => Loss: 6.72758974333928083666\n",
      "Iteration 14475 => Loss: 6.72758578008842178519\n",
      "Iteration 14476 => Loss: 6.72758181737160310121\n",
      "Iteration 14477 => Loss: 6.72757785518876350039\n",
      "Iteration 14478 => Loss: 6.72757389353983281666\n",
      "Iteration 14479 => Loss: 6.72756993242472933758\n",
      "Iteration 14480 => Loss: 6.72756597184338200890\n",
      "Iteration 14481 => Loss: 6.72756201179573221083\n",
      "Iteration 14482 => Loss: 6.72755805228169645460\n",
      "Iteration 14483 => Loss: 6.72755409330120723865\n",
      "Iteration 14484 => Loss: 6.72755013485418640329\n",
      "Iteration 14485 => Loss: 6.72754617694056999966\n",
      "Iteration 14486 => Loss: 6.72754221956028519713\n",
      "Iteration 14487 => Loss: 6.72753826271325117148\n",
      "Iteration 14488 => Loss: 6.72753430639940663838\n",
      "Iteration 14489 => Loss: 6.72753035061867787903\n",
      "Iteration 14490 => Loss: 6.72752639537098851008\n",
      "Iteration 14491 => Loss: 6.72752244065626747727\n",
      "Iteration 14492 => Loss: 6.72751848647444816720\n",
      "Iteration 14493 => Loss: 6.72751453282544975565\n",
      "Iteration 14494 => Loss: 6.72751057970921184648\n",
      "Iteration 14495 => Loss: 6.72750662712565716816\n",
      "Iteration 14496 => Loss: 6.72750267507470489647\n",
      "Iteration 14497 => Loss: 6.72749872355629818799\n",
      "Iteration 14498 => Loss: 6.72749477257035621847\n",
      "Iteration 14499 => Loss: 6.72749082211680882182\n",
      "Iteration 14500 => Loss: 6.72748687219558938466\n",
      "Iteration 14501 => Loss: 6.72748292280661797093\n",
      "Iteration 14502 => Loss: 6.72747897394983063180\n",
      "Iteration 14503 => Loss: 6.72747502562515453661\n",
      "Iteration 14504 => Loss: 6.72747107783250708479\n",
      "Iteration 14505 => Loss: 6.72746713057182876838\n",
      "Iteration 14506 => Loss: 6.72746318384305208582\n",
      "Iteration 14507 => Loss: 6.72745923764608821926\n",
      "Iteration 14508 => Loss: 6.72745529198087677258\n",
      "Iteration 14509 => Loss: 6.72745134684734757968\n",
      "Iteration 14510 => Loss: 6.72744740224542070450\n",
      "Iteration 14511 => Loss: 6.72744345817503042184\n",
      "Iteration 14512 => Loss: 6.72743951463610390107\n",
      "Iteration 14513 => Loss: 6.72743557162856653520\n",
      "Iteration 14514 => Loss: 6.72743162915235348720\n",
      "Iteration 14515 => Loss: 6.72742768720738926191\n",
      "Iteration 14516 => Loss: 6.72742374579360458142\n",
      "Iteration 14517 => Loss: 6.72741980491092395056\n",
      "Iteration 14518 => Loss: 6.72741586455927098598\n",
      "Iteration 14519 => Loss: 6.72741192473858973244\n",
      "Iteration 14520 => Loss: 6.72740798544879581300\n",
      "Iteration 14521 => Loss: 6.72740404668982172609\n",
      "Iteration 14522 => Loss: 6.72740010846160352287\n",
      "Iteration 14523 => Loss: 6.72739617076405416185\n",
      "Iteration 14524 => Loss: 6.72739223359710880601\n",
      "Iteration 14525 => Loss: 6.72738829696069551289\n",
      "Iteration 14526 => Loss: 6.72738436085474855730\n",
      "Iteration 14527 => Loss: 6.72738042527919510860\n",
      "Iteration 14528 => Loss: 6.72737649023395256620\n",
      "Iteration 14529 => Loss: 6.72737255571896408668\n",
      "Iteration 14530 => Loss: 6.72736862173414618127\n",
      "Iteration 14531 => Loss: 6.72736468827943934201\n",
      "Iteration 14532 => Loss: 6.72736075535476096832\n",
      "Iteration 14533 => Loss: 6.72735682296004799952\n",
      "Iteration 14534 => Loss: 6.72735289109522227591\n",
      "Iteration 14535 => Loss: 6.72734895976021540775\n",
      "Iteration 14536 => Loss: 6.72734502895496078168\n",
      "Iteration 14537 => Loss: 6.72734109867937934979\n",
      "Iteration 14538 => Loss: 6.72733716893340272236\n",
      "Iteration 14539 => Loss: 6.72733323971696162147\n",
      "Iteration 14540 => Loss: 6.72732931102998055195\n",
      "Iteration 14541 => Loss: 6.72732538287239023589\n",
      "Iteration 14542 => Loss: 6.72732145524412228355\n",
      "Iteration 14543 => Loss: 6.72731752814509675886\n",
      "Iteration 14544 => Loss: 6.72731360157525681842\n",
      "Iteration 14545 => Loss: 6.72730967553451364438\n",
      "Iteration 14546 => Loss: 6.72730575002281128150\n",
      "Iteration 14547 => Loss: 6.72730182504007245825\n",
      "Iteration 14548 => Loss: 6.72729790058621990312\n",
      "Iteration 14549 => Loss: 6.72729397666119055543\n",
      "Iteration 14550 => Loss: 6.72729005326490625549\n",
      "Iteration 14551 => Loss: 6.72728613039730571899\n",
      "Iteration 14552 => Loss: 6.72728220805830989804\n",
      "Iteration 14553 => Loss: 6.72727828624784951472\n",
      "Iteration 14554 => Loss: 6.72727436496585085024\n",
      "Iteration 14555 => Loss: 6.72727044421224373849\n",
      "Iteration 14556 => Loss: 6.72726652398697044788\n",
      "Iteration 14557 => Loss: 6.72726260428994127238\n",
      "Iteration 14558 => Loss: 6.72725868512108782227\n",
      "Iteration 14559 => Loss: 6.72725476648034437233\n",
      "Iteration 14560 => Loss: 6.72725084836764164464\n",
      "Iteration 14561 => Loss: 6.72724693078290059134\n",
      "Iteration 14562 => Loss: 6.72724301372605371085\n",
      "Iteration 14563 => Loss: 6.72723909719703261345\n",
      "Iteration 14564 => Loss: 6.72723518119576535668\n",
      "Iteration 14565 => Loss: 6.72723126572218355079\n",
      "Iteration 14566 => Loss: 6.72722735077620104249\n",
      "Iteration 14567 => Loss: 6.72722343635776098836\n",
      "Iteration 14568 => Loss: 6.72721952246679588683\n",
      "Iteration 14569 => Loss: 6.72721560910322136095\n",
      "Iteration 14570 => Loss: 6.72721169626697790278\n",
      "Iteration 14571 => Loss: 6.72720778395798291172\n",
      "Iteration 14572 => Loss: 6.72720387217617776798\n",
      "Iteration 14573 => Loss: 6.72719996092148431188\n",
      "Iteration 14574 => Loss: 6.72719605019383415367\n",
      "Iteration 14575 => Loss: 6.72719213999314824548\n",
      "Iteration 14576 => Loss: 6.72718823031936974388\n",
      "Iteration 14577 => Loss: 6.72718432117241871282\n",
      "Iteration 14578 => Loss: 6.72718041255222587438\n",
      "Iteration 14579 => Loss: 6.72717650445871129250\n",
      "Iteration 14580 => Loss: 6.72717259689182256466\n",
      "Iteration 14581 => Loss: 6.72716868985147620208\n",
      "Iteration 14582 => Loss: 6.72716478333759937414\n",
      "Iteration 14583 => Loss: 6.72716087735013257287\n",
      "Iteration 14584 => Loss: 6.72715697188899408587\n",
      "Iteration 14585 => Loss: 6.72715306695411552340\n",
      "Iteration 14586 => Loss: 6.72714916254543204843\n",
      "Iteration 14587 => Loss: 6.72714525866286461309\n",
      "Iteration 14588 => Loss: 6.72714135530634838034\n",
      "Iteration 14589 => Loss: 6.72713745247580252595\n",
      "Iteration 14590 => Loss: 6.72713355017117109469\n",
      "Iteration 14591 => Loss: 6.72712964839236793324\n",
      "Iteration 14592 => Loss: 6.72712574713933886272\n",
      "Iteration 14593 => Loss: 6.72712184641199506530\n",
      "Iteration 14594 => Loss: 6.72711794621028325025\n",
      "Iteration 14595 => Loss: 6.72711404653411992882\n",
      "Iteration 14596 => Loss: 6.72711014738343582309\n",
      "Iteration 14597 => Loss: 6.72710624875816964874\n",
      "Iteration 14598 => Loss: 6.72710235065823258793\n",
      "Iteration 14599 => Loss: 6.72709845308356957361\n",
      "Iteration 14600 => Loss: 6.72709455603410777513\n",
      "Iteration 14601 => Loss: 6.72709065950977347370\n",
      "Iteration 14602 => Loss: 6.72708676351049472686\n",
      "Iteration 14603 => Loss: 6.72708286803620314487\n",
      "Iteration 14604 => Loss: 6.72707897308682500892\n",
      "Iteration 14605 => Loss: 6.72707507866229192928\n",
      "Iteration 14606 => Loss: 6.72707118476253462802\n",
      "Iteration 14607 => Loss: 6.72706729138748116270\n",
      "Iteration 14608 => Loss: 6.72706339853705603815\n",
      "Iteration 14609 => Loss: 6.72705950621119885824\n",
      "Iteration 14610 => Loss: 6.72705561440983146326\n",
      "Iteration 14611 => Loss: 6.72705172313288191077\n",
      "Iteration 14612 => Loss: 6.72704783238028358738\n",
      "Iteration 14613 => Loss: 6.72704394215196277429\n",
      "Iteration 14614 => Loss: 6.72704005244785108175\n",
      "Iteration 14615 => Loss: 6.72703616326788012003\n",
      "Iteration 14616 => Loss: 6.72703227461197439396\n",
      "Iteration 14617 => Loss: 6.72702838648006373745\n",
      "Iteration 14618 => Loss: 6.72702449887208420165\n",
      "Iteration 14619 => Loss: 6.72702061178795496232\n",
      "Iteration 14620 => Loss: 6.72701672522760762973\n",
      "Iteration 14621 => Loss: 6.72701283919098269592\n",
      "Iteration 14622 => Loss: 6.72700895367799844848\n",
      "Iteration 14623 => Loss: 6.72700506868858738585\n",
      "Iteration 14624 => Loss: 6.72700118422268200646\n",
      "Iteration 14625 => Loss: 6.72699730028020592698\n",
      "Iteration 14626 => Loss: 6.72699341686108720495\n",
      "Iteration 14627 => Loss: 6.72698953396526544424\n",
      "Iteration 14628 => Loss: 6.72698565159266070879\n",
      "Iteration 14629 => Loss: 6.72698176974320727339\n",
      "Iteration 14630 => Loss: 6.72697788841683408378\n",
      "Iteration 14631 => Loss: 6.72697400761347186204\n",
      "Iteration 14632 => Loss: 6.72697012733304156029\n",
      "Iteration 14633 => Loss: 6.72696624757548633511\n",
      "Iteration 14634 => Loss: 6.72696236834072358590\n",
      "Iteration 14635 => Loss: 6.72695848962868936383\n",
      "Iteration 14636 => Loss: 6.72695461143931172643\n",
      "Iteration 14637 => Loss: 6.72695073377252050761\n",
      "Iteration 14638 => Loss: 6.72694685662824909400\n",
      "Iteration 14639 => Loss: 6.72694298000641932589\n",
      "Iteration 14640 => Loss: 6.72693910390696814261\n",
      "Iteration 14641 => Loss: 6.72693522832981738446\n",
      "Iteration 14642 => Loss: 6.72693135327490043807\n",
      "Iteration 14643 => Loss: 6.72692747874215157822\n",
      "Iteration 14644 => Loss: 6.72692360473149353339\n",
      "Iteration 14645 => Loss: 6.72691973124286324293\n",
      "Iteration 14646 => Loss: 6.72691585827617544169\n",
      "Iteration 14647 => Loss: 6.72691198583137772715\n",
      "Iteration 14648 => Loss: 6.72690811390839105144\n",
      "Iteration 14649 => Loss: 6.72690424250714880117\n",
      "Iteration 14650 => Loss: 6.72690037162757814571\n",
      "Iteration 14651 => Loss: 6.72689650126960625443\n",
      "Iteration 14652 => Loss: 6.72689263143316917848\n",
      "Iteration 14653 => Loss: 6.72688876211818431727\n",
      "Iteration 14654 => Loss: 6.72688489332459571557\n",
      "Iteration 14655 => Loss: 6.72688102505232699002\n",
      "Iteration 14656 => Loss: 6.72687715730130531000\n",
      "Iteration 14657 => Loss: 6.72687329007146761484\n",
      "Iteration 14658 => Loss: 6.72686942336273840937\n",
      "Iteration 14659 => Loss: 6.72686555717504663932\n",
      "Iteration 14660 => Loss: 6.72686169150832924402\n",
      "Iteration 14661 => Loss: 6.72685782636250628741\n",
      "Iteration 14662 => Loss: 6.72685396173751026794\n",
      "Iteration 14663 => Loss: 6.72685009763327546040\n",
      "Iteration 14664 => Loss: 6.72684623404972814598\n",
      "Iteration 14665 => Loss: 6.72684237098680082312\n",
      "Iteration 14666 => Loss: 6.72683850844442154937\n",
      "Iteration 14667 => Loss: 6.72683464642251571775\n",
      "Iteration 14668 => Loss: 6.72683078492102204393\n",
      "Iteration 14669 => Loss: 6.72682692393986769730\n",
      "Iteration 14670 => Loss: 6.72682306347897362997\n",
      "Iteration 14671 => Loss: 6.72681920353828477488\n",
      "Iteration 14672 => Loss: 6.72681534411771497872\n",
      "Iteration 14673 => Loss: 6.72681148521720917444\n",
      "Iteration 14674 => Loss: 6.72680762683668831414\n",
      "Iteration 14675 => Loss: 6.72680376897608667264\n",
      "Iteration 14676 => Loss: 6.72679991163532609022\n",
      "Iteration 14677 => Loss: 6.72679605481434439440\n",
      "Iteration 14678 => Loss: 6.72679219851307585998\n",
      "Iteration 14679 => Loss: 6.72678834273144143907\n",
      "Iteration 14680 => Loss: 6.72678448746937096558\n",
      "Iteration 14681 => Loss: 6.72678063272679960249\n",
      "Iteration 14682 => Loss: 6.72677677850365274281\n",
      "Iteration 14683 => Loss: 6.72677292479986377316\n",
      "Iteration 14684 => Loss: 6.72676907161536519197\n",
      "Iteration 14685 => Loss: 6.72676521895007706320\n",
      "Iteration 14686 => Loss: 6.72676136680394343159\n",
      "Iteration 14687 => Loss: 6.72675751517687992020\n",
      "Iteration 14688 => Loss: 6.72675366406882258019\n",
      "Iteration 14689 => Loss: 6.72674981347971190360\n",
      "Iteration 14690 => Loss: 6.72674596340946084894\n",
      "Iteration 14691 => Loss: 6.72674211385800990826\n",
      "Iteration 14692 => Loss: 6.72673826482528802728\n",
      "Iteration 14693 => Loss: 6.72673441631121526996\n",
      "Iteration 14694 => Loss: 6.72673056831573656922\n",
      "Iteration 14695 => Loss: 6.72672672083877287719\n",
      "Iteration 14696 => Loss: 6.72672287388025846866\n",
      "Iteration 14697 => Loss: 6.72671902744012584208\n",
      "Iteration 14698 => Loss: 6.72671518151829772592\n",
      "Iteration 14699 => Loss: 6.72671133611470928315\n",
      "Iteration 14700 => Loss: 6.72670749122928857133\n",
      "Iteration 14701 => Loss: 6.72670364686196187165\n",
      "Iteration 14702 => Loss: 6.72669980301266878797\n",
      "Iteration 14703 => Loss: 6.72669595968133382513\n",
      "Iteration 14704 => Loss: 6.72669211686789036975\n",
      "Iteration 14705 => Loss: 6.72668827457226381483\n",
      "Iteration 14706 => Loss: 6.72668443279438488247\n",
      "Iteration 14707 => Loss: 6.72668059153418873564\n",
      "Iteration 14708 => Loss: 6.72667675079160076734\n",
      "Iteration 14709 => Loss: 6.72667291056654992332\n",
      "Iteration 14710 => Loss: 6.72666907085897580743\n",
      "Iteration 14711 => Loss: 6.72666523166880203632\n",
      "Iteration 14712 => Loss: 6.72666139299595666756\n",
      "Iteration 14713 => Loss: 6.72665755484038108136\n",
      "Iteration 14714 => Loss: 6.72665371720198734806\n",
      "Iteration 14715 => Loss: 6.72664988008071862424\n",
      "Iteration 14716 => Loss: 6.72664604347650207927\n",
      "Iteration 14717 => Loss: 6.72664220738927198795\n",
      "Iteration 14718 => Loss: 6.72663837181895196693\n",
      "Iteration 14719 => Loss: 6.72663453676547540283\n",
      "Iteration 14720 => Loss: 6.72663070222877301774\n",
      "Iteration 14721 => Loss: 6.72662686820877109284\n",
      "Iteration 14722 => Loss: 6.72662303470540745565\n",
      "Iteration 14723 => Loss: 6.72661920171860661100\n",
      "Iteration 14724 => Loss: 6.72661536924830461004\n",
      "Iteration 14725 => Loss: 6.72661153729442595761\n",
      "Iteration 14726 => Loss: 6.72660770585690315215\n",
      "Iteration 14727 => Loss: 6.72660387493566869210\n",
      "Iteration 14728 => Loss: 6.72660004453064708230\n",
      "Iteration 14729 => Loss: 6.72659621464177526207\n",
      "Iteration 14730 => Loss: 6.72659238526898040078\n",
      "Iteration 14731 => Loss: 6.72658855641219410870\n",
      "Iteration 14732 => Loss: 6.72658472807134888427\n",
      "Iteration 14733 => Loss: 6.72658090024637278503\n",
      "Iteration 14734 => Loss: 6.72657707293719120401\n",
      "Iteration 14735 => Loss: 6.72657324614374818594\n",
      "Iteration 14736 => Loss: 6.72656941986595580119\n",
      "Iteration 14737 => Loss: 6.72656559410376431174\n",
      "Iteration 14738 => Loss: 6.72656176885708756430\n",
      "Iteration 14739 => Loss: 6.72655794412586427455\n",
      "Iteration 14740 => Loss: 6.72655411991002782912\n",
      "Iteration 14741 => Loss: 6.72655029620950095648\n",
      "Iteration 14742 => Loss: 6.72654647302421970778\n",
      "Iteration 14743 => Loss: 6.72654265035411480511\n",
      "Iteration 14744 => Loss: 6.72653882819911252966\n",
      "Iteration 14745 => Loss: 6.72653500655914360351\n",
      "Iteration 14746 => Loss: 6.72653118543414940689\n",
      "Iteration 14747 => Loss: 6.72652736482404023377\n",
      "Iteration 14748 => Loss: 6.72652354472876634617\n",
      "Iteration 14749 => Loss: 6.72651972514824869620\n",
      "Iteration 14750 => Loss: 6.72651590608242422320\n",
      "Iteration 14751 => Loss: 6.72651208753121121475\n",
      "Iteration 14752 => Loss: 6.72650826949455016290\n",
      "Iteration 14753 => Loss: 6.72650445197237534245\n",
      "Iteration 14754 => Loss: 6.72650063496460504098\n",
      "Iteration 14755 => Loss: 6.72649681847117975053\n",
      "Iteration 14756 => Loss: 6.72649300249203019320\n",
      "Iteration 14757 => Loss: 6.72648918702708620287\n",
      "Iteration 14758 => Loss: 6.72648537207626251444\n",
      "Iteration 14759 => Loss: 6.72648155763951649533\n",
      "Iteration 14760 => Loss: 6.72647774371676732130\n",
      "Iteration 14761 => Loss: 6.72647393030793594448\n",
      "Iteration 14762 => Loss: 6.72647011741296729781\n",
      "Iteration 14763 => Loss: 6.72646630503178144522\n",
      "Iteration 14764 => Loss: 6.72646249316431710241\n",
      "Iteration 14765 => Loss: 6.72645868181050232693\n",
      "Iteration 14766 => Loss: 6.72645487097027139356\n",
      "Iteration 14767 => Loss: 6.72645106064354436626\n",
      "Iteration 14768 => Loss: 6.72644725083026706614\n",
      "Iteration 14769 => Loss: 6.72644344153035778078\n",
      "Iteration 14770 => Loss: 6.72643963274375256134\n",
      "Iteration 14771 => Loss: 6.72643582447037768901\n",
      "Iteration 14772 => Loss: 6.72643201671016832677\n",
      "Iteration 14773 => Loss: 6.72642820946306496666\n",
      "Iteration 14774 => Loss: 6.72642440272898234355\n",
      "Iteration 14775 => Loss: 6.72642059650785917313\n",
      "Iteration 14776 => Loss: 6.72641679079961640753\n",
      "Iteration 14777 => Loss: 6.72641298560420075603\n",
      "Iteration 14778 => Loss: 6.72640918092153672347\n",
      "Iteration 14779 => Loss: 6.72640537675154703834\n",
      "Iteration 14780 => Loss: 6.72640157309417663356\n",
      "Iteration 14781 => Loss: 6.72639776994934823762\n",
      "Iteration 14782 => Loss: 6.72639396731699257259\n",
      "Iteration 14783 => Loss: 6.72639016519704124875\n",
      "Iteration 14784 => Loss: 6.72638636358942676452\n",
      "Iteration 14785 => Loss: 6.72638256249407717746\n",
      "Iteration 14786 => Loss: 6.72637876191092587419\n",
      "Iteration 14787 => Loss: 6.72637496183990535314\n",
      "Iteration 14788 => Loss: 6.72637116228094100734\n",
      "Iteration 14789 => Loss: 6.72636736323397510517\n",
      "Iteration 14790 => Loss: 6.72636356469892504606\n",
      "Iteration 14791 => Loss: 6.72635976667572332843\n",
      "Iteration 14792 => Loss: 6.72635596916431488523\n",
      "Iteration 14793 => Loss: 6.72635217216462066858\n",
      "Iteration 14794 => Loss: 6.72634837567656784785\n",
      "Iteration 14795 => Loss: 6.72634457970009158601\n",
      "Iteration 14796 => Loss: 6.72634078423512971057\n",
      "Iteration 14797 => Loss: 6.72633698928160050912\n",
      "Iteration 14798 => Loss: 6.72633319483944358552\n",
      "Iteration 14799 => Loss: 6.72632940090858699733\n",
      "Iteration 14800 => Loss: 6.72632560748896857206\n",
      "Iteration 14801 => Loss: 6.72632181458050926182\n",
      "Iteration 14802 => Loss: 6.72631802218314689412\n",
      "Iteration 14803 => Loss: 6.72631423029680597381\n",
      "Iteration 14804 => Loss: 6.72631043892142255203\n",
      "Iteration 14805 => Loss: 6.72630664805693001540\n",
      "Iteration 14806 => Loss: 6.72630285770325109240\n",
      "Iteration 14807 => Loss: 6.72629906786033604504\n",
      "Iteration 14808 => Loss: 6.72629527852809250277\n",
      "Iteration 14809 => Loss: 6.72629148970645918126\n",
      "Iteration 14810 => Loss: 6.72628770139537657258\n",
      "Iteration 14811 => Loss: 6.72628391359476474065\n",
      "Iteration 14812 => Loss: 6.72628012630456506571\n",
      "Iteration 14813 => Loss: 6.72627633952469672352\n",
      "Iteration 14814 => Loss: 6.72627255325510020612\n",
      "Iteration 14815 => Loss: 6.72626876749570890013\n",
      "Iteration 14816 => Loss: 6.72626498224643665225\n",
      "Iteration 14817 => Loss: 6.72626119750723727719\n",
      "Iteration 14818 => Loss: 6.72625741327803172709\n",
      "Iteration 14819 => Loss: 6.72625362955874095405\n",
      "Iteration 14820 => Loss: 6.72624984634931877281\n",
      "Iteration 14821 => Loss: 6.72624606364967725369\n",
      "Iteration 14822 => Loss: 6.72624228145975955329\n",
      "Iteration 14823 => Loss: 6.72623849977948307099\n",
      "Iteration 14824 => Loss: 6.72623471860879451611\n",
      "Iteration 14825 => Loss: 6.72623093794762194619\n",
      "Iteration 14826 => Loss: 6.72622715779589341878\n",
      "Iteration 14827 => Loss: 6.72622337815353521506\n",
      "Iteration 14828 => Loss: 6.72621959902048782709\n",
      "Iteration 14829 => Loss: 6.72621582039667753605\n",
      "Iteration 14830 => Loss: 6.72621204228203772857\n",
      "Iteration 14831 => Loss: 6.72620826467649912672\n",
      "Iteration 14832 => Loss: 6.72620448757999245259\n",
      "Iteration 14833 => Loss: 6.72620071099245375734\n",
      "Iteration 14834 => Loss: 6.72619693491380932215\n",
      "Iteration 14835 => Loss: 6.72619315934398898094\n",
      "Iteration 14836 => Loss: 6.72618938428292700848\n",
      "Iteration 14837 => Loss: 6.72618560973055767960\n",
      "Iteration 14838 => Loss: 6.72618183568680727547\n",
      "Iteration 14839 => Loss: 6.72617806215160474181\n",
      "Iteration 14840 => Loss: 6.72617428912489767612\n",
      "Iteration 14841 => Loss: 6.72617051660659548418\n",
      "Iteration 14842 => Loss: 6.72616674459664753982\n",
      "Iteration 14843 => Loss: 6.72616297309497834789\n",
      "Iteration 14844 => Loss: 6.72615920210151152503\n",
      "Iteration 14845 => Loss: 6.72615543161619200418\n",
      "Iteration 14846 => Loss: 6.72615166163894961926\n",
      "Iteration 14847 => Loss: 6.72614789216970709873\n",
      "Iteration 14848 => Loss: 6.72614412320839960557\n",
      "Iteration 14849 => Loss: 6.72614035475496230276\n",
      "Iteration 14850 => Loss: 6.72613658680932680056\n",
      "Iteration 14851 => Loss: 6.72613281937141138656\n",
      "Iteration 14852 => Loss: 6.72612905244117076364\n",
      "Iteration 14853 => Loss: 6.72612528601851700216\n",
      "Iteration 14854 => Loss: 6.72612152010339503505\n",
      "Iteration 14855 => Loss: 6.72611775469572403807\n",
      "Iteration 14856 => Loss: 6.72611398979544805599\n",
      "Iteration 14857 => Loss: 6.72611022540249336998\n",
      "Iteration 14858 => Loss: 6.72610646151677737947\n",
      "Iteration 14859 => Loss: 6.72610269813825922824\n",
      "Iteration 14860 => Loss: 6.72609893526685187481\n",
      "Iteration 14861 => Loss: 6.72609517290249225852\n",
      "Iteration 14862 => Loss: 6.72609141104511465414\n",
      "Iteration 14863 => Loss: 6.72608764969464534289\n",
      "Iteration 14864 => Loss: 6.72608388885101771137\n",
      "Iteration 14865 => Loss: 6.72608012851416692257\n",
      "Iteration 14866 => Loss: 6.72607636868401570496\n",
      "Iteration 14867 => Loss: 6.72607260936050366240\n",
      "Iteration 14868 => Loss: 6.72606885054355974063\n",
      "Iteration 14869 => Loss: 6.72606509223311999079\n",
      "Iteration 14870 => Loss: 6.72606133442911779952\n",
      "Iteration 14871 => Loss: 6.72605757713146967802\n",
      "Iteration 14872 => Loss: 6.72605382034012500014\n",
      "Iteration 14873 => Loss: 6.72605006405500382982\n",
      "Iteration 14874 => Loss: 6.72604630827604488275\n",
      "Iteration 14875 => Loss: 6.72604255300317710464\n",
      "Iteration 14876 => Loss: 6.72603879823633032942\n",
      "Iteration 14877 => Loss: 6.72603504397544682547\n",
      "Iteration 14878 => Loss: 6.72603129022043777496\n",
      "Iteration 14879 => Loss: 6.72602753697125432808\n",
      "Iteration 14880 => Loss: 6.72602378422782010148\n",
      "Iteration 14881 => Loss: 6.72602003199007292267\n",
      "Iteration 14882 => Loss: 6.72601628025792930288\n",
      "Iteration 14883 => Loss: 6.72601252903134128047\n",
      "Iteration 14884 => Loss: 6.72600877831023336029\n",
      "Iteration 14885 => Loss: 6.72600502809452915898\n",
      "Iteration 14886 => Loss: 6.72600127838417094495\n",
      "Iteration 14887 => Loss: 6.72599752917908499938\n",
      "Iteration 14888 => Loss: 6.72599378047920204438\n",
      "Iteration 14889 => Loss: 6.72599003228445635472\n",
      "Iteration 14890 => Loss: 6.72598628459478486974\n",
      "Iteration 14891 => Loss: 6.72598253741010854156\n",
      "Iteration 14892 => Loss: 6.72597879073037940856\n",
      "Iteration 14893 => Loss: 6.72597504455549621838\n",
      "Iteration 14894 => Loss: 6.72597129888541811482\n",
      "Iteration 14895 => Loss: 6.72596755372007049090\n",
      "Iteration 14896 => Loss: 6.72596380905939206230\n",
      "Iteration 14897 => Loss: 6.72596006490329667571\n",
      "Iteration 14898 => Loss: 6.72595632125172571136\n",
      "Iteration 14899 => Loss: 6.72595257810462143766\n",
      "Iteration 14900 => Loss: 6.72594883546190303036\n",
      "Iteration 14901 => Loss: 6.72594509332350209974\n",
      "Iteration 14902 => Loss: 6.72594135168935824964\n",
      "Iteration 14903 => Loss: 6.72593761055939776128\n",
      "Iteration 14904 => Loss: 6.72593386993355046855\n",
      "Iteration 14905 => Loss: 6.72593012981176041620\n",
      "Iteration 14906 => Loss: 6.72592639019394678002\n",
      "Iteration 14907 => Loss: 6.72592265108005360474\n",
      "Iteration 14908 => Loss: 6.72591891246999562526\n",
      "Iteration 14909 => Loss: 6.72591517436372399175\n",
      "Iteration 14910 => Loss: 6.72591143676116409722\n",
      "Iteration 14911 => Loss: 6.72590769966224488741\n",
      "Iteration 14912 => Loss: 6.72590396306689619621\n",
      "Iteration 14913 => Loss: 6.72590022697504874571\n",
      "Iteration 14914 => Loss: 6.72589649138665546246\n",
      "Iteration 14915 => Loss: 6.72589275630162308772\n",
      "Iteration 14916 => Loss: 6.72588902171989921897\n",
      "Iteration 14917 => Loss: 6.72588528764140303196\n",
      "Iteration 14918 => Loss: 6.72588155406607324238\n",
      "Iteration 14919 => Loss: 6.72587782099384856593\n",
      "Iteration 14920 => Loss: 6.72587408842466505376\n",
      "Iteration 14921 => Loss: 6.72587035635842855896\n",
      "Iteration 14922 => Loss: 6.72586662479509467261\n",
      "Iteration 14923 => Loss: 6.72586289373458523499\n",
      "Iteration 14924 => Loss: 6.72585916317684429089\n",
      "Iteration 14925 => Loss: 6.72585543312179545694\n",
      "Iteration 14926 => Loss: 6.72585170356936590252\n",
      "Iteration 14927 => Loss: 6.72584797451950588965\n",
      "Iteration 14928 => Loss: 6.72584424597212304775\n",
      "Iteration 14929 => Loss: 6.72584051792716675067\n",
      "Iteration 14930 => Loss: 6.72583679038455795052\n",
      "Iteration 14931 => Loss: 6.72583306334424246842\n",
      "Iteration 14932 => Loss: 6.72582933680615102645\n",
      "Iteration 14933 => Loss: 6.72582561077020102402\n",
      "Iteration 14934 => Loss: 6.72582188523634005861\n",
      "Iteration 14935 => Loss: 6.72581816020449618776\n",
      "Iteration 14936 => Loss: 6.72581443567459391630\n",
      "Iteration 14937 => Loss: 6.72581071164657817718\n",
      "Iteration 14938 => Loss: 6.72580698812037258705\n",
      "Iteration 14939 => Loss: 6.72580326509592119066\n",
      "Iteration 14940 => Loss: 6.72579954257313961108\n",
      "Iteration 14941 => Loss: 6.72579582055197100487\n",
      "Iteration 14942 => Loss: 6.72579209903234076506\n",
      "Iteration 14943 => Loss: 6.72578837801418671916\n",
      "Iteration 14944 => Loss: 6.72578465749744047741\n",
      "Iteration 14945 => Loss: 6.72578093748203365010\n",
      "Iteration 14946 => Loss: 6.72577721796790140019\n",
      "Iteration 14947 => Loss: 6.72577349895496912069\n",
      "Iteration 14948 => Loss: 6.72576978044317819183\n",
      "Iteration 14949 => Loss: 6.72576606243246732930\n",
      "Iteration 14950 => Loss: 6.72576234492274593890\n",
      "Iteration 14951 => Loss: 6.72575862791395717721\n",
      "Iteration 14952 => Loss: 6.72575491140604153628\n",
      "Iteration 14953 => Loss: 6.72575119539892618548\n",
      "Iteration 14954 => Loss: 6.72574747989254095870\n",
      "Iteration 14955 => Loss: 6.72574376488682013076\n",
      "Iteration 14956 => Loss: 6.72574005038169620008\n",
      "Iteration 14957 => Loss: 6.72573633637710610600\n",
      "Iteration 14958 => Loss: 6.72573262287297168882\n",
      "Iteration 14959 => Loss: 6.72572890986923610512\n",
      "Iteration 14960 => Loss: 6.72572519736582918881\n",
      "Iteration 14961 => Loss: 6.72572148536268077379\n",
      "Iteration 14962 => Loss: 6.72571777385972691121\n",
      "Iteration 14963 => Loss: 6.72571406285689477045\n",
      "Iteration 14964 => Loss: 6.72571035235412395537\n",
      "Iteration 14965 => Loss: 6.72570664235134163533\n",
      "Iteration 14966 => Loss: 6.72570293284848119697\n",
      "Iteration 14967 => Loss: 6.72569922384548490868\n",
      "Iteration 14968 => Loss: 6.72569551534226750533\n",
      "Iteration 14969 => Loss: 6.72569180733877214351\n",
      "Iteration 14970 => Loss: 6.72568809983493665072\n",
      "Iteration 14971 => Loss: 6.72568439283068375545\n",
      "Iteration 14972 => Loss: 6.72568068632594595613\n",
      "Iteration 14973 => Loss: 6.72567698032066996205\n",
      "Iteration 14974 => Loss: 6.72567327481477139628\n",
      "Iteration 14975 => Loss: 6.72566956980818808631\n",
      "Iteration 14976 => Loss: 6.72566586530085963602\n",
      "Iteration 14977 => Loss: 6.72566216129270877389\n",
      "Iteration 14978 => Loss: 6.72565845778367776830\n",
      "Iteration 14979 => Loss: 6.72565475477369112411\n",
      "Iteration 14980 => Loss: 6.72565105226268844518\n",
      "Iteration 14981 => Loss: 6.72564735025059867723\n",
      "Iteration 14982 => Loss: 6.72564364873736053596\n",
      "Iteration 14983 => Loss: 6.72563994772289674984\n",
      "Iteration 14984 => Loss: 6.72563624720714337002\n",
      "Iteration 14985 => Loss: 6.72563254719003467130\n",
      "Iteration 14986 => Loss: 6.72562884767150226395\n",
      "Iteration 14987 => Loss: 6.72562514865148841636\n",
      "Iteration 14988 => Loss: 6.72562145012990786341\n",
      "Iteration 14989 => Loss: 6.72561775210670820258\n",
      "Iteration 14990 => Loss: 6.72561405458182104411\n",
      "Iteration 14991 => Loss: 6.72561035755517178103\n",
      "Iteration 14992 => Loss: 6.72560666102669646449\n",
      "Iteration 14993 => Loss: 6.72560296499632759293\n",
      "Iteration 14994 => Loss: 6.72559926946400654657\n",
      "Iteration 14995 => Loss: 6.72559557442965072482\n",
      "Iteration 14996 => Loss: 6.72559187989320417245\n",
      "Iteration 14997 => Loss: 6.72558818585459405881\n",
      "Iteration 14998 => Loss: 6.72558449231375732325\n",
      "Iteration 14999 => Loss: 6.72558079927062646419\n",
      "Iteration 15000 => Loss: 6.72557710672513042738\n",
      "Iteration 15001 => Loss: 6.72557341467721148121\n",
      "Iteration 15002 => Loss: 6.72556972312678968962\n",
      "Iteration 15003 => Loss: 6.72556603207380820919\n",
      "Iteration 15004 => Loss: 6.72556234151819509748\n",
      "Iteration 15005 => Loss: 6.72555865145988907017\n",
      "Iteration 15006 => Loss: 6.72555496189881640845\n",
      "Iteration 15007 => Loss: 6.72555127283490961076\n",
      "Iteration 15008 => Loss: 6.72554758426810384009\n",
      "Iteration 15009 => Loss: 6.72554389619833337122\n",
      "Iteration 15010 => Loss: 6.72554020862553159077\n",
      "Iteration 15011 => Loss: 6.72553652154962922083\n",
      "Iteration 15012 => Loss: 6.72553283497056408891\n",
      "Iteration 15013 => Loss: 6.72552914888826691708\n",
      "Iteration 15014 => Loss: 6.72552546330266309837\n",
      "Iteration 15015 => Loss: 6.72552177821369578936\n",
      "Iteration 15016 => Loss: 6.72551809362129482395\n",
      "Iteration 15017 => Loss: 6.72551440952539625329\n",
      "Iteration 15018 => Loss: 6.72551072592592902311\n",
      "Iteration 15019 => Loss: 6.72550704282281763824\n",
      "Iteration 15020 => Loss: 6.72550336021601324887\n",
      "Iteration 15021 => Loss: 6.72549967810544035984\n",
      "Iteration 15022 => Loss: 6.72549599649102791687\n",
      "Iteration 15023 => Loss: 6.72549231537271285930\n",
      "Iteration 15024 => Loss: 6.72548863475043390281\n",
      "Iteration 15025 => Loss: 6.72548495462411555224\n",
      "Iteration 15026 => Loss: 6.72548127499369563509\n",
      "Iteration 15027 => Loss: 6.72547759585910842617\n",
      "Iteration 15028 => Loss: 6.72547391722028109484\n",
      "Iteration 15029 => Loss: 6.72547023907714791591\n",
      "Iteration 15030 => Loss: 6.72546656142965382230\n",
      "Iteration 15031 => Loss: 6.72546288427772065432\n",
      "Iteration 15032 => Loss: 6.72545920762128268677\n",
      "Iteration 15033 => Loss: 6.72545553146027241809\n",
      "Iteration 15034 => Loss: 6.72545185579462501124\n",
      "Iteration 15035 => Loss: 6.72544818062427918193\n",
      "Iteration 15036 => Loss: 6.72544450594915677044\n",
      "Iteration 15037 => Loss: 6.72544083176920004519\n",
      "Iteration 15038 => Loss: 6.72543715808433439918\n",
      "Iteration 15039 => Loss: 6.72543348489449943628\n",
      "Iteration 15040 => Loss: 6.72542981219963476036\n",
      "Iteration 15041 => Loss: 6.72542613999965777083\n",
      "Iteration 15042 => Loss: 6.72542246829451251244\n",
      "Iteration 15043 => Loss: 6.72541879708413414818\n",
      "Iteration 15044 => Loss: 6.72541512636844895923\n",
      "Iteration 15045 => Loss: 6.72541145614738766767\n",
      "Iteration 15046 => Loss: 6.72540778642089343009\n",
      "Iteration 15047 => Loss: 6.72540411718889696857\n",
      "Iteration 15048 => Loss: 6.72540044845132634066\n",
      "Iteration 15049 => Loss: 6.72539678020811937387\n",
      "Iteration 15050 => Loss: 6.72539311245921034299\n",
      "Iteration 15051 => Loss: 6.72538944520453174647\n",
      "Iteration 15052 => Loss: 6.72538577844400631278\n",
      "Iteration 15053 => Loss: 6.72538211217759585026\n",
      "Iteration 15054 => Loss: 6.72537844640519821837\n",
      "Iteration 15055 => Loss: 6.72537478112677167275\n",
      "Iteration 15056 => Loss: 6.72537111634223716550\n",
      "Iteration 15057 => Loss: 6.72536745205154051774\n",
      "Iteration 15058 => Loss: 6.72536378825459202346\n",
      "Iteration 15059 => Loss: 6.72536012495135171463\n",
      "Iteration 15060 => Loss: 6.72535646214174587243\n",
      "Iteration 15061 => Loss: 6.72535279982569544899\n",
      "Iteration 15062 => Loss: 6.72534913800314271271\n",
      "Iteration 15063 => Loss: 6.72534547667402193838\n",
      "Iteration 15064 => Loss: 6.72534181583827272988\n",
      "Iteration 15065 => Loss: 6.72533815549581159843\n",
      "Iteration 15066 => Loss: 6.72533449564658791786\n",
      "Iteration 15067 => Loss: 6.72533083629052619301\n",
      "Iteration 15068 => Loss: 6.72532717742756425139\n",
      "Iteration 15069 => Loss: 6.72532351905763370326\n",
      "Iteration 15070 => Loss: 6.72531986118066882341\n",
      "Iteration 15071 => Loss: 6.72531620379660299847\n",
      "Iteration 15072 => Loss: 6.72531254690537316776\n",
      "Iteration 15073 => Loss: 6.72530889050690650066\n",
      "Iteration 15074 => Loss: 6.72530523460113727197\n",
      "Iteration 15075 => Loss: 6.72530157918800863825\n",
      "Iteration 15076 => Loss: 6.72529792426743711076\n",
      "Iteration 15077 => Loss: 6.72529426983937028695\n",
      "Iteration 15078 => Loss: 6.72529061590374421797\n",
      "Iteration 15079 => Loss: 6.72528696246048340868\n",
      "Iteration 15080 => Loss: 6.72528330950952302203\n",
      "Iteration 15081 => Loss: 6.72527965705080088554\n",
      "Iteration 15082 => Loss: 6.72527600508424061587\n",
      "Iteration 15083 => Loss: 6.72527235360978714596\n",
      "Iteration 15084 => Loss: 6.72526870262737208606\n",
      "Iteration 15085 => Loss: 6.72526505213692527008\n",
      "Iteration 15086 => Loss: 6.72526140213838452553\n",
      "Iteration 15087 => Loss: 6.72525775263168146267\n",
      "Iteration 15088 => Loss: 6.72525410361674502724\n",
      "Iteration 15089 => Loss: 6.72525045509351748763\n",
      "Iteration 15090 => Loss: 6.72524680706193045410\n",
      "Iteration 15091 => Loss: 6.72524315952191020784\n",
      "Iteration 15092 => Loss: 6.72523951247340434634\n",
      "Iteration 15093 => Loss: 6.72523586591633026899\n",
      "Iteration 15094 => Loss: 6.72523221985063113237\n",
      "Iteration 15095 => Loss: 6.72522857427624209947\n",
      "Iteration 15096 => Loss: 6.72522492919309122783\n",
      "Iteration 15097 => Loss: 6.72522128460112167403\n",
      "Iteration 15098 => Loss: 6.72521764050025794290\n",
      "Iteration 15099 => Loss: 6.72521399689043519743\n",
      "Iteration 15100 => Loss: 6.72521035377159570601\n",
      "Iteration 15101 => Loss: 6.72520671114365597987\n",
      "Iteration 15102 => Loss: 6.72520306900657072191\n",
      "Iteration 15103 => Loss: 6.72519942736026443697\n",
      "Iteration 15104 => Loss: 6.72519578620466251806\n",
      "Iteration 15105 => Loss: 6.72519214553970900994\n",
      "Iteration 15106 => Loss: 6.72518850536533641105\n",
      "Iteration 15107 => Loss: 6.72518486568147544347\n",
      "Iteration 15108 => Loss: 6.72518122648806571107\n",
      "Iteration 15109 => Loss: 6.72517758778503527139\n",
      "Iteration 15110 => Loss: 6.72517394957232195196\n",
      "Iteration 15111 => Loss: 6.72517031184985736303\n",
      "Iteration 15112 => Loss: 6.72516667461757489122\n",
      "Iteration 15113 => Loss: 6.72516303787540525860\n",
      "Iteration 15114 => Loss: 6.72515940162329872720\n",
      "Iteration 15115 => Loss: 6.72515576586117003188\n",
      "Iteration 15116 => Loss: 6.72515213058895877651\n",
      "Iteration 15117 => Loss: 6.72514849580660722950\n",
      "Iteration 15118 => Loss: 6.72514486151403634295\n",
      "Iteration 15119 => Loss: 6.72514122771119282618\n",
      "Iteration 15120 => Loss: 6.72513759439800473672\n",
      "Iteration 15121 => Loss: 6.72513396157440013212\n",
      "Iteration 15122 => Loss: 6.72513032924031861626\n",
      "Iteration 15123 => Loss: 6.72512669739569535210\n",
      "Iteration 15124 => Loss: 6.72512306604046461445\n",
      "Iteration 15125 => Loss: 6.72511943517456423081\n",
      "Iteration 15126 => Loss: 6.72511580479791959419\n",
      "Iteration 15127 => Loss: 6.72511217491046675576\n",
      "Iteration 15128 => Loss: 6.72510854551214265484\n",
      "Iteration 15129 => Loss: 6.72510491660287712534\n",
      "Iteration 15130 => Loss: 6.72510128818261243566\n",
      "Iteration 15131 => Loss: 6.72509766025127309064\n",
      "Iteration 15132 => Loss: 6.72509403280880224685\n",
      "Iteration 15133 => Loss: 6.72509040585513062638\n",
      "Iteration 15134 => Loss: 6.72508677939018628678\n",
      "Iteration 15135 => Loss: 6.72508315341391327280\n",
      "Iteration 15136 => Loss: 6.72507952792623697746\n",
      "Iteration 15137 => Loss: 6.72507590292709611646\n",
      "Iteration 15138 => Loss: 6.72507227841642318822\n",
      "Iteration 15139 => Loss: 6.72506865439415513208\n",
      "Iteration 15140 => Loss: 6.72506503086022000559\n",
      "Iteration 15141 => Loss: 6.72506140781456362987\n",
      "Iteration 15142 => Loss: 6.72505778525710873339\n",
      "Iteration 15143 => Loss: 6.72505416318779403184\n",
      "Iteration 15144 => Loss: 6.72505054160655468820\n",
      "Iteration 15145 => Loss: 6.72504692051332408909\n",
      "Iteration 15146 => Loss: 6.72504329990803295658\n",
      "Iteration 15147 => Loss: 6.72503967979061556548\n",
      "Iteration 15148 => Loss: 6.72503606016101862508\n",
      "Iteration 15149 => Loss: 6.72503244101916219932\n",
      "Iteration 15150 => Loss: 6.72502882236498855661\n",
      "Iteration 15151 => Loss: 6.72502520419842664268\n",
      "Iteration 15152 => Loss: 6.72502158651941162049\n",
      "Iteration 15153 => Loss: 6.72501796932787776484\n",
      "Iteration 15154 => Loss: 6.72501435262376379143\n",
      "Iteration 15155 => Loss: 6.72501073640700131051\n",
      "Iteration 15156 => Loss: 6.72500712067752370871\n",
      "Iteration 15157 => Loss: 6.72500350543526526081\n",
      "Iteration 15158 => Loss: 6.72499989068016290616\n",
      "Iteration 15159 => Loss: 6.72499627641214559048\n",
      "Iteration 15160 => Loss: 6.72499266263115114128\n",
      "Iteration 15161 => Loss: 6.72498904933711649790\n",
      "Iteration 15162 => Loss: 6.72498543652997593512\n",
      "Iteration 15163 => Loss: 6.72498182420965751049\n",
      "Iteration 15164 => Loss: 6.72497821237609727518\n",
      "Iteration 15165 => Loss: 6.72497460102923749758\n",
      "Iteration 15166 => Loss: 6.72497099016900090618\n",
      "Iteration 15167 => Loss: 6.72496737979533332208\n",
      "Iteration 15168 => Loss: 6.72496376990816369101\n",
      "Iteration 15169 => Loss: 6.72496016050742451142\n",
      "Iteration 15170 => Loss: 6.72495655159305272264\n",
      "Iteration 15171 => Loss: 6.72495294316498526399\n",
      "Iteration 15172 => Loss: 6.72494933522315463392\n",
      "Iteration 15173 => Loss: 6.72494572776748267273\n",
      "Iteration 15174 => Loss: 6.72494212079792763603\n",
      "Iteration 15175 => Loss: 6.72493851431440425870\n",
      "Iteration 15176 => Loss: 6.72493490831686546727\n",
      "Iteration 15177 => Loss: 6.72493130280522333209\n",
      "Iteration 15178 => Loss: 6.72492769777943166787\n",
      "Iteration 15179 => Loss: 6.72492409323941231492\n",
      "Iteration 15180 => Loss: 6.72492048918510842981\n",
      "Iteration 15181 => Loss: 6.72491688561645251099\n",
      "Iteration 15182 => Loss: 6.72491328253338060961\n",
      "Iteration 15183 => Loss: 6.72490967993581545414\n",
      "Iteration 15184 => Loss: 6.72490607782370819479\n",
      "Iteration 15185 => Loss: 6.72490247619697889547\n",
      "Iteration 15186 => Loss: 6.72489887505557515368\n",
      "Iteration 15187 => Loss: 6.72489527439941792153\n",
      "Iteration 15188 => Loss: 6.72489167422845479649\n",
      "Iteration 15189 => Loss: 6.72488807454261827701\n",
      "Iteration 15190 => Loss: 6.72488447534183464427\n",
      "Iteration 15191 => Loss: 6.72488087662604083761\n",
      "Iteration 15192 => Loss: 6.72487727839518090178\n",
      "Iteration 15193 => Loss: 6.72487368064918733523\n",
      "Iteration 15194 => Loss: 6.72487008338797487284\n",
      "Iteration 15195 => Loss: 6.72486648661150354656\n",
      "Iteration 15196 => Loss: 6.72486289031969697305\n",
      "Iteration 15197 => Loss: 6.72485929451248942712\n",
      "Iteration 15198 => Loss: 6.72485569918982140081\n",
      "Iteration 15199 => Loss: 6.72485210435162095166\n",
      "Iteration 15200 => Loss: 6.72484850999782768355\n",
      "Iteration 15201 => Loss: 6.72484491612837054220\n",
      "Iteration 15202 => Loss: 6.72484132274318646694\n",
      "Iteration 15203 => Loss: 6.72483772984221772617\n",
      "Iteration 15204 => Loss: 6.72483413742537905478\n",
      "Iteration 15205 => Loss: 6.72483054549262782018\n",
      "Iteration 15206 => Loss: 6.72482695404389119176\n",
      "Iteration 15207 => Loss: 6.72482336307910166795\n",
      "Iteration 15208 => Loss: 6.72481977259820062898\n",
      "Iteration 15209 => Loss: 6.72481618260110192153\n",
      "Iteration 15210 => Loss: 6.72481259308776646577\n",
      "Iteration 15211 => Loss: 6.72480900405811699017\n",
      "Iteration 15212 => Loss: 6.72480541551208421680\n",
      "Iteration 15213 => Loss: 6.72480182744961663133\n",
      "Iteration 15214 => Loss: 6.72479823987063429769\n",
      "Iteration 15215 => Loss: 6.72479465277507770793\n",
      "Iteration 15216 => Loss: 6.72479106616289090681\n",
      "Iteration 15217 => Loss: 6.72478748003399662281\n",
      "Iteration 15218 => Loss: 6.72478389438833179526\n",
      "Iteration 15219 => Loss: 6.72478030922583336348\n",
      "Iteration 15220 => Loss: 6.72477672454643826683\n",
      "Iteration 15221 => Loss: 6.72477314035007456283\n",
      "Iteration 15222 => Loss: 6.72476955663668363172\n",
      "Iteration 15223 => Loss: 6.72476597340619530740\n",
      "Iteration 15224 => Loss: 6.72476239065855185828\n",
      "Iteration 15225 => Loss: 6.72475880839368578279\n",
      "Iteration 15226 => Loss: 6.72475522661152602666\n",
      "Iteration 15227 => Loss: 6.72475164531201219376\n",
      "Iteration 15228 => Loss: 6.72474806449507944706\n",
      "Iteration 15229 => Loss: 6.72474448416066117318\n",
      "Iteration 15230 => Loss: 6.72474090430869342327\n",
      "Iteration 15231 => Loss: 6.72473732493912379482\n",
      "Iteration 15232 => Loss: 6.72473374605185370001\n",
      "Iteration 15233 => Loss: 6.72473016764684938806\n",
      "Iteration 15234 => Loss: 6.72472658972402825839\n",
      "Iteration 15235 => Loss: 6.72472301228334323753\n",
      "Iteration 15236 => Loss: 6.72471943532471794214\n",
      "Iteration 15237 => Loss: 6.72471585884807954159\n",
      "Iteration 15238 => Loss: 6.72471228285336941610\n",
      "Iteration 15239 => Loss: 6.72470870734053871587\n",
      "Iteration 15240 => Loss: 6.72470513230949773487\n",
      "Iteration 15241 => Loss: 6.72470155776019495875\n",
      "Iteration 15242 => Loss: 6.72469798369256466231\n",
      "Iteration 15243 => Loss: 6.72469441010653845581\n",
      "Iteration 15244 => Loss: 6.72469083700205416676\n",
      "Iteration 15245 => Loss: 6.72468726437904251725\n",
      "Iteration 15246 => Loss: 6.72468369223745110475\n",
      "Iteration 15247 => Loss: 6.72468012057720176955\n",
      "Iteration 15248 => Loss: 6.72467654939822612192\n",
      "Iteration 15249 => Loss: 6.72467297870047442387\n",
      "Iteration 15250 => Loss: 6.72466940848387917384\n",
      "Iteration 15251 => Loss: 6.72466583874836221213\n",
      "Iteration 15252 => Loss: 6.72466226949387024803\n",
      "Iteration 15253 => Loss: 6.72465870072033489180\n",
      "Iteration 15254 => Loss: 6.72465513242769574731\n",
      "Iteration 15255 => Loss: 6.72465156461587909575\n",
      "Iteration 15256 => Loss: 6.72464799728482898189\n",
      "Iteration 15257 => Loss: 6.72464443043447790416\n",
      "Iteration 15258 => Loss: 6.72464086406475836100\n",
      "Iteration 15259 => Loss: 6.72463729817560551538\n",
      "Iteration 15260 => Loss: 6.72463373276695630665\n",
      "Iteration 15261 => Loss: 6.72463016783875033866\n",
      "Iteration 15262 => Loss: 6.72462660339091922168\n",
      "Iteration 15263 => Loss: 6.72462303942339367779\n",
      "Iteration 15264 => Loss: 6.72461947593611331087\n",
      "Iteration 15265 => Loss: 6.72461591292901150752\n",
      "Iteration 15266 => Loss: 6.72461235040203586522\n",
      "Iteration 15267 => Loss: 6.72460878835509578977\n",
      "Iteration 15268 => Loss: 6.72460522678815486586\n",
      "Iteration 15269 => Loss: 6.72460166570112605200\n",
      "Iteration 15270 => Loss: 6.72459810509396049838\n",
      "Iteration 15271 => Loss: 6.72459454496657915712\n",
      "Iteration 15272 => Loss: 6.72459098531893495476\n",
      "Iteration 15273 => Loss: 6.72458742615094795525\n",
      "Iteration 15274 => Loss: 6.72458386746256042699\n",
      "Iteration 15275 => Loss: 6.72458030925370753295\n",
      "Iteration 15276 => Loss: 6.72457675152431999521\n",
      "Iteration 15277 => Loss: 6.72457319427434008219\n",
      "Iteration 15278 => Loss: 6.72456963750369940414\n",
      "Iteration 15279 => Loss: 6.72456608121233134767\n",
      "Iteration 15280 => Loss: 6.72456252540017462849\n",
      "Iteration 15281 => Loss: 6.72455897006716707409\n",
      "Iteration 15282 => Loss: 6.72455541521323407750\n",
      "Iteration 15283 => Loss: 6.72455186083832145982\n",
      "Iteration 15284 => Loss: 6.72454830694236438404\n",
      "Iteration 15285 => Loss: 6.72454475352529712495\n",
      "Iteration 15286 => Loss: 6.72454120058704329921\n",
      "Iteration 15287 => Loss: 6.72453764812755583336\n",
      "Iteration 15288 => Loss: 6.72453409614675745587\n",
      "Iteration 15289 => Loss: 6.72453054464459576423\n",
      "Iteration 15290 => Loss: 6.72452699362099171054\n",
      "Iteration 15291 => Loss: 6.72452344307589733319\n",
      "Iteration 15292 => Loss: 6.72451989300922914339\n",
      "Iteration 15293 => Loss: 6.72451634342093651497\n",
      "Iteration 15294 => Loss: 6.72451279431095105821\n",
      "Iteration 15295 => Loss: 6.72450924567921060060\n",
      "Iteration 15296 => Loss: 6.72450569752564586423\n",
      "Iteration 15297 => Loss: 6.72450214985019556480\n",
      "Iteration 15298 => Loss: 6.72449860265279930616\n",
      "Iteration 15299 => Loss: 6.72449505593338869858\n",
      "Iteration 15300 => Loss: 6.72449150969188913507\n",
      "Iteration 15301 => Loss: 6.72448796392825354218\n",
      "Iteration 15302 => Loss: 6.72448441864240376020\n",
      "Iteration 15303 => Loss: 6.72448087383428916297\n",
      "Iteration 15304 => Loss: 6.72447732950383514350\n",
      "Iteration 15305 => Loss: 6.72447378565097597658\n",
      "Iteration 15306 => Loss: 6.72447024227566014787\n",
      "Iteration 15307 => Loss: 6.72446669937780594495\n",
      "Iteration 15308 => Loss: 6.72446315695735830076\n",
      "Iteration 15309 => Loss: 6.72445961501425237827\n",
      "Iteration 15310 => Loss: 6.72445607354842689318\n",
      "Iteration 15311 => Loss: 6.72445253255981256757\n",
      "Iteration 15312 => Loss: 6.72444899204835166984\n",
      "Iteration 15313 => Loss: 6.72444545201396337575\n",
      "Iteration 15314 => Loss: 6.72444191245660238820\n",
      "Iteration 15315 => Loss: 6.72443837337619232386\n",
      "Iteration 15316 => Loss: 6.72443483477268344473\n",
      "Iteration 15317 => Loss: 6.72443129664598870932\n",
      "Iteration 15318 => Loss: 6.72442775899606992596\n",
      "Iteration 15319 => Loss: 6.72442422182284005316\n",
      "Iteration 15320 => Loss: 6.72442068512625024113\n",
      "Iteration 15321 => Loss: 6.72441714890623565282\n",
      "Iteration 15322 => Loss: 6.72441361316271102311\n",
      "Iteration 15323 => Loss: 6.72441007789563549579\n",
      "Iteration 15324 => Loss: 6.72440654310494512202\n",
      "Iteration 15325 => Loss: 6.72440300879055996575\n",
      "Iteration 15326 => Loss: 6.72439947495242229536\n",
      "Iteration 15327 => Loss: 6.72439594159047437927\n",
      "Iteration 15328 => Loss: 6.72439240870464605138\n",
      "Iteration 15329 => Loss: 6.72438887629486981012\n",
      "Iteration 15330 => Loss: 6.72438534436109502934\n",
      "Iteration 15331 => Loss: 6.72438181290324088479\n",
      "Iteration 15332 => Loss: 6.72437828192125941484\n",
      "Iteration 15333 => Loss: 6.72437475141506713072\n",
      "Iteration 15334 => Loss: 6.72437122138462139986\n",
      "Iteration 15335 => Loss: 6.72436769182984228621\n",
      "Iteration 15336 => Loss: 6.72436416275066406456\n",
      "Iteration 15337 => Loss: 6.72436063414704054964\n",
      "Iteration 15338 => Loss: 6.72435710601889713445\n",
      "Iteration 15339 => Loss: 6.72435357836616365290\n",
      "Iteration 15340 => Loss: 6.72435005118878859065\n",
      "Iteration 15341 => Loss: 6.72434652448668845892\n",
      "Iteration 15342 => Loss: 6.72434299825981707244\n",
      "Iteration 15343 => Loss: 6.72433947250810959417\n",
      "Iteration 15344 => Loss: 6.72433594723149230532\n",
      "Iteration 15345 => Loss: 6.72433242242991013882\n",
      "Iteration 15346 => Loss: 6.72432889810329204039\n",
      "Iteration 15347 => Loss: 6.72432537425158116662\n",
      "Iteration 15348 => Loss: 6.72432185087470024598\n",
      "Iteration 15349 => Loss: 6.72431832797261019863\n",
      "Iteration 15350 => Loss: 6.72431480554522309490\n",
      "Iteration 15351 => Loss: 6.72431128359248386772\n",
      "Iteration 15352 => Loss: 6.72430776211432057465\n",
      "Iteration 15353 => Loss: 6.72430424111068525406\n",
      "Iteration 15354 => Loss: 6.72430072058150418712\n",
      "Iteration 15355 => Loss: 6.72429720052671697772\n",
      "Iteration 15356 => Loss: 6.72429368094624901886\n",
      "Iteration 15357 => Loss: 6.72429016184004790802\n",
      "Iteration 15358 => Loss: 6.72428664320805591359\n",
      "Iteration 15359 => Loss: 6.72428312505019398770\n",
      "Iteration 15360 => Loss: 6.72427960736639995787\n",
      "Iteration 15361 => Loss: 6.72427609015661875702\n",
      "Iteration 15362 => Loss: 6.72427257342078021907\n",
      "Iteration 15363 => Loss: 6.72426905715881950698\n",
      "Iteration 15364 => Loss: 6.72426554137067977734\n",
      "Iteration 15365 => Loss: 6.72426202605629175224\n",
      "Iteration 15366 => Loss: 6.72425851121558704193\n",
      "Iteration 15367 => Loss: 6.72425499684851768478\n",
      "Iteration 15368 => Loss: 6.72425148295500019202\n",
      "Iteration 15369 => Loss: 6.72424796953498660201\n",
      "Iteration 15370 => Loss: 6.72424445658839697870\n",
      "Iteration 15371 => Loss: 6.72424094411518957770\n",
      "Iteration 15372 => Loss: 6.72423743211528357477\n",
      "Iteration 15373 => Loss: 6.72423392058862123832\n",
      "Iteration 15374 => Loss: 6.72423040953513329043\n",
      "Iteration 15375 => Loss: 6.72422689895476377586\n",
      "Iteration 15376 => Loss: 6.72422338884744608123\n",
      "Iteration 15377 => Loss: 6.72421987921311092862\n",
      "Iteration 15378 => Loss: 6.72421637005169792189\n",
      "Iteration 15379 => Loss: 6.72421286136315465853\n",
      "Iteration 15380 => Loss: 6.72420935314739853794\n",
      "Iteration 15381 => Loss: 6.72420584540437360488\n",
      "Iteration 15382 => Loss: 6.72420233813402212775\n",
      "Iteration 15383 => Loss: 6.72419883133627749316\n",
      "Iteration 15384 => Loss: 6.72419532501107219957\n",
      "Iteration 15385 => Loss: 6.72419181915834496266\n",
      "Iteration 15386 => Loss: 6.72418831377802561633\n",
      "Iteration 15387 => Loss: 6.72418480887005998170\n",
      "Iteration 15388 => Loss: 6.72418130443438677446\n",
      "Iteration 15389 => Loss: 6.72417780047093049944\n",
      "Iteration 15390 => Loss: 6.72417429697963253687\n",
      "Iteration 15391 => Loss: 6.72417079396043693151\n",
      "Iteration 15392 => Loss: 6.72416729141326818819\n",
      "Iteration 15393 => Loss: 6.72416378933806591078\n",
      "Iteration 15394 => Loss: 6.72416028773477147951\n",
      "Iteration 15395 => Loss: 6.72415678660332183370\n",
      "Iteration 15396 => Loss: 6.72415328594364591908\n",
      "Iteration 15397 => Loss: 6.72414978575568156316\n",
      "Iteration 15398 => Loss: 6.72414628603937103435\n",
      "Iteration 15399 => Loss: 6.72414278679464683108\n",
      "Iteration 15400 => Loss: 6.72413928802144766905\n",
      "Iteration 15401 => Loss: 6.72413578971970871123\n",
      "Iteration 15402 => Loss: 6.72413229188936245606\n",
      "Iteration 15403 => Loss: 6.72412879453035294830\n",
      "Iteration 15404 => Loss: 6.72412529764261357457\n",
      "Iteration 15405 => Loss: 6.72412180122607860966\n",
      "Iteration 15406 => Loss: 6.72411830528068765744\n",
      "Iteration 15407 => Loss: 6.72411480980637055183\n",
      "Iteration 15408 => Loss: 6.72411131480307311392\n",
      "Iteration 15409 => Loss: 6.72410782027072517764\n",
      "Iteration 15410 => Loss: 6.72410432620926634684\n",
      "Iteration 15411 => Loss: 6.72410083261863533721\n",
      "Iteration 15412 => Loss: 6.72409733949876464720\n",
      "Iteration 15413 => Loss: 6.72409384684959032796\n",
      "Iteration 15414 => Loss: 6.72409035467104931882\n",
      "Iteration 15415 => Loss: 6.72408686296308300001\n",
      "Iteration 15416 => Loss: 6.72408337172562564632\n",
      "Iteration 15417 => Loss: 6.72407988095860975619\n",
      "Iteration 15418 => Loss: 6.72407639066197670985\n",
      "Iteration 15419 => Loss: 6.72407290083566167027\n",
      "Iteration 15420 => Loss: 6.72406941147959802407\n",
      "Iteration 15421 => Loss: 6.72406592259372537512\n",
      "Iteration 15422 => Loss: 6.72406243417798510364\n",
      "Iteration 15423 => Loss: 6.72405894623230437901\n",
      "Iteration 15424 => Loss: 6.72405545875662458144\n",
      "Iteration 15425 => Loss: 6.72405197175088797934\n",
      "Iteration 15426 => Loss: 6.72404848521502085390\n",
      "Iteration 15427 => Loss: 6.72404499914896724988\n",
      "Iteration 15428 => Loss: 6.72404151355266055390\n",
      "Iteration 15429 => Loss: 6.72403802842603148804\n",
      "Iteration 15430 => Loss: 6.72403454376903475520\n",
      "Iteration 15431 => Loss: 6.72403105958159219568\n",
      "Iteration 15432 => Loss: 6.72402757586364607789\n",
      "Iteration 15433 => Loss: 6.72402409261512357119\n",
      "Iteration 15434 => Loss: 6.72402060983597404942\n",
      "Iteration 15435 => Loss: 6.72401712752612290558\n",
      "Iteration 15436 => Loss: 6.72401364568552484258\n",
      "Iteration 15437 => Loss: 6.72401016431410436525\n",
      "Iteration 15438 => Loss: 6.72400668341178953114\n",
      "Iteration 15439 => Loss: 6.72400320297852793772\n",
      "Iteration 15440 => Loss: 6.72399972301425830068\n",
      "Iteration 15441 => Loss: 6.72399624351891933571\n",
      "Iteration 15442 => Loss: 6.72399276449243465947\n",
      "Iteration 15443 => Loss: 6.72398928593475009308\n",
      "Iteration 15444 => Loss: 6.72398580784580790493\n",
      "Iteration 15445 => Loss: 6.72398233022552549443\n",
      "Iteration 15446 => Loss: 6.72397885307386733444\n",
      "Iteration 15447 => Loss: 6.72397537639074283078\n",
      "Iteration 15448 => Loss: 6.72397190017611201540\n",
      "Iteration 15449 => Loss: 6.72396842442989672861\n",
      "Iteration 15450 => Loss: 6.72396494915204367970\n",
      "Iteration 15451 => Loss: 6.72396147434247470898\n",
      "Iteration 15452 => Loss: 6.72395800000114274297\n",
      "Iteration 15453 => Loss: 6.72395452612797850378\n",
      "Iteration 15454 => Loss: 6.72395105272291448983\n",
      "Iteration 15455 => Loss: 6.72394757978590185132\n",
      "Iteration 15456 => Loss: 6.72394410731685798766\n",
      "Iteration 15457 => Loss: 6.72394063531573138448\n",
      "Iteration 15458 => Loss: 6.72393716378246342202\n",
      "Iteration 15459 => Loss: 6.72393369271697949330\n",
      "Iteration 15460 => Loss: 6.72393022211922097853\n",
      "Iteration 15461 => Loss: 6.72392675198913103429\n",
      "Iteration 15462 => Loss: 6.72392328232663505361\n",
      "Iteration 15463 => Loss: 6.72391981313167708123\n",
      "Iteration 15464 => Loss: 6.72391634440419849739\n",
      "Iteration 15465 => Loss: 6.72391287614412291873\n",
      "Iteration 15466 => Loss: 6.72390940835140327181\n",
      "Iteration 15467 => Loss: 6.72390594102597116688\n",
      "Iteration 15468 => Loss: 6.72390247416774933242\n",
      "Iteration 15469 => Loss: 6.72389900777669868859\n",
      "Iteration 15470 => Loss: 6.72389554185274285203\n",
      "Iteration 15471 => Loss: 6.72389207639581343301\n",
      "Iteration 15472 => Loss: 6.72388861140586779896\n",
      "Iteration 15473 => Loss: 6.72388514688281357934\n",
      "Iteration 15474 => Loss: 6.72388168282661347064\n",
      "Iteration 15475 => Loss: 6.72387821923719375405\n",
      "Iteration 15476 => Loss: 6.72387475611449403345\n",
      "Iteration 15477 => Loss: 6.72387129345844947181\n",
      "Iteration 15478 => Loss: 6.72386783126899967300\n",
      "Iteration 15479 => Loss: 6.72386436954607802363\n",
      "Iteration 15480 => Loss: 6.72386090828961879851\n",
      "Iteration 15481 => Loss: 6.72385744749957137145\n",
      "Iteration 15482 => Loss: 6.72385398717586824091\n",
      "Iteration 15483 => Loss: 6.72385052731843924079\n",
      "Iteration 15484 => Loss: 6.72384706792722663948\n",
      "Iteration 15485 => Loss: 6.72384360900216293544\n",
      "Iteration 15486 => Loss: 6.72384015054319661431\n",
      "Iteration 15487 => Loss: 6.72383669255025218092\n",
      "Iteration 15488 => Loss: 6.72383323502327545640\n",
      "Iteration 15489 => Loss: 6.72382977796219893918\n",
      "Iteration 15490 => Loss: 6.72382632136696578584\n",
      "Iteration 15491 => Loss: 6.72382286523750938301\n",
      "Iteration 15492 => Loss: 6.72381940957376134094\n",
      "Iteration 15493 => Loss: 6.72381595437566570439\n",
      "Iteration 15494 => Loss: 6.72381249964316296541\n",
      "Iteration 15495 => Loss: 6.72380904537618118155\n",
      "Iteration 15496 => Loss: 6.72380559157466262121\n",
      "Iteration 15497 => Loss: 6.72380213823854244737\n",
      "Iteration 15498 => Loss: 6.72379868536776559296\n",
      "Iteration 15499 => Loss: 6.72379523296226100371\n",
      "Iteration 15500 => Loss: 6.72379178102196828348\n",
      "Iteration 15501 => Loss: 6.72378832954681815437\n",
      "Iteration 15502 => Loss: 6.72378487853676176655\n",
      "Iteration 15503 => Loss: 6.72378142799172895394\n",
      "Iteration 15504 => Loss: 6.72377797791165487951\n",
      "Iteration 15505 => Loss: 6.72377452829647825894\n",
      "Iteration 15506 => Loss: 6.72377107914613780792\n",
      "Iteration 15507 => Loss: 6.72376763046057757123\n",
      "Iteration 15508 => Loss: 6.72376418223972027732\n",
      "Iteration 15509 => Loss: 6.72376073448351441186\n",
      "Iteration 15510 => Loss: 6.72375728719189158511\n",
      "Iteration 15511 => Loss: 6.72375384036479406547\n",
      "Iteration 15512 => Loss: 6.72375039400215701590\n",
      "Iteration 15513 => Loss: 6.72374694810391826394\n",
      "Iteration 15514 => Loss: 6.72374350267001386072\n",
      "Iteration 15515 => Loss: 6.72374005770037808105\n",
      "Iteration 15516 => Loss: 6.72373661319495408151\n",
      "Iteration 15517 => Loss: 6.72373316915367880142\n",
      "Iteration 15518 => Loss: 6.72372972557649006831\n",
      "Iteration 15519 => Loss: 6.72372628246332304514\n",
      "Iteration 15520 => Loss: 6.72372283981411200671\n",
      "Iteration 15521 => Loss: 6.72371939762880188596\n",
      "Iteration 15522 => Loss: 6.72371595590732340497\n",
      "Iteration 15523 => Loss: 6.72371251464962060851\n",
      "Iteration 15524 => Loss: 6.72370907385562510683\n",
      "Iteration 15525 => Loss: 6.72370563352527828016\n",
      "Iteration 15526 => Loss: 6.72370219365850996240\n",
      "Iteration 15527 => Loss: 6.72369875425527041557\n",
      "Iteration 15528 => Loss: 6.72369531531549302628\n",
      "Iteration 15529 => Loss: 6.72369187683910851661\n",
      "Iteration 15530 => Loss: 6.72368843882606448403\n",
      "Iteration 15531 => Loss: 6.72368500127628809793\n",
      "Iteration 15532 => Loss: 6.72368156418971629762\n",
      "Iteration 15533 => Loss: 6.72367812756630112148\n",
      "Iteration 15534 => Loss: 6.72367469140596263344\n",
      "Iteration 15535 => Loss: 6.72367125570865820094\n",
      "Iteration 15536 => Loss: 6.72366782047430788793\n",
      "Iteration 15537 => Loss: 6.72366438570285929188\n",
      "Iteration 15538 => Loss: 6.72366095139424313487\n",
      "Iteration 15539 => Loss: 6.72365751754839724441\n",
      "Iteration 15540 => Loss: 6.72365408416526655344\n",
      "Iteration 15541 => Loss: 6.72365065124478800129\n",
      "Iteration 15542 => Loss: 6.72364721878688786916\n",
      "Iteration 15543 => Loss: 6.72364378679151641904\n",
      "Iteration 15544 => Loss: 6.72364035525860348486\n",
      "Iteration 15545 => Loss: 6.72363692418809133500\n",
      "Iteration 15546 => Loss: 6.72363349357992579058\n",
      "Iteration 15547 => Loss: 6.72363006343402069831\n",
      "Iteration 15548 => Loss: 6.72362663375033164925\n",
      "Iteration 15549 => Loss: 6.72362320452879735910\n",
      "Iteration 15550 => Loss: 6.72361977576935121448\n",
      "Iteration 15551 => Loss: 6.72361634747192393746\n",
      "Iteration 15552 => Loss: 6.72361291963646667824\n",
      "Iteration 15553 => Loss: 6.72360949226290482983\n",
      "Iteration 15554 => Loss: 6.72360606535118510152\n",
      "Iteration 15555 => Loss: 6.72360263890123999175\n",
      "Iteration 15556 => Loss: 6.72359921291300999258\n",
      "Iteration 15557 => Loss: 6.72359578738643293150\n",
      "Iteration 15558 => Loss: 6.72359236232144841239\n",
      "Iteration 15559 => Loss: 6.72358893771798715733\n",
      "Iteration 15560 => Loss: 6.72358551357599765197\n",
      "Iteration 15561 => Loss: 6.72358208989540528933\n",
      "Iteration 15562 => Loss: 6.72357866667615500234\n",
      "Iteration 15563 => Loss: 6.72357524391818728304\n",
      "Iteration 15564 => Loss: 6.72357182162143462989\n",
      "Iteration 15565 => Loss: 6.72356839978583220585\n",
      "Iteration 15566 => Loss: 6.72356497841132849658\n",
      "Iteration 15567 => Loss: 6.72356155749785333597\n",
      "Iteration 15568 => Loss: 6.72355813704534899244\n",
      "Iteration 15569 => Loss: 6.72355471705374707625\n",
      "Iteration 15570 => Loss: 6.72355129752299252033\n",
      "Iteration 15571 => Loss: 6.72354787845301515858\n",
      "Iteration 15572 => Loss: 6.72354445984376436485\n",
      "Iteration 15573 => Loss: 6.72354104169516020306\n",
      "Iteration 15574 => Loss: 6.72353762400716536973\n",
      "Iteration 15575 => Loss: 6.72353420677969282337\n",
      "Iteration 15576 => Loss: 6.72353079001270259596\n",
      "Iteration 15577 => Loss: 6.72352737370611919232\n",
      "Iteration 15578 => Loss: 6.72352395785987955179\n",
      "Iteration 15579 => Loss: 6.72352054247392594277\n",
      "Iteration 15580 => Loss: 6.72351712754819974549\n",
      "Iteration 15581 => Loss: 6.72351371308263079385\n",
      "Iteration 15582 => Loss: 6.72351029907715780354\n",
      "Iteration 15583 => Loss: 6.72350688553173103656\n",
      "Iteration 15584 => Loss: 6.72350347244627144505\n",
      "Iteration 15585 => Loss: 6.72350005982073106736\n",
      "Iteration 15586 => Loss: 6.72349664765504240194\n",
      "Iteration 15587 => Loss: 6.72349323594913972357\n",
      "Iteration 15588 => Loss: 6.72348982470296618885\n",
      "Iteration 15589 => Loss: 6.72348641391645696075\n",
      "Iteration 15590 => Loss: 6.72348300358955874856\n",
      "Iteration 15591 => Loss: 6.72347959372219428076\n",
      "Iteration 15592 => Loss: 6.72347618431431381936\n",
      "Iteration 15593 => Loss: 6.72347277536585163915\n",
      "Iteration 15594 => Loss: 6.72346936687674467947\n",
      "Iteration 15595 => Loss: 6.72346595884692899148\n",
      "Iteration 15596 => Loss: 6.72346255127634950810\n",
      "Iteration 15597 => Loss: 6.72345914416493606325\n",
      "Iteration 15598 => Loss: 6.72345573751263536622\n",
      "Iteration 15599 => Loss: 6.72345233131938169180\n",
      "Iteration 15600 => Loss: 6.72344892558510842662\n",
      "Iteration 15601 => Loss: 6.72344552030975872725\n",
      "Iteration 15602 => Loss: 6.72344211549327219757\n",
      "Iteration 15603 => Loss: 6.72343871113559199415\n",
      "Iteration 15604 => Loss: 6.72343530723663906912\n",
      "Iteration 15605 => Loss: 6.72343190379636101994\n",
      "Iteration 15606 => Loss: 6.72342850081470011503\n",
      "Iteration 15607 => Loss: 6.72342509829159151735\n",
      "Iteration 15608 => Loss: 6.72342169622697838349\n",
      "Iteration 15609 => Loss: 6.72341829462078965918\n",
      "Iteration 15610 => Loss: 6.72341489347296317192\n",
      "Iteration 15611 => Loss: 6.72341149278344563101\n",
      "Iteration 15612 => Loss: 6.72340809255217308760\n",
      "Iteration 15613 => Loss: 6.72340469277907626378\n",
      "Iteration 15614 => Loss: 6.72340129346410364519\n",
      "Iteration 15615 => Loss: 6.72339789460718240122\n",
      "Iteration 15616 => Loss: 6.72339449620826723475\n",
      "Iteration 15617 => Loss: 6.72339109826728353880\n",
      "Iteration 15618 => Loss: 6.72338770078416914089\n",
      "Iteration 15619 => Loss: 6.72338430375886897394\n",
      "Iteration 15620 => Loss: 6.72338090719131464823\n",
      "Iteration 15621 => Loss: 6.72337751108145287304\n",
      "Iteration 15622 => Loss: 6.72337411542921348229\n",
      "Iteration 15623 => Loss: 6.72337072023454229708\n",
      "Iteration 15624 => Loss: 6.72336732549737270404\n",
      "Iteration 15625 => Loss: 6.72336393121764430703\n",
      "Iteration 15626 => Loss: 6.72336053739529138085\n",
      "Iteration 15627 => Loss: 6.72335714403025797026\n",
      "Iteration 15628 => Loss: 6.72335375112247923823\n",
      "Iteration 15629 => Loss: 6.72335035867189745318\n",
      "Iteration 15630 => Loss: 6.72334696667845488349\n",
      "Iteration 15631 => Loss: 6.72334357514207692219\n",
      "Iteration 15632 => Loss: 6.72334018406270494950\n",
      "Iteration 15633 => Loss: 6.72333679344029100378\n",
      "Iteration 15634 => Loss: 6.72333340327475603715\n",
      "Iteration 15635 => Loss: 6.72333001356604942345\n",
      "Iteration 15636 => Loss: 6.72332662431410366111\n",
      "Iteration 15637 => Loss: 6.72332323551886279489\n",
      "Iteration 15638 => Loss: 6.72331984718026287595\n",
      "Iteration 15639 => Loss: 6.72331645929823729091\n",
      "Iteration 15640 => Loss: 6.72331307187273186088\n",
      "Iteration 15641 => Loss: 6.72330968490368174884\n",
      "Iteration 15642 => Loss: 6.72330629839102655865\n",
      "Iteration 15643 => Loss: 6.72330291233470589418\n",
      "Iteration 15644 => Loss: 6.72329952673465580659\n",
      "Iteration 15645 => Loss: 6.72329614159080968250\n",
      "Iteration 15646 => Loss: 6.72329275690311867208\n",
      "Iteration 15647 => Loss: 6.72328937267151527379\n",
      "Iteration 15648 => Loss: 6.72328598889593376242\n",
      "Iteration 15649 => Loss: 6.72328260557631995908\n",
      "Iteration 15650 => Loss: 6.72327922271260458587\n",
      "Iteration 15651 => Loss: 6.72327584030473524024\n",
      "Iteration 15652 => Loss: 6.72327245835263820339\n",
      "Iteration 15653 => Loss: 6.72326907685626018463\n",
      "Iteration 15654 => Loss: 6.72326569581555144595\n",
      "Iteration 15655 => Loss: 6.72326231523042672222\n",
      "Iteration 15656 => Loss: 6.72325893510083538729\n",
      "Iteration 15657 => Loss: 6.72325555542672059772\n",
      "Iteration 15658 => Loss: 6.72325217620802018104\n",
      "Iteration 15659 => Loss: 6.72324879744466397113\n",
      "Iteration 15660 => Loss: 6.72324541913660223003\n",
      "Iteration 15661 => Loss: 6.72324204128375946254\n",
      "Iteration 15662 => Loss: 6.72323866388608859523\n",
      "Iteration 15663 => Loss: 6.72323528694351857382\n",
      "Iteration 15664 => Loss: 6.72323191045599610760\n",
      "Iteration 15665 => Loss: 6.72322853442345813590\n",
      "Iteration 15666 => Loss: 6.72322515884583626899\n",
      "Iteration 15667 => Loss: 6.72322178372307277527\n",
      "Iteration 15668 => Loss: 6.72321840905510725861\n",
      "Iteration 15669 => Loss: 6.72321503484188198740\n",
      "Iteration 15670 => Loss: 6.72321166108333034828\n",
      "Iteration 15671 => Loss: 6.72320828777938839238\n",
      "Iteration 15672 => Loss: 6.72320491493000726990\n",
      "Iteration 15673 => Loss: 6.72320154253511148568\n",
      "Iteration 15674 => Loss: 6.72319817059464686082\n",
      "Iteration 15675 => Loss: 6.72319479910855477556\n",
      "Iteration 15676 => Loss: 6.72319142807677128104\n",
      "Iteration 15677 => Loss: 6.72318805749923065207\n",
      "Iteration 15678 => Loss: 6.72318468737588492701\n",
      "Iteration 15679 => Loss: 6.72318131770665150526\n",
      "Iteration 15680 => Loss: 6.72317794849148597791\n",
      "Iteration 15681 => Loss: 6.72317457973031817886\n",
      "Iteration 15682 => Loss: 6.72317121142309837012\n",
      "Iteration 15683 => Loss: 6.72316784356975905013\n",
      "Iteration 15684 => Loss: 6.72316447617023538186\n",
      "Iteration 15685 => Loss: 6.72316110922446430465\n",
      "Iteration 15686 => Loss: 6.72315774273239608050\n",
      "Iteration 15687 => Loss: 6.72315437669395432607\n",
      "Iteration 15688 => Loss: 6.72315101110909818516\n",
      "Iteration 15689 => Loss: 6.72314764597774772170\n",
      "Iteration 15690 => Loss: 6.72314428129985230953\n",
      "Iteration 15691 => Loss: 6.72314091707534355891\n",
      "Iteration 15692 => Loss: 6.72313755330416995548\n",
      "Iteration 15693 => Loss: 6.72313418998625955680\n",
      "Iteration 15694 => Loss: 6.72313082712156084852\n",
      "Iteration 15695 => Loss: 6.72312746471000277637\n",
      "Iteration 15696 => Loss: 6.72312410275153293782\n",
      "Iteration 15697 => Loss: 6.72312074124608738401\n",
      "Iteration 15698 => Loss: 6.72311738019360305429\n",
      "Iteration 15699 => Loss: 6.72311401959402488160\n",
      "Iteration 15700 => Loss: 6.72311065944728447619\n",
      "Iteration 15701 => Loss: 6.72310729975332588282\n",
      "Iteration 15702 => Loss: 6.72310394051208515265\n",
      "Iteration 15703 => Loss: 6.72310058172349922501\n",
      "Iteration 15704 => Loss: 6.72309722338751836190\n",
      "Iteration 15705 => Loss: 6.72309386550407150906\n",
      "Iteration 15706 => Loss: 6.72309050807309027675\n",
      "Iteration 15707 => Loss: 6.72308715109453114422\n",
      "Iteration 15708 => Loss: 6.72308379456832305721\n",
      "Iteration 15709 => Loss: 6.72308043849440828410\n",
      "Iteration 15710 => Loss: 6.72307708287272642877\n",
      "Iteration 15711 => Loss: 6.72307372770321798328\n",
      "Iteration 15712 => Loss: 6.72307037298581278151\n",
      "Iteration 15713 => Loss: 6.72306701872045486823\n",
      "Iteration 15714 => Loss: 6.72306366490708917638\n",
      "Iteration 15715 => Loss: 6.72306031154564820440\n",
      "Iteration 15716 => Loss: 6.72305695863606711526\n",
      "Iteration 15717 => Loss: 6.72305360617830150005\n",
      "Iteration 15718 => Loss: 6.72305025417227408724\n",
      "Iteration 15719 => Loss: 6.72304690261792625705\n",
      "Iteration 15720 => Loss: 6.72304355151520738332\n",
      "Iteration 15721 => Loss: 6.72304020086405085266\n",
      "Iteration 15722 => Loss: 6.72303685066438649898\n",
      "Iteration 15723 => Loss: 6.72303350091616191975\n",
      "Iteration 15724 => Loss: 6.72303015161932560062\n",
      "Iteration 15725 => Loss: 6.72302680277380204643\n",
      "Iteration 15726 => Loss: 6.72302345437953441376\n",
      "Iteration 15727 => Loss: 6.72302010643646585919\n",
      "Iteration 15728 => Loss: 6.72301675894453065752\n",
      "Iteration 15729 => Loss: 6.72301341190367374168\n",
      "Iteration 15730 => Loss: 6.72301006531382938647\n",
      "Iteration 15731 => Loss: 6.72300671917493630758\n",
      "Iteration 15732 => Loss: 6.72300337348693588524\n",
      "Iteration 15733 => Loss: 6.72300002824976683513\n",
      "Iteration 15734 => Loss: 6.72299668346336876112\n",
      "Iteration 15735 => Loss: 6.72299333912768570798\n",
      "Iteration 15736 => Loss: 6.72298999524264839778\n",
      "Iteration 15737 => Loss: 6.72298665180819821074\n",
      "Iteration 15738 => Loss: 6.72298330882428096800\n",
      "Iteration 15739 => Loss: 6.72297996629082650344\n",
      "Iteration 15740 => Loss: 6.72297662420777886183\n",
      "Iteration 15741 => Loss: 6.72297328257508119975\n",
      "Iteration 15742 => Loss: 6.72296994139266157475\n",
      "Iteration 15743 => Loss: 6.72296660066047913062\n",
      "Iteration 15744 => Loss: 6.72296326037845126677\n",
      "Iteration 15745 => Loss: 6.72295992054652380432\n",
      "Iteration 15746 => Loss: 6.72295658116463989984\n",
      "Iteration 15747 => Loss: 6.72295324223274892717\n",
      "Iteration 15748 => Loss: 6.72294990375077006206\n",
      "Iteration 15749 => Loss: 6.72294656571865179018\n",
      "Iteration 15750 => Loss: 6.72294322813633282721\n",
      "Iteration 15751 => Loss: 6.72293989100375988244\n",
      "Iteration 15752 => Loss: 6.72293655432085657253\n",
      "Iteration 15753 => Loss: 6.72293321808757671221\n",
      "Iteration 15754 => Loss: 6.72292988230385635262\n",
      "Iteration 15755 => Loss: 6.72292654696963509764\n",
      "Iteration 15756 => Loss: 6.72292321208484011663\n",
      "Iteration 15757 => Loss: 6.72291987764942700068\n",
      "Iteration 15758 => Loss: 6.72291654366333091275\n",
      "Iteration 15759 => Loss: 6.72291321012648968036\n",
      "Iteration 15760 => Loss: 6.72290987703884557192\n",
      "Iteration 15761 => Loss: 6.72290654440033019767\n",
      "Iteration 15762 => Loss: 6.72290321221089204329\n",
      "Iteration 15763 => Loss: 6.72289988047045472541\n",
      "Iteration 15764 => Loss: 6.72289654917898271691\n",
      "Iteration 15765 => Loss: 6.72289321833639874626\n",
      "Iteration 15766 => Loss: 6.72288988794264508186\n",
      "Iteration 15767 => Loss: 6.72288655799766576848\n",
      "Iteration 15768 => Loss: 6.72288322850139419273\n",
      "Iteration 15769 => Loss: 6.72287989945377262302\n",
      "Iteration 15770 => Loss: 6.72287657085473888685\n",
      "Iteration 15771 => Loss: 6.72287324270423347627\n",
      "Iteration 15772 => Loss: 6.72286991500220576512\n",
      "Iteration 15773 => Loss: 6.72286658774857581733\n",
      "Iteration 15774 => Loss: 6.72286326094329034220\n",
      "Iteration 15775 => Loss: 6.72285993458630670716\n",
      "Iteration 15776 => Loss: 6.72285660867754053527\n",
      "Iteration 15777 => Loss: 6.72285328321694386489\n",
      "Iteration 15778 => Loss: 6.72284995820444830628\n",
      "Iteration 15779 => Loss: 6.72284663364000589780\n",
      "Iteration 15780 => Loss: 6.72284330952354203248\n",
      "Iteration 15781 => Loss: 6.72283998585500341960\n",
      "Iteration 15782 => Loss: 6.72283666263433499211\n",
      "Iteration 15783 => Loss: 6.72283333986146214301\n",
      "Iteration 15784 => Loss: 6.72283001753634756881\n",
      "Iteration 15785 => Loss: 6.72282669565890600438\n",
      "Iteration 15786 => Loss: 6.72282337422908504720\n",
      "Iteration 15787 => Loss: 6.72282005324682607750\n",
      "Iteration 15788 => Loss: 6.72281673271208113363\n",
      "Iteration 15789 => Loss: 6.72281341262477205589\n",
      "Iteration 15790 => Loss: 6.72281009298484377723\n",
      "Iteration 15791 => Loss: 6.72280677379223856605\n",
      "Iteration 15792 => Loss: 6.72280345504689513803\n",
      "Iteration 15793 => Loss: 6.72280013674875309704\n",
      "Iteration 15794 => Loss: 6.72279681889775027059\n",
      "Iteration 15795 => Loss: 6.72279350149383336799\n",
      "Iteration 15796 => Loss: 6.72279018453693133495\n",
      "Iteration 15797 => Loss: 6.72278686802699532166\n",
      "Iteration 15798 => Loss: 6.72278355196395072113\n",
      "Iteration 15799 => Loss: 6.72278023634775134809\n",
      "Iteration 15800 => Loss: 6.72277692117833058916\n",
      "Iteration 15801 => Loss: 6.72277360645562804820\n",
      "Iteration 15802 => Loss: 6.72277029217958865814\n",
      "Iteration 15803 => Loss: 6.72276697835014669380\n",
      "Iteration 15804 => Loss: 6.72276366496723554178\n",
      "Iteration 15805 => Loss: 6.72276035203080724045\n",
      "Iteration 15806 => Loss: 6.72275703954080494640\n",
      "Iteration 15807 => Loss: 6.72275372749715138809\n",
      "Iteration 15808 => Loss: 6.72275041589980837387\n",
      "Iteration 15809 => Loss: 6.72274710474869152677\n",
      "Iteration 15810 => Loss: 6.72274379404375554969\n",
      "Iteration 15811 => Loss: 6.72274048378493560563\n",
      "Iteration 15812 => Loss: 6.72273717397217662750\n",
      "Iteration 15813 => Loss: 6.72273386460541555465\n",
      "Iteration 15814 => Loss: 6.72273055568458666187\n",
      "Iteration 15815 => Loss: 6.72272724720963754663\n",
      "Iteration 15816 => Loss: 6.72272393918050781281\n",
      "Iteration 15817 => Loss: 6.72272063159714150515\n",
      "Iteration 15818 => Loss: 6.72271732445946135215\n",
      "Iteration 15819 => Loss: 6.72271401776742028034\n",
      "Iteration 15820 => Loss: 6.72271071152096233448\n",
      "Iteration 15821 => Loss: 6.72270740572001734847\n",
      "Iteration 15822 => Loss: 6.72270410036452492619\n",
      "Iteration 15823 => Loss: 6.72270079545443444147\n",
      "Iteration 15824 => Loss: 6.72269749098968372181\n",
      "Iteration 15825 => Loss: 6.72269418697020704201\n",
      "Iteration 15826 => Loss: 6.72269088339594045323\n",
      "Iteration 15827 => Loss: 6.72268758026684132290\n",
      "Iteration 15828 => Loss: 6.72268427758283770856\n",
      "Iteration 15829 => Loss: 6.72268097534386122049\n",
      "Iteration 15830 => Loss: 6.72267767354987100248\n",
      "Iteration 15831 => Loss: 6.72267437220079955296\n",
      "Iteration 15832 => Loss: 6.72267107129657670583\n",
      "Iteration 15833 => Loss: 6.72266777083715805219\n",
      "Iteration 15834 => Loss: 6.72266447082246898503\n",
      "Iteration 15835 => Loss: 6.72266117125246331909\n",
      "Iteration 15836 => Loss: 6.72265787212706911191\n",
      "Iteration 15837 => Loss: 6.72265457344623929004\n",
      "Iteration 15838 => Loss: 6.72265127520990635190\n",
      "Iteration 15839 => Loss: 6.72264797741800546049\n",
      "Iteration 15840 => Loss: 6.72264468007048687781\n",
      "Iteration 15841 => Loss: 6.72264138316729109590\n",
      "Iteration 15842 => Loss: 6.72263808670834173142\n",
      "Iteration 15843 => Loss: 6.72263479069359526363\n",
      "Iteration 15844 => Loss: 6.72263149512298685551\n",
      "Iteration 15845 => Loss: 6.72262819999646143998\n",
      "Iteration 15846 => Loss: 6.72262490531394885096\n",
      "Iteration 15847 => Loss: 6.72262161107539846228\n",
      "Iteration 15848 => Loss: 6.72261831728074366055\n",
      "Iteration 15849 => Loss: 6.72261502392993381960\n",
      "Iteration 15850 => Loss: 6.72261173102289699699\n",
      "Iteration 15851 => Loss: 6.72260843855958079018\n",
      "Iteration 15852 => Loss: 6.72260514653992391487\n",
      "Iteration 15853 => Loss: 6.72260185496386863946\n",
      "Iteration 15854 => Loss: 6.72259856383135279145\n",
      "Iteration 15855 => Loss: 6.72259527314231952744\n",
      "Iteration 15856 => Loss: 6.72259198289670045767\n",
      "Iteration 15857 => Loss: 6.72258869309444850870\n",
      "Iteration 15858 => Loss: 6.72258540373549884350\n",
      "Iteration 15859 => Loss: 6.72258211481978751323\n",
      "Iteration 15860 => Loss: 6.72257882634725945081\n",
      "Iteration 15861 => Loss: 6.72257553831784715470\n",
      "Iteration 15862 => Loss: 6.72257225073150088690\n",
      "Iteration 15863 => Loss: 6.72256896358816380399\n",
      "Iteration 15864 => Loss: 6.72256567688775685809\n",
      "Iteration 15865 => Loss: 6.72256239063024185754\n",
      "Iteration 15866 => Loss: 6.72255910481554774805\n",
      "Iteration 15867 => Loss: 6.72255581944362390345\n",
      "Iteration 15868 => Loss: 6.72255253451439305223\n",
      "Iteration 15869 => Loss: 6.72254925002781078547\n",
      "Iteration 15870 => Loss: 6.72254596598381493067\n",
      "Iteration 15871 => Loss: 6.72254268238234420352\n",
      "Iteration 15872 => Loss: 6.72253939922334087242\n",
      "Iteration 15873 => Loss: 6.72253611650674187672\n",
      "Iteration 15874 => Loss: 6.72253283423248593209\n",
      "Iteration 15875 => Loss: 6.72252955240052063601\n",
      "Iteration 15876 => Loss: 6.72252627101077937510\n",
      "Iteration 15877 => Loss: 6.72252299006320885866\n",
      "Iteration 15878 => Loss: 6.72251970955774691419\n",
      "Iteration 15879 => Loss: 6.72251642949432959284\n",
      "Iteration 15880 => Loss: 6.72251314987290538028\n",
      "Iteration 15881 => Loss: 6.72250987069340233404\n",
      "Iteration 15882 => Loss: 6.72250659195578048610\n",
      "Iteration 15883 => Loss: 6.72250331365995901223\n",
      "Iteration 15884 => Loss: 6.72250003580589083896\n",
      "Iteration 15885 => Loss: 6.72249675839351734652\n",
      "Iteration 15886 => Loss: 6.72249348142277280971\n",
      "Iteration 15887 => Loss: 6.72249020489360038511\n",
      "Iteration 15888 => Loss: 6.72248692880593790022\n",
      "Iteration 15889 => Loss: 6.72248365315972939982\n",
      "Iteration 15890 => Loss: 6.72248037795492159319\n",
      "Iteration 15891 => Loss: 6.72247710319144076152\n",
      "Iteration 15892 => Loss: 6.72247382886923805501\n",
      "Iteration 15893 => Loss: 6.72247055498824863662\n",
      "Iteration 15894 => Loss: 6.72246728154841477476\n",
      "Iteration 15895 => Loss: 6.72246400854967607330\n",
      "Iteration 15896 => Loss: 6.72246073599197213611\n",
      "Iteration 15897 => Loss: 6.72245746387525411336\n",
      "Iteration 15898 => Loss: 6.72245419219944917444\n",
      "Iteration 15899 => Loss: 6.72245092096450136410\n",
      "Iteration 15900 => Loss: 6.72244765017035472709\n",
      "Iteration 15901 => Loss: 6.72244437981694709094\n",
      "Iteration 15902 => Loss: 6.72244110990422161223\n",
      "Iteration 15903 => Loss: 6.72243784043211345391\n",
      "Iteration 15904 => Loss: 6.72243457140056666077\n",
      "Iteration 15905 => Loss: 6.72243130280952794209\n",
      "Iteration 15906 => Loss: 6.72242803465892269088\n",
      "Iteration 15907 => Loss: 6.72242476694871360365\n",
      "Iteration 15908 => Loss: 6.72242149967882074435\n",
      "Iteration 15909 => Loss: 6.72241823284919082226\n",
      "Iteration 15910 => Loss: 6.72241496645977232305\n",
      "Iteration 15911 => Loss: 6.72241170051049508061\n",
      "Iteration 15912 => Loss: 6.72240843500130491606\n",
      "Iteration 15913 => Loss: 6.72240516993215297958\n",
      "Iteration 15914 => Loss: 6.72240190530295311788\n",
      "Iteration 15915 => Loss: 6.72239864111367069199\n",
      "Iteration 15916 => Loss: 6.72239537736423908854\n",
      "Iteration 15917 => Loss: 6.72239211405459702320\n",
      "Iteration 15918 => Loss: 6.72238885118468498803\n",
      "Iteration 15919 => Loss: 6.72238558875444791596\n",
      "Iteration 15920 => Loss: 6.72238232676381830544\n",
      "Iteration 15921 => Loss: 6.72237906521274641847\n",
      "Iteration 15922 => Loss: 6.72237580410116386531\n",
      "Iteration 15923 => Loss: 6.72237254342902179616\n",
      "Iteration 15924 => Loss: 6.72236928319625182127\n",
      "Iteration 15925 => Loss: 6.72236602340280331447\n",
      "Iteration 15926 => Loss: 6.72236276404860966238\n",
      "Iteration 15927 => Loss: 6.72235950513360958070\n",
      "Iteration 15928 => Loss: 6.72235624665775333142\n",
      "Iteration 15929 => Loss: 6.72235298862097785388\n",
      "Iteration 15930 => Loss: 6.72234973102322364014\n",
      "Iteration 15931 => Loss: 6.72234647386442851769\n",
      "Iteration 15932 => Loss: 6.72234321714453297858\n",
      "Iteration 15933 => Loss: 6.72233996086348373211\n",
      "Iteration 15934 => Loss: 6.72233670502122038215\n",
      "Iteration 15935 => Loss: 6.72233344961767986803\n",
      "Iteration 15936 => Loss: 6.72233019465280801086\n",
      "Iteration 15937 => Loss: 6.72232694012653997362\n",
      "Iteration 15938 => Loss: 6.72232368603882068925\n",
      "Iteration 15939 => Loss: 6.72232043238959064979\n",
      "Iteration 15940 => Loss: 6.72231717917878768276\n",
      "Iteration 15941 => Loss: 6.72231392640635228020\n",
      "Iteration 15942 => Loss: 6.72231067407223381593\n",
      "Iteration 15943 => Loss: 6.72230742217636212388\n",
      "Iteration 15944 => Loss: 6.72230417071869190693\n",
      "Iteration 15945 => Loss: 6.72230091969914855810\n",
      "Iteration 15946 => Loss: 6.72229766911768766846\n",
      "Iteration 15947 => Loss: 6.72229441897423196650\n",
      "Iteration 15948 => Loss: 6.72229116926874947779\n",
      "Iteration 15949 => Loss: 6.72228792000114960814\n",
      "Iteration 15950 => Loss: 6.72228467117139771858\n",
      "Iteration 15951 => Loss: 6.72228142277942275484\n",
      "Iteration 15952 => Loss: 6.72227817482516787351\n",
      "Iteration 15953 => Loss: 6.72227492730857623116\n",
      "Iteration 15954 => Loss: 6.72227168022958743165\n",
      "Iteration 15955 => Loss: 6.72226843358814463159\n",
      "Iteration 15956 => Loss: 6.72226518738418210575\n",
      "Iteration 15957 => Loss: 6.72226194161765633339\n",
      "Iteration 15958 => Loss: 6.72225869628848826665\n",
      "Iteration 15959 => Loss: 6.72225545139663172023\n",
      "Iteration 15960 => Loss: 6.72225220694202096894\n",
      "Iteration 15961 => Loss: 6.72224896292460805114\n",
      "Iteration 15962 => Loss: 6.72224571934432013620\n",
      "Iteration 15963 => Loss: 6.72224247620110659796\n",
      "Iteration 15964 => Loss: 6.72223923349490881662\n",
      "Iteration 15965 => Loss: 6.72223599122566373154\n",
      "Iteration 15966 => Loss: 6.72223274939331627564\n",
      "Iteration 15967 => Loss: 6.72222950799780516462\n",
      "Iteration 15968 => Loss: 6.72222626703907266688\n",
      "Iteration 15969 => Loss: 6.72222302651705660992\n",
      "Iteration 15970 => Loss: 6.72221978643170725576\n",
      "Iteration 15971 => Loss: 6.72221654678295443830\n",
      "Iteration 15972 => Loss: 6.72221330757074397866\n",
      "Iteration 15973 => Loss: 6.72221006879501992159\n",
      "Iteration 15974 => Loss: 6.72220683045572631187\n",
      "Iteration 15975 => Loss: 6.72220359255278943067\n",
      "Iteration 15976 => Loss: 6.72220035508616753361\n",
      "Iteration 15977 => Loss: 6.72219711805579223096\n",
      "Iteration 15978 => Loss: 6.72219388146160490294\n",
      "Iteration 15979 => Loss: 6.72219064530355137066\n",
      "Iteration 15980 => Loss: 6.72218740958157390253\n",
      "Iteration 15981 => Loss: 6.72218417429560233245\n",
      "Iteration 15982 => Loss: 6.72218093944558692243\n",
      "Iteration 15983 => Loss: 6.72217770503147260541\n",
      "Iteration 15984 => Loss: 6.72217447105319365619\n",
      "Iteration 15985 => Loss: 6.72217123751069145499\n",
      "Iteration 15986 => Loss: 6.72216800440391093474\n",
      "Iteration 15987 => Loss: 6.72216477173279525203\n",
      "Iteration 15988 => Loss: 6.72216153949727690531\n",
      "Iteration 15989 => Loss: 6.72215830769730704475\n",
      "Iteration 15990 => Loss: 6.72215507633281994515\n",
      "Iteration 15991 => Loss: 6.72215184540376142763\n",
      "Iteration 15992 => Loss: 6.72214861491006576699\n",
      "Iteration 15993 => Loss: 6.72214538485168855431\n",
      "Iteration 15994 => Loss: 6.72214215522854896534\n",
      "Iteration 15995 => Loss: 6.72213892604061324931\n",
      "Iteration 15996 => Loss: 6.72213569728780768742\n",
      "Iteration 15997 => Loss: 6.72213246897007099534\n",
      "Iteration 15998 => Loss: 6.72212924108735787598\n",
      "Iteration 15999 => Loss: 6.72212601363959905143\n",
      "Iteration 16000 => Loss: 6.72212278662673590190\n",
      "Iteration 16001 => Loss: 6.72211956004872135395\n",
      "Iteration 16002 => Loss: 6.72211633390548168876\n",
      "Iteration 16003 => Loss: 6.72211310819696628016\n",
      "Iteration 16004 => Loss: 6.72210988292311650838\n",
      "Iteration 16005 => Loss: 6.72210665808387197728\n",
      "Iteration 16006 => Loss: 6.72210343367916784985\n",
      "Iteration 16007 => Loss: 6.72210020970896060533\n",
      "Iteration 16008 => Loss: 6.72209698617318718306\n",
      "Iteration 16009 => Loss: 6.72209376307177919330\n",
      "Iteration 16010 => Loss: 6.72209054040468512170\n",
      "Iteration 16011 => Loss: 6.72208731817185078938\n",
      "Iteration 16012 => Loss: 6.72208409637320603025\n",
      "Iteration 16013 => Loss: 6.72208087500870377085\n",
      "Iteration 16014 => Loss: 6.72207765407827473325\n",
      "Iteration 16015 => Loss: 6.72207443358186740312\n",
      "Iteration 16016 => Loss: 6.72207121351942493703\n",
      "Iteration 16017 => Loss: 6.72206799389088605068\n",
      "Iteration 16018 => Loss: 6.72206477469619390064\n",
      "Iteration 16019 => Loss: 6.72206155593528720260\n",
      "Iteration 16020 => Loss: 6.72205833760810733679\n",
      "Iteration 16021 => Loss: 6.72205511971459923615\n",
      "Iteration 16022 => Loss: 6.72205190225470072818\n",
      "Iteration 16023 => Loss: 6.72204868522835319311\n",
      "Iteration 16024 => Loss: 6.72204546863550156388\n",
      "Iteration 16025 => Loss: 6.72204225247608899707\n",
      "Iteration 16026 => Loss: 6.72203903675005509655\n",
      "Iteration 16027 => Loss: 6.72203582145733591346\n",
      "Iteration 16028 => Loss: 6.72203260659787904530\n",
      "Iteration 16029 => Loss: 6.72202939217162320773\n",
      "Iteration 16030 => Loss: 6.72202617817851244553\n",
      "Iteration 16031 => Loss: 6.72202296461848902709\n",
      "Iteration 16032 => Loss: 6.72201975149149522082\n",
      "Iteration 16033 => Loss: 6.72201653879746974241\n",
      "Iteration 16034 => Loss: 6.72201332653635397207\n",
      "Iteration 16035 => Loss: 6.72201011470808751369\n",
      "Iteration 16036 => Loss: 6.72200690331262151744\n",
      "Iteration 16037 => Loss: 6.72200369234988315270\n",
      "Iteration 16038 => Loss: 6.72200048181982889872\n",
      "Iteration 16039 => Loss: 6.72199727172239303030\n",
      "Iteration 16040 => Loss: 6.72199406205751337495\n",
      "Iteration 16041 => Loss: 6.72199085282514019468\n",
      "Iteration 16042 => Loss: 6.72198764402520776429\n",
      "Iteration 16043 => Loss: 6.72198443565766901031\n",
      "Iteration 16044 => Loss: 6.72198122772244754941\n",
      "Iteration 16045 => Loss: 6.72197802021950607809\n",
      "Iteration 16046 => Loss: 6.72197481314876821301\n",
      "Iteration 16047 => Loss: 6.72197160651018865707\n",
      "Iteration 16048 => Loss: 6.72196840030370257324\n",
      "Iteration 16049 => Loss: 6.72196519452924601268\n",
      "Iteration 16050 => Loss: 6.72196198918678078371\n",
      "Iteration 16051 => Loss: 6.72195878427622339757\n",
      "Iteration 16052 => Loss: 6.72195557979753743894\n",
      "Iteration 16053 => Loss: 6.72195237575065185354\n",
      "Iteration 16054 => Loss: 6.72194917213550180435\n",
      "Iteration 16055 => Loss: 6.72194596895205354059\n",
      "Iteration 16056 => Loss: 6.72194276620022979074\n",
      "Iteration 16057 => Loss: 6.72193956387997904045\n",
      "Iteration 16058 => Loss: 6.72193636199123467634\n",
      "Iteration 16059 => Loss: 6.72193316053395228948\n",
      "Iteration 16060 => Loss: 6.72192995950806171379\n",
      "Iteration 16061 => Loss: 6.72192675891351143491\n",
      "Iteration 16062 => Loss: 6.72192355875024460943\n",
      "Iteration 16063 => Loss: 6.72192035901819373578\n",
      "Iteration 16064 => Loss: 6.72191715971730818779\n",
      "Iteration 16065 => Loss: 6.72191396084753201023\n",
      "Iteration 16066 => Loss: 6.72191076240880569515\n",
      "Iteration 16067 => Loss: 6.72190756440106529368\n",
      "Iteration 16068 => Loss: 6.72190436682425218606\n",
      "Iteration 16069 => Loss: 6.72190116967831841066\n",
      "Iteration 16070 => Loss: 6.72189797296319913045\n",
      "Iteration 16071 => Loss: 6.72189477667884194290\n",
      "Iteration 16072 => Loss: 6.72189158082517757009\n",
      "Iteration 16073 => Loss: 6.72188838540216071493\n",
      "Iteration 16074 => Loss: 6.72188519040972209950\n",
      "Iteration 16075 => Loss: 6.72188199584781820306\n",
      "Iteration 16076 => Loss: 6.72187880171636820137\n",
      "Iteration 16077 => Loss: 6.72187560801533390276\n",
      "Iteration 16078 => Loss: 6.72187241474465313473\n",
      "Iteration 16079 => Loss: 6.72186922190426550117\n",
      "Iteration 16080 => Loss: 6.72186602949411238228\n",
      "Iteration 16081 => Loss: 6.72186283751413604648\n",
      "Iteration 16082 => Loss: 6.72185964596427698581\n",
      "Iteration 16083 => Loss: 6.72185645484448723863\n",
      "Iteration 16084 => Loss: 6.72185326415469397432\n",
      "Iteration 16085 => Loss: 6.72185007389484301399\n",
      "Iteration 16086 => Loss: 6.72184688406489261325\n",
      "Iteration 16087 => Loss: 6.72184369466475839516\n",
      "Iteration 16088 => Loss: 6.72184050569440572076\n",
      "Iteration 16089 => Loss: 6.72183731715375998306\n",
      "Iteration 16090 => Loss: 6.72183412904277677313\n",
      "Iteration 16091 => Loss: 6.72183094136139036578\n",
      "Iteration 16092 => Loss: 6.72182775410953503581\n",
      "Iteration 16093 => Loss: 6.72182456728717081518\n",
      "Iteration 16094 => Loss: 6.72182138089423730776\n",
      "Iteration 16095 => Loss: 6.72181819493065990656\n",
      "Iteration 16096 => Loss: 6.72181500939640041992\n",
      "Iteration 16097 => Loss: 6.72181182429138690537\n",
      "Iteration 16098 => Loss: 6.72180863961556251951\n",
      "Iteration 16099 => Loss: 6.72180545536887663616\n",
      "Iteration 16100 => Loss: 6.72180227155126708283\n",
      "Iteration 16101 => Loss: 6.72179908816268412153\n",
      "Iteration 16102 => Loss: 6.72179590520305580981\n",
      "Iteration 16103 => Loss: 6.72179272267233685056\n",
      "Iteration 16104 => Loss: 6.72178954057045618953\n",
      "Iteration 16105 => Loss: 6.72178635889737297049\n",
      "Iteration 16106 => Loss: 6.72178317765300992193\n",
      "Iteration 16107 => Loss: 6.72177999683733062852\n",
      "Iteration 16108 => Loss: 6.72177681645025959511\n",
      "Iteration 16109 => Loss: 6.72177363649174886007\n",
      "Iteration 16110 => Loss: 6.72177045696173269818\n",
      "Iteration 16111 => Loss: 6.72176727786016492416\n",
      "Iteration 16112 => Loss: 6.72176409918697892465\n",
      "Iteration 16113 => Loss: 6.72176092094212140893\n",
      "Iteration 16114 => Loss: 6.72175774312552842815\n",
      "Iteration 16115 => Loss: 6.72175456573715202069\n",
      "Iteration 16116 => Loss: 6.72175138877692468498\n",
      "Iteration 16117 => Loss: 6.72174821224479401849\n",
      "Iteration 16118 => Loss: 6.72174503614070051327\n",
      "Iteration 16119 => Loss: 6.72174186046458999044\n",
      "Iteration 16120 => Loss: 6.72173868521639850115\n",
      "Iteration 16121 => Loss: 6.72173551039607808377\n",
      "Iteration 16122 => Loss: 6.72173233600355857220\n",
      "Iteration 16123 => Loss: 6.72172916203879289299\n",
      "Iteration 16124 => Loss: 6.72172598850171887364\n",
      "Iteration 16125 => Loss: 6.72172281539227256530\n",
      "Iteration 16126 => Loss: 6.72171964271040867089\n",
      "Iteration 16127 => Loss: 6.72171647045606679427\n",
      "Iteration 16128 => Loss: 6.72171329862917943387\n",
      "Iteration 16129 => Loss: 6.72171012722970306896\n",
      "Iteration 16130 => Loss: 6.72170695625756486891\n",
      "Iteration 16131 => Loss: 6.72170378571271864843\n",
      "Iteration 16132 => Loss: 6.72170061559511022864\n",
      "Iteration 16133 => Loss: 6.72169744590467033163\n",
      "Iteration 16134 => Loss: 6.72169427664134389033\n",
      "Iteration 16135 => Loss: 6.72169110780507672587\n",
      "Iteration 16136 => Loss: 6.72168793939581465935\n",
      "Iteration 16137 => Loss: 6.72168477141349463011\n",
      "Iteration 16138 => Loss: 6.72168160385805890655\n",
      "Iteration 16139 => Loss: 6.72167843672944620437\n",
      "Iteration 16140 => Loss: 6.72167527002760945010\n",
      "Iteration 16141 => Loss: 6.72167210375248735943\n",
      "Iteration 16142 => Loss: 6.72166893790401864806\n",
      "Iteration 16143 => Loss: 6.72166577248215357798\n",
      "Iteration 16144 => Loss: 6.72166260748682375947\n",
      "Iteration 16145 => Loss: 6.72165944291798478361\n",
      "Iteration 16146 => Loss: 6.72165627877555671432\n",
      "Iteration 16147 => Loss: 6.72165311505951112991\n",
      "Iteration 16148 => Loss: 6.72164995176977075886\n",
      "Iteration 16149 => Loss: 6.72164678890628319863\n",
      "Iteration 16150 => Loss: 6.72164362646899515852\n",
      "Iteration 16151 => Loss: 6.72164046445784446604\n",
      "Iteration 16152 => Loss: 6.72163730287277605413\n",
      "Iteration 16153 => Loss: 6.72163414171373396755\n",
      "Iteration 16154 => Loss: 6.72163098098065070474\n",
      "Iteration 16155 => Loss: 6.72162782067347919224\n",
      "Iteration 16156 => Loss: 6.72162466079215903392\n",
      "Iteration 16157 => Loss: 6.72162150133663427454\n",
      "Iteration 16158 => Loss: 6.72161834230684984703\n",
      "Iteration 16159 => Loss: 6.72161518370274002621\n",
      "Iteration 16160 => Loss: 6.72161202552425507406\n",
      "Iteration 16161 => Loss: 6.72160886777133370629\n",
      "Iteration 16162 => Loss: 6.72160571044391996764\n",
      "Iteration 16163 => Loss: 6.72160255354195879107\n",
      "Iteration 16164 => Loss: 6.72159939706538445137\n",
      "Iteration 16165 => Loss: 6.72159624101415609232\n",
      "Iteration 16166 => Loss: 6.72159308538819111334\n",
      "Iteration 16167 => Loss: 6.72158993018745309911\n",
      "Iteration 16168 => Loss: 6.72158677541188787075\n",
      "Iteration 16169 => Loss: 6.72158362106141815673\n",
      "Iteration 16170 => Loss: 6.72158046713600398903\n",
      "Iteration 16171 => Loss: 6.72157731363557253701\n",
      "Iteration 16172 => Loss: 6.72157416056008472083\n",
      "Iteration 16173 => Loss: 6.72157100790946859803\n",
      "Iteration 16174 => Loss: 6.72156785568367443062\n",
      "Iteration 16175 => Loss: 6.72156470388264626337\n",
      "Iteration 16176 => Loss: 6.72156155250631215381\n",
      "Iteration 16177 => Loss: 6.72155840155463479846\n",
      "Iteration 16178 => Loss: 6.72155525102754491940\n",
      "Iteration 16179 => Loss: 6.72155210092499633134\n",
      "Iteration 16180 => Loss: 6.72154895124691975639\n",
      "Iteration 16181 => Loss: 6.72154580199325923928\n",
      "Iteration 16182 => Loss: 6.72154265316396593022\n",
      "Iteration 16183 => Loss: 6.72153950475897676853\n",
      "Iteration 16184 => Loss: 6.72153635677823224626\n",
      "Iteration 16185 => Loss: 6.72153320922167818452\n",
      "Iteration 16186 => Loss: 6.72153006208926218079\n",
      "Iteration 16187 => Loss: 6.72152691538092295076\n",
      "Iteration 16188 => Loss: 6.72152376909659654558\n",
      "Iteration 16189 => Loss: 6.72152062323623500362\n",
      "Iteration 16190 => Loss: 6.72151747779978414599\n",
      "Iteration 16191 => Loss: 6.72151433278716758934\n",
      "Iteration 16192 => Loss: 6.72151118819835424745\n",
      "Iteration 16193 => Loss: 6.72150804403327395420\n",
      "Iteration 16194 => Loss: 6.72150490029186986618\n",
      "Iteration 16195 => Loss: 6.72150175697407714637\n",
      "Iteration 16196 => Loss: 6.72149861407985227402\n",
      "Iteration 16197 => Loss: 6.72149547160913485300\n",
      "Iteration 16198 => Loss: 6.72149232956185826993\n",
      "Iteration 16199 => Loss: 6.72148918793797811588\n",
      "Iteration 16200 => Loss: 6.72148604673742955384\n",
      "Iteration 16201 => Loss: 6.72148290596016195764\n",
      "Iteration 16202 => Loss: 6.72147976560611404295\n",
      "Iteration 16203 => Loss: 6.72147662567522186094\n",
      "Iteration 16204 => Loss: 6.72147348616744633176\n",
      "Iteration 16205 => Loss: 6.72147034708271107206\n",
      "Iteration 16206 => Loss: 6.72146720842097167292\n",
      "Iteration 16207 => Loss: 6.72146407018216596185\n",
      "Iteration 16208 => Loss: 6.72146093236623887179\n",
      "Iteration 16209 => Loss: 6.72145779497313089479\n",
      "Iteration 16210 => Loss: 6.72145465800279229285\n",
      "Iteration 16211 => Loss: 6.72145152145515911712\n",
      "Iteration 16212 => Loss: 6.72144838533016919513\n",
      "Iteration 16213 => Loss: 6.72144524962778078248\n",
      "Iteration 16214 => Loss: 6.72144211434792371307\n",
      "Iteration 16215 => Loss: 6.72143897949054647256\n",
      "Iteration 16216 => Loss: 6.72143584505558955300\n",
      "Iteration 16217 => Loss: 6.72143271104300410457\n",
      "Iteration 16218 => Loss: 6.72142957745271907299\n",
      "Iteration 16219 => Loss: 6.72142644428469360207\n",
      "Iteration 16220 => Loss: 6.72142331153885397299\n",
      "Iteration 16221 => Loss: 6.72142017921516110590\n",
      "Iteration 16222 => Loss: 6.72141704731354305835\n",
      "Iteration 16223 => Loss: 6.72141391583394742781\n",
      "Iteration 16224 => Loss: 6.72141078477633069355\n",
      "Iteration 16225 => Loss: 6.72140765414061114313\n",
      "Iteration 16226 => Loss: 6.72140452392675236126\n",
      "Iteration 16227 => Loss: 6.72140139413468507001\n",
      "Iteration 16228 => Loss: 6.72139826476435864322\n",
      "Iteration 16229 => Loss: 6.72139513581571446110\n",
      "Iteration 16230 => Loss: 6.72139200728870100932\n",
      "Iteration 16231 => Loss: 6.72138887918325345083\n",
      "Iteration 16232 => Loss: 6.72138575149931583042\n",
      "Iteration 16233 => Loss: 6.72138262423683841007\n",
      "Iteration 16234 => Loss: 6.72137949739576168184\n",
      "Iteration 16235 => Loss: 6.72137637097601992053\n",
      "Iteration 16236 => Loss: 6.72137324497757049357\n",
      "Iteration 16237 => Loss: 6.72137011940034945212\n",
      "Iteration 16238 => Loss: 6.72136699424429284733\n",
      "Iteration 16239 => Loss: 6.72136386950935538209\n",
      "Iteration 16240 => Loss: 6.72136074519547932482\n",
      "Iteration 16241 => Loss: 6.72135762130260516756\n",
      "Iteration 16242 => Loss: 6.72135449783067340235\n",
      "Iteration 16243 => Loss: 6.72135137477962718577\n",
      "Iteration 16244 => Loss: 6.72134825214940967442\n",
      "Iteration 16245 => Loss: 6.72134512993997557118\n",
      "Iteration 16246 => Loss: 6.72134200815125293360\n",
      "Iteration 16247 => Loss: 6.72133888678319468823\n",
      "Iteration 16248 => Loss: 6.72133576583574043894\n",
      "Iteration 16249 => Loss: 6.72133264530883511867\n",
      "Iteration 16250 => Loss: 6.72132952520241655492\n",
      "Iteration 16251 => Loss: 6.72132640551643500970\n",
      "Iteration 16252 => Loss: 6.72132328625083541596\n",
      "Iteration 16253 => Loss: 6.72132016740555382484\n",
      "Iteration 16254 => Loss: 6.72131704898053516928\n",
      "Iteration 16255 => Loss: 6.72131393097572704676\n",
      "Iteration 16256 => Loss: 6.72131081339106994932\n",
      "Iteration 16257 => Loss: 6.72130769622650348083\n",
      "Iteration 16258 => Loss: 6.72130457948198323237\n",
      "Iteration 16259 => Loss: 6.72130146315743459695\n",
      "Iteration 16260 => Loss: 6.72129834725282115926\n",
      "Iteration 16261 => Loss: 6.72129523176807008866\n",
      "Iteration 16262 => Loss: 6.72129211670313519988\n",
      "Iteration 16263 => Loss: 6.72128900205795254408\n",
      "Iteration 16264 => Loss: 6.72128588783246172511\n",
      "Iteration 16265 => Loss: 6.72128277402662366313\n",
      "Iteration 16266 => Loss: 6.72127966064036908023\n",
      "Iteration 16267 => Loss: 6.72127654767364024480\n",
      "Iteration 16268 => Loss: 6.72127343512638919520\n",
      "Iteration 16269 => Loss: 6.72127032299854487718\n",
      "Iteration 16270 => Loss: 6.72126721129006909905\n",
      "Iteration 16271 => Loss: 6.72126410000089258290\n",
      "Iteration 16272 => Loss: 6.72126098913096292620\n",
      "Iteration 16273 => Loss: 6.72125787868022239735\n",
      "Iteration 16274 => Loss: 6.72125476864861237658\n",
      "Iteration 16275 => Loss: 6.72125165903607957318\n",
      "Iteration 16276 => Loss: 6.72124854984257691370\n",
      "Iteration 16277 => Loss: 6.72124544106803245569\n",
      "Iteration 16278 => Loss: 6.72124233271239646115\n",
      "Iteration 16279 => Loss: 6.72123922477560764577\n",
      "Iteration 16280 => Loss: 6.72123611725762071245\n",
      "Iteration 16281 => Loss: 6.72123301015836904782\n",
      "Iteration 16282 => Loss: 6.72122990347779225573\n",
      "Iteration 16283 => Loss: 6.72122679721584770363\n",
      "Iteration 16284 => Loss: 6.72122369137247233084\n",
      "Iteration 16285 => Loss: 6.72122058594761018213\n",
      "Iteration 16286 => Loss: 6.72121748094120086137\n",
      "Iteration 16287 => Loss: 6.72121437635319107784\n",
      "Iteration 16288 => Loss: 6.72121127218352398813\n",
      "Iteration 16289 => Loss: 6.72120816843215251879\n",
      "Iteration 16290 => Loss: 6.72120506509900561554\n",
      "Iteration 16291 => Loss: 6.72120196218403354038\n",
      "Iteration 16292 => Loss: 6.72119885968717589719\n",
      "Iteration 16293 => Loss: 6.72119575760838738887\n",
      "Iteration 16294 => Loss: 6.72119265594759607296\n",
      "Iteration 16295 => Loss: 6.72118955470476020508\n",
      "Iteration 16296 => Loss: 6.72118645387981583639\n",
      "Iteration 16297 => Loss: 6.72118335347270789981\n",
      "Iteration 16298 => Loss: 6.72118025348337599922\n",
      "Iteration 16299 => Loss: 6.72117715391176773210\n",
      "Iteration 16300 => Loss: 6.72117405475783602498\n",
      "Iteration 16301 => Loss: 6.72117095602150715905\n",
      "Iteration 16302 => Loss: 6.72116785770273761358\n",
      "Iteration 16303 => Loss: 6.72116475980145899882\n",
      "Iteration 16304 => Loss: 6.72116166231762690586\n",
      "Iteration 16305 => Loss: 6.72115856525118715581\n",
      "Iteration 16306 => Loss: 6.72115546860207313529\n",
      "Iteration 16307 => Loss: 6.72115237237023599448\n",
      "Iteration 16308 => Loss: 6.72114927655561622544\n",
      "Iteration 16309 => Loss: 6.72114618115814810295\n",
      "Iteration 16310 => Loss: 6.72114308617779343535\n",
      "Iteration 16311 => Loss: 6.72113999161448649744\n",
      "Iteration 16312 => Loss: 6.72113689746817488668\n",
      "Iteration 16313 => Loss: 6.72113380373879287788\n",
      "Iteration 16314 => Loss: 6.72113071042629339757\n",
      "Iteration 16315 => Loss: 6.72112761753062137871\n",
      "Iteration 16316 => Loss: 6.72112452505171198425\n",
      "Iteration 16317 => Loss: 6.72112143298951725257\n",
      "Iteration 16318 => Loss: 6.72111834134397856388\n",
      "Iteration 16319 => Loss: 6.72111525011503996296\n",
      "Iteration 16320 => Loss: 6.72111215930264638274\n",
      "Iteration 16321 => Loss: 6.72110906890673387437\n",
      "Iteration 16322 => Loss: 6.72110597892725891711\n",
      "Iteration 16323 => Loss: 6.72110288936415756211\n",
      "Iteration 16324 => Loss: 6.72109980021737207778\n",
      "Iteration 16325 => Loss: 6.72109671148685183795\n",
      "Iteration 16326 => Loss: 6.72109362317253555830\n",
      "Iteration 16327 => Loss: 6.72109053527437261266\n",
      "Iteration 16328 => Loss: 6.72108744779230260491\n",
      "Iteration 16329 => Loss: 6.72108436072627757341\n",
      "Iteration 16330 => Loss: 6.72108127407622113481\n",
      "Iteration 16331 => Loss: 6.72107818784209865015\n",
      "Iteration 16332 => Loss: 6.72107510202384528242\n",
      "Iteration 16333 => Loss: 6.72107201662140862908\n",
      "Iteration 16334 => Loss: 6.72106893163473007036\n",
      "Iteration 16335 => Loss: 6.72106584706375542737\n",
      "Iteration 16336 => Loss: 6.72106276290842696852\n",
      "Iteration 16337 => Loss: 6.72105967916868518586\n",
      "Iteration 16338 => Loss: 6.72105659584447678867\n",
      "Iteration 16339 => Loss: 6.72105351293574848626\n",
      "Iteration 16340 => Loss: 6.72105043044244165884\n",
      "Iteration 16341 => Loss: 6.72104734836450479207\n",
      "Iteration 16342 => Loss: 6.72104426670186683168\n",
      "Iteration 16343 => Loss: 6.72104118545449580324\n",
      "Iteration 16344 => Loss: 6.72103810462231354705\n",
      "Iteration 16345 => Loss: 6.72103502420528275962\n",
      "Iteration 16346 => Loss: 6.72103194420333771575\n",
      "Iteration 16347 => Loss: 6.72102886461641890747\n",
      "Iteration 16348 => Loss: 6.72102578544447837317\n",
      "Iteration 16349 => Loss: 6.72102270668745127580\n",
      "Iteration 16350 => Loss: 6.72101962834528610102\n",
      "Iteration 16351 => Loss: 6.72101655041793222267\n",
      "Iteration 16352 => Loss: 6.72101347290532569190\n",
      "Iteration 16353 => Loss: 6.72101039580742476431\n",
      "Iteration 16354 => Loss: 6.72100731912415128022\n",
      "Iteration 16355 => Loss: 6.72100424285545994252\n",
      "Iteration 16356 => Loss: 6.72100116700130811864\n",
      "Iteration 16357 => Loss: 6.72099809156161498436\n",
      "Iteration 16358 => Loss: 6.72099501653634678888\n",
      "Iteration 16359 => Loss: 6.72099194192543247794\n",
      "Iteration 16360 => Loss: 6.72098886772882231355\n",
      "Iteration 16361 => Loss: 6.72098579394646389318\n",
      "Iteration 16362 => Loss: 6.72098272057829504433\n",
      "Iteration 16363 => Loss: 6.72097964762426336449\n",
      "Iteration 16364 => Loss: 6.72097657508431911566\n",
      "Iteration 16365 => Loss: 6.72097350295839213175\n",
      "Iteration 16366 => Loss: 6.72097043124643445111\n",
      "Iteration 16367 => Loss: 6.72096735994839811212\n",
      "Iteration 16368 => Loss: 6.72096428906421472504\n",
      "Iteration 16369 => Loss: 6.72096121859382744645\n",
      "Iteration 16370 => Loss: 6.72095814853719009108\n",
      "Iteration 16371 => Loss: 6.72095507889424670367\n",
      "Iteration 16372 => Loss: 6.72095200966493333539\n",
      "Iteration 16373 => Loss: 6.72094894084919847188\n",
      "Iteration 16374 => Loss: 6.72094587244698882245\n",
      "Iteration 16375 => Loss: 6.72094280445825020820\n",
      "Iteration 16376 => Loss: 6.72093973688291690394\n",
      "Iteration 16377 => Loss: 6.72093666972094805345\n",
      "Iteration 16378 => Loss: 6.72093360297226372069\n",
      "Iteration 16379 => Loss: 6.72093053663683992482\n",
      "Iteration 16380 => Loss: 6.72092747071459939434\n",
      "Iteration 16381 => Loss: 6.72092440520549416760\n",
      "Iteration 16382 => Loss: 6.72092134010945940759\n",
      "Iteration 16383 => Loss: 6.72091827542645514626\n",
      "Iteration 16384 => Loss: 6.72091521115641210571\n",
      "Iteration 16385 => Loss: 6.72091214729928410065\n",
      "Iteration 16386 => Loss: 6.72090908385500540589\n",
      "Iteration 16387 => Loss: 6.72090602082353161251\n",
      "Iteration 16388 => Loss: 6.72090295820479166622\n",
      "Iteration 16389 => Loss: 6.72089989599875359261\n",
      "Iteration 16390 => Loss: 6.72089683420533923197\n",
      "Iteration 16391 => Loss: 6.72089377282450772810\n",
      "Iteration 16392 => Loss: 6.72089071185619157944\n",
      "Iteration 16393 => Loss: 6.72088765130034371253\n",
      "Iteration 16394 => Loss: 6.72088459115690373125\n",
      "Iteration 16395 => Loss: 6.72088153142582545030\n",
      "Iteration 16396 => Loss: 6.72087847210703426271\n",
      "Iteration 16397 => Loss: 6.72087541320049997040\n",
      "Iteration 16398 => Loss: 6.72087235470614530186\n",
      "Iteration 16399 => Loss: 6.72086929662392229545\n",
      "Iteration 16400 => Loss: 6.72086623895377943683\n",
      "Iteration 16401 => Loss: 6.72086318169565632985\n",
      "Iteration 16402 => Loss: 6.72086012484949701928\n",
      "Iteration 16403 => Loss: 6.72085706841525443167\n",
      "Iteration 16404 => Loss: 6.72085401239286106545\n",
      "Iteration 16405 => Loss: 6.72085095678226984717\n",
      "Iteration 16406 => Loss: 6.72084790158341593980\n",
      "Iteration 16407 => Loss: 6.72084484679625848713\n",
      "Iteration 16408 => Loss: 6.72084179242072821125\n",
      "Iteration 16409 => Loss: 6.72083873845677892689\n",
      "Iteration 16410 => Loss: 6.72083568490435645515\n",
      "Iteration 16411 => Loss: 6.72083263176338618905\n",
      "Iteration 16412 => Loss: 6.72082957903383793052\n",
      "Iteration 16413 => Loss: 6.72082652671564328983\n",
      "Iteration 16414 => Loss: 6.72082347480874364720\n",
      "Iteration 16415 => Loss: 6.72082042331309637007\n",
      "Iteration 16416 => Loss: 6.72081737222863395687\n",
      "Iteration 16417 => Loss: 6.72081432155530489325\n",
      "Iteration 16418 => Loss: 6.72081127129305855306\n",
      "Iteration 16419 => Loss: 6.72080822144182832290\n",
      "Iteration 16420 => Loss: 6.72080517200157423474\n",
      "Iteration 16421 => Loss: 6.72080212297222523432\n",
      "Iteration 16422 => Loss: 6.72079907435373780089\n",
      "Iteration 16423 => Loss: 6.72079602614604532107\n",
      "Iteration 16424 => Loss: 6.72079297834911315590\n",
      "Iteration 16425 => Loss: 6.72078993096285781661\n",
      "Iteration 16426 => Loss: 6.72078688398724377606\n",
      "Iteration 16427 => Loss: 6.72078383742220797359\n",
      "Iteration 16428 => Loss: 6.72078079126770244756\n",
      "Iteration 16429 => Loss: 6.72077774552366058458\n",
      "Iteration 16430 => Loss: 6.72077470019003175850\n",
      "Iteration 16431 => Loss: 6.72077165526676800766\n",
      "Iteration 16432 => Loss: 6.72076861075380271870\n",
      "Iteration 16433 => Loss: 6.72076556665108970634\n",
      "Iteration 16434 => Loss: 6.72076252295856857444\n",
      "Iteration 16435 => Loss: 6.72075947967618336776\n",
      "Iteration 16436 => Loss: 6.72075643680387990742\n",
      "Iteration 16437 => Loss: 6.72075339434160223817\n",
      "Iteration 16438 => Loss: 6.72075035228930151021\n",
      "Iteration 16439 => Loss: 6.72074731064691821558\n",
      "Iteration 16440 => Loss: 6.72074426941439373451\n",
      "Iteration 16441 => Loss: 6.72074122859167388810\n",
      "Iteration 16442 => Loss: 6.72073818817871337927\n",
      "Iteration 16443 => Loss: 6.72073514817543760103\n",
      "Iteration 16444 => Loss: 6.72073210858181013805\n",
      "Iteration 16445 => Loss: 6.72072906939776348878\n",
      "Iteration 16446 => Loss: 6.72072603062325146794\n",
      "Iteration 16447 => Loss: 6.72072299225820835034\n",
      "Iteration 16448 => Loss: 6.72071995430258972704\n",
      "Iteration 16449 => Loss: 6.72071691675633431373\n",
      "Iteration 16450 => Loss: 6.72071387961939148425\n",
      "Iteration 16451 => Loss: 6.72071084289170261883\n",
      "Iteration 16452 => Loss: 6.72070780657321265039\n",
      "Iteration 16453 => Loss: 6.72070477066386029463\n",
      "Iteration 16454 => Loss: 6.72070173516360469534\n",
      "Iteration 16455 => Loss: 6.72069870007238545639\n",
      "Iteration 16456 => Loss: 6.72069566539013418804\n",
      "Iteration 16457 => Loss: 6.72069263111681269862\n",
      "Iteration 16458 => Loss: 6.72068959725235970382\n",
      "Iteration 16459 => Loss: 6.72068656379672013657\n",
      "Iteration 16460 => Loss: 6.72068353074984159434\n",
      "Iteration 16461 => Loss: 6.72068049811166545737\n",
      "Iteration 16462 => Loss: 6.72067746588213221770\n",
      "Iteration 16463 => Loss: 6.72067443406119746641\n",
      "Iteration 16464 => Loss: 6.72067140264880169553\n",
      "Iteration 16465 => Loss: 6.72066837164488184442\n",
      "Iteration 16466 => Loss: 6.72066534104939350414\n",
      "Iteration 16467 => Loss: 6.72066231086228160763\n",
      "Iteration 16468 => Loss: 6.72065928108348309422\n",
      "Iteration 16469 => Loss: 6.72065625171295089046\n",
      "Iteration 16470 => Loss: 6.72065322275062815294\n",
      "Iteration 16471 => Loss: 6.72065019419645981458\n",
      "Iteration 16472 => Loss: 6.72064716605038192654\n",
      "Iteration 16473 => Loss: 6.72064413831234741536\n",
      "Iteration 16474 => Loss: 6.72064111098230387853\n",
      "Iteration 16475 => Loss: 6.72063808406019180808\n",
      "Iteration 16476 => Loss: 6.72063505754596057784\n",
      "Iteration 16477 => Loss: 6.72063203143955067986\n",
      "Iteration 16478 => Loss: 6.72062900574090793526\n",
      "Iteration 16479 => Loss: 6.72062598044997994151\n",
      "Iteration 16480 => Loss: 6.72062295556670630248\n",
      "Iteration 16481 => Loss: 6.72061993109104083288\n",
      "Iteration 16482 => Loss: 6.72061690702292136024\n",
      "Iteration 16483 => Loss: 6.72061388336229370566\n",
      "Iteration 16484 => Loss: 6.72061086010910457844\n",
      "Iteration 16485 => Loss: 6.72060783726331045784\n",
      "Iteration 16486 => Loss: 6.72060481482482607873\n",
      "Iteration 16487 => Loss: 6.72060179279362923666\n",
      "Iteration 16488 => Loss: 6.72059877116964621280\n",
      "Iteration 16489 => Loss: 6.72059574995283171006\n",
      "Iteration 16490 => Loss: 6.72059272914311911507\n",
      "Iteration 16491 => Loss: 6.72058970874046757160\n",
      "Iteration 16492 => Loss: 6.72058668874481135447\n",
      "Iteration 16493 => Loss: 6.72058366915610516656\n",
      "Iteration 16494 => Loss: 6.72058064997427972997\n",
      "Iteration 16495 => Loss: 6.72057763119929685303\n",
      "Iteration 16496 => Loss: 6.72057461283109169869\n",
      "Iteration 16497 => Loss: 6.72057159486961097627\n",
      "Iteration 16498 => Loss: 6.72056857731480494778\n",
      "Iteration 16499 => Loss: 6.72056556016660877617\n",
      "Iteration 16500 => Loss: 6.72056254342497449983\n",
      "Iteration 16501 => Loss: 6.72055952708985593347\n",
      "Iteration 16502 => Loss: 6.72055651116117491739\n",
      "Iteration 16503 => Loss: 6.72055349563889592446\n",
      "Iteration 16504 => Loss: 6.72055048052296122307\n",
      "Iteration 16505 => Loss: 6.72054746581331130528\n",
      "Iteration 16506 => Loss: 6.72054445150989199220\n",
      "Iteration 16507 => Loss: 6.72054143761265176948\n",
      "Iteration 16508 => Loss: 6.72053842412153290553\n",
      "Iteration 16509 => Loss: 6.72053541103648743871\n",
      "Iteration 16510 => Loss: 6.72053239835745142017\n",
      "Iteration 16511 => Loss: 6.72052938608437600010\n",
      "Iteration 16512 => Loss: 6.72052637421720255873\n",
      "Iteration 16513 => Loss: 6.72052336275588402259\n",
      "Iteration 16514 => Loss: 6.72052035170034756106\n",
      "Iteration 16515 => Loss: 6.72051734105056297608\n",
      "Iteration 16516 => Loss: 6.72051433080646365426\n",
      "Iteration 16517 => Loss: 6.72051132096798831128\n",
      "Iteration 16518 => Loss: 6.72050831153509253824\n",
      "Iteration 16519 => Loss: 6.72050530250771593899\n",
      "Iteration 16520 => Loss: 6.72050229388580699919\n",
      "Iteration 16521 => Loss: 6.72049928566931420448\n",
      "Iteration 16522 => Loss: 6.72049627785817449421\n",
      "Iteration 16523 => Loss: 6.72049327045233546585\n",
      "Iteration 16524 => Loss: 6.72049026345175182229\n",
      "Iteration 16525 => Loss: 6.72048725685636050287\n",
      "Iteration 16526 => Loss: 6.72048425066610111145\n",
      "Iteration 16527 => Loss: 6.72048124488092923912\n",
      "Iteration 16528 => Loss: 6.72047823950078360156\n",
      "Iteration 16529 => Loss: 6.72047523452562245438\n",
      "Iteration 16530 => Loss: 6.72047222995537563150\n",
      "Iteration 16531 => Loss: 6.72046922578999517128\n",
      "Iteration 16532 => Loss: 6.72046622202943488844\n",
      "Iteration 16533 => Loss: 6.72046321867362461688\n",
      "Iteration 16534 => Loss: 6.72046021572251373044\n",
      "Iteration 16535 => Loss: 6.72045721317604716205\n",
      "Iteration 16536 => Loss: 6.72045421103418227915\n",
      "Iteration 16537 => Loss: 6.72045120929685424471\n",
      "Iteration 16538 => Loss: 6.72044820796400887986\n",
      "Iteration 16539 => Loss: 6.72044520703559644659\n",
      "Iteration 16540 => Loss: 6.72044220651156276602\n",
      "Iteration 16541 => Loss: 6.72043920639183856025\n",
      "Iteration 16542 => Loss: 6.72043620667638830213\n",
      "Iteration 16543 => Loss: 6.72043320736514893099\n",
      "Iteration 16544 => Loss: 6.72043020845806449159\n",
      "Iteration 16545 => Loss: 6.72042720995508791049\n",
      "Iteration 16546 => Loss: 6.72042421185614902157\n",
      "Iteration 16547 => Loss: 6.72042121416121940314\n",
      "Iteration 16548 => Loss: 6.72041821687022356002\n",
      "Iteration 16549 => Loss: 6.72041521998310997787\n",
      "Iteration 16550 => Loss: 6.72041222349983069506\n",
      "Iteration 16551 => Loss: 6.72040922742032886816\n",
      "Iteration 16552 => Loss: 6.72040623174454143651\n",
      "Iteration 16553 => Loss: 6.72040323647242665572\n",
      "Iteration 16554 => Loss: 6.72040024160392501784\n",
      "Iteration 16555 => Loss: 6.72039724713898145581\n",
      "Iteration 16556 => Loss: 6.72039425307754267891\n",
      "Iteration 16557 => Loss: 6.72039125941955628463\n",
      "Iteration 16558 => Loss: 6.72038826616496010047\n",
      "Iteration 16559 => Loss: 6.72038527331371060569\n",
      "Iteration 16560 => Loss: 6.72038228086574296327\n",
      "Iteration 16561 => Loss: 6.72037928882101098793\n",
      "Iteration 16562 => Loss: 6.72037629717945605989\n",
      "Iteration 16563 => Loss: 6.72037330594102400028\n",
      "Iteration 16564 => Loss: 6.72037031510566418291\n",
      "Iteration 16565 => Loss: 6.72036732467332065255\n",
      "Iteration 16566 => Loss: 6.72036433464393567760\n",
      "Iteration 16567 => Loss: 6.72036134501745685554\n",
      "Iteration 16568 => Loss: 6.72035835579383178384\n",
      "Iteration 16569 => Loss: 6.72035536697300184272\n",
      "Iteration 16570 => Loss: 6.72035237855492084691\n",
      "Iteration 16571 => Loss: 6.72034939053953017662\n",
      "Iteration 16572 => Loss: 6.72034640292676765938\n",
      "Iteration 16573 => Loss: 6.72034341571659421533\n",
      "Iteration 16574 => Loss: 6.72034042890893879019\n",
      "Iteration 16575 => Loss: 6.72033744250376319229\n",
      "Iteration 16576 => Loss: 6.72033445650100258462\n",
      "Iteration 16577 => Loss: 6.72033147090060545281\n",
      "Iteration 16578 => Loss: 6.72032848570252561160\n",
      "Iteration 16579 => Loss: 6.72032550090669111853\n",
      "Iteration 16580 => Loss: 6.72032251651306822282\n",
      "Iteration 16581 => Loss: 6.72031953252157521206\n",
      "Iteration 16582 => Loss: 6.72031654893218988178\n",
      "Iteration 16583 => Loss: 6.72031356574484473043\n",
      "Iteration 16584 => Loss: 6.72031058295948025005\n",
      "Iteration 16585 => Loss: 6.72030760057604314994\n",
      "Iteration 16586 => Loss: 6.72030461859448546846\n",
      "Iteration 16587 => Loss: 6.72030163701474858584\n",
      "Iteration 16588 => Loss: 6.72029865583678542862\n",
      "Iteration 16589 => Loss: 6.72029567506052760706\n",
      "Iteration 16590 => Loss: 6.72029269468593604131\n",
      "Iteration 16591 => Loss: 6.72028971471295122342\n",
      "Iteration 16592 => Loss: 6.72028673514151364543\n",
      "Iteration 16593 => Loss: 6.72028375597157445753\n",
      "Iteration 16594 => Loss: 6.72028077720308125720\n",
      "Iteration 16595 => Loss: 6.72027779883597187194\n",
      "Iteration 16596 => Loss: 6.72027482087020100465\n",
      "Iteration 16597 => Loss: 6.72027184330571270010\n",
      "Iteration 16598 => Loss: 6.72026886614245011486\n",
      "Iteration 16599 => Loss: 6.72026588938036262277\n",
      "Iteration 16600 => Loss: 6.72026291301939426859\n",
      "Iteration 16601 => Loss: 6.72025993705948465617\n",
      "Iteration 16602 => Loss: 6.72025696150058937661\n",
      "Iteration 16603 => Loss: 6.72025398634264625741\n",
      "Iteration 16604 => Loss: 6.72025101158561710690\n",
      "Iteration 16605 => Loss: 6.72024803722942998263\n",
      "Iteration 16606 => Loss: 6.72024506327403781114\n",
      "Iteration 16607 => Loss: 6.72024208971938641355\n",
      "Iteration 16608 => Loss: 6.72023911656542249915\n",
      "Iteration 16609 => Loss: 6.72023614381208833635\n",
      "Iteration 16610 => Loss: 6.72023317145934218075\n",
      "Iteration 16611 => Loss: 6.72023019950710764903\n",
      "Iteration 16612 => Loss: 6.72022722795534743767\n",
      "Iteration 16613 => Loss: 6.72022425680401092052\n",
      "Iteration 16614 => Loss: 6.72022128605303237237\n",
      "Iteration 16615 => Loss: 6.72021831570235939068\n",
      "Iteration 16616 => Loss: 6.72021534575194667838\n",
      "Iteration 16617 => Loss: 6.72021237620173828020\n",
      "Iteration 16618 => Loss: 6.72020940705167024731\n",
      "Iteration 16619 => Loss: 6.72020643830170083532\n",
      "Iteration 16620 => Loss: 6.72020346995176076632\n",
      "Iteration 16621 => Loss: 6.72020050200181362499\n",
      "Iteration 16622 => Loss: 6.72019753445180434426\n",
      "Iteration 16623 => Loss: 6.72019456730165654079\n",
      "Iteration 16624 => Loss: 6.72019160055134623377\n",
      "Iteration 16625 => Loss: 6.72018863420080148074\n",
      "Iteration 16626 => Loss: 6.72018566824997343190\n",
      "Iteration 16627 => Loss: 6.72018270269880257928\n",
      "Iteration 16628 => Loss: 6.72017973754724540214\n",
      "Iteration 16629 => Loss: 6.72017677279524150435\n",
      "Iteration 16630 => Loss: 6.72017380844274114793\n",
      "Iteration 16631 => Loss: 6.72017084448967860766\n",
      "Iteration 16632 => Loss: 6.72016788093601569187\n",
      "Iteration 16633 => Loss: 6.72016491778168845173\n",
      "Iteration 16634 => Loss: 6.72016195502665070194\n",
      "Iteration 16635 => Loss: 6.72015899267084204638\n",
      "Iteration 16636 => Loss: 6.72015603071421008252\n",
      "Iteration 16637 => Loss: 6.72015306915670418419\n",
      "Iteration 16638 => Loss: 6.72015010799826750798\n",
      "Iteration 16639 => Loss: 6.72014714723884587499\n",
      "Iteration 16640 => Loss: 6.72014418687838865907\n",
      "Iteration 16641 => Loss: 6.72014122691684079314\n",
      "Iteration 16642 => Loss: 6.72013826735414365743\n",
      "Iteration 16643 => Loss: 6.72013530819025195484\n",
      "Iteration 16644 => Loss: 6.72013234942510528924\n",
      "Iteration 16645 => Loss: 6.72012939105865125811\n",
      "Iteration 16646 => Loss: 6.72012643309084278798\n",
      "Iteration 16647 => Loss: 6.72012347552161948272\n",
      "Iteration 16648 => Loss: 6.72012051835092361074\n",
      "Iteration 16649 => Loss: 6.72011756157871520401\n",
      "Iteration 16650 => Loss: 6.72011460520492143189\n",
      "Iteration 16651 => Loss: 6.72011164922950587908\n",
      "Iteration 16652 => Loss: 6.72010869365240903761\n",
      "Iteration 16653 => Loss: 6.72010573847357317590\n",
      "Iteration 16654 => Loss: 6.72010278369295299683\n",
      "Iteration 16655 => Loss: 6.72009982931048366339\n",
      "Iteration 16656 => Loss: 6.72009687532612343119\n",
      "Iteration 16657 => Loss: 6.72009392173980746321\n",
      "Iteration 16658 => Loss: 6.72009096855149046235\n",
      "Iteration 16659 => Loss: 6.72008801576111203246\n",
      "Iteration 16660 => Loss: 6.72008506336863131736\n",
      "Iteration 16661 => Loss: 6.72008211137398081547\n",
      "Iteration 16662 => Loss: 6.72007915977710990063\n",
      "Iteration 16663 => Loss: 6.72007620857797327574\n",
      "Iteration 16664 => Loss: 6.72007325777649544563\n",
      "Iteration 16665 => Loss: 6.72007030737264798859\n",
      "Iteration 16666 => Loss: 6.72006735736637406120\n",
      "Iteration 16667 => Loss: 6.72006440775760438555\n",
      "Iteration 16668 => Loss: 6.72006145854629632908\n",
      "Iteration 16669 => Loss: 6.72005850973239748924\n",
      "Iteration 16670 => Loss: 6.72005556131584835811\n",
      "Iteration 16671 => Loss: 6.72005261329660008585\n",
      "Iteration 16672 => Loss: 6.72004966567460293447\n",
      "Iteration 16673 => Loss: 6.72004671844979117878\n",
      "Iteration 16674 => Loss: 6.72004377162211952168\n",
      "Iteration 16675 => Loss: 6.72004082519153467246\n",
      "Iteration 16676 => Loss: 6.72003787915798422858\n",
      "Iteration 16677 => Loss: 6.72003493352140690575\n",
      "Iteration 16678 => Loss: 6.72003198828175474233\n",
      "Iteration 16679 => Loss: 6.72002904343897533579\n",
      "Iteration 16680 => Loss: 6.72002609899301539542\n",
      "Iteration 16681 => Loss: 6.72002315494381718963\n",
      "Iteration 16682 => Loss: 6.72002021129133009225\n",
      "Iteration 16683 => Loss: 6.72001726803550258893\n",
      "Iteration 16684 => Loss: 6.72001432517627872443\n",
      "Iteration 16685 => Loss: 6.72001138271360165533\n",
      "Iteration 16686 => Loss: 6.72000844064742341999\n",
      "Iteration 16687 => Loss: 6.72000549897769516861\n",
      "Iteration 16688 => Loss: 6.72000255770434762326\n",
      "Iteration 16689 => Loss: 6.71999961682733903956\n",
      "Iteration 16690 => Loss: 6.71999667634661967952\n",
      "Iteration 16691 => Loss: 6.71999373626212559429\n",
      "Iteration 16692 => Loss: 6.71999079657380704589\n",
      "Iteration 16693 => Loss: 6.71998785728161251996\n",
      "Iteration 16694 => Loss: 6.71998491838548339672\n",
      "Iteration 16695 => Loss: 6.71998197988538592540\n",
      "Iteration 16696 => Loss: 6.71997904178123928176\n",
      "Iteration 16697 => Loss: 6.71997610407300349777\n",
      "Iteration 16698 => Loss: 6.71997316676062617091\n",
      "Iteration 16699 => Loss: 6.71997022984405489865\n",
      "Iteration 16700 => Loss: 6.71996729332322662032\n",
      "Iteration 16701 => Loss: 6.71996435719809248610\n",
      "Iteration 16702 => Loss: 6.71996142146861075162\n",
      "Iteration 16703 => Loss: 6.71995848613471657984\n",
      "Iteration 16704 => Loss: 6.71995555119635135100\n",
      "Iteration 16705 => Loss: 6.71995261665348220248\n",
      "Iteration 16706 => Loss: 6.71994968250602919824\n",
      "Iteration 16707 => Loss: 6.71994674875396302838\n",
      "Iteration 16708 => Loss: 6.71994381539721619134\n",
      "Iteration 16709 => Loss: 6.71994088243573983732\n",
      "Iteration 16710 => Loss: 6.71993794986947801107\n",
      "Iteration 16711 => Loss: 6.71993501769838452731\n",
      "Iteration 16712 => Loss: 6.71993208592239898991\n",
      "Iteration 16713 => Loss: 6.71992915454146721999\n",
      "Iteration 16714 => Loss: 6.71992622355554836133\n",
      "Iteration 16715 => Loss: 6.71992329296456869514\n",
      "Iteration 16716 => Loss: 6.71992036276849091792\n",
      "Iteration 16717 => Loss: 6.71991743296725640988\n",
      "Iteration 16718 => Loss: 6.71991450356082076212\n",
      "Iteration 16719 => Loss: 6.71991157454911114399\n",
      "Iteration 16720 => Loss: 6.71990864593209380473\n",
      "Iteration 16721 => Loss: 6.71990571770970834820\n",
      "Iteration 16722 => Loss: 6.71990278988189970732\n",
      "Iteration 16723 => Loss: 6.71989986244861015052\n",
      "Iteration 16724 => Loss: 6.71989693540980326247\n",
      "Iteration 16725 => Loss: 6.71989400876540710073\n",
      "Iteration 16726 => Loss: 6.71989108251537281546\n",
      "Iteration 16727 => Loss: 6.71988815665965955048\n",
      "Iteration 16728 => Loss: 6.71988523119819891605\n",
      "Iteration 16729 => Loss: 6.71988230613095094412\n",
      "Iteration 16730 => Loss: 6.71987938145785435040\n",
      "Iteration 16731 => Loss: 6.71987645717885317964\n",
      "Iteration 16732 => Loss: 6.71987353329389769385\n",
      "Iteration 16733 => Loss: 6.71987060980294348411\n",
      "Iteration 16734 => Loss: 6.71986768670592837793\n",
      "Iteration 16735 => Loss: 6.71986476400279553189\n",
      "Iteration 16736 => Loss: 6.71986184169350231343\n",
      "Iteration 16737 => Loss: 6.71985891977799276731\n",
      "Iteration 16738 => Loss: 6.71985599825620649739\n",
      "Iteration 16739 => Loss: 6.71985307712809731839\n",
      "Iteration 16740 => Loss: 6.71985015639360927509\n",
      "Iteration 16741 => Loss: 6.71984723605269085311\n",
      "Iteration 16742 => Loss: 6.71984431610528698542\n",
      "Iteration 16743 => Loss: 6.71984139655134882219\n",
      "Iteration 16744 => Loss: 6.71983847739081863182\n",
      "Iteration 16745 => Loss: 6.71983555862365200539\n",
      "Iteration 16746 => Loss: 6.71983264024977966500\n",
      "Iteration 16747 => Loss: 6.71982972226915631353\n",
      "Iteration 16748 => Loss: 6.71982680468174731203\n",
      "Iteration 16749 => Loss: 6.71982388748747272444\n",
      "Iteration 16750 => Loss: 6.71982097068628991821\n",
      "Iteration 16751 => Loss: 6.71981805427814649079\n",
      "Iteration 16752 => Loss: 6.71981513826299625691\n",
      "Iteration 16753 => Loss: 6.71981222264077437956\n",
      "Iteration 16754 => Loss: 6.71980930741142579166\n",
      "Iteration 16755 => Loss: 6.71980639257491230154\n",
      "Iteration 16756 => Loss: 6.71980347813116996036\n",
      "Iteration 16757 => Loss: 6.71980056408015613556\n",
      "Iteration 16758 => Loss: 6.71979765042180510193\n",
      "Iteration 16759 => Loss: 6.71979473715606889783\n",
      "Iteration 16760 => Loss: 6.71979182428289423257\n",
      "Iteration 16761 => Loss: 6.71978891180223403268\n",
      "Iteration 16762 => Loss: 6.71978599971403234292\n",
      "Iteration 16763 => Loss: 6.71978308801823320806\n",
      "Iteration 16764 => Loss: 6.71978017671478067285\n",
      "Iteration 16765 => Loss: 6.71977726580362855202\n",
      "Iteration 16766 => Loss: 6.71977435528472355486\n",
      "Iteration 16767 => Loss: 6.71977144515801061431\n",
      "Iteration 16768 => Loss: 6.71976853542343732784\n",
      "Iteration 16769 => Loss: 6.71976562608095306928\n",
      "Iteration 16770 => Loss: 6.71976271713050188339\n",
      "Iteration 16771 => Loss: 6.71975980857203047947\n",
      "Iteration 16772 => Loss: 6.71975690040548734316\n",
      "Iteration 16773 => Loss: 6.71975399263081563106\n",
      "Iteration 16774 => Loss: 6.71975108524797626330\n",
      "Iteration 16775 => Loss: 6.71974817825690085016\n",
      "Iteration 16776 => Loss: 6.71974527165754498270\n",
      "Iteration 16777 => Loss: 6.71974236544985714659\n",
      "Iteration 16778 => Loss: 6.71973945963377783386\n",
      "Iteration 16779 => Loss: 6.71973655420925108928\n",
      "Iteration 16780 => Loss: 6.71973364917623960935\n",
      "Iteration 16781 => Loss: 6.71973074453467678069\n",
      "Iteration 16782 => Loss: 6.71972784028451552985\n",
      "Iteration 16783 => Loss: 6.71972493642570167793\n",
      "Iteration 16784 => Loss: 6.71972203295818193425\n",
      "Iteration 16785 => Loss: 6.71971912988191100169\n",
      "Iteration 16786 => Loss: 6.71971622719682226688\n",
      "Iteration 16787 => Loss: 6.71971332490287576178\n",
      "Iteration 16788 => Loss: 6.71971042300000664937\n",
      "Iteration 16789 => Loss: 6.71970752148817851435\n",
      "Iteration 16790 => Loss: 6.71970462036732030242\n",
      "Iteration 16791 => Loss: 6.71970171963739382193\n",
      "Iteration 16792 => Loss: 6.71969881929834311762\n",
      "Iteration 16793 => Loss: 6.71969591935010956973\n",
      "Iteration 16794 => Loss: 6.71969301979264610480\n",
      "Iteration 16795 => Loss: 6.71969012062589854395\n",
      "Iteration 16796 => Loss: 6.71968722184980915557\n",
      "Iteration 16797 => Loss: 6.71968432346433175439\n",
      "Iteration 16798 => Loss: 6.71968142546941127335\n",
      "Iteration 16799 => Loss: 6.71967852786500063900\n",
      "Iteration 16800 => Loss: 6.71967563065104300790\n",
      "Iteration 16801 => Loss: 6.71967273382748064847\n",
      "Iteration 16802 => Loss: 6.71966983739427181632\n",
      "Iteration 16803 => Loss: 6.71966694135134545718\n",
      "Iteration 16804 => Loss: 6.71966404569867403751\n",
      "Iteration 16805 => Loss: 6.71966115043618383851\n",
      "Iteration 16806 => Loss: 6.71965825556383133943\n",
      "Iteration 16807 => Loss: 6.71965536108156413775\n",
      "Iteration 16808 => Loss: 6.71965246698932983094\n",
      "Iteration 16809 => Loss: 6.71964957328707068740\n",
      "Iteration 16810 => Loss: 6.71964667997474318639\n",
      "Iteration 16811 => Loss: 6.71964378705229048450\n",
      "Iteration 16812 => Loss: 6.71964089451965218558\n",
      "Iteration 16813 => Loss: 6.71963800237679542704\n",
      "Iteration 16814 => Loss: 6.71963511062364649007\n",
      "Iteration 16815 => Loss: 6.71963221926016096575\n",
      "Iteration 16816 => Loss: 6.71962932828629000426\n",
      "Iteration 16817 => Loss: 6.71962643770197765036\n",
      "Iteration 16818 => Loss: 6.71962354750716528429\n",
      "Iteration 16819 => Loss: 6.71962065770181560254\n",
      "Iteration 16820 => Loss: 6.71961776828586110355\n",
      "Iteration 16821 => Loss: 6.71961487925926359566\n",
      "Iteration 16822 => Loss: 6.71961199062195824183\n",
      "Iteration 16823 => Loss: 6.71960910237389352773\n",
      "Iteration 16824 => Loss: 6.71960621451502060353\n",
      "Iteration 16825 => Loss: 6.71960332704529061942\n",
      "Iteration 16826 => Loss: 6.71960043996464673199\n",
      "Iteration 16827 => Loss: 6.71959755327302943329\n",
      "Iteration 16828 => Loss: 6.71959466697040586070\n",
      "Iteration 16829 => Loss: 6.71959178105670673631\n",
      "Iteration 16830 => Loss: 6.71958889553188587485\n",
      "Iteration 16831 => Loss: 6.71958601039588820925\n",
      "Iteration 16832 => Loss: 6.71958312564866311334\n",
      "Iteration 16833 => Loss: 6.71958024129015907278\n",
      "Iteration 16834 => Loss: 6.71957735732032368503\n",
      "Iteration 16835 => Loss: 6.71957447373910365940\n",
      "Iteration 16836 => Loss: 6.71957159054644481699\n",
      "Iteration 16837 => Loss: 6.71956870774229120258\n",
      "Iteration 16838 => Loss: 6.71956582532660107177\n",
      "Iteration 16839 => Loss: 6.71956294329932024567\n",
      "Iteration 16840 => Loss: 6.71956006166038743999\n",
      "Iteration 16841 => Loss: 6.71955718040975380489\n",
      "Iteration 16842 => Loss: 6.71955429954737404330\n",
      "Iteration 16843 => Loss: 6.71955141907318775907\n",
      "Iteration 16844 => Loss: 6.71954853898715054328\n",
      "Iteration 16845 => Loss: 6.71954565928919578255\n",
      "Iteration 16846 => Loss: 6.71954277997929061428\n",
      "Iteration 16847 => Loss: 6.71953990105736131966\n",
      "Iteration 16848 => Loss: 6.71953702252337770062\n",
      "Iteration 16849 => Loss: 6.71953414437727314379\n",
      "Iteration 16850 => Loss: 6.71953126661899702299\n",
      "Iteration 16851 => Loss: 6.71952838924850404112\n",
      "Iteration 16852 => Loss: 6.71952551226573380205\n",
      "Iteration 16853 => Loss: 6.71952263567063479144\n",
      "Iteration 16854 => Loss: 6.71951975946315815946\n",
      "Iteration 16855 => Loss: 6.71951688364325061542\n",
      "Iteration 16856 => Loss: 6.71951400821086686221\n",
      "Iteration 16857 => Loss: 6.71951113316594117464\n",
      "Iteration 16858 => Loss: 6.71950825850843003195\n",
      "Iteration 16859 => Loss: 6.71950538423828014345\n",
      "Iteration 16860 => Loss: 6.71950251035543377753\n",
      "Iteration 16861 => Loss: 6.71949963685984652528\n",
      "Iteration 16862 => Loss: 6.71949676375145976692\n",
      "Iteration 16863 => Loss: 6.71949389103022731717\n",
      "Iteration 16864 => Loss: 6.71949101869609943805\n",
      "Iteration 16865 => Loss: 6.71948814674901129251\n",
      "Iteration 16866 => Loss: 6.71948527518891935983\n",
      "Iteration 16867 => Loss: 6.71948240401577567837\n",
      "Iteration 16868 => Loss: 6.71947953322951452293\n",
      "Iteration 16869 => Loss: 6.71947666283009859001\n",
      "Iteration 16870 => Loss: 6.71947379281747014801\n",
      "Iteration 16871 => Loss: 6.71947092319157324170\n",
      "Iteration 16872 => Loss: 6.71946805395235280400\n",
      "Iteration 16873 => Loss: 6.71946518509977508415\n",
      "Iteration 16874 => Loss: 6.71946231663376458698\n",
      "Iteration 16875 => Loss: 6.71945944855428578535\n",
      "Iteration 16876 => Loss: 6.71945658086127739494\n",
      "Iteration 16877 => Loss: 6.71945371355469056596\n",
      "Iteration 16878 => Loss: 6.71945084663448266582\n",
      "Iteration 16879 => Loss: 6.71944798010058352844\n",
      "Iteration 16880 => Loss: 6.71944511395294874490\n",
      "Iteration 16881 => Loss: 6.71944224819152857719\n",
      "Iteration 16882 => Loss: 6.71943938281627950460\n",
      "Iteration 16883 => Loss: 6.71943651782712869647\n",
      "Iteration 16884 => Loss: 6.71943365322403973749\n",
      "Iteration 16885 => Loss: 6.71943078900695578426\n",
      "Iteration 16886 => Loss: 6.71942792517582088152\n",
      "Iteration 16887 => Loss: 6.71942506173059861396\n",
      "Iteration 16888 => Loss: 6.71942219867121703913\n",
      "Iteration 16889 => Loss: 6.71941933599763263629\n",
      "Iteration 16890 => Loss: 6.71941647370979389109\n",
      "Iteration 16891 => Loss: 6.71941361180765461825\n",
      "Iteration 16892 => Loss: 6.71941075029115442163\n",
      "Iteration 16893 => Loss: 6.71940788916024533961\n",
      "Iteration 16894 => Loss: 6.71940502841486875241\n",
      "Iteration 16895 => Loss: 6.71940216805498291563\n",
      "Iteration 16896 => Loss: 6.71939930808052565681\n",
      "Iteration 16897 => Loss: 6.71939644849144990246\n",
      "Iteration 16898 => Loss: 6.71939358928770857915\n",
      "Iteration 16899 => Loss: 6.71939073046924129073\n",
      "Iteration 16900 => Loss: 6.71938787203600451647\n",
      "Iteration 16901 => Loss: 6.71938501398794230113\n",
      "Iteration 16902 => Loss: 6.71938215632499691310\n",
      "Iteration 16903 => Loss: 6.71937929904712927254\n",
      "Iteration 16904 => Loss: 6.71937644215426832517\n",
      "Iteration 16905 => Loss: 6.71937358564638653746\n",
      "Iteration 16906 => Loss: 6.71937072952341285514\n",
      "Iteration 16907 => Loss: 6.71936787378530020476\n",
      "Iteration 16908 => Loss: 6.71936501843200151285\n",
      "Iteration 16909 => Loss: 6.71936216346346348871\n",
      "Iteration 16910 => Loss: 6.71935930887962928892\n",
      "Iteration 16911 => Loss: 6.71935645468044651096\n",
      "Iteration 16912 => Loss: 6.71935360086587518680\n",
      "Iteration 16913 => Loss: 6.71935074743585580848\n",
      "Iteration 16914 => Loss: 6.71934789439033330893\n",
      "Iteration 16915 => Loss: 6.71934504172926061472\n",
      "Iteration 16916 => Loss: 6.71934218945258088240\n",
      "Iteration 16917 => Loss: 6.71933933756024703854\n",
      "Iteration 16918 => Loss: 6.71933648605220934513\n",
      "Iteration 16919 => Loss: 6.71933363492840474152\n",
      "Iteration 16920 => Loss: 6.71933078418879681237\n",
      "Iteration 16921 => Loss: 6.71932793383332427339\n",
      "Iteration 16922 => Loss: 6.71932508386193738659\n",
      "Iteration 16923 => Loss: 6.71932223427458819032\n",
      "Iteration 16924 => Loss: 6.71931938507120740667\n",
      "Iteration 16925 => Loss: 6.71931653625176661393\n",
      "Iteration 16926 => Loss: 6.71931368781620719233\n",
      "Iteration 16927 => Loss: 6.71931083976447141026\n",
      "Iteration 16928 => Loss: 6.71930799209651219428\n",
      "Iteration 16929 => Loss: 6.71930514481227003643\n",
      "Iteration 16930 => Loss: 6.71930229791170763320\n",
      "Iteration 16931 => Loss: 6.71929945139475925941\n",
      "Iteration 16932 => Loss: 6.71929660526138672338\n",
      "Iteration 16933 => Loss: 6.71929375951152341173\n",
      "Iteration 16934 => Loss: 6.71929091414513024461\n",
      "Iteration 16935 => Loss: 6.71928806916214504952\n",
      "Iteration 16936 => Loss: 6.71928522456252785844\n",
      "Iteration 16937 => Loss: 6.71928238034621649888\n",
      "Iteration 16938 => Loss: 6.71927953651316389738\n",
      "Iteration 16939 => Loss: 6.71927669306332120414\n",
      "Iteration 16940 => Loss: 6.71927384999663424026\n",
      "Iteration 16941 => Loss: 6.71927100731304527415\n",
      "Iteration 16942 => Loss: 6.71926816501251167324\n",
      "Iteration 16943 => Loss: 6.71926532309497126505\n",
      "Iteration 16944 => Loss: 6.71926248156038319337\n",
      "Iteration 16945 => Loss: 6.71925964040870749017\n",
      "Iteration 16946 => Loss: 6.71925679963986066667\n",
      "Iteration 16947 => Loss: 6.71925395925380541939\n",
      "Iteration 16948 => Loss: 6.71925111925050178030\n",
      "Iteration 16949 => Loss: 6.71924827962988402419\n",
      "Iteration 16950 => Loss: 6.71924544039190685396\n",
      "Iteration 16951 => Loss: 6.71924260153651786709\n",
      "Iteration 16952 => Loss: 6.71923976306366643740\n",
      "Iteration 16953 => Loss: 6.71923692497329128059\n",
      "Iteration 16954 => Loss: 6.71923408726535775770\n",
      "Iteration 16955 => Loss: 6.71923124993980191988\n",
      "Iteration 16956 => Loss: 6.71922841299657580549\n",
      "Iteration 16957 => Loss: 6.71922557643562967655\n",
      "Iteration 16958 => Loss: 6.71922274025690846599\n",
      "Iteration 16959 => Loss: 6.71921990446036510036\n",
      "Iteration 16960 => Loss: 6.71921706904594362442\n",
      "Iteration 16961 => Loss: 6.71921423401359962924\n",
      "Iteration 16962 => Loss: 6.71921139936326827780\n",
      "Iteration 16963 => Loss: 6.71920856509491137842\n",
      "Iteration 16964 => Loss: 6.71920573120846675863\n",
      "Iteration 16965 => Loss: 6.71920289770389178585\n",
      "Iteration 16966 => Loss: 6.71920006458113316938\n",
      "Iteration 16967 => Loss: 6.71919723184013584216\n",
      "Iteration 16968 => Loss: 6.71919439948085539527\n",
      "Iteration 16969 => Loss: 6.71919156750323054439\n",
      "Iteration 16970 => Loss: 6.71918873590721599243\n",
      "Iteration 16971 => Loss: 6.71918590469276022503\n",
      "Iteration 16972 => Loss: 6.71918307385980817514\n",
      "Iteration 16973 => Loss: 6.71918024340831987473\n",
      "Iteration 16974 => Loss: 6.71917741333822338134\n",
      "Iteration 16975 => Loss: 6.71917458364948227967\n",
      "Iteration 16976 => Loss: 6.71917175434204771989\n",
      "Iteration 16977 => Loss: 6.71916892541586197041\n",
      "Iteration 16978 => Loss: 6.71916609687086463509\n",
      "Iteration 16979 => Loss: 6.71916326870702107499\n",
      "Iteration 16980 => Loss: 6.71916044092427533485\n",
      "Iteration 16981 => Loss: 6.71915761352256879491\n",
      "Iteration 16982 => Loss: 6.71915478650185882259\n",
      "Iteration 16983 => Loss: 6.71915195986208857448\n",
      "Iteration 16984 => Loss: 6.71914913360320387170\n",
      "Iteration 16985 => Loss: 6.71914630772516030532\n",
      "Iteration 16986 => Loss: 6.71914348222790547283\n",
      "Iteration 16987 => Loss: 6.71914065711138963621\n",
      "Iteration 16988 => Loss: 6.71913783237555151118\n",
      "Iteration 16989 => Loss: 6.71913500802035290604\n",
      "Iteration 16990 => Loss: 6.71913218404573253650\n",
      "Iteration 16991 => Loss: 6.71912936045164688181\n",
      "Iteration 16992 => Loss: 6.71912653723803110495\n",
      "Iteration 16993 => Loss: 6.71912371440485500784\n",
      "Iteration 16994 => Loss: 6.71912089195204842440\n",
      "Iteration 16995 => Loss: 6.71911806987957049841\n",
      "Iteration 16996 => Loss: 6.71911524818736882736\n",
      "Iteration 16997 => Loss: 6.71911242687539012053\n",
      "Iteration 16998 => Loss: 6.71910960594358197540\n",
      "Iteration 16999 => Loss: 6.71910678539189731850\n",
      "Iteration 17000 => Loss: 6.71910396522027308919\n",
      "Iteration 17001 => Loss: 6.71910114542867642484\n",
      "Iteration 17002 => Loss: 6.71909832601704692934\n",
      "Iteration 17003 => Loss: 6.71909550698532953561\n",
      "Iteration 17004 => Loss: 6.71909268833347450567\n",
      "Iteration 17005 => Loss: 6.71908987006143920695\n",
      "Iteration 17006 => Loss: 6.71908705216916235514\n",
      "Iteration 17007 => Loss: 6.71908423465659954132\n",
      "Iteration 17008 => Loss: 6.71908141752369214572\n",
      "Iteration 17009 => Loss: 6.71907860077039309488\n",
      "Iteration 17010 => Loss: 6.71907578439665620351\n",
      "Iteration 17011 => Loss: 6.71907296840242906910\n",
      "Iteration 17012 => Loss: 6.71907015278764507826\n",
      "Iteration 17013 => Loss: 6.71906733755228025018\n",
      "Iteration 17014 => Loss: 6.71906452269626175422\n",
      "Iteration 17015 => Loss: 6.71906170821954784600\n",
      "Iteration 17016 => Loss: 6.71905889412208701117\n",
      "Iteration 17017 => Loss: 6.71905608040381441270\n",
      "Iteration 17018 => Loss: 6.71905326706469629983\n",
      "Iteration 17019 => Loss: 6.71905045410467671729\n",
      "Iteration 17020 => Loss: 6.71904764152370503894\n",
      "Iteration 17021 => Loss: 6.71904482932173507947\n",
      "Iteration 17022 => Loss: 6.71904201749869844917\n",
      "Iteration 17023 => Loss: 6.71903920605455784454\n",
      "Iteration 17024 => Loss: 6.71903639498926352758\n",
      "Iteration 17025 => Loss: 6.71903358430275332580\n",
      "Iteration 17026 => Loss: 6.71903077399498904754\n",
      "Iteration 17027 => Loss: 6.71902796406591473755\n",
      "Iteration 17028 => Loss: 6.71902515451547355241\n",
      "Iteration 17029 => Loss: 6.71902234534362730045\n",
      "Iteration 17030 => Loss: 6.71901953655030848012\n",
      "Iteration 17031 => Loss: 6.71901672813547978791\n",
      "Iteration 17032 => Loss: 6.71901392009908438041\n",
      "Iteration 17033 => Loss: 6.71901111244107429599\n",
      "Iteration 17034 => Loss: 6.71900830516139357940\n",
      "Iteration 17035 => Loss: 6.71900549825999426901\n",
      "Iteration 17036 => Loss: 6.71900269173682307411\n",
      "Iteration 17037 => Loss: 6.71899988559183114489\n",
      "Iteration 17038 => Loss: 6.71899707982497673697\n",
      "Iteration 17039 => Loss: 6.71899427443618968425\n",
      "Iteration 17040 => Loss: 6.71899146942542735417\n",
      "Iteration 17041 => Loss: 6.71898866479264711415\n",
      "Iteration 17042 => Loss: 6.71898586053779478533\n",
      "Iteration 17043 => Loss: 6.71898305666080908338\n",
      "Iteration 17044 => Loss: 6.71898025316165092846\n",
      "Iteration 17045 => Loss: 6.71897745004025548354\n",
      "Iteration 17046 => Loss: 6.71897464729658899785\n",
      "Iteration 17047 => Loss: 6.71897184493058396981\n",
      "Iteration 17048 => Loss: 6.71896904294220576048\n",
      "Iteration 17049 => Loss: 6.71896624133139219737\n",
      "Iteration 17050 => Loss: 6.71896344009810331244\n",
      "Iteration 17051 => Loss: 6.71896063924227249231\n",
      "Iteration 17052 => Loss: 6.71895783876386065714\n",
      "Iteration 17053 => Loss: 6.71895503866281096350\n",
      "Iteration 17054 => Loss: 6.71895223893907722612\n",
      "Iteration 17055 => Loss: 6.71894943959260615429\n",
      "Iteration 17056 => Loss: 6.71894664062334801002\n",
      "Iteration 17057 => Loss: 6.71894384203124772625\n",
      "Iteration 17058 => Loss: 6.71894104381625911770\n",
      "Iteration 17059 => Loss: 6.71893824597833067003\n",
      "Iteration 17060 => Loss: 6.71893544851741353341\n",
      "Iteration 17061 => Loss: 6.71893265143345264079\n",
      "Iteration 17062 => Loss: 6.71892985472640624778\n",
      "Iteration 17063 => Loss: 6.71892705839620241193\n",
      "Iteration 17064 => Loss: 6.71892426244281448788\n",
      "Iteration 17065 => Loss: 6.71892146686618030316\n",
      "Iteration 17066 => Loss: 6.71891867166624390251\n",
      "Iteration 17067 => Loss: 6.71891587684296798244\n",
      "Iteration 17068 => Loss: 6.71891308239628681775\n",
      "Iteration 17069 => Loss: 6.71891028832616754585\n",
      "Iteration 17070 => Loss: 6.71890749463254266516\n",
      "Iteration 17071 => Loss: 6.71890470131536865495\n",
      "Iteration 17072 => Loss: 6.71890190837460021811\n",
      "Iteration 17073 => Loss: 6.71889911581017607034\n",
      "Iteration 17074 => Loss: 6.71889632362204736182\n",
      "Iteration 17075 => Loss: 6.71889353181017234817\n",
      "Iteration 17076 => Loss: 6.71889074037449152144\n",
      "Iteration 17077 => Loss: 6.71888794931495780816\n",
      "Iteration 17078 => Loss: 6.71888515863151258856\n",
      "Iteration 17079 => Loss: 6.71888236832412211186\n",
      "Iteration 17080 => Loss: 6.71887957839272509375\n",
      "Iteration 17081 => Loss: 6.71887678883726291446\n",
      "Iteration 17082 => Loss: 6.71887399965770182320\n",
      "Iteration 17083 => Loss: 6.71887121085397609477\n",
      "Iteration 17084 => Loss: 6.71886842242605286657\n",
      "Iteration 17085 => Loss: 6.71886563437385753161\n",
      "Iteration 17086 => Loss: 6.71886284669735278641\n",
      "Iteration 17087 => Loss: 6.71886005939649688656\n",
      "Iteration 17088 => Loss: 6.71885727247122588324\n",
      "Iteration 17089 => Loss: 6.71885448592149181479\n",
      "Iteration 17090 => Loss: 6.71885169974724760777\n",
      "Iteration 17091 => Loss: 6.71884891394844352419\n",
      "Iteration 17092 => Loss: 6.71884612852502183244\n",
      "Iteration 17093 => Loss: 6.71884334347693723544\n",
      "Iteration 17094 => Loss: 6.71884055880413821882\n",
      "Iteration 17095 => Loss: 6.71883777450656971553\n",
      "Iteration 17096 => Loss: 6.71883499058419353389\n",
      "Iteration 17097 => Loss: 6.71883220703695016596\n",
      "Iteration 17098 => Loss: 6.71882942386478010377\n",
      "Iteration 17099 => Loss: 6.71882664106765670198\n",
      "Iteration 17100 => Loss: 6.71882385864550801813\n",
      "Iteration 17101 => Loss: 6.71882107659828609059\n",
      "Iteration 17102 => Loss: 6.71881829492595539222\n",
      "Iteration 17103 => Loss: 6.71881551362845197417\n",
      "Iteration 17104 => Loss: 6.71881273270571721667\n",
      "Iteration 17105 => Loss: 6.71880995215773246798\n",
      "Iteration 17106 => Loss: 6.71880717198441335114\n",
      "Iteration 17107 => Loss: 6.71880439218572345084\n",
      "Iteration 17108 => Loss: 6.71880161276161036454\n",
      "Iteration 17109 => Loss: 6.71879883371202701881\n",
      "Iteration 17110 => Loss: 6.71879605503692722834\n",
      "Iteration 17111 => Loss: 6.71879327673624526795\n",
      "Iteration 17112 => Loss: 6.71879049880994827504\n",
      "Iteration 17113 => Loss: 6.71878772125797052439\n",
      "Iteration 17114 => Loss: 6.71878494408027027163\n",
      "Iteration 17115 => Loss: 6.71878216727679333786\n",
      "Iteration 17116 => Loss: 6.71877939084749087328\n",
      "Iteration 17117 => Loss: 6.71877661479231402808\n",
      "Iteration 17118 => Loss: 6.71877383911121039972\n",
      "Iteration 17119 => Loss: 6.71877106380413025022\n",
      "Iteration 17120 => Loss: 6.71876828887102650611\n",
      "Iteration 17121 => Loss: 6.71876551431184232399\n",
      "Iteration 17122 => Loss: 6.71876274012652796586\n",
      "Iteration 17123 => Loss: 6.71875996631503191736\n",
      "Iteration 17124 => Loss: 6.71875719287731332230\n",
      "Iteration 17125 => Loss: 6.71875441981331089636\n",
      "Iteration 17126 => Loss: 6.71875164712298911240\n",
      "Iteration 17127 => Loss: 6.71874887480627780434\n",
      "Iteration 17128 => Loss: 6.71874610286313789231\n",
      "Iteration 17129 => Loss: 6.71874333129351963834\n",
      "Iteration 17130 => Loss: 6.71874056009736975170\n",
      "Iteration 17131 => Loss: 6.71873778927463849442\n",
      "Iteration 17132 => Loss: 6.71873501882527968121\n",
      "Iteration 17133 => Loss: 6.71873224874923291594\n",
      "Iteration 17134 => Loss: 6.71872947904645467787\n",
      "Iteration 17135 => Loss: 6.71872670971689611719\n",
      "Iteration 17136 => Loss: 6.71872394076050838407\n",
      "Iteration 17137 => Loss: 6.71872117217723818783\n",
      "Iteration 17138 => Loss: 6.71871840396702335596\n",
      "Iteration 17139 => Loss: 6.71871563612983546676\n",
      "Iteration 17140 => Loss: 6.71871286866561323592\n",
      "Iteration 17141 => Loss: 6.71871010157429804366\n",
      "Iteration 17142 => Loss: 6.71870733485585791556\n",
      "Iteration 17143 => Loss: 6.71870456851023067912\n",
      "Iteration 17144 => Loss: 6.71870180253737370180\n",
      "Iteration 17145 => Loss: 6.71869903693722658744\n",
      "Iteration 17146 => Loss: 6.71869627170974759167\n",
      "Iteration 17147 => Loss: 6.71869350685487809471\n",
      "Iteration 17148 => Loss: 6.71869074237257191129\n",
      "Iteration 17149 => Loss: 6.71868797826278196794\n",
      "Iteration 17150 => Loss: 6.71868521452546119122\n",
      "Iteration 17151 => Loss: 6.71868245116054740862\n",
      "Iteration 17152 => Loss: 6.71867968816800598120\n",
      "Iteration 17153 => Loss: 6.71867692554777118374\n",
      "Iteration 17154 => Loss: 6.71867416329979949552\n",
      "Iteration 17155 => Loss: 6.71867140142404117853\n",
      "Iteration 17156 => Loss: 6.71866863992045182385\n",
      "Iteration 17157 => Loss: 6.71866587878896659447\n",
      "Iteration 17158 => Loss: 6.71866311802954552235\n",
      "Iteration 17159 => Loss: 6.71866035764214419856\n",
      "Iteration 17160 => Loss: 6.71865759762670311517\n",
      "Iteration 17161 => Loss: 6.71865483798316542874\n",
      "Iteration 17162 => Loss: 6.71865207871149472396\n",
      "Iteration 17163 => Loss: 6.71864931981164126285\n",
      "Iteration 17164 => Loss: 6.71864656128354020836\n",
      "Iteration 17165 => Loss: 6.71864380312716136245\n",
      "Iteration 17166 => Loss: 6.71864104534243988809\n",
      "Iteration 17167 => Loss: 6.71863828792932959999\n",
      "Iteration 17168 => Loss: 6.71863553088778342470\n",
      "Iteration 17169 => Loss: 6.71863277421775162424\n",
      "Iteration 17170 => Loss: 6.71863001791916758521\n",
      "Iteration 17171 => Loss: 6.71862726199200999133\n",
      "Iteration 17172 => Loss: 6.71862450643620245927\n",
      "Iteration 17173 => Loss: 6.71862175125171212642\n",
      "Iteration 17174 => Loss: 6.71861899643848481389\n",
      "Iteration 17175 => Loss: 6.71861624199646545463\n",
      "Iteration 17176 => Loss: 6.71861348792560608700\n",
      "Iteration 17177 => Loss: 6.71861073422586052573\n",
      "Iteration 17178 => Loss: 6.71860798089717725645\n",
      "Iteration 17179 => Loss: 6.71860522793950654119\n",
      "Iteration 17180 => Loss: 6.71860247535279420106\n",
      "Iteration 17181 => Loss: 6.71859972313698783353\n",
      "Iteration 17182 => Loss: 6.71859697129205013511\n",
      "Iteration 17183 => Loss: 6.71859421981792603873\n",
      "Iteration 17184 => Loss: 6.71859146871455870098\n",
      "Iteration 17185 => Loss: 6.71858871798189927205\n",
      "Iteration 17186 => Loss: 6.71858596761990600754\n",
      "Iteration 17187 => Loss: 6.71858321762853094583\n",
      "Iteration 17188 => Loss: 6.71858046800770658535\n",
      "Iteration 17189 => Loss: 6.71857771875739651080\n",
      "Iteration 17190 => Loss: 6.71857496987755098417\n",
      "Iteration 17191 => Loss: 6.71857222136811671476\n",
      "Iteration 17192 => Loss: 6.71856947322904307640\n",
      "Iteration 17193 => Loss: 6.71856672546027944293\n",
      "Iteration 17194 => Loss: 6.71856397806178140542\n",
      "Iteration 17195 => Loss: 6.71856123103348856773\n",
      "Iteration 17196 => Loss: 6.71855848437536540274\n",
      "Iteration 17197 => Loss: 6.71855573808735684338\n",
      "Iteration 17198 => Loss: 6.71855299216940071716\n",
      "Iteration 17199 => Loss: 6.71855024662146149694\n",
      "Iteration 17200 => Loss: 6.71854750144349122110\n",
      "Iteration 17201 => Loss: 6.71854475663542505259\n",
      "Iteration 17202 => Loss: 6.71854201219722924066\n",
      "Iteration 17203 => Loss: 6.71853926812883983644\n",
      "Iteration 17204 => Loss: 6.71853652443021953644\n",
      "Iteration 17205 => Loss: 6.71853378110131149725\n",
      "Iteration 17206 => Loss: 6.71853103814206509270\n",
      "Iteration 17207 => Loss: 6.71852829555243591386\n",
      "Iteration 17208 => Loss: 6.71852555333236889368\n",
      "Iteration 17209 => Loss: 6.71852281148181340598\n",
      "Iteration 17210 => Loss: 6.71852007000072681819\n",
      "Iteration 17211 => Loss: 6.71851732888905317509\n",
      "Iteration 17212 => Loss: 6.71851458814674096232\n",
      "Iteration 17213 => Loss: 6.71851184777375021184\n",
      "Iteration 17214 => Loss: 6.71850910777002585661\n",
      "Iteration 17215 => Loss: 6.71850636813551194138\n",
      "Iteration 17216 => Loss: 6.71850362887016849811\n",
      "Iteration 17217 => Loss: 6.71850088997393601886\n",
      "Iteration 17218 => Loss: 6.71849815144677631196\n",
      "Iteration 17219 => Loss: 6.71849541328863075762\n",
      "Iteration 17220 => Loss: 6.71849267549945228239\n",
      "Iteration 17221 => Loss: 6.71848993807919558918\n",
      "Iteration 17222 => Loss: 6.71848720102779939367\n",
      "Iteration 17223 => Loss: 6.71848446434521484605\n",
      "Iteration 17224 => Loss: 6.71848172803141174825\n",
      "Iteration 17225 => Loss: 6.71847899208631815782\n",
      "Iteration 17226 => Loss: 6.71847625650990121216\n",
      "Iteration 17227 => Loss: 6.71847352130209962695\n",
      "Iteration 17228 => Loss: 6.71847078646286099968\n",
      "Iteration 17229 => Loss: 6.71846805199215069138\n",
      "Iteration 17230 => Loss: 6.71846531788990919409\n",
      "Iteration 17231 => Loss: 6.71846258415608765802\n",
      "Iteration 17232 => Loss: 6.71845985079063723333\n",
      "Iteration 17233 => Loss: 6.71845711779350640569\n",
      "Iteration 17234 => Loss: 6.71845438516464632528\n",
      "Iteration 17235 => Loss: 6.71845165290401791225\n",
      "Iteration 17236 => Loss: 6.71844892101155188868\n",
      "Iteration 17237 => Loss: 6.71844618948721006291\n",
      "Iteration 17238 => Loss: 6.71844345833093914422\n",
      "Iteration 17239 => Loss: 6.71844072754269738823\n",
      "Iteration 17240 => Loss: 6.71843799712242351063\n",
      "Iteration 17241 => Loss: 6.71843526707007665522\n",
      "Iteration 17242 => Loss: 6.71843253738560619581\n",
      "Iteration 17243 => Loss: 6.71842980806896239443\n",
      "Iteration 17244 => Loss: 6.71842707912009018401\n",
      "Iteration 17245 => Loss: 6.71842435053894426744\n",
      "Iteration 17246 => Loss: 6.71842162232547046585\n",
      "Iteration 17247 => Loss: 6.71841889447963680482\n",
      "Iteration 17248 => Loss: 6.71841616700136956553\n",
      "Iteration 17249 => Loss: 6.71841343989063677355\n",
      "Iteration 17250 => Loss: 6.71841071314737803277\n",
      "Iteration 17251 => Loss: 6.71840798677154982244\n",
      "Iteration 17252 => Loss: 6.71840526076309441095\n",
      "Iteration 17253 => Loss: 6.71840253512197893571\n",
      "Iteration 17254 => Loss: 6.71839980984814033604\n",
      "Iteration 17255 => Loss: 6.71839708494153775575\n",
      "Iteration 17256 => Loss: 6.71839436040210369327\n",
      "Iteration 17257 => Loss: 6.71839163622981061508\n",
      "Iteration 17258 => Loss: 6.71838891242460256592\n",
      "Iteration 17259 => Loss: 6.71838618898642270238\n",
      "Iteration 17260 => Loss: 6.71838346591522217466\n",
      "Iteration 17261 => Loss: 6.71838074321096812014\n",
      "Iteration 17262 => Loss: 6.71837802087358859637\n",
      "Iteration 17263 => Loss: 6.71837529890304452351\n",
      "Iteration 17264 => Loss: 6.71837257729928793992\n",
      "Iteration 17265 => Loss: 6.71836985606226555490\n",
      "Iteration 17266 => Loss: 6.71836713519193651223\n",
      "Iteration 17267 => Loss: 6.71836441468823686307\n",
      "Iteration 17268 => Loss: 6.71836169455112663940\n",
      "Iteration 17269 => Loss: 6.71835897478055699139\n",
      "Iteration 17270 => Loss: 6.71835625537648173378\n",
      "Iteration 17271 => Loss: 6.71835353633884047042\n",
      "Iteration 17272 => Loss: 6.71835081766758968058\n",
      "Iteration 17273 => Loss: 6.71834809936268317898\n",
      "Iteration 17274 => Loss: 6.71834538142407033945\n",
      "Iteration 17275 => Loss: 6.71834266385169254221\n",
      "Iteration 17276 => Loss: 6.71833994664551248377\n",
      "Iteration 17277 => Loss: 6.71833722980547598524\n",
      "Iteration 17278 => Loss: 6.71833451333153597318\n",
      "Iteration 17279 => Loss: 6.71833179722363738051\n",
      "Iteration 17280 => Loss: 6.71832908148173757468\n",
      "Iteration 17281 => Loss: 6.71832636610577527136\n",
      "Iteration 17282 => Loss: 6.71832365109571938433\n",
      "Iteration 17283 => Loss: 6.71832093645151573469\n",
      "Iteration 17284 => Loss: 6.71831822217310392631\n",
      "Iteration 17285 => Loss: 6.71831550826043955027\n",
      "Iteration 17286 => Loss: 6.71831279471347730947\n",
      "Iteration 17287 => Loss: 6.71831008153216213685\n",
      "Iteration 17288 => Loss: 6.71830736871645228803\n",
      "Iteration 17289 => Loss: 6.71830465626629891318\n",
      "Iteration 17290 => Loss: 6.71830194418164250436\n",
      "Iteration 17291 => Loss: 6.71829923246243865265\n",
      "Iteration 17292 => Loss: 6.71829652110863850822\n",
      "Iteration 17293 => Loss: 6.71829381012019855035\n",
      "Iteration 17294 => Loss: 6.71829109949706460014\n",
      "Iteration 17295 => Loss: 6.71828838923918336690\n",
      "Iteration 17296 => Loss: 6.71828567934650777715\n",
      "Iteration 17297 => Loss: 6.71828296981899963924\n",
      "Iteration 17298 => Loss: 6.71828026065658878707\n",
      "Iteration 17299 => Loss: 6.71827755185924324621\n",
      "Iteration 17300 => Loss: 6.71827484342691150232\n",
      "Iteration 17301 => Loss: 6.71827213535953848833\n",
      "Iteration 17302 => Loss: 6.71826942765707268990\n",
      "Iteration 17303 => Loss: 6.71826672031947591535\n",
      "Iteration 17304 => Loss: 6.71826401334669576215\n",
      "Iteration 17305 => Loss: 6.71826130673867272236\n",
      "Iteration 17306 => Loss: 6.71825860049536949248\n",
      "Iteration 17307 => Loss: 6.71825589461673189362\n",
      "Iteration 17308 => Loss: 6.71825318910271018780\n",
      "Iteration 17309 => Loss: 6.71825048395326351880\n",
      "Iteration 17310 => Loss: 6.71824777916833415503\n",
      "Iteration 17311 => Loss: 6.71824507474787147032\n",
      "Iteration 17312 => Loss: 6.71824237069182750304\n",
      "Iteration 17313 => Loss: 6.71823966700015340336\n",
      "Iteration 17314 => Loss: 6.71823696367281275599\n",
      "Iteration 17315 => Loss: 6.71823426070973805935\n",
      "Iteration 17316 => Loss: 6.71823155811089023359\n",
      "Iteration 17317 => Loss: 6.71822885587621865255\n",
      "Iteration 17318 => Loss: 6.71822615400566380828\n",
      "Iteration 17319 => Loss: 6.71822345249919816723\n",
      "Iteration 17320 => Loss: 6.71822075135675689239\n",
      "Iteration 17321 => Loss: 6.71821805057829823937\n",
      "Iteration 17322 => Loss: 6.71821535016376270022\n",
      "Iteration 17323 => Loss: 6.71821265011311297144\n",
      "Iteration 17324 => Loss: 6.71820995042629398597\n",
      "Iteration 17325 => Loss: 6.71820725110325689400\n",
      "Iteration 17326 => Loss: 6.71820455214395373389\n",
      "Iteration 17327 => Loss: 6.71820185354834098490\n",
      "Iteration 17328 => Loss: 6.71819915531635469819\n",
      "Iteration 17329 => Loss: 6.71819645744796556386\n",
      "Iteration 17330 => Loss: 6.71819375994310341582\n",
      "Iteration 17331 => Loss: 6.71819106280173627965\n",
      "Iteration 17332 => Loss: 6.71818836602380642375\n",
      "Iteration 17333 => Loss: 6.71818566960927476828\n",
      "Iteration 17334 => Loss: 6.71818297355808002891\n",
      "Iteration 17335 => Loss: 6.71818027787017513219\n",
      "Iteration 17336 => Loss: 6.71817758254552277464\n",
      "Iteration 17337 => Loss: 6.71817488758405900739\n",
      "Iteration 17338 => Loss: 6.71817219298573942154\n",
      "Iteration 17339 => Loss: 6.71816949875052671359\n",
      "Iteration 17340 => Loss: 6.71816680487835071744\n",
      "Iteration 17341 => Loss: 6.71816411136919100500\n",
      "Iteration 17342 => Loss: 6.71816141822296675201\n",
      "Iteration 17343 => Loss: 6.71815872543965131314\n",
      "Iteration 17344 => Loss: 6.71815603301918251589\n",
      "Iteration 17345 => Loss: 6.71815334096152483312\n",
      "Iteration 17346 => Loss: 6.71815064926661786870\n",
      "Iteration 17347 => Loss: 6.71814795793441543736\n",
      "Iteration 17348 => Loss: 6.71814526696486957746\n",
      "Iteration 17349 => Loss: 6.71814257635793765644\n",
      "Iteration 17350 => Loss: 6.71813988611356283087\n",
      "Iteration 17351 => Loss: 6.71813719623170424455\n",
      "Iteration 17352 => Loss: 6.71813450671229439592\n",
      "Iteration 17353 => Loss: 6.71813181755530486328\n",
      "Iteration 17354 => Loss: 6.71812912876067613865\n",
      "Iteration 17355 => Loss: 6.71812644032836825403\n",
      "Iteration 17356 => Loss: 6.71812375225832614234\n",
      "Iteration 17357 => Loss: 6.71812106455049828924\n",
      "Iteration 17358 => Loss: 6.71811837720484117398\n",
      "Iteration 17359 => Loss: 6.71811569022130239404\n",
      "Iteration 17360 => Loss: 6.71811300359983487596\n",
      "Iteration 17361 => Loss: 6.71811031734039154628\n",
      "Iteration 17362 => Loss: 6.71810763144292266702\n",
      "Iteration 17363 => Loss: 6.71810494590738205289\n",
      "Iteration 17364 => Loss: 6.71810226073370753141\n",
      "Iteration 17365 => Loss: 6.71809957592186979269\n",
      "Iteration 17366 => Loss: 6.71809689147180133517\n",
      "Iteration 17367 => Loss: 6.71809420738347018442\n",
      "Iteration 17368 => Loss: 6.71809152365682038521\n",
      "Iteration 17369 => Loss: 6.71808884029179864683\n",
      "Iteration 17370 => Loss: 6.71808615728836056036\n",
      "Iteration 17371 => Loss: 6.71808347464645816416\n",
      "Iteration 17372 => Loss: 6.71808079236605060203\n",
      "Iteration 17373 => Loss: 6.71807811044706948422\n",
      "Iteration 17374 => Loss: 6.71807542888948194815\n",
      "Iteration 17375 => Loss: 6.71807274769323026220\n",
      "Iteration 17376 => Loss: 6.71807006685827534653\n",
      "Iteration 17377 => Loss: 6.71806738638456035773\n",
      "Iteration 17378 => Loss: 6.71806470627204177504\n",
      "Iteration 17379 => Loss: 6.71806202652066453140\n",
      "Iteration 17380 => Loss: 6.71805934713038332973\n",
      "Iteration 17381 => Loss: 6.71805666810115464926\n",
      "Iteration 17382 => Loss: 6.71805398943292253477\n",
      "Iteration 17383 => Loss: 6.71805131112564080098\n",
      "Iteration 17384 => Loss: 6.71804863317926326260\n",
      "Iteration 17385 => Loss: 6.71804595559374018165\n",
      "Iteration 17386 => Loss: 6.71804327836902004378\n",
      "Iteration 17387 => Loss: 6.71804060150506110460\n",
      "Iteration 17388 => Loss: 6.71803792500179763891\n",
      "Iteration 17389 => Loss: 6.71803524885920388954\n",
      "Iteration 17390 => Loss: 6.71803257307722390124\n",
      "Iteration 17391 => Loss: 6.71802989765579994241\n",
      "Iteration 17392 => Loss: 6.71802722259488138690\n",
      "Iteration 17393 => Loss: 6.71802454789443981298\n",
      "Iteration 17394 => Loss: 6.71802187355440239003\n",
      "Iteration 17395 => Loss: 6.71801919957474513723\n",
      "Iteration 17396 => Loss: 6.71801652595540055302\n",
      "Iteration 17397 => Loss: 6.71801385269632245212\n",
      "Iteration 17398 => Loss: 6.71801117979747530740\n",
      "Iteration 17399 => Loss: 6.71800850725879339365\n",
      "Iteration 17400 => Loss: 6.71800583508024384827\n",
      "Iteration 17401 => Loss: 6.71800316326176716331\n",
      "Iteration 17402 => Loss: 6.71800049180331271259\n",
      "Iteration 17403 => Loss: 6.71799782070484230445\n",
      "Iteration 17404 => Loss: 6.71799514996630797725\n",
      "Iteration 17405 => Loss: 6.71799247958764844668\n",
      "Iteration 17406 => Loss: 6.71798980956882552107\n",
      "Iteration 17407 => Loss: 6.71798713990978590971\n",
      "Iteration 17408 => Loss: 6.71798447061048431550\n",
      "Iteration 17409 => Loss: 6.71798180167086922410\n",
      "Iteration 17410 => Loss: 6.71797913309089711476\n",
      "Iteration 17411 => Loss: 6.71797646487051292041\n",
      "Iteration 17412 => Loss: 6.71797379700967489669\n",
      "Iteration 17413 => Loss: 6.71797112950832797651\n",
      "Iteration 17414 => Loss: 6.71796846236643308004\n",
      "Iteration 17415 => Loss: 6.71796579558393069931\n",
      "Iteration 17416 => Loss: 6.71796312916077553723\n",
      "Iteration 17417 => Loss: 6.71796046309692584941\n",
      "Iteration 17418 => Loss: 6.71795779739232745698\n",
      "Iteration 17419 => Loss: 6.71795513204693417464\n",
      "Iteration 17420 => Loss: 6.71795246706068915898\n",
      "Iteration 17421 => Loss: 6.71794980243356221195\n",
      "Iteration 17422 => Loss: 6.71794713816548760832\n",
      "Iteration 17423 => Loss: 6.71794447425641916283\n",
      "Iteration 17424 => Loss: 6.71794181070631868380\n",
      "Iteration 17425 => Loss: 6.71793914751512843964\n",
      "Iteration 17426 => Loss: 6.71793648468280668595\n",
      "Iteration 17427 => Loss: 6.71793382220930279658\n",
      "Iteration 17428 => Loss: 6.71793116009456436899\n",
      "Iteration 17429 => Loss: 6.71792849833854788244\n",
      "Iteration 17430 => Loss: 6.71792583694119738169\n",
      "Iteration 17431 => Loss: 6.71792317590248089232\n",
      "Iteration 17432 => Loss: 6.71792051522233713001\n",
      "Iteration 17433 => Loss: 6.71791785490071724496\n",
      "Iteration 17434 => Loss: 6.71791519493757505188\n",
      "Iteration 17435 => Loss: 6.71791253533286170097\n",
      "Iteration 17436 => Loss: 6.71790987608653189511\n",
      "Iteration 17437 => Loss: 6.71790721719853500815\n",
      "Iteration 17438 => Loss: 6.71790455866882929570\n",
      "Iteration 17439 => Loss: 6.71790190049735791433\n",
      "Iteration 17440 => Loss: 6.71789924268406846153\n",
      "Iteration 17441 => Loss: 6.71789658522892541015\n",
      "Iteration 17442 => Loss: 6.71789392813187813402\n",
      "Iteration 17443 => Loss: 6.71789127139286890156\n",
      "Iteration 17444 => Loss: 6.71788861501185596836\n",
      "Iteration 17445 => Loss: 6.71788595898879137280\n",
      "Iteration 17446 => Loss: 6.71788330332362626507\n",
      "Iteration 17447 => Loss: 6.71788064801631268352\n",
      "Iteration 17448 => Loss: 6.71787799306680266653\n",
      "Iteration 17449 => Loss: 6.71787533847504292339\n",
      "Iteration 17450 => Loss: 6.71787268424099970332\n",
      "Iteration 17451 => Loss: 6.71787003036460284022\n",
      "Iteration 17452 => Loss: 6.71786737684582568875\n",
      "Iteration 17453 => Loss: 6.71786472368460518823\n",
      "Iteration 17454 => Loss: 6.71786207088089692974\n",
      "Iteration 17455 => Loss: 6.71785941843465916889\n",
      "Iteration 17456 => Loss: 6.71785676634583595046\n",
      "Iteration 17457 => Loss: 6.71785411461438197733\n",
      "Iteration 17458 => Loss: 6.71785146324024928788\n",
      "Iteration 17459 => Loss: 6.71784881222338992046\n",
      "Iteration 17460 => Loss: 6.71784616156374791984\n",
      "Iteration 17461 => Loss: 6.71784351126129219978\n",
      "Iteration 17462 => Loss: 6.71784086131595881142\n",
      "Iteration 17463 => Loss: 6.71783821172771400398\n",
      "Iteration 17464 => Loss: 6.71783556249649560499\n",
      "Iteration 17465 => Loss: 6.71783291362226275822\n",
      "Iteration 17466 => Loss: 6.71783026510496839023\n",
      "Iteration 17467 => Loss: 6.71782761694455832213\n",
      "Iteration 17468 => Loss: 6.71782496914098725682\n",
      "Iteration 17469 => Loss: 6.71782232169420812085\n",
      "Iteration 17470 => Loss: 6.71781967460417384075\n",
      "Iteration 17471 => Loss: 6.71781702787083734307\n",
      "Iteration 17472 => Loss: 6.71781438149414711347\n",
      "Iteration 17473 => Loss: 6.71781173547405163760\n",
      "Iteration 17474 => Loss: 6.71780908981051538831\n",
      "Iteration 17475 => Loss: 6.71780644450347441676\n",
      "Iteration 17476 => Loss: 6.71780379955289497218\n",
      "Iteration 17477 => Loss: 6.71780115495872287568\n",
      "Iteration 17478 => Loss: 6.71779851072090572472\n",
      "Iteration 17479 => Loss: 6.71779586683939555769\n",
      "Iteration 17480 => Loss: 6.71779322331416040015\n",
      "Iteration 17481 => Loss: 6.71779058014513807962\n",
      "Iteration 17482 => Loss: 6.71778793733227619356\n",
      "Iteration 17483 => Loss: 6.71778529487553655031\n",
      "Iteration 17484 => Loss: 6.71778265277487385276\n",
      "Iteration 17485 => Loss: 6.71778001103022770479\n",
      "Iteration 17486 => Loss: 6.71777736964155369748\n",
      "Iteration 17487 => Loss: 6.71777472860881719185\n",
      "Iteration 17488 => Loss: 6.71777208793195423908\n",
      "Iteration 17489 => Loss: 6.71776944761092487113\n",
      "Iteration 17490 => Loss: 6.71776680764567579729\n",
      "Iteration 17491 => Loss: 6.71776416803616349682\n",
      "Iteration 17492 => Loss: 6.71776152878233379084\n",
      "Iteration 17493 => Loss: 6.71775888988415470493\n",
      "Iteration 17494 => Loss: 6.71775625134156495477\n",
      "Iteration 17495 => Loss: 6.71775361315450947330\n",
      "Iteration 17496 => Loss: 6.71775097532296161518\n",
      "Iteration 17497 => Loss: 6.71774833784685121429\n",
      "Iteration 17498 => Loss: 6.71774570072614984895\n",
      "Iteration 17499 => Loss: 6.71774306396079801118\n",
      "Iteration 17500 => Loss: 6.71774042755074685118\n",
      "Iteration 17501 => Loss: 6.71773779149595728910\n",
      "Iteration 17502 => Loss: 6.71773515579637514605\n",
      "Iteration 17503 => Loss: 6.71773252045194979587\n",
      "Iteration 17504 => Loss: 6.71772988546264038234\n",
      "Iteration 17505 => Loss: 6.71772725082839361477\n",
      "Iteration 17506 => Loss: 6.71772461654916508422\n",
      "Iteration 17507 => Loss: 6.71772198262491126997\n",
      "Iteration 17508 => Loss: 6.71771934905557266404\n",
      "Iteration 17509 => Loss: 6.71771671584110574571\n",
      "Iteration 17510 => Loss: 6.71771408298147409965\n",
      "Iteration 17511 => Loss: 6.71771145047661022431\n",
      "Iteration 17512 => Loss: 6.71770881832647948073\n",
      "Iteration 17513 => Loss: 6.71770618653103657181\n",
      "Iteration 17514 => Loss: 6.71770355509022287777\n",
      "Iteration 17515 => Loss: 6.71770092400399665422\n",
      "Iteration 17516 => Loss: 6.71769829327231171590\n",
      "Iteration 17517 => Loss: 6.71769566289512010115\n",
      "Iteration 17518 => Loss: 6.71769303287236585476\n",
      "Iteration 17519 => Loss: 6.71769040320400723232\n",
      "Iteration 17520 => Loss: 6.71768777389000426581\n",
      "Iteration 17521 => Loss: 6.71768514493029478274\n",
      "Iteration 17522 => Loss: 6.71768251632483881508\n",
      "Iteration 17523 => Loss: 6.71767988807359284209\n",
      "Iteration 17524 => Loss: 6.71767726017649557946\n",
      "Iteration 17525 => Loss: 6.71767463263351327640\n",
      "Iteration 17526 => Loss: 6.71767200544459797129\n",
      "Iteration 17527 => Loss: 6.71766937860968660345\n",
      "Iteration 17528 => Loss: 6.71766675212874098122\n",
      "Iteration 17529 => Loss: 6.71766412600172202474\n",
      "Iteration 17530 => Loss: 6.71766150022857111423\n",
      "Iteration 17531 => Loss: 6.71765887480923939989\n",
      "Iteration 17532 => Loss: 6.71765624974369135458\n",
      "Iteration 17533 => Loss: 6.71765362503186391763\n",
      "Iteration 17534 => Loss: 6.71765100067371268011\n",
      "Iteration 17535 => Loss: 6.71764837666919945036\n",
      "Iteration 17536 => Loss: 6.71764575301827182585\n",
      "Iteration 17537 => Loss: 6.71764312972087918041\n",
      "Iteration 17538 => Loss: 6.71764050677697532876\n",
      "Iteration 17539 => Loss: 6.71763788418651941470\n",
      "Iteration 17540 => Loss: 6.71763526194945459480\n",
      "Iteration 17541 => Loss: 6.71763264006572935472\n",
      "Iteration 17542 => Loss: 6.71763001853531793728\n",
      "Iteration 17543 => Loss: 6.71762739735814484732\n",
      "Iteration 17544 => Loss: 6.71762477653417811041\n",
      "Iteration 17545 => Loss: 6.71762215606337242946\n",
      "Iteration 17546 => Loss: 6.71761953594567096104\n",
      "Iteration 17547 => Loss: 6.71761691618103196078\n",
      "Iteration 17548 => Loss: 6.71761429676940746702\n",
      "Iteration 17549 => Loss: 6.71761167771075307087\n",
      "Iteration 17550 => Loss: 6.71760905900500748800\n",
      "Iteration 17551 => Loss: 6.71760644065213075038\n",
      "Iteration 17552 => Loss: 6.71760382265209354813\n",
      "Iteration 17553 => Loss: 6.71760120500482038608\n",
      "Iteration 17554 => Loss: 6.71759858771027307256\n",
      "Iteration 17555 => Loss: 6.71759597076840986318\n",
      "Iteration 17556 => Loss: 6.71759335417917569089\n",
      "Iteration 17557 => Loss: 6.71759073794254124579\n",
      "Iteration 17558 => Loss: 6.71758812205843458543\n",
      "Iteration 17559 => Loss: 6.71758550652681662996\n",
      "Iteration 17560 => Loss: 6.71758289134764741135\n",
      "Iteration 17561 => Loss: 6.71758027652086742165\n",
      "Iteration 17562 => Loss: 6.71757766204643758101\n",
      "Iteration 17563 => Loss: 6.71757504792431525686\n",
      "Iteration 17564 => Loss: 6.71757243415443827672\n",
      "Iteration 17565 => Loss: 6.71756982073677200162\n",
      "Iteration 17566 => Loss: 6.71756720767125603544\n",
      "Iteration 17567 => Loss: 6.71756459495785929192\n",
      "Iteration 17568 => Loss: 6.71756198259651871041\n",
      "Iteration 17569 => Loss: 6.71755937058720764554\n",
      "Iteration 17570 => Loss: 6.71755675892985060216\n",
      "Iteration 17571 => Loss: 6.71755414762441827037\n",
      "Iteration 17572 => Loss: 6.71755153667086357672\n",
      "Iteration 17573 => Loss: 6.71754892606913411868\n",
      "Iteration 17574 => Loss: 6.71754631581918104644\n",
      "Iteration 17575 => Loss: 6.71754370592095551018\n",
      "Iteration 17576 => Loss: 6.71754109637442375913\n",
      "Iteration 17577 => Loss: 6.71753848717952095626\n",
      "Iteration 17578 => Loss: 6.71753587833620535719\n",
      "Iteration 17579 => Loss: 6.71753326984443788206\n",
      "Iteration 17580 => Loss: 6.71753066170416079927\n",
      "Iteration 17581 => Loss: 6.71752805391533858170\n",
      "Iteration 17582 => Loss: 6.71752544647790461596\n",
      "Iteration 17583 => Loss: 6.71752283939183669759\n",
      "Iteration 17584 => Loss: 6.71752023265705666688\n",
      "Iteration 17585 => Loss: 6.71751762627355031299\n",
      "Iteration 17586 => Loss: 6.71751502024125102253\n",
      "Iteration 17587 => Loss: 6.71751241456010639297\n",
      "Iteration 17588 => Loss: 6.71750980923008089718\n",
      "Iteration 17589 => Loss: 6.71750720425112923806\n",
      "Iteration 17590 => Loss: 6.71750459962318835494\n",
      "Iteration 17591 => Loss: 6.71750199534622893793\n",
      "Iteration 17592 => Loss: 6.71749939142020124905\n",
      "Iteration 17593 => Loss: 6.71749678784504578033\n",
      "Iteration 17594 => Loss: 6.71749418462071457014\n",
      "Iteration 17595 => Loss: 6.71749158174718541403\n",
      "Iteration 17596 => Loss: 6.71748897922438370500\n",
      "Iteration 17597 => Loss: 6.71748637705227391592\n",
      "Iteration 17598 => Loss: 6.71748377523080453244\n",
      "Iteration 17599 => Loss: 6.71748117375993114564\n",
      "Iteration 17600 => Loss: 6.71747857263960490570\n",
      "Iteration 17601 => Loss: 6.71747597186978939732\n",
      "Iteration 17602 => Loss: 6.71747337145041623074\n",
      "Iteration 17603 => Loss: 6.71747077138145520792\n",
      "Iteration 17604 => Loss: 6.71746817166285659084\n",
      "Iteration 17605 => Loss: 6.71746557229456708882\n",
      "Iteration 17606 => Loss: 6.71746297327653874021\n",
      "Iteration 17607 => Loss: 6.71746037460873246516\n",
      "Iteration 17608 => Loss: 6.71745777629109497298\n",
      "Iteration 17609 => Loss: 6.71745517832358540744\n",
      "Iteration 17610 => Loss: 6.71745258070614958967\n",
      "Iteration 17611 => Loss: 6.71744998343874222257\n",
      "Iteration 17612 => Loss: 6.71744738652131179180\n",
      "Iteration 17613 => Loss: 6.71744478995382277020\n",
      "Iteration 17614 => Loss: 6.71744219373621831437\n",
      "Iteration 17615 => Loss: 6.71743959786845934445\n",
      "Iteration 17616 => Loss: 6.71743700235048990521\n",
      "Iteration 17617 => Loss: 6.71743440718226825226\n",
      "Iteration 17618 => Loss: 6.71743181236373931853\n",
      "Iteration 17619 => Loss: 6.71742921789487112960\n",
      "Iteration 17620 => Loss: 6.71742662377559973663\n",
      "Iteration 17621 => Loss: 6.71742403000589138884\n",
      "Iteration 17622 => Loss: 6.71742143658569723641\n",
      "Iteration 17623 => Loss: 6.71741884351496221228\n",
      "Iteration 17624 => Loss: 6.71741625079364812478\n",
      "Iteration 17625 => Loss: 6.71741365842169368960\n",
      "Iteration 17626 => Loss: 6.71741106639906782050\n",
      "Iteration 17627 => Loss: 6.71740847472571900312\n",
      "Iteration 17628 => Loss: 6.71740588340159661129\n",
      "Iteration 17629 => Loss: 6.71740329242665801246\n",
      "Iteration 17630 => Loss: 6.71740070180084458684\n",
      "Iteration 17631 => Loss: 6.71739811152412702455\n",
      "Iteration 17632 => Loss: 6.71739552159644404128\n",
      "Iteration 17633 => Loss: 6.71739293201775744535\n",
      "Iteration 17634 => Loss: 6.71739034278801394606\n",
      "Iteration 17635 => Loss: 6.71738775390717091085\n",
      "Iteration 17636 => Loss: 6.71738516537518126626\n",
      "Iteration 17637 => Loss: 6.71738257719199616247\n",
      "Iteration 17638 => Loss: 6.71737998935756674967\n",
      "Iteration 17639 => Loss: 6.71737740187185128349\n",
      "Iteration 17640 => Loss: 6.71737481473479380867\n",
      "Iteration 17641 => Loss: 6.71737222794636235079\n",
      "Iteration 17642 => Loss: 6.71736964150649384919\n",
      "Iteration 17643 => Loss: 6.71736705541514655948\n",
      "Iteration 17644 => Loss: 6.71736446967228584271\n",
      "Iteration 17645 => Loss: 6.71736188427784064459\n",
      "Iteration 17646 => Loss: 6.71735929923178698431\n",
      "Iteration 17647 => Loss: 6.71735671453406446574\n",
      "Iteration 17648 => Loss: 6.71735413018463134449\n",
      "Iteration 17649 => Loss: 6.71735154618344232347\n",
      "Iteration 17650 => Loss: 6.71734896253044055925\n",
      "Iteration 17651 => Loss: 6.71734637922558963652\n",
      "Iteration 17652 => Loss: 6.71734379626883892911\n",
      "Iteration 17653 => Loss: 6.71734121366014225174\n",
      "Iteration 17654 => Loss: 6.71733863139945430731\n",
      "Iteration 17655 => Loss: 6.71733604948672713419\n",
      "Iteration 17656 => Loss: 6.71733346792191010621\n",
      "Iteration 17657 => Loss: 6.71733088670496591988\n",
      "Iteration 17658 => Loss: 6.71732830583582884998\n",
      "Iteration 17659 => Loss: 6.71732572531446958664\n",
      "Iteration 17660 => Loss: 6.71732314514083839185\n",
      "Iteration 17661 => Loss: 6.71732056531488730400\n",
      "Iteration 17662 => Loss: 6.71731798583656214419\n",
      "Iteration 17663 => Loss: 6.71731540670582383257\n",
      "Iteration 17664 => Loss: 6.71731282792262707204\n",
      "Iteration 17665 => Loss: 6.71731024948691413101\n",
      "Iteration 17666 => Loss: 6.71730767139864770598\n",
      "Iteration 17667 => Loss: 6.71730509365778249986\n",
      "Iteration 17668 => Loss: 6.71730251626426344558\n",
      "Iteration 17669 => Loss: 6.71729993921805412782\n",
      "Iteration 17670 => Loss: 6.71729736251910036771\n",
      "Iteration 17671 => Loss: 6.71729478616735331542\n",
      "Iteration 17672 => Loss: 6.71729221016277389111\n",
      "Iteration 17673 => Loss: 6.71728963450531058044\n",
      "Iteration 17674 => Loss: 6.71728705919491275722\n",
      "Iteration 17675 => Loss: 6.71728448423154755886\n",
      "Iteration 17676 => Loss: 6.71728190961515014834\n",
      "Iteration 17677 => Loss: 6.71727933534568233398\n",
      "Iteration 17678 => Loss: 6.71727676142310592411\n",
      "Iteration 17679 => Loss: 6.71727418784735696988\n",
      "Iteration 17680 => Loss: 6.71727161461840083234\n",
      "Iteration 17681 => Loss: 6.71726904173619132621\n",
      "Iteration 17682 => Loss: 6.71726646920067693713\n",
      "Iteration 17683 => Loss: 6.71726389701180881531\n",
      "Iteration 17684 => Loss: 6.71726132516954077545\n",
      "Iteration 17685 => Loss: 6.71725875367383373771\n",
      "Iteration 17686 => Loss: 6.71725618252463618774\n",
      "Iteration 17687 => Loss: 6.71725361172189927572\n",
      "Iteration 17688 => Loss: 6.71725104126557592821\n",
      "Iteration 17689 => Loss: 6.71724847115561995992\n",
      "Iteration 17690 => Loss: 6.71724590139199051464\n",
      "Iteration 17691 => Loss: 6.71724333197464140710\n",
      "Iteration 17692 => Loss: 6.71724076290351934659\n",
      "Iteration 17693 => Loss: 6.71723819417857725966\n",
      "Iteration 17694 => Loss: 6.71723562579976896103\n",
      "Iteration 17695 => Loss: 6.71723305776704737724\n",
      "Iteration 17696 => Loss: 6.71723049008037609298\n",
      "Iteration 17697 => Loss: 6.71722792273970004118\n",
      "Iteration 17698 => Loss: 6.71722535574497126021\n",
      "Iteration 17699 => Loss: 6.71722278909614090026\n",
      "Iteration 17700 => Loss: 6.71722022279317609872\n",
      "Iteration 17701 => Loss: 6.71721765683601379493\n",
      "Iteration 17702 => Loss: 6.71721509122461579722\n",
      "Iteration 17703 => Loss: 6.71721252595893592030\n",
      "Iteration 17704 => Loss: 6.71720996103892353801\n",
      "Iteration 17705 => Loss: 6.71720739646453424143\n",
      "Iteration 17706 => Loss: 6.71720483223572717435\n",
      "Iteration 17707 => Loss: 6.71720226835244105246\n",
      "Iteration 17708 => Loss: 6.71719970481464567769\n",
      "Iteration 17709 => Loss: 6.71719714162228243026\n",
      "Iteration 17710 => Loss: 6.71719457877530867762\n",
      "Iteration 17711 => Loss: 6.71719201627368356355\n",
      "Iteration 17712 => Loss: 6.71718945411735557371\n",
      "Iteration 17713 => Loss: 6.71718689230627674647\n",
      "Iteration 17714 => Loss: 6.71718433084040711378\n",
      "Iteration 17715 => Loss: 6.71718176971968361499\n",
      "Iteration 17716 => Loss: 6.71717920894407871657\n",
      "Iteration 17717 => Loss: 6.71717664851353557509\n",
      "Iteration 17718 => Loss: 6.71717408842802043978\n",
      "Iteration 17719 => Loss: 6.71717152868746669725\n",
      "Iteration 17720 => Loss: 6.71716896929183882037\n",
      "Iteration 17721 => Loss: 6.71716641024109240021\n",
      "Iteration 17722 => Loss: 6.71716385153517236972\n",
      "Iteration 17723 => Loss: 6.71716129317405030719\n",
      "Iteration 17724 => Loss: 6.71715873515765604651\n",
      "Iteration 17725 => Loss: 6.71715617748596383052\n",
      "Iteration 17726 => Loss: 6.71715362015891148673\n",
      "Iteration 17727 => Loss: 6.71715106317645993528\n",
      "Iteration 17728 => Loss: 6.71714850653856476725\n",
      "Iteration 17729 => Loss: 6.71714595024517269195\n",
      "Iteration 17730 => Loss: 6.71714339429624818223\n",
      "Iteration 17731 => Loss: 6.71714083869173084196\n",
      "Iteration 17732 => Loss: 6.71713828343158603218\n",
      "Iteration 17733 => Loss: 6.71713572851576046219\n",
      "Iteration 17734 => Loss: 6.71713317394420617035\n",
      "Iteration 17735 => Loss: 6.71713061971688762952\n",
      "Iteration 17736 => Loss: 6.71712806583374621994\n",
      "Iteration 17737 => Loss: 6.71712551229475263170\n",
      "Iteration 17738 => Loss: 6.71712295909983136966\n",
      "Iteration 17739 => Loss: 6.71712040624896022933\n",
      "Iteration 17740 => Loss: 6.71711785374209302546\n",
      "Iteration 17741 => Loss: 6.71711530157916669737\n",
      "Iteration 17742 => Loss: 6.71711274976014749427\n",
      "Iteration 17743 => Loss: 6.71711019828498923090\n",
      "Iteration 17744 => Loss: 6.71710764715364128108\n",
      "Iteration 17745 => Loss: 6.71710509636605479500\n",
      "Iteration 17746 => Loss: 6.71710254592219335734\n",
      "Iteration 17747 => Loss: 6.71709999582199035473\n",
      "Iteration 17748 => Loss: 6.71709744606542891177\n",
      "Iteration 17749 => Loss: 6.71709489665244063872\n",
      "Iteration 17750 => Loss: 6.71709234758299000845\n",
      "Iteration 17751 => Loss: 6.71708979885702284207\n",
      "Iteration 17752 => Loss: 6.71708725047449739520\n",
      "Iteration 17753 => Loss: 6.71708470243536748256\n",
      "Iteration 17754 => Loss: 6.71708215473958425434\n",
      "Iteration 17755 => Loss: 6.71707960738711395976\n",
      "Iteration 17756 => Loss: 6.71707706037788554454\n",
      "Iteration 17757 => Loss: 6.71707451371187502787\n",
      "Iteration 17758 => Loss: 6.71707196738902112543\n",
      "Iteration 17759 => Loss: 6.71706942140929186280\n",
      "Iteration 17760 => Loss: 6.71706687577262684385\n",
      "Iteration 17761 => Loss: 6.71706433047899054145\n",
      "Iteration 17762 => Loss: 6.71706178552832788853\n",
      "Iteration 17763 => Loss: 6.71705924092059891706\n",
      "Iteration 17764 => Loss: 6.71705669665576099447\n",
      "Iteration 17765 => Loss: 6.71705415273376260643\n",
      "Iteration 17766 => Loss: 6.71705160915455223858\n",
      "Iteration 17767 => Loss: 6.71704906591809614014\n",
      "Iteration 17768 => Loss: 6.71704652302433924405\n",
      "Iteration 17769 => Loss: 6.71704398047323447685\n",
      "Iteration 17770 => Loss: 6.71704143826474187051\n",
      "Iteration 17771 => Loss: 6.71703889639880724616\n",
      "Iteration 17772 => Loss: 6.71703635487539330029\n",
      "Iteration 17773 => Loss: 6.71703381369444674220\n",
      "Iteration 17774 => Loss: 6.71703127285593115658\n",
      "Iteration 17775 => Loss: 6.71702873235978970001\n",
      "Iteration 17776 => Loss: 6.71702619220597973992\n",
      "Iteration 17777 => Loss: 6.71702365239445153833\n",
      "Iteration 17778 => Loss: 6.71702111292517223262\n",
      "Iteration 17779 => Loss: 6.71701857379808231485\n",
      "Iteration 17780 => Loss: 6.71701603501314004063\n",
      "Iteration 17781 => Loss: 6.71701349657030100104\n",
      "Iteration 17782 => Loss: 6.71701095846951190538\n",
      "Iteration 17783 => Loss: 6.71700842071073900286\n",
      "Iteration 17784 => Loss: 6.71700588329392367370\n",
      "Iteration 17785 => Loss: 6.71700334621902506171\n",
      "Iteration 17786 => Loss: 6.71700080948600142250\n",
      "Iteration 17787 => Loss: 6.71699827309479946535\n",
      "Iteration 17788 => Loss: 6.71699573704538366314\n",
      "Iteration 17789 => Loss: 6.71699320133768917884\n",
      "Iteration 17790 => Loss: 6.71699066597168936710\n",
      "Iteration 17791 => Loss: 6.71698813094732827267\n",
      "Iteration 17792 => Loss: 6.71698559626456148663\n",
      "Iteration 17793 => Loss: 6.71698306192334548825\n",
      "Iteration 17794 => Loss: 6.71698052792363675678\n",
      "Iteration 17795 => Loss: 6.71697799426537223155\n",
      "Iteration 17796 => Loss: 6.71697546094852260268\n",
      "Iteration 17797 => Loss: 6.71697292797304168488\n",
      "Iteration 17798 => Loss: 6.71697039533887796381\n",
      "Iteration 17799 => Loss: 6.71696786304598436601\n",
      "Iteration 17800 => Loss: 6.71696533109432447617\n",
      "Iteration 17801 => Loss: 6.71696279948383967451\n",
      "Iteration 17802 => Loss: 6.71696026821448999300\n",
      "Iteration 17803 => Loss: 6.71695773728623013454\n",
      "Iteration 17804 => Loss: 6.71695520669901213751\n",
      "Iteration 17805 => Loss: 6.71695267645279159296\n",
      "Iteration 17806 => Loss: 6.71695014654752053929\n",
      "Iteration 17807 => Loss: 6.71694761698315456755\n",
      "Iteration 17808 => Loss: 6.71694508775964926883\n",
      "Iteration 17809 => Loss: 6.71694255887695579332\n",
      "Iteration 17810 => Loss: 6.71694003033503417299\n",
      "Iteration 17811 => Loss: 6.71693750213382223535\n",
      "Iteration 17812 => Loss: 6.71693497427329599958\n",
      "Iteration 17813 => Loss: 6.71693244675338974048\n",
      "Iteration 17814 => Loss: 6.71692991957407681269\n",
      "Iteration 17815 => Loss: 6.71692739273530037281\n",
      "Iteration 17816 => Loss: 6.71692486623701690007\n",
      "Iteration 17817 => Loss: 6.71692234007917399197\n",
      "Iteration 17818 => Loss: 6.71691981426172901593\n",
      "Iteration 17819 => Loss: 6.71691728878464111574\n",
      "Iteration 17820 => Loss: 6.71691476364786055342\n",
      "Iteration 17821 => Loss: 6.71691223885134647276\n",
      "Iteration 17822 => Loss: 6.71690971439504647122\n",
      "Iteration 17823 => Loss: 6.71690719027891525172\n",
      "Iteration 17824 => Loss: 6.71690466650291018169\n",
      "Iteration 17825 => Loss: 6.71690214306698951674\n",
      "Iteration 17826 => Loss: 6.71689961997109108438\n",
      "Iteration 17827 => Loss: 6.71689709721518912744\n",
      "Iteration 17828 => Loss: 6.71689457479922680250\n",
      "Iteration 17829 => Loss: 6.71689205272315703610\n",
      "Iteration 17830 => Loss: 6.71688953098693897203\n",
      "Iteration 17831 => Loss: 6.71688700959052642503\n",
      "Iteration 17832 => Loss: 6.71688448853386788073\n",
      "Iteration 17833 => Loss: 6.71688196781692870019\n",
      "Iteration 17834 => Loss: 6.71687944743964848726\n",
      "Iteration 17835 => Loss: 6.71687692740199171482\n",
      "Iteration 17836 => Loss: 6.71687440770391397393\n",
      "Iteration 17837 => Loss: 6.71687188834536641480\n",
      "Iteration 17838 => Loss: 6.71686936932629929942\n",
      "Iteration 17839 => Loss: 6.71686685064667354794\n",
      "Iteration 17840 => Loss: 6.71686433230643675785\n",
      "Iteration 17841 => Loss: 6.71686181430554629657\n",
      "Iteration 17842 => Loss: 6.71685929664396397243\n",
      "Iteration 17843 => Loss: 6.71685677932162228387\n",
      "Iteration 17844 => Loss: 6.71685426233850169098\n",
      "Iteration 17845 => Loss: 6.71685174569453824489\n",
      "Iteration 17846 => Loss: 6.71684922938969553030\n",
      "Iteration 17847 => Loss: 6.71684671342393180282\n",
      "Iteration 17848 => Loss: 6.71684419779718488996\n",
      "Iteration 17849 => Loss: 6.71684168250942192913\n",
      "Iteration 17850 => Loss: 6.71683916756059229414\n",
      "Iteration 17851 => Loss: 6.71683665295065601697\n",
      "Iteration 17852 => Loss: 6.71683413867955980692\n",
      "Iteration 17853 => Loss: 6.71683162474726014324\n",
      "Iteration 17854 => Loss: 6.71682911115371972244\n",
      "Iteration 17855 => Loss: 6.71682659789888703017\n",
      "Iteration 17856 => Loss: 6.71682408498270966390\n",
      "Iteration 17857 => Loss: 6.71682157240515120833\n",
      "Iteration 17858 => Loss: 6.71681906016616281363\n",
      "Iteration 17859 => Loss: 6.71681654826569651817\n",
      "Iteration 17860 => Loss: 6.71681403670370880121\n",
      "Iteration 17861 => Loss: 6.71681152548015525383\n",
      "Iteration 17862 => Loss: 6.71680901459499057893\n",
      "Iteration 17863 => Loss: 6.71680650404816859123\n",
      "Iteration 17864 => Loss: 6.71680399383964310545\n",
      "Iteration 17865 => Loss: 6.71680148396936349542\n",
      "Iteration 17866 => Loss: 6.71679897443729512219\n",
      "Iteration 17867 => Loss: 6.71679646524337581326\n",
      "Iteration 17868 => Loss: 6.71679395638758602871\n",
      "Iteration 17869 => Loss: 6.71679144786985649063\n",
      "Iteration 17870 => Loss: 6.71678893969014456644\n",
      "Iteration 17871 => Loss: 6.71678643184842538716\n",
      "Iteration 17872 => Loss: 6.71678392434462345761\n",
      "Iteration 17873 => Loss: 6.71678141717871479699\n",
      "Iteration 17874 => Loss: 6.71677891035064078551\n",
      "Iteration 17875 => Loss: 6.71677640386036411968\n",
      "Iteration 17876 => Loss: 6.71677389770784483147\n",
      "Iteration 17877 => Loss: 6.71677139189302163658\n",
      "Iteration 17878 => Loss: 6.71676888641586344875\n",
      "Iteration 17879 => Loss: 6.71676638127631431274\n",
      "Iteration 17880 => Loss: 6.71676387647433603689\n",
      "Iteration 17881 => Loss: 6.71676137200987088960\n",
      "Iteration 17882 => Loss: 6.71675886788289400187\n",
      "Iteration 17883 => Loss: 6.71675636409334320120\n",
      "Iteration 17884 => Loss: 6.71675386064117585505\n",
      "Iteration 17885 => Loss: 6.71675135752635465991\n",
      "Iteration 17886 => Loss: 6.71674885474882366054\n",
      "Iteration 17887 => Loss: 6.71674635230854732981\n",
      "Iteration 17888 => Loss: 6.71674385020547060066\n",
      "Iteration 17889 => Loss: 6.71674134843955528140\n",
      "Iteration 17890 => Loss: 6.71673884701074275227\n",
      "Iteration 17891 => Loss: 6.71673634591901613788\n",
      "Iteration 17892 => Loss: 6.71673384516429639035\n",
      "Iteration 17893 => Loss: 6.71673134474656663429\n",
      "Iteration 17894 => Loss: 6.71672884466575759177\n",
      "Iteration 17895 => Loss: 6.71672634492184084110\n",
      "Iteration 17896 => Loss: 6.71672384551475509795\n",
      "Iteration 17897 => Loss: 6.71672134644447726970\n",
      "Iteration 17898 => Loss: 6.71671884771093985478\n",
      "Iteration 17899 => Loss: 6.71671634931411443148\n",
      "Iteration 17900 => Loss: 6.71671385125394238003\n",
      "Iteration 17901 => Loss: 6.71671135353038550875\n",
      "Iteration 17902 => Loss: 6.71670885614340029690\n",
      "Iteration 17903 => Loss: 6.71670635909293700649\n",
      "Iteration 17904 => Loss: 6.71670386237894501136\n",
      "Iteration 17905 => Loss: 6.71670136600139144889\n",
      "Iteration 17906 => Loss: 6.71669886996022480474\n",
      "Iteration 17907 => Loss: 6.71669637425540599907\n",
      "Iteration 17908 => Loss: 6.71669387888687463573\n",
      "Iteration 17909 => Loss: 6.71669138385459607576\n",
      "Iteration 17910 => Loss: 6.71668888915852768662\n",
      "Iteration 17911 => Loss: 6.71668639479861351305\n",
      "Iteration 17912 => Loss: 6.71668390077481269884\n",
      "Iteration 17913 => Loss: 6.71668140708708794051\n",
      "Iteration 17914 => Loss: 6.71667891373538861188\n",
      "Iteration 17915 => Loss: 6.71667642071966675132\n",
      "Iteration 17916 => Loss: 6.71667392803987706174\n",
      "Iteration 17917 => Loss: 6.71667143569597246966\n",
      "Iteration 17918 => Loss: 6.71666894368791833614\n",
      "Iteration 17919 => Loss: 6.71666645201565870593\n",
      "Iteration 17920 => Loss: 6.71666396067915894008\n",
      "Iteration 17921 => Loss: 6.71666146967835864245\n",
      "Iteration 17922 => Loss: 6.71665897901322672681\n",
      "Iteration 17923 => Loss: 6.71665648868371256697\n",
      "Iteration 17924 => Loss: 6.71665399868976198405\n",
      "Iteration 17925 => Loss: 6.71665150903134478000\n",
      "Iteration 17926 => Loss: 6.71664901970841032863\n",
      "Iteration 17927 => Loss: 6.71664653072090622743\n",
      "Iteration 17928 => Loss: 6.71664404206880316650\n",
      "Iteration 17929 => Loss: 6.71664155375203453247\n",
      "Iteration 17930 => Loss: 6.71663906577057456815\n",
      "Iteration 17931 => Loss: 6.71663657812437175920\n",
      "Iteration 17932 => Loss: 6.71663409081337015039\n",
      "Iteration 17933 => Loss: 6.71663160383753776728\n",
      "Iteration 17934 => Loss: 6.71662911719683108913\n",
      "Iteration 17935 => Loss: 6.71662663089119593707\n",
      "Iteration 17936 => Loss: 6.71662414492058879034\n",
      "Iteration 17937 => Loss: 6.71662165928496612821\n",
      "Iteration 17938 => Loss: 6.71661917398428798265\n",
      "Iteration 17939 => Loss: 6.71661668901850728020\n",
      "Iteration 17940 => Loss: 6.71661420438756273654\n",
      "Iteration 17941 => Loss: 6.71661172009143125905\n",
      "Iteration 17942 => Loss: 6.71660923613005600430\n",
      "Iteration 17943 => Loss: 6.71660675250339789244\n",
      "Iteration 17944 => Loss: 6.71660426921140629730\n",
      "Iteration 17945 => Loss: 6.71660178625404302721\n",
      "Iteration 17946 => Loss: 6.71659930363125390329\n",
      "Iteration 17947 => Loss: 6.71659682134300162204\n",
      "Iteration 17948 => Loss: 6.71659433938923200458\n",
      "Iteration 17949 => Loss: 6.71659185776990419470\n",
      "Iteration 17950 => Loss: 6.71658937648498444162\n",
      "Iteration 17951 => Loss: 6.71658689553441146103\n",
      "Iteration 17952 => Loss: 6.71658441491815150215\n",
      "Iteration 17953 => Loss: 6.71658193463615216245\n",
      "Iteration 17954 => Loss: 6.71657945468837258574\n",
      "Iteration 17955 => Loss: 6.71657697507476036947\n",
      "Iteration 17956 => Loss: 6.71657449579528265104\n",
      "Iteration 17957 => Loss: 6.71657201684988525159\n",
      "Iteration 17958 => Loss: 6.71656953823852109764\n",
      "Iteration 17959 => Loss: 6.71656705996116176749\n",
      "Iteration 17960 => Loss: 6.71656458201774064776\n",
      "Iteration 17961 => Loss: 6.71656210440822842855\n",
      "Iteration 17962 => Loss: 6.71655962713257004282\n",
      "Iteration 17963 => Loss: 6.71655715019072552252\n",
      "Iteration 17964 => Loss: 6.71655467358265312328\n",
      "Iteration 17965 => Loss: 6.71655219730830044256\n",
      "Iteration 17966 => Loss: 6.71654972136762395962\n",
      "Iteration 17967 => Loss: 6.71654724576058548280\n",
      "Iteration 17968 => Loss: 6.71654477048712994502\n",
      "Iteration 17969 => Loss: 6.71654229554722448370\n",
      "Iteration 17970 => Loss: 6.71653982094081225540\n",
      "Iteration 17971 => Loss: 6.71653734666785062757\n",
      "Iteration 17972 => Loss: 6.71653487272830318489\n",
      "Iteration 17973 => Loss: 6.71653239912212018936\n",
      "Iteration 17974 => Loss: 6.71652992584925190300\n",
      "Iteration 17975 => Loss: 6.71652745290965746960\n",
      "Iteration 17976 => Loss: 6.71652498030328981571\n",
      "Iteration 17977 => Loss: 6.71652250803011252600\n",
      "Iteration 17978 => Loss: 6.71652003609007142160\n",
      "Iteration 17979 => Loss: 6.71651756448312298176\n",
      "Iteration 17980 => Loss: 6.71651509320922190938\n",
      "Iteration 17981 => Loss: 6.71651262226833090097\n",
      "Iteration 17982 => Loss: 6.71651015166040021853\n",
      "Iteration 17983 => Loss: 6.71650768138537834773\n",
      "Iteration 17984 => Loss: 6.71650521144322976141\n",
      "Iteration 17985 => Loss: 6.71650274183390205707\n",
      "Iteration 17986 => Loss: 6.71650027255735881937\n",
      "Iteration 17987 => Loss: 6.71649780361355031033\n",
      "Iteration 17988 => Loss: 6.71649533500242501560\n",
      "Iteration 17989 => Loss: 6.71649286672395362530\n",
      "Iteration 17990 => Loss: 6.71649039877807929599\n",
      "Iteration 17991 => Loss: 6.71648793116476827691\n",
      "Iteration 17992 => Loss: 6.71648546388395573103\n",
      "Iteration 17993 => Loss: 6.71648299693561323664\n",
      "Iteration 17994 => Loss: 6.71648053031969638482\n",
      "Iteration 17995 => Loss: 6.71647806403615366122\n",
      "Iteration 17996 => Loss: 6.71647559808493976874\n",
      "Iteration 17997 => Loss: 6.71647313246602095660\n",
      "Iteration 17998 => Loss: 6.71647066717934038138\n",
      "Iteration 17999 => Loss: 6.71646820222486162777\n",
      "Iteration 18000 => Loss: 6.71646573760252696417\n",
      "Iteration 18001 => Loss: 6.71646327331230885704\n",
      "Iteration 18002 => Loss: 6.71646080935415135116\n",
      "Iteration 18003 => Loss: 6.71645834572801092577\n",
      "Iteration 18004 => Loss: 6.71645588243384317195\n",
      "Iteration 18005 => Loss: 6.71645341947160634533\n",
      "Iteration 18006 => Loss: 6.71645095684125070790\n",
      "Iteration 18007 => Loss: 6.71644849454273806799\n",
      "Iteration 18008 => Loss: 6.71644603257601602309\n",
      "Iteration 18009 => Loss: 6.71644357094105171058\n",
      "Iteration 18010 => Loss: 6.71644110963778651069\n",
      "Iteration 18011 => Loss: 6.71643864866618756082\n",
      "Iteration 18012 => Loss: 6.71643618802619979391\n",
      "Iteration 18013 => Loss: 6.71643372771778679464\n",
      "Iteration 18014 => Loss: 6.71643126774089793685\n",
      "Iteration 18015 => Loss: 6.71642880809549680521\n",
      "Iteration 18016 => Loss: 6.71642634878152389177\n",
      "Iteration 18017 => Loss: 6.71642388979894899848\n",
      "Iteration 18018 => Loss: 6.71642143114772327550\n",
      "Iteration 18019 => Loss: 6.71641897282779964939\n",
      "Iteration 18020 => Loss: 6.71641651483913459941\n",
      "Iteration 18021 => Loss: 6.71641405718168282846\n",
      "Iteration 18022 => Loss: 6.71641159985540259214\n",
      "Iteration 18023 => Loss: 6.71640914286024326429\n",
      "Iteration 18024 => Loss: 6.71640668619616754142\n",
      "Iteration 18025 => Loss: 6.71640422986312835008\n",
      "Iteration 18026 => Loss: 6.71640177386108128132\n",
      "Iteration 18027 => Loss: 6.71639931818997570900\n",
      "Iteration 18028 => Loss: 6.71639686284977521780\n",
      "Iteration 18029 => Loss: 6.71639440784043184607\n",
      "Iteration 18030 => Loss: 6.71639195316190118490\n",
      "Iteration 18031 => Loss: 6.71638949881413616083\n",
      "Iteration 18032 => Loss: 6.71638704479709858219\n",
      "Iteration 18033 => Loss: 6.71638459111073871100\n",
      "Iteration 18034 => Loss: 6.71638213775501302649\n",
      "Iteration 18035 => Loss: 6.71637968472987711976\n",
      "Iteration 18036 => Loss: 6.71637723203528658189\n",
      "Iteration 18037 => Loss: 6.71637477967119966848\n",
      "Iteration 18038 => Loss: 6.71637232763756042431\n",
      "Iteration 18039 => Loss: 6.71636987593433598676\n",
      "Iteration 18040 => Loss: 6.71636742456148638780\n",
      "Iteration 18041 => Loss: 6.71636497351894945496\n",
      "Iteration 18042 => Loss: 6.71636252280669854287\n",
      "Iteration 18043 => Loss: 6.71636007242467858447\n",
      "Iteration 18044 => Loss: 6.71635762237284783538\n",
      "Iteration 18045 => Loss: 6.71635517265116099850\n",
      "Iteration 18046 => Loss: 6.71635272325957721762\n",
      "Iteration 18047 => Loss: 6.71635027419804320203\n",
      "Iteration 18048 => Loss: 6.71634782546652431279\n",
      "Iteration 18049 => Loss: 6.71634537706497258824\n",
      "Iteration 18050 => Loss: 6.71634292899334095495\n",
      "Iteration 18051 => Loss: 6.71634048125159210940\n",
      "Iteration 18052 => Loss: 6.71633803383967098455\n",
      "Iteration 18053 => Loss: 6.71633558675754294143\n",
      "Iteration 18054 => Loss: 6.71633314000515557751\n",
      "Iteration 18055 => Loss: 6.71633069358247336567\n",
      "Iteration 18056 => Loss: 6.71632824748944123883\n",
      "Iteration 18057 => Loss: 6.71632580172602011714\n",
      "Iteration 18058 => Loss: 6.71632335629216825623\n",
      "Iteration 18059 => Loss: 6.71632091118784124717\n",
      "Iteration 18060 => Loss: 6.71631846641299201650\n",
      "Iteration 18061 => Loss: 6.71631602196757615530\n",
      "Iteration 18062 => Loss: 6.71631357785154303741\n",
      "Iteration 18063 => Loss: 6.71631113406485802386\n",
      "Iteration 18064 => Loss: 6.71630869060747492938\n",
      "Iteration 18065 => Loss: 6.71630624747934756869\n",
      "Iteration 18066 => Loss: 6.71630380468043330922\n",
      "Iteration 18067 => Loss: 6.71630136221068951841\n",
      "Iteration 18068 => Loss: 6.71629892007005846466\n",
      "Iteration 18069 => Loss: 6.71629647825851350262\n",
      "Iteration 18070 => Loss: 6.71629403677600045341\n",
      "Iteration 18071 => Loss: 6.71629159562247757265\n",
      "Iteration 18072 => Loss: 6.71628915479790133958\n",
      "Iteration 18073 => Loss: 6.71628671430222645711\n",
      "Iteration 18074 => Loss: 6.71628427413540496360\n",
      "Iteration 18075 => Loss: 6.71628183429740399646\n",
      "Iteration 18076 => Loss: 6.71627939478816138319\n",
      "Iteration 18077 => Loss: 6.71627695560765047844\n",
      "Iteration 18078 => Loss: 6.71627451675582154422\n",
      "Iteration 18079 => Loss: 6.71627207823261862529\n",
      "Iteration 18080 => Loss: 6.71626964003801951719\n",
      "Iteration 18081 => Loss: 6.71626720217195050111\n",
      "Iteration 18082 => Loss: 6.71626476463439736619\n",
      "Iteration 18083 => Loss: 6.71626232742529793995\n",
      "Iteration 18084 => Loss: 6.71625989054461225436\n",
      "Iteration 18085 => Loss: 6.71625745399229767685\n",
      "Iteration 18086 => Loss: 6.71625501776831246303\n",
      "Iteration 18087 => Loss: 6.71625258187260154585\n",
      "Iteration 18088 => Loss: 6.71625014630512673364\n",
      "Iteration 18089 => Loss: 6.71624771106585249925\n",
      "Iteration 18090 => Loss: 6.71624527615471933473\n",
      "Iteration 18091 => Loss: 6.71624284157169615384\n",
      "Iteration 18092 => Loss: 6.71624040731672877769\n",
      "Iteration 18093 => Loss: 6.71623797338978612004\n",
      "Iteration 18094 => Loss: 6.71623553979080778475\n",
      "Iteration 18095 => Loss: 6.71623310651976179741\n",
      "Iteration 18096 => Loss: 6.71623067357659753185\n",
      "Iteration 18097 => Loss: 6.71622824096126702642\n",
      "Iteration 18098 => Loss: 6.71622580867373297764\n",
      "Iteration 18099 => Loss: 6.71622337671395452929\n",
      "Iteration 18100 => Loss: 6.71622094508188638429\n",
      "Iteration 18101 => Loss: 6.71621851377747081102\n",
      "Iteration 18102 => Loss: 6.71621608280068116414\n",
      "Iteration 18103 => Loss: 6.71621365215146592931\n",
      "Iteration 18104 => Loss: 6.71621122182977270398\n",
      "Iteration 18105 => Loss: 6.71620879183557306646\n",
      "Iteration 18106 => Loss: 6.71620636216881372604\n",
      "Iteration 18107 => Loss: 6.71620393282944849744\n",
      "Iteration 18108 => Loss: 6.71620150381744185353\n",
      "Iteration 18109 => Loss: 6.71619907513274494448\n",
      "Iteration 18110 => Loss: 6.71619664677530803232\n",
      "Iteration 18111 => Loss: 6.71619421874509736625\n",
      "Iteration 18112 => Loss: 6.71619179104206143194\n",
      "Iteration 18113 => Loss: 6.71618936366615937317\n",
      "Iteration 18114 => Loss: 6.71618693661734766920\n",
      "Iteration 18115 => Loss: 6.71618450989558457564\n",
      "Iteration 18116 => Loss: 6.71618208350081147273\n",
      "Iteration 18117 => Loss: 6.71617965743300171511\n",
      "Iteration 18118 => Loss: 6.71617723169210911749\n",
      "Iteration 18119 => Loss: 6.71617480627807683646\n",
      "Iteration 18120 => Loss: 6.71617238119087645032\n",
      "Iteration 18121 => Loss: 6.71616995643045111564\n",
      "Iteration 18122 => Loss: 6.71616753199676885799\n",
      "Iteration 18123 => Loss: 6.71616510788977727486\n",
      "Iteration 18124 => Loss: 6.71616268410943284550\n",
      "Iteration 18125 => Loss: 6.71616026065569027281\n",
      "Iteration 18126 => Loss: 6.71615783752851136512\n",
      "Iteration 18127 => Loss: 6.71615541472784638444\n",
      "Iteration 18128 => Loss: 6.71615299225365536273\n",
      "Iteration 18129 => Loss: 6.71615057010589122655\n",
      "Iteration 18130 => Loss: 6.71614814828451756057\n",
      "Iteration 18131 => Loss: 6.71614572678948018591\n",
      "Iteration 18132 => Loss: 6.71614330562073913455\n",
      "Iteration 18133 => Loss: 6.71614088477825443846\n",
      "Iteration 18134 => Loss: 6.71613846426197635964\n",
      "Iteration 18135 => Loss: 6.71613604407186226553\n",
      "Iteration 18136 => Loss: 6.71613362420786863538\n",
      "Iteration 18137 => Loss: 6.71613120466995283664\n",
      "Iteration 18138 => Loss: 6.71612878545806690767\n",
      "Iteration 18139 => Loss: 6.71612636657217088043\n",
      "Iteration 18140 => Loss: 6.71612394801222389873\n",
      "Iteration 18141 => Loss: 6.71612152977817622457\n",
      "Iteration 18142 => Loss: 6.71611911186998344903\n",
      "Iteration 18143 => Loss: 6.71611669428760738043\n",
      "Iteration 18144 => Loss: 6.71611427703099916897\n",
      "Iteration 18145 => Loss: 6.71611186010011884662\n",
      "Iteration 18146 => Loss: 6.71610944349491845173\n",
      "Iteration 18147 => Loss: 6.71610702721534913451\n",
      "Iteration 18148 => Loss: 6.71610461126137980870\n",
      "Iteration 18149 => Loss: 6.71610219563296606538\n",
      "Iteration 18150 => Loss: 6.71609978033005550202\n",
      "Iteration 18151 => Loss: 6.71609736535260015700\n",
      "Iteration 18152 => Loss: 6.71609495070056539134\n",
      "Iteration 18153 => Loss: 6.71609253637390768432\n",
      "Iteration 18154 => Loss: 6.71609012237257640976\n",
      "Iteration 18155 => Loss: 6.71608770869654492230\n",
      "Iteration 18156 => Loss: 6.71608529534574305586\n",
      "Iteration 18157 => Loss: 6.71608288232014594143\n",
      "Iteration 18158 => Loss: 6.71608046961970295285\n",
      "Iteration 18159 => Loss: 6.71607805724436790484\n",
      "Iteration 18160 => Loss: 6.71607564519410704662\n",
      "Iteration 18161 => Loss: 6.71607323346886531112\n",
      "Iteration 18162 => Loss: 6.71607082206860805940\n",
      "Iteration 18163 => Loss: 6.71606841099328555345\n",
      "Iteration 18164 => Loss: 6.71606600024285249617\n",
      "Iteration 18165 => Loss: 6.71606358981727691315\n",
      "Iteration 18166 => Loss: 6.71606117971649574372\n",
      "Iteration 18167 => Loss: 6.71605876994048056616\n",
      "Iteration 18168 => Loss: 6.71605636048918519521\n",
      "Iteration 18169 => Loss: 6.71605395136256255739\n",
      "Iteration 18170 => Loss: 6.71605154256056646744\n",
      "Iteration 18171 => Loss: 6.71604913408315518097\n",
      "Iteration 18172 => Loss: 6.71604672593029317085\n",
      "Iteration 18173 => Loss: 6.71604431810192714636\n",
      "Iteration 18174 => Loss: 6.71604191059801269859\n",
      "Iteration 18175 => Loss: 6.71603950341851607675\n",
      "Iteration 18176 => Loss: 6.71603709656338310197\n",
      "Iteration 18177 => Loss: 6.71603469003257291803\n",
      "Iteration 18178 => Loss: 6.71603228382604111601\n",
      "Iteration 18179 => Loss: 6.71602987794375572150\n",
      "Iteration 18180 => Loss: 6.71602747238565189747\n",
      "Iteration 18181 => Loss: 6.71602506715170388674\n",
      "Iteration 18182 => Loss: 6.71602266224185573407\n",
      "Iteration 18183 => Loss: 6.71602025765607724139\n",
      "Iteration 18184 => Loss: 6.71601785339431334165\n",
      "Iteration 18185 => Loss: 6.71601544945652673135\n",
      "Iteration 18186 => Loss: 6.71601304584266145525\n",
      "Iteration 18187 => Loss: 6.71601064255269530889\n",
      "Iteration 18188 => Loss: 6.71600823958656167889\n",
      "Iteration 18189 => Loss: 6.71600583694423569625\n",
      "Iteration 18190 => Loss: 6.71600343462566140573\n",
      "Iteration 18191 => Loss: 6.71600103263079795113\n",
      "Iteration 18192 => Loss: 6.71599863095961158166\n",
      "Iteration 18193 => Loss: 6.71599622961204723026\n",
      "Iteration 18194 => Loss: 6.71599382858806315255\n",
      "Iteration 18195 => Loss: 6.71599142788761405143\n",
      "Iteration 18196 => Loss: 6.71598902751066884065\n",
      "Iteration 18197 => Loss: 6.71598662745716179501\n",
      "Iteration 18198 => Loss: 6.71598422772707159822\n",
      "Iteration 18199 => Loss: 6.71598182832034673595\n",
      "Iteration 18200 => Loss: 6.71597942923693391748\n",
      "Iteration 18201 => Loss: 6.71597703047680738564\n",
      "Iteration 18202 => Loss: 6.71597463203990763247\n",
      "Iteration 18203 => Loss: 6.71597223392619557814\n",
      "Iteration 18204 => Loss: 6.71596983613563036641\n",
      "Iteration 18205 => Loss: 6.71596743866817202928\n",
      "Iteration 18206 => Loss: 6.71596504152377171692\n",
      "Iteration 18207 => Loss: 6.71596264470238235589\n",
      "Iteration 18208 => Loss: 6.71596024820396930721\n",
      "Iteration 18209 => Loss: 6.71595785202848194473\n",
      "Iteration 18210 => Loss: 6.71595545617588207676\n",
      "Iteration 18211 => Loss: 6.71595306064612529440\n",
      "Iteration 18212 => Loss: 6.71595066543915741875\n",
      "Iteration 18213 => Loss: 6.71594827055495269263\n",
      "Iteration 18214 => Loss: 6.71594587599345693718\n",
      "Iteration 18215 => Loss: 6.71594348175462396711\n",
      "Iteration 18216 => Loss: 6.71594108783842536070\n",
      "Iteration 18217 => Loss: 6.71593869424479184005\n",
      "Iteration 18218 => Loss: 6.71593630097370830612\n",
      "Iteration 18219 => Loss: 6.71593390802510992188\n",
      "Iteration 18220 => Loss: 6.71593151539896471292\n",
      "Iteration 18221 => Loss: 6.71592912309522560577\n",
      "Iteration 18222 => Loss: 6.71592673111385352058\n",
      "Iteration 18223 => Loss: 6.71592433945479516666\n",
      "Iteration 18224 => Loss: 6.71592194811801768140\n",
      "Iteration 18225 => Loss: 6.71591955710347399133\n",
      "Iteration 18226 => Loss: 6.71591716641111258213\n",
      "Iteration 18227 => Loss: 6.71591477604090059117\n",
      "Iteration 18228 => Loss: 6.71591238599279360955\n",
      "Iteration 18229 => Loss: 6.71590999626674545198\n",
      "Iteration 18230 => Loss: 6.71590760686270993318\n",
      "Iteration 18231 => Loss: 6.71590521778064530878\n",
      "Iteration 18232 => Loss: 6.71590282902051605163\n",
      "Iteration 18233 => Loss: 6.71590044058226887103\n",
      "Iteration 18234 => Loss: 6.71589805246586823984\n",
      "Iteration 18235 => Loss: 6.71589566467126264371\n",
      "Iteration 18236 => Loss: 6.71589327719840767372\n",
      "Iteration 18237 => Loss: 6.71589089004727313181\n",
      "Iteration 18238 => Loss: 6.71588850321780395092\n",
      "Iteration 18239 => Loss: 6.71588611670995749847\n",
      "Iteration 18240 => Loss: 6.71588373052369913552\n",
      "Iteration 18241 => Loss: 6.71588134465897557135\n",
      "Iteration 18242 => Loss: 6.71587895911574683794\n",
      "Iteration 18243 => Loss: 6.71587657389397030272\n",
      "Iteration 18244 => Loss: 6.71587418899360599767\n",
      "Iteration 18245 => Loss: 6.71587180441460596114\n",
      "Iteration 18246 => Loss: 6.71586942015692578423\n",
      "Iteration 18247 => Loss: 6.71586703622052105800\n",
      "Iteration 18248 => Loss: 6.71586465260536158439\n",
      "Iteration 18249 => Loss: 6.71586226931139318452\n",
      "Iteration 18250 => Loss: 6.71585988633856700858\n",
      "Iteration 18251 => Loss: 6.71585750368685729939\n",
      "Iteration 18252 => Loss: 6.71585512135620099627\n",
      "Iteration 18253 => Loss: 6.71585273934655990757\n",
      "Iteration 18254 => Loss: 6.71585035765789850615\n",
      "Iteration 18255 => Loss: 6.71584797629017682397\n",
      "Iteration 18256 => Loss: 6.71584559524333979397\n",
      "Iteration 18257 => Loss: 6.71584321451735100084\n",
      "Iteration 18258 => Loss: 6.71584083411215893022\n",
      "Iteration 18259 => Loss: 6.71583845402773160771\n",
      "Iteration 18260 => Loss: 6.71583607426402107166\n",
      "Iteration 18261 => Loss: 6.71583369482098557768\n",
      "Iteration 18262 => Loss: 6.71583131569857805232\n",
      "Iteration 18263 => Loss: 6.71582893689675408666\n",
      "Iteration 18264 => Loss: 6.71582655841547637721\n",
      "Iteration 18265 => Loss: 6.71582418025469785050\n",
      "Iteration 18266 => Loss: 6.71582180241438120305\n",
      "Iteration 18267 => Loss: 6.71581942489448024958\n",
      "Iteration 18268 => Loss: 6.71581704769493903484\n",
      "Iteration 18269 => Loss: 6.71581467081573357802\n",
      "Iteration 18270 => Loss: 6.71581229425681236478\n",
      "Iteration 18271 => Loss: 6.71580991801813453890\n",
      "Iteration 18272 => Loss: 6.71580754209965036239\n",
      "Iteration 18273 => Loss: 6.71580516650132430811\n",
      "Iteration 18274 => Loss: 6.71580279122310930262\n",
      "Iteration 18275 => Loss: 6.71580041626496626606\n",
      "Iteration 18276 => Loss: 6.71579804162684368407\n",
      "Iteration 18277 => Loss: 6.71579566730870869407\n",
      "Iteration 18278 => Loss: 6.71579329331050622898\n",
      "Iteration 18279 => Loss: 6.71579091963221053163\n",
      "Iteration 18280 => Loss: 6.71578854627376120590\n",
      "Iteration 18281 => Loss: 6.71578617323512006010\n",
      "Iteration 18282 => Loss: 6.71578380051625156710\n",
      "Iteration 18283 => Loss: 6.71578142811711042981\n",
      "Iteration 18284 => Loss: 6.71577905603763891662\n",
      "Iteration 18285 => Loss: 6.71577668427781127036\n",
      "Iteration 18286 => Loss: 6.71577431283758308211\n",
      "Iteration 18287 => Loss: 6.71577194171689750846\n",
      "Iteration 18288 => Loss: 6.71576957091572790404\n",
      "Iteration 18289 => Loss: 6.71576720043402186633\n",
      "Iteration 18290 => Loss: 6.71576483027173676277\n",
      "Iteration 18291 => Loss: 6.71576246042882640808\n",
      "Iteration 18292 => Loss: 6.71576009090526593326\n",
      "Iteration 18293 => Loss: 6.71575772170099138947\n",
      "Iteration 18294 => Loss: 6.71575535281596547321\n",
      "Iteration 18295 => Loss: 6.71575298425015265735\n",
      "Iteration 18296 => Loss: 6.71575061600349432211\n",
      "Iteration 18297 => Loss: 6.71574824807596648668\n",
      "Iteration 18298 => Loss: 6.71574588046750964310\n",
      "Iteration 18299 => Loss: 6.71574351317809981055\n",
      "Iteration 18300 => Loss: 6.71574114620767126382\n",
      "Iteration 18301 => Loss: 6.71573877955619558122\n",
      "Iteration 18302 => Loss: 6.71573641322362746564\n",
      "Iteration 18303 => Loss: 6.71573404720992339634\n",
      "Iteration 18304 => Loss: 6.71573168151503718803\n",
      "Iteration 18305 => Loss: 6.71572931613893508995\n",
      "Iteration 18306 => Loss: 6.71572695108155581778\n",
      "Iteration 18307 => Loss: 6.71572458634287805523\n",
      "Iteration 18308 => Loss: 6.71572222192284495890\n",
      "Iteration 18309 => Loss: 6.71571985782142011345\n",
      "Iteration 18310 => Loss: 6.71571749403855822180\n",
      "Iteration 18311 => Loss: 6.71571513057421487503\n",
      "Iteration 18312 => Loss: 6.71571276742834921691\n",
      "Iteration 18313 => Loss: 6.71571040460091595037\n",
      "Iteration 18314 => Loss: 6.71570804209187510736\n",
      "Iteration 18315 => Loss: 6.71570567990117783808\n",
      "Iteration 18316 => Loss: 6.71570331802879572081\n",
      "Iteration 18317 => Loss: 6.71570095647466569488\n",
      "Iteration 18318 => Loss: 6.71569859523876377949\n",
      "Iteration 18319 => Loss: 6.71569623432103313121\n",
      "Iteration 18320 => Loss: 6.71569387372144266379\n",
      "Iteration 18321 => Loss: 6.71569151343993286929\n",
      "Iteration 18322 => Loss: 6.71568915347648420777\n",
      "Iteration 18323 => Loss: 6.71568679383103006586\n",
      "Iteration 18324 => Loss: 6.71568443450354024549\n",
      "Iteration 18325 => Loss: 6.71568207549397122591\n",
      "Iteration 18326 => Loss: 6.71567971680227859821\n",
      "Iteration 18327 => Loss: 6.71567735842842150618\n",
      "Iteration 18328 => Loss: 6.71567500037235731725\n",
      "Iteration 18329 => Loss: 6.71567264263403718161\n",
      "Iteration 18330 => Loss: 6.71567028521342468395\n",
      "Iteration 18331 => Loss: 6.71566792811047452716\n",
      "Iteration 18332 => Loss: 6.71566557132514407868\n",
      "Iteration 18333 => Loss: 6.71566321485738981778\n",
      "Iteration 18334 => Loss: 6.71566085870717177642\n",
      "Iteration 18335 => Loss: 6.71565850287444376931\n",
      "Iteration 18336 => Loss: 6.71565614735916138756\n",
      "Iteration 18337 => Loss: 6.71565379216128466311\n",
      "Iteration 18338 => Loss: 6.71565143728078073337\n",
      "Iteration 18339 => Loss: 6.71564908271758742586\n",
      "Iteration 18340 => Loss: 6.71564672847167898340\n",
      "Iteration 18341 => Loss: 6.71564437454300477981\n",
      "Iteration 18342 => Loss: 6.71564202093151241257\n",
      "Iteration 18343 => Loss: 6.71563966763717434816\n",
      "Iteration 18344 => Loss: 6.71563731465994884218\n",
      "Iteration 18345 => Loss: 6.71563496199978171575\n",
      "Iteration 18346 => Loss: 6.71563260965663566537\n",
      "Iteration 18347 => Loss: 6.71563025763046717032\n",
      "Iteration 18348 => Loss: 6.71562790592123981526\n",
      "Iteration 18349 => Loss: 6.71562555452890386221\n",
      "Iteration 18350 => Loss: 6.71562320345341401406\n",
      "Iteration 18351 => Loss: 6.71562085269473296734\n",
      "Iteration 18352 => Loss: 6.71561850225281720128\n",
      "Iteration 18353 => Loss: 6.71561615212763118876\n",
      "Iteration 18354 => Loss: 6.71561380231911631000\n",
      "Iteration 18355 => Loss: 6.71561145282723614969\n",
      "Iteration 18356 => Loss: 6.71560910365195695704\n",
      "Iteration 18357 => Loss: 6.71560675479322544135\n",
      "Iteration 18358 => Loss: 6.71560440625100429912\n",
      "Iteration 18359 => Loss: 6.71560205802525000962\n",
      "Iteration 18360 => Loss: 6.71559971011592260481\n",
      "Iteration 18361 => Loss: 6.71559736252296879400\n",
      "Iteration 18362 => Loss: 6.71559501524635482639\n",
      "Iteration 18363 => Loss: 6.71559266828603629307\n",
      "Iteration 18364 => Loss: 6.71559032164197500236\n",
      "Iteration 18365 => Loss: 6.71558797531412476900\n",
      "Iteration 18366 => Loss: 6.71558562930243585498\n",
      "Iteration 18367 => Loss: 6.71558328360687450953\n",
      "Iteration 18368 => Loss: 6.71558093822739810008\n",
      "Iteration 18369 => Loss: 6.71557859316396310589\n",
      "Iteration 18370 => Loss: 6.71557624841652600622\n",
      "Iteration 18371 => Loss: 6.71557390398503706308\n",
      "Iteration 18372 => Loss: 6.71557155986946519022\n",
      "Iteration 18373 => Loss: 6.71556921606976331418\n",
      "Iteration 18374 => Loss: 6.71556687258588613787\n",
      "Iteration 18375 => Loss: 6.71556452941779458143\n",
      "Iteration 18376 => Loss: 6.71556218656544867684\n",
      "Iteration 18377 => Loss: 6.71555984402879779793\n",
      "Iteration 18378 => Loss: 6.71555750180780552938\n",
      "Iteration 18379 => Loss: 6.71555515990243367952\n",
      "Iteration 18380 => Loss: 6.71555281831262540493\n",
      "Iteration 18381 => Loss: 6.71555047703834517847\n",
      "Iteration 18382 => Loss: 6.71554813607955924937\n",
      "Iteration 18383 => Loss: 6.71554579543621166238\n",
      "Iteration 18384 => Loss: 6.71554345510826866672\n",
      "Iteration 18385 => Loss: 6.71554111509568318894\n",
      "Iteration 18386 => Loss: 6.71553877539841792554\n",
      "Iteration 18387 => Loss: 6.71553643601642225036\n",
      "Iteration 18388 => Loss: 6.71553409694966862986\n",
      "Iteration 18389 => Loss: 6.71553175819809489155\n",
      "Iteration 18390 => Loss: 6.71552941976166994920\n",
      "Iteration 18391 => Loss: 6.71552708164034761751\n",
      "Iteration 18392 => Loss: 6.71552474383408704028\n",
      "Iteration 18393 => Loss: 6.71552240634285180221\n",
      "Iteration 18394 => Loss: 6.71552006916658950075\n",
      "Iteration 18395 => Loss: 6.71551773230526816150\n",
      "Iteration 18396 => Loss: 6.71551539575883005284\n",
      "Iteration 18397 => Loss: 6.71551305952724320036\n",
      "Iteration 18398 => Loss: 6.71551072361046852421\n",
      "Iteration 18399 => Loss: 6.71550838800845628640\n",
      "Iteration 18400 => Loss: 6.71550605272116474254\n",
      "Iteration 18401 => Loss: 6.71550371774855658913\n",
      "Iteration 18402 => Loss: 6.71550138309058297637\n",
      "Iteration 18403 => Loss: 6.71549904874721281800\n",
      "Iteration 18404 => Loss: 6.71549671471838482972\n",
      "Iteration 18405 => Loss: 6.71549438100407680707\n",
      "Iteration 18406 => Loss: 6.71549204760422746574\n",
      "Iteration 18407 => Loss: 6.71548971451880571948\n",
      "Iteration 18408 => Loss: 6.71548738174777604115\n",
      "Iteration 18409 => Loss: 6.71548504929108158734\n",
      "Iteration 18410 => Loss: 6.71548271714868771909\n",
      "Iteration 18411 => Loss: 6.71548038532054736294\n",
      "Iteration 18412 => Loss: 6.71547805380661788632\n",
      "Iteration 18413 => Loss: 6.71547572260686376211\n",
      "Iteration 18414 => Loss: 6.71547339172124146955\n",
      "Iteration 18415 => Loss: 6.71547106114970215884\n",
      "Iteration 18416 => Loss: 6.71546873089221207920\n",
      "Iteration 18417 => Loss: 6.71546640094872859805\n",
      "Iteration 18418 => Loss: 6.71546407131919398381\n",
      "Iteration 18419 => Loss: 6.71546174200357803841\n",
      "Iteration 18420 => Loss: 6.71545941300183901745\n",
      "Iteration 18421 => Loss: 6.71545708431393872928\n",
      "Iteration 18422 => Loss: 6.71545475593982654772\n",
      "Iteration 18423 => Loss: 6.71545242787946339291\n",
      "Iteration 18424 => Loss: 6.71545010013280396777\n",
      "Iteration 18425 => Loss: 6.71544777269981452150\n",
      "Iteration 18426 => Loss: 6.71544544558043998705\n",
      "Iteration 18427 => Loss: 6.71544311877464661364\n",
      "Iteration 18428 => Loss: 6.71544079228239265689\n",
      "Iteration 18429 => Loss: 6.71543846610363459604\n",
      "Iteration 18430 => Loss: 6.71543614023832624582\n",
      "Iteration 18431 => Loss: 6.71543381468643030274\n",
      "Iteration 18432 => Loss: 6.71543148944789969335\n",
      "Iteration 18433 => Loss: 6.71542916452269711414\n",
      "Iteration 18434 => Loss: 6.71542683991077815620\n",
      "Iteration 18435 => Loss: 6.71542451561209752242\n",
      "Iteration 18436 => Loss: 6.71542219162662679111\n",
      "Iteration 18437 => Loss: 6.71541986795430378976\n",
      "Iteration 18438 => Loss: 6.71541754459509920849\n",
      "Iteration 18439 => Loss: 6.71541522154896952657\n",
      "Iteration 18440 => Loss: 6.71541289881586589416\n",
      "Iteration 18441 => Loss: 6.71541057639574834326\n",
      "Iteration 18442 => Loss: 6.71540825428858401125\n",
      "Iteration 18443 => Loss: 6.71540593249431783107\n",
      "Iteration 18444 => Loss: 6.71540361101292582191\n",
      "Iteration 18445 => Loss: 6.71540128984433870585\n",
      "Iteration 18446 => Loss: 6.71539896898853871932\n",
      "Iteration 18447 => Loss: 6.71539664844547079525\n",
      "Iteration 18448 => Loss: 6.71539432821509496563\n",
      "Iteration 18449 => Loss: 6.71539200829736682152\n",
      "Iteration 18450 => Loss: 6.71538968869225527669\n",
      "Iteration 18451 => Loss: 6.71538736939971059314\n",
      "Iteration 18452 => Loss: 6.71538505041968747378\n",
      "Iteration 18453 => Loss: 6.71538273175214683874\n",
      "Iteration 18454 => Loss: 6.71538041339704516730\n",
      "Iteration 18455 => Loss: 6.71537809535435048502\n",
      "Iteration 18456 => Loss: 6.71537577762400594850\n",
      "Iteration 18457 => Loss: 6.71537346020596892515\n",
      "Iteration 18458 => Loss: 6.71537114310021454600\n",
      "Iteration 18459 => Loss: 6.71536882630668952032\n",
      "Iteration 18460 => Loss: 6.71536650982534499832\n",
      "Iteration 18461 => Loss: 6.71536419365615611099\n",
      "Iteration 18462 => Loss: 6.71536187779906335038\n",
      "Iteration 18463 => Loss: 6.71535956225403651842\n",
      "Iteration 18464 => Loss: 6.71535724702103120620\n",
      "Iteration 18465 => Loss: 6.71535493210000300479\n",
      "Iteration 18466 => Loss: 6.71535261749090306438\n",
      "Iteration 18467 => Loss: 6.71535030319370562779\n",
      "Iteration 18468 => Loss: 6.71534798920835385161\n",
      "Iteration 18469 => Loss: 6.71534567553482020230\n",
      "Iteration 18470 => Loss: 6.71534336217304339556\n",
      "Iteration 18471 => Loss: 6.71534104912300033874\n",
      "Iteration 18472 => Loss: 6.71533873638463774114\n",
      "Iteration 18473 => Loss: 6.71533642395792451651\n",
      "Iteration 18474 => Loss: 6.71533411184280115691\n",
      "Iteration 18475 => Loss: 6.71533180003924012880\n",
      "Iteration 18476 => Loss: 6.71532948854719613507\n",
      "Iteration 18477 => Loss: 6.71532717736661854957\n",
      "Iteration 18478 => Loss: 6.71532486649747895058\n",
      "Iteration 18479 => Loss: 6.71532255593972404739\n",
      "Iteration 18480 => Loss: 6.71532024569332008923\n",
      "Iteration 18481 => Loss: 6.71531793575822177900\n",
      "Iteration 18482 => Loss: 6.71531562613439181320\n",
      "Iteration 18483 => Loss: 6.71531331682177778930\n",
      "Iteration 18484 => Loss: 6.71531100782034684471\n",
      "Iteration 18485 => Loss: 6.71530869913004835325\n",
      "Iteration 18486 => Loss: 6.71530639075085034051\n",
      "Iteration 18487 => Loss: 6.71530408268271106209\n",
      "Iteration 18488 => Loss: 6.71530177492558166819\n",
      "Iteration 18489 => Loss: 6.71529946747941686169\n",
      "Iteration 18490 => Loss: 6.71529716034418466819\n",
      "Iteration 18491 => Loss: 6.71529485351983890240\n",
      "Iteration 18492 => Loss: 6.71529254700634137265\n",
      "Iteration 18493 => Loss: 6.71529024080364322913\n",
      "Iteration 18494 => Loss: 6.71528793491170894470\n",
      "Iteration 18495 => Loss: 6.71528562933049233408\n",
      "Iteration 18496 => Loss: 6.71528332405994810017\n",
      "Iteration 18497 => Loss: 6.71528101910004426855\n",
      "Iteration 18498 => Loss: 6.71527871445073110124\n",
      "Iteration 18499 => Loss: 6.71527641011197218290\n",
      "Iteration 18500 => Loss: 6.71527410608372399281\n",
      "Iteration 18501 => Loss: 6.71527180236594389839\n",
      "Iteration 18502 => Loss: 6.71526949895858749073\n",
      "Iteration 18503 => Loss: 6.71526719586161746633\n",
      "Iteration 18504 => Loss: 6.71526489307498586356\n",
      "Iteration 18505 => Loss: 6.71526259059865804346\n",
      "Iteration 18506 => Loss: 6.71526028843258959711\n",
      "Iteration 18507 => Loss: 6.71525798657673611558\n",
      "Iteration 18508 => Loss: 6.71525568503105940721\n",
      "Iteration 18509 => Loss: 6.71525338379551861578\n",
      "Iteration 18510 => Loss: 6.71525108287006045060\n",
      "Iteration 18511 => Loss: 6.71524878225466181902\n",
      "Iteration 18512 => Loss: 6.71524648194927564759\n",
      "Iteration 18513 => Loss: 6.71524418195383798746\n",
      "Iteration 18514 => Loss: 6.71524188226833462778\n",
      "Iteration 18515 => Loss: 6.71523958289271494237\n",
      "Iteration 18516 => Loss: 6.71523728382694251593\n",
      "Iteration 18517 => Loss: 6.71523498507096050503\n",
      "Iteration 18518 => Loss: 6.71523268662473515889\n",
      "Iteration 18519 => Loss: 6.71523038848823095037\n",
      "Iteration 18520 => Loss: 6.71522809066139014789\n",
      "Iteration 18521 => Loss: 6.71522579314419587604\n",
      "Iteration 18522 => Loss: 6.71522349593658596234\n",
      "Iteration 18523 => Loss: 6.71522119903852576783\n",
      "Iteration 18524 => Loss: 6.71521890244997354813\n",
      "Iteration 18525 => Loss: 6.71521660617088489431\n",
      "Iteration 18526 => Loss: 6.71521431020121717381\n",
      "Iteration 18527 => Loss: 6.71521201454093663585\n",
      "Iteration 18528 => Loss: 6.71520971918999798334\n",
      "Iteration 18529 => Loss: 6.71520742414835591916\n",
      "Iteration 18530 => Loss: 6.71520512941596781076\n",
      "Iteration 18531 => Loss: 6.71520283499279901918\n",
      "Iteration 18532 => Loss: 6.71520054087880602367\n",
      "Iteration 18533 => Loss: 6.71519824707393464536\n",
      "Iteration 18534 => Loss: 6.71519595357816356795\n",
      "Iteration 18535 => Loss: 6.71519366039143683622\n",
      "Iteration 18536 => Loss: 6.71519136751371892302\n",
      "Iteration 18537 => Loss: 6.71518907494496808397\n",
      "Iteration 18538 => Loss: 6.71518678268513902196\n",
      "Iteration 18539 => Loss: 6.71518449073418821627\n",
      "Iteration 18540 => Loss: 6.71518219909207836338\n",
      "Iteration 18541 => Loss: 6.71517990775877482434\n",
      "Iteration 18542 => Loss: 6.71517761673422342028\n",
      "Iteration 18543 => Loss: 6.71517532601838862405\n",
      "Iteration 18544 => Loss: 6.71517303561122691491\n",
      "Iteration 18545 => Loss: 6.71517074551269921301\n",
      "Iteration 18546 => Loss: 6.71516845572276199761\n",
      "Iteration 18547 => Loss: 6.71516616624137263614\n",
      "Iteration 18548 => Loss: 6.71516387706849737782\n",
      "Iteration 18549 => Loss: 6.71516158820408293195\n",
      "Iteration 18550 => Loss: 6.71515929964809732411\n",
      "Iteration 18551 => Loss: 6.71515701140049170448\n",
      "Iteration 18552 => Loss: 6.71515472346122876957\n",
      "Iteration 18553 => Loss: 6.71515243583026233409\n",
      "Iteration 18554 => Loss: 6.71515014850755331821\n",
      "Iteration 18555 => Loss: 6.71514786149306885932\n",
      "Iteration 18556 => Loss: 6.71514557478675033764\n",
      "Iteration 18557 => Loss: 6.71514328838857110782\n",
      "Iteration 18558 => Loss: 6.71514100229848498458\n",
      "Iteration 18559 => Loss: 6.71513871651645466443\n",
      "Iteration 18560 => Loss: 6.71513643104242508031\n",
      "Iteration 18561 => Loss: 6.71513414587636336961\n",
      "Iteration 18562 => Loss: 6.71513186101822334706\n",
      "Iteration 18563 => Loss: 6.71512957646798192002\n",
      "Iteration 18564 => Loss: 6.71512729222557780417\n",
      "Iteration 18565 => Loss: 6.71512500829097280786\n",
      "Iteration 18566 => Loss: 6.71512272466412696303\n",
      "Iteration 18567 => Loss: 6.71512044134500829529\n",
      "Iteration 18568 => Loss: 6.71511815833355729666\n",
      "Iteration 18569 => Loss: 6.71511587562974909815\n",
      "Iteration 18570 => Loss: 6.71511359323353307360\n",
      "Iteration 18571 => Loss: 6.71511131114487191951\n",
      "Iteration 18572 => Loss: 6.71510902936371945060\n",
      "Iteration 18573 => Loss: 6.71510674789003925156\n",
      "Iteration 18574 => Loss: 6.71510446672379135435\n",
      "Iteration 18575 => Loss: 6.71510218586492335646\n",
      "Iteration 18576 => Loss: 6.71509990531340683617\n",
      "Iteration 18577 => Loss: 6.71509762506918939096\n",
      "Iteration 18578 => Loss: 6.71509534513223460550\n",
      "Iteration 18579 => Loss: 6.71509306550250961720\n",
      "Iteration 18580 => Loss: 6.71509078617995758265\n",
      "Iteration 18581 => Loss: 6.71508850716454563923\n",
      "Iteration 18582 => Loss: 6.71508622845623293074\n",
      "Iteration 18583 => Loss: 6.71508395005498304187\n",
      "Iteration 18584 => Loss: 6.71508167196074179373\n",
      "Iteration 18585 => Loss: 6.71507939417346655375\n",
      "Iteration 18586 => Loss: 6.71507711669312978842\n",
      "Iteration 18587 => Loss: 6.71507483951968442426\n",
      "Iteration 18588 => Loss: 6.71507256265308605236\n",
      "Iteration 18589 => Loss: 6.71507028609329470470\n",
      "Iteration 18590 => Loss: 6.71506800984027663048\n",
      "Iteration 18591 => Loss: 6.71506573389398120355\n",
      "Iteration 18592 => Loss: 6.71506345825436756769\n",
      "Iteration 18593 => Loss: 6.71506118292139664305\n",
      "Iteration 18594 => Loss: 6.71505890789502579707\n",
      "Iteration 18595 => Loss: 6.71505663317522127898\n",
      "Iteration 18596 => Loss: 6.71505435876192890987\n",
      "Iteration 18597 => Loss: 6.71505208465511760352\n",
      "Iteration 18598 => Loss: 6.71504981085473406921\n",
      "Iteration 18599 => Loss: 6.71504753736075077342\n",
      "Iteration 18600 => Loss: 6.71504526417312508357\n",
      "Iteration 18601 => Loss: 6.71504299129180992622\n",
      "Iteration 18602 => Loss: 6.71504071871676266881\n",
      "Iteration 18603 => Loss: 6.71503844644794778418\n",
      "Iteration 18604 => Loss: 6.71503617448532086343\n",
      "Iteration 18605 => Loss: 6.71503390282884193851\n",
      "Iteration 18606 => Loss: 6.71503163147846571235\n",
      "Iteration 18607 => Loss: 6.71502936043415843415\n",
      "Iteration 18608 => Loss: 6.71502708969586770138\n",
      "Iteration 18609 => Loss: 6.71502481926356065145\n",
      "Iteration 18610 => Loss: 6.71502254913719998086\n",
      "Iteration 18611 => Loss: 6.71502027931673595162\n",
      "Iteration 18612 => Loss: 6.71501800980213570114\n",
      "Iteration 18613 => Loss: 6.71501574059334238598\n",
      "Iteration 18614 => Loss: 6.71501347169033291351\n",
      "Iteration 18615 => Loss: 6.71501120309305310485\n",
      "Iteration 18616 => Loss: 6.71500893480147098558\n",
      "Iteration 18617 => Loss: 6.71500666681553859405\n",
      "Iteration 18618 => Loss: 6.71500439913522484403\n",
      "Iteration 18619 => Loss: 6.71500213176047200392\n",
      "Iteration 18620 => Loss: 6.71499986469124898747\n",
      "Iteration 18621 => Loss: 6.71499759792751760301\n",
      "Iteration 18622 => Loss: 6.71499533146923699434\n",
      "Iteration 18623 => Loss: 6.71499306531635120621\n",
      "Iteration 18624 => Loss: 6.71499079946883625780\n",
      "Iteration 18625 => Loss: 6.71498853392664418749\n",
      "Iteration 18626 => Loss: 6.71498626868973058635\n",
      "Iteration 18627 => Loss: 6.71498400375806347995\n",
      "Iteration 18628 => Loss: 6.71498173913159135395\n",
      "Iteration 18629 => Loss: 6.71497947481027690486\n",
      "Iteration 18630 => Loss: 6.71497721079408282918\n",
      "Iteration 18631 => Loss: 6.71497494708296738253\n",
      "Iteration 18632 => Loss: 6.71497268367688082691\n",
      "Iteration 18633 => Loss: 6.71497042057579474061\n",
      "Iteration 18634 => Loss: 6.71496815777965494476\n",
      "Iteration 18635 => Loss: 6.71496589528843035311\n",
      "Iteration 18636 => Loss: 6.71496363310207655672\n",
      "Iteration 18637 => Loss: 6.71496137122055802848\n",
      "Iteration 18638 => Loss: 6.71495910964382058950\n",
      "Iteration 18639 => Loss: 6.71495684837183492988\n",
      "Iteration 18640 => Loss: 6.71495458740455042346\n",
      "Iteration 18641 => Loss: 6.71495232674193420763\n",
      "Iteration 18642 => Loss: 6.71495006638394542620\n",
      "Iteration 18643 => Loss: 6.71494780633054144658\n",
      "Iteration 18644 => Loss: 6.71494554658167519534\n",
      "Iteration 18645 => Loss: 6.71494328713731469804\n",
      "Iteration 18646 => Loss: 6.71494102799741199306\n",
      "Iteration 18647 => Loss: 6.71493876916192355964\n",
      "Iteration 18648 => Loss: 6.71493651063081742336\n",
      "Iteration 18649 => Loss: 6.71493425240405006349\n",
      "Iteration 18650 => Loss: 6.71493199448158062381\n",
      "Iteration 18651 => Loss: 6.71492973686336469541\n",
      "Iteration 18652 => Loss: 6.71492747954936231025\n",
      "Iteration 18653 => Loss: 6.71492522253953438849\n",
      "Iteration 18654 => Loss: 6.71492296583383652120\n",
      "Iteration 18655 => Loss: 6.71492070943222874035\n",
      "Iteration 18656 => Loss: 6.71491845333467463064\n",
      "Iteration 18657 => Loss: 6.71491619754112534224\n",
      "Iteration 18658 => Loss: 6.71491394205154978891\n",
      "Iteration 18659 => Loss: 6.71491168686589912085\n",
      "Iteration 18660 => Loss: 6.71490943198413425819\n",
      "Iteration 18661 => Loss: 6.71490717740621612108\n",
      "Iteration 18662 => Loss: 6.71490492313210207698\n",
      "Iteration 18663 => Loss: 6.71490266916175304601\n",
      "Iteration 18664 => Loss: 6.71490041549512461927\n",
      "Iteration 18665 => Loss: 6.71489816213217860508\n",
      "Iteration 18666 => Loss: 6.71489590907286881816\n",
      "Iteration 18667 => Loss: 6.71489365631716683680\n",
      "Iteration 18668 => Loss: 6.71489140386502114666\n",
      "Iteration 18669 => Loss: 6.71488915171639266788\n",
      "Iteration 18670 => Loss: 6.71488689987124054426\n",
      "Iteration 18671 => Loss: 6.71488464832952658412\n",
      "Iteration 18672 => Loss: 6.71488239709121259580\n",
      "Iteration 18673 => Loss: 6.71488014615624351222\n",
      "Iteration 18674 => Loss: 6.71487789552459268805\n",
      "Iteration 18675 => Loss: 6.71487564519621571435\n",
      "Iteration 18676 => Loss: 6.71487339517106907039\n",
      "Iteration 18677 => Loss: 6.71487114544911189995\n",
      "Iteration 18678 => Loss: 6.71486889603030867590\n",
      "Iteration 18679 => Loss: 6.71486664691461676568\n",
      "Iteration 18680 => Loss: 6.71486439810198820766\n",
      "Iteration 18681 => Loss: 6.71486214959239013922\n",
      "Iteration 18682 => Loss: 6.71485990138577903963\n",
      "Iteration 18683 => Loss: 6.71485765348210517089\n",
      "Iteration 18684 => Loss: 6.71485540588135254580\n",
      "Iteration 18685 => Loss: 6.71485315858345277462\n",
      "Iteration 18686 => Loss: 6.71485091158837654746\n",
      "Iteration 18687 => Loss: 6.71484866489608744899\n",
      "Iteration 18688 => Loss: 6.71484641850653929396\n",
      "Iteration 18689 => Loss: 6.71484417241968856160\n",
      "Iteration 18690 => Loss: 6.71484192663550505387\n",
      "Iteration 18691 => Loss: 6.71483968115393548004\n",
      "Iteration 18692 => Loss: 6.71483743597494875388\n",
      "Iteration 18693 => Loss: 6.71483519109849780193\n",
      "Iteration 18694 => Loss: 6.71483294652454354434\n",
      "Iteration 18695 => Loss: 6.71483070225304512491\n",
      "Iteration 18696 => Loss: 6.71482845828396257559\n",
      "Iteration 18697 => Loss: 6.71482621461726036927\n",
      "Iteration 18698 => Loss: 6.71482397125288876794\n",
      "Iteration 18699 => Loss: 6.71482172819081046811\n",
      "Iteration 18700 => Loss: 6.71481948543098283722\n",
      "Iteration 18701 => Loss: 6.71481724297336857177\n",
      "Iteration 18702 => Loss: 6.71481500081792948009\n",
      "Iteration 18703 => Loss: 6.71481275896461848873\n",
      "Iteration 18704 => Loss: 6.71481051741339474148\n",
      "Iteration 18705 => Loss: 6.71480827616422448756\n",
      "Iteration 18706 => Loss: 6.71480603521706065351\n",
      "Iteration 18707 => Loss: 6.71480379457186504766\n",
      "Iteration 18708 => Loss: 6.71480155422859681380\n",
      "Iteration 18709 => Loss: 6.71479931418720887848\n",
      "Iteration 18710 => Loss: 6.71479707444767281999\n",
      "Iteration 18711 => Loss: 6.71479483500994067668\n",
      "Iteration 18712 => Loss: 6.71479259587397603326\n",
      "Iteration 18713 => Loss: 6.71479035703973181626\n",
      "Iteration 18714 => Loss: 6.71478811850717072218\n",
      "Iteration 18715 => Loss: 6.71478588027625367118\n",
      "Iteration 18716 => Loss: 6.71478364234694069523\n",
      "Iteration 18717 => Loss: 6.71478140471918472088\n",
      "Iteration 18718 => Loss: 6.71477916739294578008\n",
      "Iteration 18719 => Loss: 6.71477693036819456296\n",
      "Iteration 18720 => Loss: 6.71477469364487422609\n",
      "Iteration 18721 => Loss: 6.71477245722296434138\n",
      "Iteration 18722 => Loss: 6.71477022110240095998\n",
      "Iteration 18723 => Loss: 6.71476798528316276560\n",
      "Iteration 18724 => Loss: 6.71476574976519646754\n",
      "Iteration 18725 => Loss: 6.71476351454846565048\n",
      "Iteration 18726 => Loss: 6.71476127963293656364\n",
      "Iteration 18727 => Loss: 6.71475904501855946904\n",
      "Iteration 18728 => Loss: 6.71475681070529706318\n",
      "Iteration 18729 => Loss: 6.71475457669310760167\n",
      "Iteration 18730 => Loss: 6.71475234298195022831\n",
      "Iteration 18731 => Loss: 6.71475010957178675142\n",
      "Iteration 18732 => Loss: 6.71474787646257986751\n",
      "Iteration 18733 => Loss: 6.71474564365427095680\n",
      "Iteration 18734 => Loss: 6.71474341114685024934\n",
      "Iteration 18735 => Loss: 6.71474117894025201991\n",
      "Iteration 18736 => Loss: 6.71473894703444251775\n",
      "Iteration 18737 => Loss: 6.71473671542938355117\n",
      "Iteration 18738 => Loss: 6.71473448412503515215\n",
      "Iteration 18739 => Loss: 6.71473225312135468812\n",
      "Iteration 18740 => Loss: 6.71473002241830396741\n",
      "Iteration 18741 => Loss: 6.71472779201584035746\n",
      "Iteration 18742 => Loss: 6.71472556191391500846\n",
      "Iteration 18743 => Loss: 6.71472333211250660412\n",
      "Iteration 18744 => Loss: 6.71472110261155830102\n",
      "Iteration 18745 => Loss: 6.71471887341103901292\n",
      "Iteration 18746 => Loss: 6.71471664451090344272\n",
      "Iteration 18747 => Loss: 6.71471441591110895786\n",
      "Iteration 18748 => Loss: 6.71471218761162269573\n",
      "Iteration 18749 => Loss: 6.71470995961240291194\n",
      "Iteration 18750 => Loss: 6.71470773191340164487\n",
      "Iteration 18751 => Loss: 6.71470550451458070285\n",
      "Iteration 18752 => Loss: 6.71470327741590367054\n",
      "Iteration 18753 => Loss: 6.71470105061732791540\n",
      "Iteration 18754 => Loss: 6.71469882411881791029\n",
      "Iteration 18755 => Loss: 6.71469659792032746992\n",
      "Iteration 18756 => Loss: 6.71469437202181218538\n",
      "Iteration 18757 => Loss: 6.71469214642324807585\n",
      "Iteration 18758 => Loss: 6.71468992112457296884\n",
      "Iteration 18759 => Loss: 6.71468769612575400174\n",
      "Iteration 18760 => Loss: 6.71468547142676275286\n",
      "Iteration 18761 => Loss: 6.71468324702754681965\n",
      "Iteration 18762 => Loss: 6.71468102292806978681\n",
      "Iteration 18763 => Loss: 6.71467879912828724542\n",
      "Iteration 18764 => Loss: 6.71467657562816189198\n",
      "Iteration 18765 => Loss: 6.71467435242765908754\n",
      "Iteration 18766 => Loss: 6.71467212952673264681\n",
      "Iteration 18767 => Loss: 6.71466990692533194363\n",
      "Iteration 18768 => Loss: 6.71466768462343477353\n",
      "Iteration 18769 => Loss: 6.71466546262099583942\n",
      "Iteration 18770 => Loss: 6.71466324091796806783\n",
      "Iteration 18771 => Loss: 6.71466101951431060257\n",
      "Iteration 18772 => Loss: 6.71465879840999679828\n",
      "Iteration 18773 => Loss: 6.71465657760496981155\n",
      "Iteration 18774 => Loss: 6.71465435709919855611\n",
      "Iteration 18775 => Loss: 6.71465213689264306396\n",
      "Iteration 18776 => Loss: 6.71464991698525448527\n",
      "Iteration 18777 => Loss: 6.71464769737699995744\n",
      "Iteration 18778 => Loss: 6.71464547806784484152\n",
      "Iteration 18779 => Loss: 6.71464325905773495862\n",
      "Iteration 18780 => Loss: 6.71464104034664366338\n",
      "Iteration 18781 => Loss: 6.71463882193451588876\n",
      "Iteration 18782 => Loss: 6.71463660382132143667\n",
      "Iteration 18783 => Loss: 6.71463438600702211545\n",
      "Iteration 18784 => Loss: 6.71463216849157262800\n",
      "Iteration 18785 => Loss: 6.71462995127492767722\n",
      "Iteration 18786 => Loss: 6.71462773435705262415\n",
      "Iteration 18787 => Loss: 6.71462551773791638254\n",
      "Iteration 18788 => Loss: 6.71462330141746299716\n",
      "Iteration 18789 => Loss: 6.71462108539565871723\n",
      "Iteration 18790 => Loss: 6.71461886967247245650\n",
      "Iteration 18791 => Loss: 6.71461665424784470702\n",
      "Iteration 18792 => Loss: 6.71461443912174615889\n",
      "Iteration 18793 => Loss: 6.71461222429414217316\n",
      "Iteration 18794 => Loss: 6.71461000976498301185\n",
      "Iteration 18795 => Loss: 6.71460779553423758870\n",
      "Iteration 18796 => Loss: 6.71460558160184817211\n",
      "Iteration 18797 => Loss: 6.71460336796779433399\n",
      "Iteration 18798 => Loss: 6.71460115463202189545\n",
      "Iteration 18799 => Loss: 6.71459894159449888207\n",
      "Iteration 18800 => Loss: 6.71459672885518710217\n",
      "Iteration 18801 => Loss: 6.71459451641403770594\n",
      "Iteration 18802 => Loss: 6.71459230427101960714\n",
      "Iteration 18803 => Loss: 6.71459009242608750867\n",
      "Iteration 18804 => Loss: 6.71458788087919611343\n",
      "Iteration 18805 => Loss: 6.71458566963031255881\n",
      "Iteration 18806 => Loss: 6.71458345867939865315\n",
      "Iteration 18807 => Loss: 6.71458124802641087570\n",
      "Iteration 18808 => Loss: 6.71457903767130126482\n",
      "Iteration 18809 => Loss: 6.71457682761404406335\n",
      "Iteration 18810 => Loss: 6.71457461785458864512\n",
      "Iteration 18811 => Loss: 6.71457240839290481205\n",
      "Iteration 18812 => Loss: 6.71457019922894549069\n",
      "Iteration 18813 => Loss: 6.71456799036266360758\n",
      "Iteration 18814 => Loss: 6.71456578179403162920\n",
      "Iteration 18815 => Loss: 6.71456357352300425845\n",
      "Iteration 18816 => Loss: 6.71456136554953708639\n",
      "Iteration 18817 => Loss: 6.71455915787359813862\n",
      "Iteration 18818 => Loss: 6.71455695049514122985\n",
      "Iteration 18819 => Loss: 6.71455474341413083295\n",
      "Iteration 18820 => Loss: 6.71455253663051987445\n",
      "Iteration 18821 => Loss: 6.71455033014428614990\n",
      "Iteration 18822 => Loss: 6.71454812395536659864\n",
      "Iteration 18823 => Loss: 6.71454591806373279894\n",
      "Iteration 18824 => Loss: 6.71454371246934034190\n",
      "Iteration 18825 => Loss: 6.71454150717215902944\n",
      "Iteration 18826 => Loss: 6.71453930217213823539\n",
      "Iteration 18827 => Loss: 6.71453709746924154445\n",
      "Iteration 18828 => Loss: 6.71453489306342810039\n",
      "Iteration 18829 => Loss: 6.71453268895465704702\n",
      "Iteration 18830 => Loss: 6.71453048514288752813\n",
      "Iteration 18831 => Loss: 6.71452828162809023382\n",
      "Iteration 18832 => Loss: 6.71452607841020565616\n",
      "Iteration 18833 => Loss: 6.71452387548922047245\n",
      "Iteration 18834 => Loss: 6.71452167286506451660\n",
      "Iteration 18835 => Loss: 6.71451947053771824869\n",
      "Iteration 18836 => Loss: 6.71451726850713193073\n",
      "Iteration 18837 => Loss: 6.71451506677327447647\n",
      "Iteration 18838 => Loss: 6.71451286533609614793\n",
      "Iteration 18839 => Loss: 6.71451066419556585885\n",
      "Iteration 18840 => Loss: 6.71450846335163831213\n",
      "Iteration 18841 => Loss: 6.71450626280427709247\n",
      "Iteration 18842 => Loss: 6.71450406255343512640\n",
      "Iteration 18843 => Loss: 6.71450186259907955133\n",
      "Iteration 18844 => Loss: 6.71449966294116951104\n",
      "Iteration 18845 => Loss: 6.71449746357966148480\n",
      "Iteration 18846 => Loss: 6.71449526451451905729\n",
      "Iteration 18847 => Loss: 6.71449306574569959594\n",
      "Iteration 18848 => Loss: 6.71449086727315869183\n",
      "Iteration 18849 => Loss: 6.71448866909687502869\n",
      "Iteration 18850 => Loss: 6.71448647121678376948\n",
      "Iteration 18851 => Loss: 6.71448427363286182157\n",
      "Iteration 18852 => Loss: 6.71448207634506299968\n",
      "Iteration 18853 => Loss: 6.71447987935335266485\n",
      "Iteration 18854 => Loss: 6.71447768265768374363\n",
      "Iteration 18855 => Loss: 6.71447548625802426159\n",
      "Iteration 18856 => Loss: 6.71447329015432448074\n",
      "Iteration 18857 => Loss: 6.71447109434654887394\n",
      "Iteration 18858 => Loss: 6.71446889883466457860\n",
      "Iteration 18859 => Loss: 6.71446670361861652765\n",
      "Iteration 18860 => Loss: 6.71446450869838606934\n",
      "Iteration 18861 => Loss: 6.71446231407391191937\n",
      "Iteration 18862 => Loss: 6.71446011974516743237\n",
      "Iteration 18863 => Loss: 6.71445792571210642308\n",
      "Iteration 18864 => Loss: 6.71445573197469425253\n",
      "Iteration 18865 => Loss: 6.71445353853288917634\n",
      "Iteration 18866 => Loss: 6.71445134538664589741\n",
      "Iteration 18867 => Loss: 6.71444915253593244131\n",
      "Iteration 18868 => Loss: 6.71444695998070351095\n",
      "Iteration 18869 => Loss: 6.71444476772092357919\n",
      "Iteration 18870 => Loss: 6.71444257575655356618\n",
      "Iteration 18871 => Loss: 6.71444038408754728664\n",
      "Iteration 18872 => Loss: 6.71443819271387187797\n",
      "Iteration 18873 => Loss: 6.71443600163547849036\n",
      "Iteration 18874 => Loss: 6.71443381085233692573\n",
      "Iteration 18875 => Loss: 6.71443162036440899243\n",
      "Iteration 18876 => Loss: 6.71442943017163784702\n",
      "Iteration 18877 => Loss: 6.71442724027400483777\n",
      "Iteration 18878 => Loss: 6.71442505067145667397\n",
      "Iteration 18879 => Loss: 6.71442286136395427576\n",
      "Iteration 18880 => Loss: 6.71442067235146478055\n",
      "Iteration 18881 => Loss: 6.71441848363394822030\n",
      "Iteration 18882 => Loss: 6.71441629521136018610\n",
      "Iteration 18883 => Loss: 6.71441410708365715720\n",
      "Iteration 18884 => Loss: 6.71441191925080449465\n",
      "Iteration 18885 => Loss: 6.71440973171276489495\n",
      "Iteration 18886 => Loss: 6.71440754446950105461\n",
      "Iteration 18887 => Loss: 6.71440535752096767652\n",
      "Iteration 18888 => Loss: 6.71440317086712301631\n",
      "Iteration 18889 => Loss: 6.71440098450792444140\n",
      "Iteration 18890 => Loss: 6.71439879844334353010\n",
      "Iteration 18891 => Loss: 6.71439661267333409711\n",
      "Iteration 18892 => Loss: 6.71439442719785706259\n",
      "Iteration 18893 => Loss: 6.71439224201687689941\n",
      "Iteration 18894 => Loss: 6.71439005713034919864\n",
      "Iteration 18895 => Loss: 6.71438787253822955137\n",
      "Iteration 18896 => Loss: 6.71438568824048864769\n",
      "Iteration 18897 => Loss: 6.71438350423707763781\n",
      "Iteration 18898 => Loss: 6.71438132052796188276\n",
      "Iteration 18899 => Loss: 6.71437913711310674358\n",
      "Iteration 18900 => Loss: 6.71437695399245981775\n",
      "Iteration 18901 => Loss: 6.71437477116599357174\n",
      "Iteration 18902 => Loss: 6.71437258863366448480\n",
      "Iteration 18903 => Loss: 6.71437040639542281895\n",
      "Iteration 18904 => Loss: 6.71436822445125258696\n",
      "Iteration 18905 => Loss: 6.71436604280108895182\n",
      "Iteration 18906 => Loss: 6.71436386144490615635\n",
      "Iteration 18907 => Loss: 6.71436168038266245617\n",
      "Iteration 18908 => Loss: 6.71435949961431433053\n",
      "Iteration 18909 => Loss: 6.71435731913981825869\n",
      "Iteration 18910 => Loss: 6.71435513895915292437\n",
      "Iteration 18911 => Loss: 6.71435295907226947776\n",
      "Iteration 18912 => Loss: 6.71435077947911373997\n",
      "Iteration 18913 => Loss: 6.71434860017966173018\n",
      "Iteration 18914 => Loss: 6.71434642117387081584\n",
      "Iteration 18915 => Loss: 6.71434424246170458161\n",
      "Iteration 18916 => Loss: 6.71434206404312039496\n",
      "Iteration 18917 => Loss: 6.71433988591807828783\n",
      "Iteration 18918 => Loss: 6.71433770808653829221\n",
      "Iteration 18919 => Loss: 6.71433553054845599917\n",
      "Iteration 18920 => Loss: 6.71433335330380565154\n",
      "Iteration 18921 => Loss: 6.71433117635252951771\n",
      "Iteration 18922 => Loss: 6.71432899969460272871\n",
      "Iteration 18923 => Loss: 6.71432682332998265196\n",
      "Iteration 18924 => Loss: 6.71432464725862399035\n",
      "Iteration 18925 => Loss: 6.71432247148049654584\n",
      "Iteration 18926 => Loss: 6.71432029599554702770\n",
      "Iteration 18927 => Loss: 6.71431812080375056695\n",
      "Iteration 18928 => Loss: 6.71431594590505653741\n",
      "Iteration 18929 => Loss: 6.71431377129943296467\n",
      "Iteration 18930 => Loss: 6.71431159698684432158\n",
      "Iteration 18931 => Loss: 6.71430942296723731744\n",
      "Iteration 18932 => Loss: 6.71430724924057908964\n",
      "Iteration 18933 => Loss: 6.71430507580683233471\n",
      "Iteration 18934 => Loss: 6.71430290266595974913\n",
      "Iteration 18935 => Loss: 6.71430072981790981856\n",
      "Iteration 18936 => Loss: 6.71429855726265323312\n",
      "Iteration 18937 => Loss: 6.71429638500015446567\n",
      "Iteration 18938 => Loss: 6.71429421303036733093\n",
      "Iteration 18939 => Loss: 6.71429204135324741998\n",
      "Iteration 18940 => Loss: 6.71428986996876453475\n",
      "Iteration 18941 => Loss: 6.71428769887688137175\n",
      "Iteration 18942 => Loss: 6.71428552807754019938\n",
      "Iteration 18943 => Loss: 6.71428335757072147771\n",
      "Iteration 18944 => Loss: 6.71428118735638079784\n",
      "Iteration 18945 => Loss: 6.71427901743447819172\n",
      "Iteration 18946 => Loss: 6.71427684780496569772\n",
      "Iteration 18947 => Loss: 6.71427467846782111138\n",
      "Iteration 18948 => Loss: 6.71427250942298226022\n",
      "Iteration 18949 => Loss: 6.71427034067042871612\n",
      "Iteration 18950 => Loss: 6.71426817221012139925\n",
      "Iteration 18951 => Loss: 6.71426600404200168981\n",
      "Iteration 18952 => Loss: 6.71426383616604915971\n",
      "Iteration 18953 => Loss: 6.71426166858221940004\n",
      "Iteration 18954 => Loss: 6.71425950129046800186\n",
      "Iteration 18955 => Loss: 6.71425733429076210257\n",
      "Iteration 18956 => Loss: 6.71425516758305818144\n",
      "Iteration 18957 => Loss: 6.71425300116732159950\n",
      "Iteration 18958 => Loss: 6.71425083504350617147\n",
      "Iteration 18959 => Loss: 6.71424866921157370570\n",
      "Iteration 18960 => Loss: 6.71424650367148601049\n",
      "Iteration 18961 => Loss: 6.71424433842321111143\n",
      "Iteration 18962 => Loss: 6.71424217346670371143\n",
      "Iteration 18963 => Loss: 6.71424000880192650698\n",
      "Iteration 18964 => Loss: 6.71423784442883153645\n",
      "Iteration 18965 => Loss: 6.71423568034739037813\n",
      "Iteration 18966 => Loss: 6.71423351655755151768\n",
      "Iteration 18967 => Loss: 6.71423135305929186245\n",
      "Iteration 18968 => Loss: 6.71422918985256789171\n",
      "Iteration 18969 => Loss: 6.71422702693732365020\n",
      "Iteration 18970 => Loss: 6.71422486431354137437\n",
      "Iteration 18971 => Loss: 6.71422270198116333262\n",
      "Iteration 18972 => Loss: 6.71422053994017264955\n",
      "Iteration 18973 => Loss: 6.71421837819051336993\n",
      "Iteration 18974 => Loss: 6.71421621673214463755\n",
      "Iteration 18975 => Loss: 6.71421405556503358980\n",
      "Iteration 18976 => Loss: 6.71421189468914736409\n",
      "Iteration 18977 => Loss: 6.71420973410443089335\n",
      "Iteration 18978 => Loss: 6.71420757381085397952\n",
      "Iteration 18979 => Loss: 6.71420541380837754275\n",
      "Iteration 18980 => Loss: 6.71420325409696605590\n",
      "Iteration 18981 => Loss: 6.71420109467657333369\n",
      "Iteration 18982 => Loss: 6.71419893554715674355\n",
      "Iteration 18983 => Loss: 6.71419677670869319286\n",
      "Iteration 18984 => Loss: 6.71419461816113116726\n",
      "Iteration 18985 => Loss: 6.71419245990443336325\n",
      "Iteration 18986 => Loss: 6.71419030193855448374\n",
      "Iteration 18987 => Loss: 6.71418814426346965973\n",
      "Iteration 18988 => Loss: 6.71418598687912471235\n",
      "Iteration 18989 => Loss: 6.71418382978549477258\n",
      "Iteration 18990 => Loss: 6.71418167298252921427\n",
      "Iteration 18991 => Loss: 6.71417951647018984573\n",
      "Iteration 18992 => Loss: 6.71417736024844380438\n",
      "Iteration 18993 => Loss: 6.71417520431724579311\n",
      "Iteration 18994 => Loss: 6.71417304867656561385\n",
      "Iteration 18995 => Loss: 6.71417089332635175225\n",
      "Iteration 18996 => Loss: 6.71416873826657667479\n",
      "Iteration 18997 => Loss: 6.71416658349719153165\n",
      "Iteration 18998 => Loss: 6.71416442901817145383\n",
      "Iteration 18999 => Loss: 6.71416227482945693339\n",
      "Iteration 19000 => Loss: 6.71416012093102398950\n",
      "Iteration 19001 => Loss: 6.71415796732282732506\n",
      "Iteration 19002 => Loss: 6.71415581400482963659\n",
      "Iteration 19003 => Loss: 6.71415366097699095604\n",
      "Iteration 19004 => Loss: 6.71415150823928108537\n",
      "Iteration 19005 => Loss: 6.71414935579164495749\n",
      "Iteration 19006 => Loss: 6.71414720363404970982\n",
      "Iteration 19007 => Loss: 6.71414505176646425610\n",
      "Iteration 19008 => Loss: 6.71414290018884063471\n",
      "Iteration 19009 => Loss: 6.71414074890113887761\n",
      "Iteration 19010 => Loss: 6.71413859790332878674\n",
      "Iteration 19011 => Loss: 6.71413644719536062411\n",
      "Iteration 19012 => Loss: 6.71413429677719530986\n",
      "Iteration 19013 => Loss: 6.71413214664881241589\n",
      "Iteration 19014 => Loss: 6.71412999681015509879\n",
      "Iteration 19015 => Loss: 6.71412784726118605505\n",
      "Iteration 19016 => Loss: 6.71412569800187153390\n",
      "Iteration 19017 => Loss: 6.71412354903216890278\n",
      "Iteration 19018 => Loss: 6.71412140035203819366\n",
      "Iteration 19019 => Loss: 6.71411925196144476757\n",
      "Iteration 19020 => Loss: 6.71411710386034688014\n",
      "Iteration 19021 => Loss: 6.71411495604870633969\n",
      "Iteration 19022 => Loss: 6.71411280852648051365\n",
      "Iteration 19023 => Loss: 6.71411066129363209853\n",
      "Iteration 19024 => Loss: 6.71410851435012734356\n",
      "Iteration 19025 => Loss: 6.71410636769592450435\n",
      "Iteration 19026 => Loss: 6.71410422133098627739\n",
      "Iteration 19027 => Loss: 6.71410207525525759564\n",
      "Iteration 19028 => Loss: 6.71409992946872158370\n",
      "Iteration 19029 => Loss: 6.71409778397133116812\n",
      "Iteration 19030 => Loss: 6.71409563876304904539\n",
      "Iteration 19031 => Loss: 6.71409349384382903025\n",
      "Iteration 19032 => Loss: 6.71409134921363559556\n",
      "Iteration 19033 => Loss: 6.71408920487243676689\n",
      "Iteration 19034 => Loss: 6.71408706082018191807\n",
      "Iteration 19035 => Loss: 6.71408491705684173922\n",
      "Iteration 19036 => Loss: 6.71408277358237715049\n",
      "Iteration 19037 => Loss: 6.71408063039674107841\n",
      "Iteration 19038 => Loss: 6.71407848749990421311\n",
      "Iteration 19039 => Loss: 6.71407634489181859294\n",
      "Iteration 19040 => Loss: 6.71407420257245224349\n",
      "Iteration 19041 => Loss: 6.71407206054175720311\n",
      "Iteration 19042 => Loss: 6.71406991879970771464\n",
      "Iteration 19043 => Loss: 6.71406777734625936915\n",
      "Iteration 19044 => Loss: 6.71406563618136864591\n",
      "Iteration 19045 => Loss: 6.71406349530500179412\n",
      "Iteration 19046 => Loss: 6.71406135471711706941\n",
      "Iteration 19047 => Loss: 6.71405921441767272739\n",
      "Iteration 19048 => Loss: 6.71405707440664034635\n",
      "Iteration 19049 => Loss: 6.71405493468397018830\n",
      "Iteration 19050 => Loss: 6.71405279524962939064\n",
      "Iteration 19051 => Loss: 6.71405065610358153805\n",
      "Iteration 19052 => Loss: 6.71404851724577600436\n",
      "Iteration 19053 => Loss: 6.71404637867618880875\n",
      "Iteration 19054 => Loss: 6.71404424039476843689\n",
      "Iteration 19055 => Loss: 6.71404210240148824340\n",
      "Iteration 19056 => Loss: 6.71403996469629582577\n",
      "Iteration 19057 => Loss: 6.71403782727916187412\n",
      "Iteration 19058 => Loss: 6.71403569015004286769\n",
      "Iteration 19059 => Loss: 6.71403355330890949659\n",
      "Iteration 19060 => Loss: 6.71403141675570847013\n",
      "Iteration 19061 => Loss: 6.71402928049041047842\n",
      "Iteration 19062 => Loss: 6.71402714451297732978\n",
      "Iteration 19063 => Loss: 6.71402500882335928623\n",
      "Iteration 19064 => Loss: 6.71402287342153147875\n",
      "Iteration 19065 => Loss: 6.71402073830745393934\n",
      "Iteration 19066 => Loss: 6.71401860348107071275\n",
      "Iteration 19067 => Loss: 6.71401646894237025265\n",
      "Iteration 19068 => Loss: 6.71401433469128594567\n",
      "Iteration 19069 => Loss: 6.71401220072779736370\n",
      "Iteration 19070 => Loss: 6.71401006705186453871\n",
      "Iteration 19071 => Loss: 6.71400793366343773272\n",
      "Iteration 19072 => Loss: 6.71400580056249296490\n",
      "Iteration 19073 => Loss: 6.71400366774897694455\n",
      "Iteration 19074 => Loss: 6.71400153522285769725\n",
      "Iteration 19075 => Loss: 6.71399940298410058404\n",
      "Iteration 19076 => Loss: 6.71399727103265675510\n",
      "Iteration 19077 => Loss: 6.71399513936850578233\n",
      "Iteration 19078 => Loss: 6.71399300799157927599\n",
      "Iteration 19079 => Loss: 6.71399087690186924249\n",
      "Iteration 19080 => Loss: 6.71398874609932239110\n",
      "Iteration 19081 => Loss: 6.71398661558390141835\n",
      "Iteration 19082 => Loss: 6.71398448535556102712\n",
      "Iteration 19083 => Loss: 6.71398235541426213757\n",
      "Iteration 19084 => Loss: 6.71398022575999586792\n",
      "Iteration 19085 => Loss: 6.71397809639268583481\n",
      "Iteration 19086 => Loss: 6.71397596731230983380\n",
      "Iteration 19087 => Loss: 6.71397383851883144956\n",
      "Iteration 19088 => Loss: 6.71397171001220183228\n",
      "Iteration 19089 => Loss: 6.71396958179239433662\n",
      "Iteration 19090 => Loss: 6.71396745385936188910\n",
      "Iteration 19091 => Loss: 6.71396532621306985078\n",
      "Iteration 19092 => Loss: 6.71396319885347470091\n",
      "Iteration 19093 => Loss: 6.71396107178054357689\n",
      "Iteration 19094 => Loss: 6.71395894499424628066\n",
      "Iteration 19095 => Loss: 6.71395681849451797518\n",
      "Iteration 19096 => Loss: 6.71395469228134178508\n",
      "Iteration 19097 => Loss: 6.71395256635467685413\n",
      "Iteration 19098 => Loss: 6.71395044071447255618\n",
      "Iteration 19099 => Loss: 6.71394831536069958133\n",
      "Iteration 19100 => Loss: 6.71394619029332329063\n",
      "Iteration 19101 => Loss: 6.71394406551229572244\n",
      "Iteration 19102 => Loss: 6.71394194101758401416\n",
      "Iteration 19103 => Loss: 6.71393981680915263865\n",
      "Iteration 19104 => Loss: 6.71393769288695541064\n",
      "Iteration 19105 => Loss: 6.71393556925095236210\n",
      "Iteration 19106 => Loss: 6.71393344590111862402\n",
      "Iteration 19107 => Loss: 6.71393132283739912936\n",
      "Iteration 19108 => Loss: 6.71392920005976634457\n",
      "Iteration 19109 => Loss: 6.71392707756817586073\n",
      "Iteration 19110 => Loss: 6.71392495536258682165\n",
      "Iteration 19111 => Loss: 6.71392283344297524650\n",
      "Iteration 19112 => Loss: 6.71392071180928518004\n",
      "Iteration 19113 => Loss: 6.71391859046148642420\n",
      "Iteration 19114 => Loss: 6.71391646939953901096\n",
      "Iteration 19115 => Loss: 6.71391434862340563683\n",
      "Iteration 19116 => Loss: 6.71391222813305166284\n",
      "Iteration 19117 => Loss: 6.71391010792842646282\n",
      "Iteration 19118 => Loss: 6.71390798800950605596\n",
      "Iteration 19119 => Loss: 6.71390586837624159244\n",
      "Iteration 19120 => Loss: 6.71390374902859665696\n",
      "Iteration 19121 => Loss: 6.71390162996652861693\n",
      "Iteration 19122 => Loss: 6.71389951119001704427\n",
      "Iteration 19123 => Loss: 6.71389739269900509555\n",
      "Iteration 19124 => Loss: 6.71389527449345813181\n",
      "Iteration 19125 => Loss: 6.71389315657333440868\n",
      "Iteration 19126 => Loss: 6.71389103893861438621\n",
      "Iteration 19127 => Loss: 6.71388892158923500375\n",
      "Iteration 19128 => Loss: 6.71388680452517405683\n",
      "Iteration 19129 => Loss: 6.71388468774638802472\n",
      "Iteration 19130 => Loss: 6.71388257125283249849\n",
      "Iteration 19131 => Loss: 6.71388045504447816825\n",
      "Iteration 19132 => Loss: 6.71387833912128151326\n",
      "Iteration 19133 => Loss: 6.71387622348320967092\n",
      "Iteration 19134 => Loss: 6.71387410813021645595\n",
      "Iteration 19135 => Loss: 6.71387199306226634121\n",
      "Iteration 19136 => Loss: 6.71386987827932202322\n",
      "Iteration 19137 => Loss: 6.71386776378134886301\n",
      "Iteration 19138 => Loss: 6.71386564956830156348\n",
      "Iteration 19139 => Loss: 6.71386353564014637385\n",
      "Iteration 19140 => Loss: 6.71386142199684154974\n",
      "Iteration 19141 => Loss: 6.71385930863834889948\n",
      "Iteration 19142 => Loss: 6.71385719556463467228\n",
      "Iteration 19143 => Loss: 6.71385508277565357105\n",
      "Iteration 19144 => Loss: 6.71385297027137628589\n",
      "Iteration 19145 => Loss: 6.71385085805175485518\n",
      "Iteration 19146 => Loss: 6.71384874611675463996\n",
      "Iteration 19147 => Loss: 6.71384663446633922490\n",
      "Iteration 19148 => Loss: 6.71384452310047308288\n",
      "Iteration 19149 => Loss: 6.71384241201910647590\n",
      "Iteration 19150 => Loss: 6.71384030122221808767\n",
      "Iteration 19151 => Loss: 6.71383819070974663390\n",
      "Iteration 19152 => Loss: 6.71383608048167790372\n",
      "Iteration 19153 => Loss: 6.71383397053795860643\n",
      "Iteration 19154 => Loss: 6.71383186087855232671\n",
      "Iteration 19155 => Loss: 6.71382975150341998472\n",
      "Iteration 19156 => Loss: 6.71382764241253404691\n",
      "Iteration 19157 => Loss: 6.71382553360584477531\n",
      "Iteration 19158 => Loss: 6.71382342508331397823\n",
      "Iteration 19159 => Loss: 6.71382131684491234580\n",
      "Iteration 19160 => Loss: 6.71381920889059369273\n",
      "Iteration 19161 => Loss: 6.71381710122032249188\n",
      "Iteration 19162 => Loss: 6.71381499383405699888\n",
      "Iteration 19163 => Loss: 6.71381288673176435111\n",
      "Iteration 19164 => Loss: 6.71381077991340813327\n",
      "Iteration 19165 => Loss: 6.71380867337893683100\n",
      "Iteration 19166 => Loss: 6.71380656712833179256\n",
      "Iteration 19167 => Loss: 6.71380446116153883906\n",
      "Iteration 19168 => Loss: 6.71380235547852155520\n",
      "Iteration 19169 => Loss: 6.71380025007925063107\n",
      "Iteration 19170 => Loss: 6.71379814496367810506\n",
      "Iteration 19171 => Loss: 6.71379604013177733179\n",
      "Iteration 19172 => Loss: 6.71379393558349502058\n",
      "Iteration 19173 => Loss: 6.71379183131880630242\n",
      "Iteration 19174 => Loss: 6.71378972733766055114\n",
      "Iteration 19175 => Loss: 6.71378762364003023322\n",
      "Iteration 19176 => Loss: 6.71378552022587449244\n",
      "Iteration 19177 => Loss: 6.71378341709515602531\n",
      "Iteration 19178 => Loss: 6.71378131424782775838\n",
      "Iteration 19179 => Loss: 6.71377921168386482265\n",
      "Iteration 19180 => Loss: 6.71377710940322103284\n",
      "Iteration 19181 => Loss: 6.71377500740585908545\n",
      "Iteration 19182 => Loss: 6.71377290569173990065\n",
      "Iteration 19183 => Loss: 6.71377080426082617493\n",
      "Iteration 19184 => Loss: 6.71376870311307882844\n",
      "Iteration 19185 => Loss: 6.71376660224846677494\n",
      "Iteration 19186 => Loss: 6.71376450166694738186\n",
      "Iteration 19187 => Loss: 6.71376240136847624029\n",
      "Iteration 19188 => Loss: 6.71376030135302492852\n",
      "Iteration 19189 => Loss: 6.71375820162054992579\n",
      "Iteration 19190 => Loss: 6.71375610217101215227\n",
      "Iteration 19191 => Loss: 6.71375400300437430445\n",
      "Iteration 19192 => Loss: 6.71375190412060174339\n",
      "Iteration 19193 => Loss: 6.71374980551965627740\n",
      "Iteration 19194 => Loss: 6.71374770720148994485\n",
      "Iteration 19195 => Loss: 6.71374560916607787675\n",
      "Iteration 19196 => Loss: 6.71374351141337832871\n",
      "Iteration 19197 => Loss: 6.71374141394334689181\n",
      "Iteration 19198 => Loss: 6.71373931675595070345\n",
      "Iteration 19199 => Loss: 6.71373721985114979560\n",
      "Iteration 19200 => Loss: 6.71373512322890331205\n",
      "Iteration 19201 => Loss: 6.71373302688918549563\n",
      "Iteration 19202 => Loss: 6.71373093083194660835\n",
      "Iteration 19203 => Loss: 6.71372883505715023489\n",
      "Iteration 19204 => Loss: 6.71372673956475463086\n",
      "Iteration 19205 => Loss: 6.71372464435473759181\n",
      "Iteration 19206 => Loss: 6.71372254942704493885\n",
      "Iteration 19207 => Loss: 6.71372045478164203303\n",
      "Iteration 19208 => Loss: 6.71371836041849245902\n",
      "Iteration 19209 => Loss: 6.71371626633755980151\n",
      "Iteration 19210 => Loss: 6.71371417253881030973\n",
      "Iteration 19211 => Loss: 6.71371207902218980479\n",
      "Iteration 19212 => Loss: 6.71370998578767874676\n",
      "Iteration 19213 => Loss: 6.71370789283522562130\n",
      "Iteration 19214 => Loss: 6.71370580016480555940\n",
      "Iteration 19215 => Loss: 6.71370370777636704673\n",
      "Iteration 19216 => Loss: 6.71370161566987810886\n",
      "Iteration 19217 => Loss: 6.71369952384530677136\n",
      "Iteration 19218 => Loss: 6.71369743230260596079\n",
      "Iteration 19219 => Loss: 6.71369534104173215638\n",
      "Iteration 19220 => Loss: 6.71369325006266315370\n",
      "Iteration 19221 => Loss: 6.71369115936535543199\n",
      "Iteration 19222 => Loss: 6.71368906894976547051\n",
      "Iteration 19223 => Loss: 6.71368697881586218301\n",
      "Iteration 19224 => Loss: 6.71368488896360293694\n",
      "Iteration 19225 => Loss: 6.71368279939295575787\n",
      "Iteration 19226 => Loss: 6.71368071010387446051\n",
      "Iteration 19227 => Loss: 6.71367862109632618228\n",
      "Iteration 19228 => Loss: 6.71367653237026829061\n",
      "Iteration 19229 => Loss: 6.71367444392567147560\n",
      "Iteration 19230 => Loss: 6.71367235576248688744\n",
      "Iteration 19231 => Loss: 6.71367026788068432808\n",
      "Iteration 19232 => Loss: 6.71366818028022827036\n",
      "Iteration 19233 => Loss: 6.71366609296107164084\n",
      "Iteration 19234 => Loss: 6.71366400592318601781\n",
      "Iteration 19235 => Loss: 6.71366191916652521599\n",
      "Iteration 19236 => Loss: 6.71365983269105370823\n",
      "Iteration 19237 => Loss: 6.71365774649674129648\n",
      "Iteration 19238 => Loss: 6.71365566058353557821\n",
      "Iteration 19239 => Loss: 6.71365357495141434896\n",
      "Iteration 19240 => Loss: 6.71365148960033497616\n",
      "Iteration 19241 => Loss: 6.71364940453025038636\n",
      "Iteration 19242 => Loss: 6.71364731974112682877\n",
      "Iteration 19243 => Loss: 6.71364523523293144081\n",
      "Iteration 19244 => Loss: 6.71364315100562336625\n",
      "Iteration 19245 => Loss: 6.71364106705916530160\n",
      "Iteration 19246 => Loss: 6.71363898339351639066\n",
      "Iteration 19247 => Loss: 6.71363690000865087626\n",
      "Iteration 19248 => Loss: 6.71363481690451457951\n",
      "Iteration 19249 => Loss: 6.71363273408107463780\n",
      "Iteration 19250 => Loss: 6.71363065153829996490\n",
      "Iteration 19251 => Loss: 6.71362856927614526370\n",
      "Iteration 19252 => Loss: 6.71362648729457234253\n",
      "Iteration 19253 => Loss: 6.71362440559355277969\n",
      "Iteration 19254 => Loss: 6.71362232417304305443\n",
      "Iteration 19255 => Loss: 6.71362024303299964600\n",
      "Iteration 19256 => Loss: 6.71361816217339413271\n",
      "Iteration 19257 => Loss: 6.71361608159418477015\n",
      "Iteration 19258 => Loss: 6.71361400129533159031\n",
      "Iteration 19259 => Loss: 6.71361192127680173058\n",
      "Iteration 19260 => Loss: 6.71360984153854811751\n",
      "Iteration 19261 => Loss: 6.71360776208054765846\n",
      "Iteration 19262 => Loss: 6.71360568290274972725\n",
      "Iteration 19263 => Loss: 6.71360360400511613221\n",
      "Iteration 19264 => Loss: 6.71360152538762733343\n",
      "Iteration 19265 => Loss: 6.71359944705021938205\n",
      "Iteration 19266 => Loss: 6.71359736899287717904\n",
      "Iteration 19267 => Loss: 6.71359529121554654552\n",
      "Iteration 19268 => Loss: 6.71359321371820083613\n",
      "Iteration 19269 => Loss: 6.71359113650080008284\n",
      "Iteration 19270 => Loss: 6.71358905956329543585\n",
      "Iteration 19271 => Loss: 6.71358698290566646705\n",
      "Iteration 19272 => Loss: 6.71358490652786432662\n",
      "Iteration 19273 => Loss: 6.71358283042984904654\n",
      "Iteration 19274 => Loss: 6.71358075461159842234\n",
      "Iteration 19275 => Loss: 6.71357867907306360422\n",
      "Iteration 19276 => Loss: 6.71357660381419751872\n",
      "Iteration 19277 => Loss: 6.71357452883497973772\n",
      "Iteration 19278 => Loss: 6.71357245413536851686\n",
      "Iteration 19279 => Loss: 6.71357037971532033538\n",
      "Iteration 19280 => Loss: 6.71356830557479522525\n",
      "Iteration 19281 => Loss: 6.71356623171376831749\n",
      "Iteration 19282 => Loss: 6.71356415813219076227\n",
      "Iteration 19283 => Loss: 6.71356208483002880882\n",
      "Iteration 19284 => Loss: 6.71356001180724337729\n",
      "Iteration 19285 => Loss: 6.71355793906380160507\n",
      "Iteration 19286 => Loss: 6.71355586659965819507\n",
      "Iteration 19287 => Loss: 6.71355379441477762015\n",
      "Iteration 19288 => Loss: 6.71355172250912968224\n",
      "Iteration 19289 => Loss: 6.71354965088266730788\n",
      "Iteration 19290 => Loss: 6.71354757953535852266\n",
      "Iteration 19291 => Loss: 6.71354550846716247037\n",
      "Iteration 19292 => Loss: 6.71354343767804628840\n",
      "Iteration 19293 => Loss: 6.71354136716796823237\n",
      "Iteration 19294 => Loss: 6.71353929693688744607\n",
      "Iteration 19295 => Loss: 6.71353722698477906050\n",
      "Iteration 19296 => Loss: 6.71353515731159156132\n",
      "Iteration 19297 => Loss: 6.71353308791728853322\n",
      "Iteration 19298 => Loss: 6.71353101880184599537\n",
      "Iteration 19299 => Loss: 6.71352894996520532800\n",
      "Iteration 19300 => Loss: 6.71352688140734876754\n",
      "Iteration 19301 => Loss: 6.71352481312822479964\n",
      "Iteration 19302 => Loss: 6.71352274512780944349\n",
      "Iteration 19303 => Loss: 6.71352067740604940838\n",
      "Iteration 19304 => Loss: 6.71351860996292248984\n",
      "Iteration 19305 => Loss: 6.71351654279837362083\n",
      "Iteration 19306 => Loss: 6.71351447591238592594\n",
      "Iteration 19307 => Loss: 6.71351240930490522629\n",
      "Iteration 19308 => Loss: 6.71351034297590754107\n",
      "Iteration 19309 => Loss: 6.71350827692533869140\n",
      "Iteration 19310 => Loss: 6.71350621115317380827\n",
      "Iteration 19311 => Loss: 6.71350414565937292366\n",
      "Iteration 19312 => Loss: 6.71350208044389695772\n",
      "Iteration 19313 => Loss: 6.71350001550670594241\n",
      "Iteration 19314 => Loss: 6.71349795084776612697\n",
      "Iteration 19315 => Loss: 6.71349588646704464878\n",
      "Iteration 19316 => Loss: 6.71349382236449176986\n",
      "Iteration 19317 => Loss: 6.71349175854008350939\n",
      "Iteration 19318 => Loss: 6.71348969499377012937\n",
      "Iteration 19319 => Loss: 6.71348763172552231993\n",
      "Iteration 19320 => Loss: 6.71348556873530011302\n",
      "Iteration 19321 => Loss: 6.71348350602306442880\n",
      "Iteration 19322 => Loss: 6.71348144358878506921\n",
      "Iteration 19323 => Loss: 6.71347938143240963171\n",
      "Iteration 19324 => Loss: 6.71347731955391413550\n",
      "Iteration 19325 => Loss: 6.71347525795326038889\n",
      "Iteration 19326 => Loss: 6.71347319663039865389\n",
      "Iteration 19327 => Loss: 6.71347113558530672606\n",
      "Iteration 19328 => Loss: 6.71346907481794197281\n",
      "Iteration 19329 => Loss: 6.71346701432826886702\n",
      "Iteration 19330 => Loss: 6.71346495411623855887\n",
      "Iteration 19331 => Loss: 6.71346289418182173847\n",
      "Iteration 19332 => Loss: 6.71346083452498643140\n",
      "Iteration 19333 => Loss: 6.71345877514569089328\n",
      "Iteration 19334 => Loss: 6.71345671604389604425\n",
      "Iteration 19335 => Loss: 6.71345465721956280447\n",
      "Iteration 19336 => Loss: 6.71345259867265475862\n",
      "Iteration 19337 => Loss: 6.71345054040314348498\n",
      "Iteration 19338 => Loss: 6.71344848241097746921\n",
      "Iteration 19339 => Loss: 6.71344642469613006597\n",
      "Iteration 19340 => Loss: 6.71344436725855597814\n",
      "Iteration 19341 => Loss: 6.71344231009822411949\n",
      "Iteration 19342 => Loss: 6.71344025321509274562\n",
      "Iteration 19343 => Loss: 6.71343819660913077030\n",
      "Iteration 19344 => Loss: 6.71343614028028845553\n",
      "Iteration 19345 => Loss: 6.71343408422854714956\n",
      "Iteration 19346 => Loss: 6.71343202845385267352\n",
      "Iteration 19347 => Loss: 6.71342997295617749387\n",
      "Iteration 19348 => Loss: 6.71342791773547542533\n",
      "Iteration 19349 => Loss: 6.71342586279171804620\n",
      "Iteration 19350 => Loss: 6.71342380812486805297\n",
      "Iteration 19351 => Loss: 6.71342175373488281309\n",
      "Iteration 19352 => Loss: 6.71341969962172147035\n",
      "Iteration 19353 => Loss: 6.71341764578535116215\n",
      "Iteration 19354 => Loss: 6.71341559222573991406\n",
      "Iteration 19355 => Loss: 6.71341353894284509352\n",
      "Iteration 19356 => Loss: 6.71341148593662939703\n",
      "Iteration 19357 => Loss: 6.71340943320705996200\n",
      "Iteration 19358 => Loss: 6.71340738075409415586\n",
      "Iteration 19359 => Loss: 6.71340532857769733965\n",
      "Iteration 19360 => Loss: 6.71340327667783220988\n",
      "Iteration 19361 => Loss: 6.71340122505445968670\n",
      "Iteration 19362 => Loss: 6.71339917370753802572\n",
      "Iteration 19363 => Loss: 6.71339712263703614070\n",
      "Iteration 19364 => Loss: 6.71339507184292383357\n",
      "Iteration 19365 => Loss: 6.71339302132515047816\n",
      "Iteration 19366 => Loss: 6.71339097108368587641\n",
      "Iteration 19367 => Loss: 6.71338892111849183664\n",
      "Iteration 19368 => Loss: 6.71338687142953904896\n",
      "Iteration 19369 => Loss: 6.71338482201676800543\n",
      "Iteration 19370 => Loss: 6.71338277288016804789\n",
      "Iteration 19371 => Loss: 6.71338072401968055658\n",
      "Iteration 19372 => Loss: 6.71337867543527710978\n",
      "Iteration 19373 => Loss: 6.71337662712692218037\n",
      "Iteration 19374 => Loss: 6.71337457909458201755\n",
      "Iteration 19375 => Loss: 6.71337253133820510698\n",
      "Iteration 19376 => Loss: 6.71337048385777457327\n",
      "Iteration 19377 => Loss: 6.71336843665323534935\n",
      "Iteration 19378 => Loss: 6.71336638972455279628\n",
      "Iteration 19379 => Loss: 6.71336434307170559777\n",
      "Iteration 19380 => Loss: 6.71336229669463957492\n",
      "Iteration 19381 => Loss: 6.71336025059331387155\n",
      "Iteration 19382 => Loss: 6.71335820476771161225\n",
      "Iteration 19383 => Loss: 6.71335615921778305903\n",
      "Iteration 19384 => Loss: 6.71335411394349357295\n",
      "Iteration 19385 => Loss: 6.71335206894480052142\n",
      "Iteration 19386 => Loss: 6.71335002422166926550\n",
      "Iteration 19387 => Loss: 6.71334797977407404801\n",
      "Iteration 19388 => Loss: 6.71334593560196069006\n",
      "Iteration 19389 => Loss: 6.71334389170529810542\n",
      "Iteration 19390 => Loss: 6.71334184808406142508\n",
      "Iteration 19391 => Loss: 6.71333980473819735835\n",
      "Iteration 19392 => Loss: 6.71333776166766860172\n",
      "Iteration 19393 => Loss: 6.71333571887244584531\n",
      "Iteration 19394 => Loss: 6.71333367635249267380\n",
      "Iteration 19395 => Loss: 6.71333163410776556645\n",
      "Iteration 19396 => Loss: 6.71332959213823610156\n",
      "Iteration 19397 => Loss: 6.71332755044386075838\n",
      "Iteration 19398 => Loss: 6.71332550902460400977\n",
      "Iteration 19399 => Loss: 6.71332346788042766406\n",
      "Iteration 19400 => Loss: 6.71332142701129352957\n",
      "Iteration 19401 => Loss: 6.71331938641717229643\n",
      "Iteration 19402 => Loss: 6.71331734609801689118\n",
      "Iteration 19403 => Loss: 6.71331530605379445120\n",
      "Iteration 19404 => Loss: 6.71331326628446944937\n",
      "Iteration 19405 => Loss: 6.71331122679000991127\n",
      "Iteration 19406 => Loss: 6.71330918757036165800\n",
      "Iteration 19407 => Loss: 6.71330714862550603783\n",
      "Iteration 19408 => Loss: 6.71330510995539331276\n",
      "Iteration 19409 => Loss: 6.71330307155999506108\n",
      "Iteration 19410 => Loss: 6.71330103343926598569\n",
      "Iteration 19411 => Loss: 6.71329899559317766489\n",
      "Iteration 19412 => Loss: 6.71329695802168746610\n",
      "Iteration 19413 => Loss: 6.71329492072476252673\n",
      "Iteration 19414 => Loss: 6.71329288370236465511\n",
      "Iteration 19415 => Loss: 6.71329084695444588959\n",
      "Iteration 19416 => Loss: 6.71328881048099024298\n",
      "Iteration 19417 => Loss: 6.71328677428194797727\n",
      "Iteration 19418 => Loss: 6.71328473835727912444\n",
      "Iteration 19419 => Loss: 6.71328270270695792732\n",
      "Iteration 19420 => Loss: 6.71328066733093287155\n",
      "Iteration 19421 => Loss: 6.71327863222918264086\n",
      "Iteration 19422 => Loss: 6.71327659740165838542\n",
      "Iteration 19423 => Loss: 6.71327456284832191358\n",
      "Iteration 19424 => Loss: 6.71327252856915102086\n",
      "Iteration 19425 => Loss: 6.71327049456409419292\n",
      "Iteration 19426 => Loss: 6.71326846083312034352\n",
      "Iteration 19427 => Loss: 6.71326642737619305734\n",
      "Iteration 19428 => Loss: 6.71326439419327325453\n",
      "Iteration 19429 => Loss: 6.71326236128432984884\n",
      "Iteration 19430 => Loss: 6.71326032864931310229\n",
      "Iteration 19431 => Loss: 6.71325829628819459316\n",
      "Iteration 19432 => Loss: 6.71325626420094589974\n",
      "Iteration 19433 => Loss: 6.71325423238751195498\n",
      "Iteration 19434 => Loss: 6.71325220084786522534\n",
      "Iteration 19435 => Loss: 6.71325016958196929551\n",
      "Iteration 19436 => Loss: 6.71324813858979485559\n",
      "Iteration 19437 => Loss: 6.71324610787129394396\n",
      "Iteration 19438 => Loss: 6.71324407742642481622\n",
      "Iteration 19439 => Loss: 6.71324204725516082704\n",
      "Iteration 19440 => Loss: 6.71324001735746467290\n",
      "Iteration 19441 => Loss: 6.71323798773329905032\n",
      "Iteration 19442 => Loss: 6.71323595838262487945\n",
      "Iteration 19443 => Loss: 6.71323392930540219226\n",
      "Iteration 19444 => Loss: 6.71323190050160611975\n",
      "Iteration 19445 => Loss: 6.71322987197118337122\n",
      "Iteration 19446 => Loss: 6.71322784371410730131\n",
      "Iteration 19447 => Loss: 6.71322581573033527746\n",
      "Iteration 19448 => Loss: 6.71322378801984331886\n",
      "Iteration 19449 => Loss: 6.71322176058258257569\n",
      "Iteration 19450 => Loss: 6.71321973341851219175\n",
      "Iteration 19451 => Loss: 6.71321770652761173892\n",
      "Iteration 19452 => Loss: 6.71321567990982703833\n",
      "Iteration 19453 => Loss: 6.71321365356513410916\n",
      "Iteration 19454 => Loss: 6.71321162749349120702\n",
      "Iteration 19455 => Loss: 6.71320960169486191660\n",
      "Iteration 19456 => Loss: 6.71320757616920271715\n",
      "Iteration 19457 => Loss: 6.71320555091649318058\n",
      "Iteration 19458 => Loss: 6.71320352593667557528\n",
      "Iteration 19459 => Loss: 6.71320150122973213769\n",
      "Iteration 19460 => Loss: 6.71319947679561845888\n",
      "Iteration 19461 => Loss: 6.71319745263429989990\n",
      "Iteration 19462 => Loss: 6.71319542874572672275\n",
      "Iteration 19463 => Loss: 6.71319340512988116387\n",
      "Iteration 19464 => Loss: 6.71319138178671792616\n",
      "Iteration 19465 => Loss: 6.71318935871618815980\n",
      "Iteration 19466 => Loss: 6.71318733591828209484\n",
      "Iteration 19467 => Loss: 6.71318531339293755877\n",
      "Iteration 19468 => Loss: 6.71318329114014034076\n",
      "Iteration 19469 => Loss: 6.71318126915983182101\n",
      "Iteration 19470 => Loss: 6.71317924745198890690\n",
      "Iteration 19471 => Loss: 6.71317722601657695947\n",
      "Iteration 19472 => Loss: 6.71317520485354712889\n",
      "Iteration 19473 => Loss: 6.71317318396286832893\n",
      "Iteration 19474 => Loss: 6.71317116334450680881\n",
      "Iteration 19475 => Loss: 6.71316914299842260050\n",
      "Iteration 19476 => Loss: 6.71316712292457928868\n",
      "Iteration 19477 => Loss: 6.71316510312294667528\n",
      "Iteration 19478 => Loss: 6.71316308359347413415\n",
      "Iteration 19479 => Loss: 6.71316106433613768445\n",
      "Iteration 19480 => Loss: 6.71315904535089646998\n",
      "Iteration 19481 => Loss: 6.71315702663771318726\n",
      "Iteration 19482 => Loss: 6.71315500819655142095\n",
      "Iteration 19483 => Loss: 6.71315299002737742029\n",
      "Iteration 19484 => Loss: 6.71315097213015032906\n",
      "Iteration 19485 => Loss: 6.71314895450482929107\n",
      "Iteration 19486 => Loss: 6.71314693715139476637\n",
      "Iteration 19487 => Loss: 6.71314492006978991157\n",
      "Iteration 19488 => Loss: 6.71314290325998985764\n",
      "Iteration 19489 => Loss: 6.71314088672195463658\n",
      "Iteration 19490 => Loss: 6.71313887045565049760\n",
      "Iteration 19491 => Loss: 6.71313685446103569632\n",
      "Iteration 19492 => Loss: 6.71313483873807559377\n",
      "Iteration 19493 => Loss: 6.71313282328673199828\n",
      "Iteration 19494 => Loss: 6.71313080810697648815\n",
      "Iteration 19495 => Loss: 6.71312879319876198991\n",
      "Iteration 19496 => Loss: 6.71312677856206097005\n",
      "Iteration 19497 => Loss: 6.71312476419682990780\n",
      "Iteration 19498 => Loss: 6.71312275010303771694\n",
      "Iteration 19499 => Loss: 6.71312073628063821218\n",
      "Iteration 19500 => Loss: 6.71311872272960652452\n",
      "Iteration 19501 => Loss: 6.71311670944990179777\n",
      "Iteration 19502 => Loss: 6.71311469644147962299\n",
      "Iteration 19503 => Loss: 6.71311268370431601937\n",
      "Iteration 19504 => Loss: 6.71311067123836835435\n",
      "Iteration 19505 => Loss: 6.71310865904360287715\n",
      "Iteration 19506 => Loss: 6.71310664711998228427\n",
      "Iteration 19507 => Loss: 6.71310463546746394314\n",
      "Iteration 19508 => Loss: 6.71310262408601943207\n",
      "Iteration 19509 => Loss: 6.71310061297560256577\n",
      "Iteration 19510 => Loss: 6.71309860213618758706\n",
      "Iteration 19511 => Loss: 6.71309659156773452793\n",
      "Iteration 19512 => Loss: 6.71309458127020164397\n",
      "Iteration 19513 => Loss: 6.71309257124355784896\n",
      "Iteration 19514 => Loss: 6.71309056148776228667\n",
      "Iteration 19515 => Loss: 6.71308855200278919995\n",
      "Iteration 19516 => Loss: 6.71308654278859329168\n",
      "Iteration 19517 => Loss: 6.71308453384513992290\n",
      "Iteration 19518 => Loss: 6.71308252517238202017\n",
      "Iteration 19519 => Loss: 6.71308051677029915538\n",
      "Iteration 19520 => Loss: 6.71307850863884869597\n",
      "Iteration 19521 => Loss: 6.71307650077798889754\n",
      "Iteration 19522 => Loss: 6.71307449318769577928\n",
      "Iteration 19523 => Loss: 6.71307248586792404410\n",
      "Iteration 19524 => Loss: 6.71307047881864615846\n",
      "Iteration 19525 => Loss: 6.71306847203980527894\n",
      "Iteration 19526 => Loss: 6.71306646553138097744\n",
      "Iteration 19527 => Loss: 6.71306445929334127953\n",
      "Iteration 19528 => Loss: 6.71306245332563289452\n",
      "Iteration 19529 => Loss: 6.71306044762822917704\n",
      "Iteration 19530 => Loss: 6.71305844220109459997\n",
      "Iteration 19531 => Loss: 6.71305643704420340612\n",
      "Iteration 19532 => Loss: 6.71305443215749431118\n",
      "Iteration 19533 => Loss: 6.71305242754094244617\n",
      "Iteration 19534 => Loss: 6.71305042319452027755\n",
      "Iteration 19535 => Loss: 6.71304841911818339639\n",
      "Iteration 19536 => Loss: 6.71304641531189183468\n",
      "Iteration 19537 => Loss: 6.71304441177561539433\n",
      "Iteration 19538 => Loss: 6.71304240850931233098\n",
      "Iteration 19539 => Loss: 6.71304040551295511108\n",
      "Iteration 19540 => Loss: 6.71303840278650465478\n",
      "Iteration 19541 => Loss: 6.71303640032990944775\n",
      "Iteration 19542 => Loss: 6.71303439814315172640\n",
      "Iteration 19543 => Loss: 6.71303239622619152271\n",
      "Iteration 19544 => Loss: 6.71303039457898798048\n",
      "Iteration 19545 => Loss: 6.71302839320151090163\n",
      "Iteration 19546 => Loss: 6.71302639209371498907\n",
      "Iteration 19547 => Loss: 6.71302439125556915656\n",
      "Iteration 19548 => Loss: 6.71302239068703077152\n",
      "Iteration 19549 => Loss: 6.71302039038807762950\n",
      "Iteration 19550 => Loss: 6.71301839035866443339\n",
      "Iteration 19551 => Loss: 6.71301639059875032700\n",
      "Iteration 19552 => Loss: 6.71301439110830688861\n",
      "Iteration 19553 => Loss: 6.71301239188729148566\n",
      "Iteration 19554 => Loss: 6.71301039293567303190\n",
      "Iteration 19555 => Loss: 6.71300839425341688838\n",
      "Iteration 19556 => Loss: 6.71300639584047953434\n",
      "Iteration 19557 => Loss: 6.71300439769683165991\n",
      "Iteration 19558 => Loss: 6.71300239982242974435\n",
      "Iteration 19559 => Loss: 6.71300040221724891865\n",
      "Iteration 19560 => Loss: 6.71299840488124299753\n",
      "Iteration 19561 => Loss: 6.71299640781437112480\n",
      "Iteration 19562 => Loss: 6.71299441101660843145\n",
      "Iteration 19563 => Loss: 6.71299241448791228493\n",
      "Iteration 19564 => Loss: 6.71299041822825426351\n",
      "Iteration 19565 => Loss: 6.71298842223759084646\n",
      "Iteration 19566 => Loss: 6.71298642651588384211\n",
      "Iteration 19567 => Loss: 6.71298443106310216422\n",
      "Iteration 19568 => Loss: 6.71298243587920850928\n",
      "Iteration 19569 => Loss: 6.71298044096417001469\n",
      "Iteration 19570 => Loss: 6.71297844631793960701\n",
      "Iteration 19571 => Loss: 6.71297645194048886452\n",
      "Iteration 19572 => Loss: 6.71297445783178137191\n",
      "Iteration 19573 => Loss: 6.71297246399178515475\n",
      "Iteration 19574 => Loss: 6.71297047042044692233\n",
      "Iteration 19575 => Loss: 6.71296847711775246381\n",
      "Iteration 19576 => Loss: 6.71296648408365381755\n",
      "Iteration 19577 => Loss: 6.71296449131811812094\n",
      "Iteration 19578 => Loss: 6.71296249882110807050\n",
      "Iteration 19579 => Loss: 6.71296050659258636273\n",
      "Iteration 19580 => Loss: 6.71295851463251391777\n",
      "Iteration 19581 => Loss: 6.71295652294085964940\n",
      "Iteration 19582 => Loss: 6.71295453151758980681\n",
      "Iteration 19583 => Loss: 6.71295254036266086928\n",
      "Iteration 19584 => Loss: 6.71295054947604175055\n",
      "Iteration 19585 => Loss: 6.71294855885769248260\n",
      "Iteration 19586 => Loss: 6.71294656850758375555\n",
      "Iteration 19587 => Loss: 6.71294457842567116046\n",
      "Iteration 19588 => Loss: 6.71294258861191917021\n",
      "Iteration 19589 => Loss: 6.71294059906629847490\n",
      "Iteration 19590 => Loss: 6.71293860978876644197\n",
      "Iteration 19591 => Loss: 6.71293662077929909060\n",
      "Iteration 19592 => Loss: 6.71293463203784401827\n",
      "Iteration 19593 => Loss: 6.71293264356437102691\n",
      "Iteration 19594 => Loss: 6.71293065535884814210\n",
      "Iteration 19595 => Loss: 6.71292866742123273127\n",
      "Iteration 19596 => Loss: 6.71292667975149193182\n",
      "Iteration 19597 => Loss: 6.71292469234959821023\n",
      "Iteration 19598 => Loss: 6.71292270521550271667\n",
      "Iteration 19599 => Loss: 6.71292071834916548312\n",
      "Iteration 19600 => Loss: 6.71291873175056430512\n",
      "Iteration 19601 => Loss: 6.71291674541965388556\n",
      "Iteration 19602 => Loss: 6.71291475935641202000\n",
      "Iteration 19603 => Loss: 6.71291277356078275318\n",
      "Iteration 19604 => Loss: 6.71291078803274299247\n",
      "Iteration 19605 => Loss: 6.71290880277224832895\n",
      "Iteration 19606 => Loss: 6.71290681777927389362\n",
      "Iteration 19607 => Loss: 6.71290483305378060663\n",
      "Iteration 19608 => Loss: 6.71290284859572139453\n",
      "Iteration 19609 => Loss: 6.71290086440506961196\n",
      "Iteration 19610 => Loss: 6.71289888048178795543\n",
      "Iteration 19611 => Loss: 6.71289689682584800323\n",
      "Iteration 19612 => Loss: 6.71289491343719380012\n",
      "Iteration 19613 => Loss: 6.71289293031581291160\n",
      "Iteration 19614 => Loss: 6.71289094746164494154\n",
      "Iteration 19615 => Loss: 6.71288896487467035001\n",
      "Iteration 19616 => Loss: 6.71288698255486071531\n",
      "Iteration 19617 => Loss: 6.71288500050216363491\n",
      "Iteration 19618 => Loss: 6.71288301871654002895\n",
      "Iteration 19619 => Loss: 6.71288103719796946933\n",
      "Iteration 19620 => Loss: 6.71287905594640310625\n",
      "Iteration 19621 => Loss: 6.71287707496181518252\n",
      "Iteration 19622 => Loss: 6.71287509424416750647\n",
      "Iteration 19623 => Loss: 6.71287311379341655737\n",
      "Iteration 19624 => Loss: 6.71287113360953302532\n",
      "Iteration 19625 => Loss: 6.71286915369248049501\n",
      "Iteration 19626 => Loss: 6.71286717404221811023\n",
      "Iteration 19627 => Loss: 6.71286519465871922563\n",
      "Iteration 19628 => Loss: 6.71286321554193587957\n",
      "Iteration 19629 => Loss: 6.71286123669184142670\n",
      "Iteration 19630 => Loss: 6.71285925810840300443\n",
      "Iteration 19631 => Loss: 6.71285727979157353928\n",
      "Iteration 19632 => Loss: 6.71285530174132283321\n",
      "Iteration 19633 => Loss: 6.71285332395761713542\n",
      "Iteration 19634 => Loss: 6.71285134644041470153\n",
      "Iteration 19635 => Loss: 6.71284936918968444530\n",
      "Iteration 19636 => Loss: 6.71284739220539083959\n",
      "Iteration 19637 => Loss: 6.71284541548748947548\n",
      "Iteration 19638 => Loss: 6.71284343903595193126\n",
      "Iteration 19639 => Loss: 6.71284146285075156158\n",
      "Iteration 19640 => Loss: 6.71283948693183329937\n",
      "Iteration 19641 => Loss: 6.71283751127916961110\n",
      "Iteration 19642 => Loss: 6.71283553589272852236\n",
      "Iteration 19643 => Loss: 6.71283356077246740057\n",
      "Iteration 19644 => Loss: 6.71283158591835960038\n",
      "Iteration 19645 => Loss: 6.71282961133035716017\n",
      "Iteration 19646 => Loss: 6.71282763700843698729\n",
      "Iteration 19647 => Loss: 6.71282566295255644917\n",
      "Iteration 19648 => Loss: 6.71282368916267380143\n",
      "Iteration 19649 => Loss: 6.71282171563876595144\n",
      "Iteration 19650 => Loss: 6.71281974238078937844\n",
      "Iteration 19651 => Loss: 6.71281776938870589078\n",
      "Iteration 19652 => Loss: 6.71281579666248440219\n",
      "Iteration 19653 => Loss: 6.71281382420209116191\n",
      "Iteration 19654 => Loss: 6.71281185200748442554\n",
      "Iteration 19655 => Loss: 6.71280988007863399503\n",
      "Iteration 19656 => Loss: 6.71280790841549279691\n",
      "Iteration 19657 => Loss: 6.71280593701804484397\n",
      "Iteration 19658 => Loss: 6.71280396588623506915\n",
      "Iteration 19659 => Loss: 6.71280199502003593892\n",
      "Iteration 19660 => Loss: 6.71280002441941192615\n",
      "Iteration 19661 => Loss: 6.71279805408432839187\n",
      "Iteration 19662 => Loss: 6.71279608401474980894\n",
      "Iteration 19663 => Loss: 6.71279411421063532117\n",
      "Iteration 19664 => Loss: 6.71279214467195028959\n",
      "Iteration 19665 => Loss: 6.71279017539866362796\n",
      "Iteration 19666 => Loss: 6.71278820639073092735\n",
      "Iteration 19667 => Loss: 6.71278623764813531238\n",
      "Iteration 19668 => Loss: 6.71278426917081549874\n",
      "Iteration 19669 => Loss: 6.71278230095875105832\n",
      "Iteration 19670 => Loss: 6.71278033301190646398\n",
      "Iteration 19671 => Loss: 6.71277836533023819499\n",
      "Iteration 19672 => Loss: 6.71277639791372315869\n",
      "Iteration 19673 => Loss: 6.71277443076230717622\n",
      "Iteration 19674 => Loss: 6.71277246387597070765\n",
      "Iteration 19675 => Loss: 6.71277049725466934404\n",
      "Iteration 19676 => Loss: 6.71276853089837644006\n",
      "Iteration 19677 => Loss: 6.71276656480704669860\n",
      "Iteration 19678 => Loss: 6.71276459898064103982\n",
      "Iteration 19679 => Loss: 6.71276263341913637106\n",
      "Iteration 19680 => Loss: 6.71276066812249805338\n",
      "Iteration 19681 => Loss: 6.71275870309067457242\n",
      "Iteration 19682 => Loss: 6.71275673832364283555\n",
      "Iteration 19683 => Loss: 6.71275477382136731563\n",
      "Iteration 19684 => Loss: 6.71275280958380360374\n",
      "Iteration 19685 => Loss: 6.71275084561092061364\n",
      "Iteration 19686 => Loss: 6.71274888190268370636\n",
      "Iteration 19687 => Loss: 6.71274691845905469023\n",
      "Iteration 19688 => Loss: 6.71274495528000336719\n",
      "Iteration 19689 => Loss: 6.71274299236548799286\n",
      "Iteration 19690 => Loss: 6.71274102971547836916\n",
      "Iteration 19691 => Loss: 6.71273906732993452806\n",
      "Iteration 19692 => Loss: 6.71273710520882449515\n",
      "Iteration 19693 => Loss: 6.71273514335210741422\n",
      "Iteration 19694 => Loss: 6.71273318175975042266\n",
      "Iteration 19695 => Loss: 6.71273122043172065787\n",
      "Iteration 19696 => Loss: 6.71272925936797904001\n",
      "Iteration 19697 => Loss: 6.71272729856849270647\n",
      "Iteration 19698 => Loss: 6.71272533803322257739\n",
      "Iteration 19699 => Loss: 6.71272337776213312566\n",
      "Iteration 19700 => Loss: 6.71272141775518793594\n",
      "Iteration 19701 => Loss: 6.71271945801236746831\n",
      "Iteration 19702 => Loss: 6.71271749853360244487\n",
      "Iteration 19703 => Loss: 6.71271553931889286559\n",
      "Iteration 19704 => Loss: 6.71271358036817922255\n",
      "Iteration 19705 => Loss: 6.71271162168144019944\n",
      "Iteration 19706 => Loss: 6.71270966325863227553\n",
      "Iteration 19707 => Loss: 6.71270770509972081186\n",
      "Iteration 19708 => Loss: 6.71270574720467205765\n",
      "Iteration 19709 => Loss: 6.71270378957345048576\n",
      "Iteration 19710 => Loss: 6.71270183220601879270\n",
      "Iteration 19711 => Loss: 6.71269987510234056316\n",
      "Iteration 19712 => Loss: 6.71269791826238293453\n",
      "Iteration 19713 => Loss: 6.71269596168611482057\n",
      "Iteration 19714 => Loss: 6.71269400537348914781\n",
      "Iteration 19715 => Loss: 6.71269204932447749457\n",
      "Iteration 19716 => Loss: 6.71269009353904966275\n",
      "Iteration 19717 => Loss: 6.71268813801715946710\n",
      "Iteration 19718 => Loss: 6.71268618275877670953\n",
      "Iteration 19719 => Loss: 6.71268422776386408657\n",
      "Iteration 19720 => Loss: 6.71268227303238695924\n",
      "Iteration 19721 => Loss: 6.71268031856431157678\n",
      "Iteration 19722 => Loss: 6.71267836435960063568\n",
      "Iteration 19723 => Loss: 6.71267641041822482606\n",
      "Iteration 19724 => Loss: 6.71267445674013174539\n",
      "Iteration 19725 => Loss: 6.71267250332529918921\n",
      "Iteration 19726 => Loss: 6.71267055017369251857\n",
      "Iteration 19727 => Loss: 6.71266859728527265361\n",
      "Iteration 19728 => Loss: 6.71266664466000850808\n",
      "Iteration 19729 => Loss: 6.71266469229786011397\n",
      "Iteration 19730 => Loss: 6.71266274019878395052\n",
      "Iteration 19731 => Loss: 6.71266078836275870145\n",
      "Iteration 19732 => Loss: 6.71265883678974262239\n",
      "Iteration 19733 => Loss: 6.71265688547970373889\n",
      "Iteration 19734 => Loss: 6.71265493443260208295\n",
      "Iteration 19735 => Loss: 6.71265298364840035106\n",
      "Iteration 19736 => Loss: 6.71265103312707989147\n",
      "Iteration 19737 => Loss: 6.71264908286858119624\n",
      "Iteration 19738 => Loss: 6.71264713287288472543\n",
      "Iteration 19739 => Loss: 6.71264518313994340559\n",
      "Iteration 19740 => Loss: 6.71264323366972970319\n",
      "Iteration 19741 => Loss: 6.71264128446221786106\n",
      "Iteration 19742 => Loss: 6.71263933551735014760\n",
      "Iteration 19743 => Loss: 6.71263738683511501648\n",
      "Iteration 19744 => Loss: 6.71263543841545651247\n",
      "Iteration 19745 => Loss: 6.71263349025835065476\n",
      "Iteration 19746 => Loss: 6.71263154236375303441\n",
      "Iteration 19747 => Loss: 6.71262959473163789426\n",
      "Iteration 19748 => Loss: 6.71262764736196970716\n",
      "Iteration 19749 => Loss: 6.71262570025470672874\n",
      "Iteration 19750 => Loss: 6.71262375340981698457\n",
      "Iteration 19751 => Loss: 6.71262180682726850023\n",
      "Iteration 19752 => Loss: 6.71261986050701242590\n",
      "Iteration 19753 => Loss: 6.71261791444902744530\n",
      "Iteration 19754 => Loss: 6.71261596865327891948\n",
      "Iteration 19755 => Loss: 6.71261402311971622225\n",
      "Iteration 19756 => Loss: 6.71261207784832780732\n",
      "Iteration 19757 => Loss: 6.71261013283906038396\n",
      "Iteration 19758 => Loss: 6.71260818809187753686\n",
      "Iteration 19759 => Loss: 6.71260624360675617339\n",
      "Iteration 19760 => Loss: 6.71260429938364833191\n",
      "Iteration 19761 => Loss: 6.71260235542253003160\n",
      "Iteration 19762 => Loss: 6.71260041172336130444\n",
      "Iteration 19763 => Loss: 6.71259846828610218239\n",
      "Iteration 19764 => Loss: 6.71259652511072157921\n",
      "Iteration 19765 => Loss: 6.71259458219718752048\n",
      "Iteration 19766 => Loss: 6.71259263954545737363\n",
      "Iteration 19767 => Loss: 6.71259069715550005242\n",
      "Iteration 19768 => Loss: 6.71258875502728002971\n",
      "Iteration 19769 => Loss: 6.71258681316076089018\n",
      "Iteration 19770 => Loss: 6.71258487155590621853\n",
      "Iteration 19771 => Loss: 6.71258293021268936940\n",
      "Iteration 19772 => Loss: 6.71258098913106948658\n",
      "Iteration 19773 => Loss: 6.71257904831100837839\n",
      "Iteration 19774 => Loss: 6.71257710775246607682\n",
      "Iteration 19775 => Loss: 6.71257516745542393011\n",
      "Iteration 19776 => Loss: 6.71257322741982953573\n",
      "Iteration 19777 => Loss: 6.71257128764565802470\n",
      "Iteration 19778 => Loss: 6.71256934813287653441\n",
      "Iteration 19779 => Loss: 6.71256740888143266233\n",
      "Iteration 19780 => Loss: 6.71256546989131663850\n",
      "Iteration 19781 => Loss: 6.71256353116247339585\n",
      "Iteration 19782 => Loss: 6.71256159269487184815\n",
      "Iteration 19783 => Loss: 6.71255965448848268551\n",
      "Iteration 19784 => Loss: 6.71255771654326327536\n",
      "Iteration 19785 => Loss: 6.71255577885918608416\n",
      "Iteration 19786 => Loss: 6.71255384143620581483\n",
      "Iteration 19787 => Loss: 6.71255190427429582201\n",
      "Iteration 19788 => Loss: 6.71254996737342501945\n",
      "Iteration 19789 => Loss: 6.71254803073354722187\n",
      "Iteration 19790 => Loss: 6.71254609435463134304\n",
      "Iteration 19791 => Loss: 6.71254415823664452034\n",
      "Iteration 19792 => Loss: 6.71254222237954856212\n",
      "Iteration 19793 => Loss: 6.71254028678330882940\n",
      "Iteration 19794 => Loss: 6.71253835144788890688\n",
      "Iteration 19795 => Loss: 6.71253641637325415559\n",
      "Iteration 19796 => Loss: 6.71253448155938325925\n",
      "Iteration 19797 => Loss: 6.71253254700620960449\n",
      "Iteration 19798 => Loss: 6.71253061271373052676\n",
      "Iteration 19799 => Loss: 6.71252867868189451173\n",
      "Iteration 19800 => Loss: 6.71252674491067669038\n",
      "Iteration 19801 => Loss: 6.71252481140002199567\n",
      "Iteration 19802 => Loss: 6.71252287814990999948\n",
      "Iteration 19803 => Loss: 6.71252094516031139193\n",
      "Iteration 19804 => Loss: 6.71251901243117732321\n",
      "Iteration 19805 => Loss: 6.71251707996248025978\n",
      "Iteration 19806 => Loss: 6.71251514775418112180\n",
      "Iteration 19807 => Loss: 6.71251321580625148755\n",
      "Iteration 19808 => Loss: 6.71251128411864961265\n",
      "Iteration 19809 => Loss: 6.71250935269134529904\n",
      "Iteration 19810 => Loss: 6.71250742152429502596\n",
      "Iteration 19811 => Loss: 6.71250549061747125990\n",
      "Iteration 19812 => Loss: 6.71250355997084646731\n",
      "Iteration 19813 => Loss: 6.71250162958437091021\n",
      "Iteration 19814 => Loss: 6.71249969945800817328\n",
      "Iteration 19815 => Loss: 6.71249776959173960478\n",
      "Iteration 19816 => Loss: 6.71249583998551635489\n",
      "Iteration 19817 => Loss: 6.71249391063930911372\n",
      "Iteration 19818 => Loss: 6.71249198155308057778\n",
      "Iteration 19819 => Loss: 6.71249005272679166723\n",
      "Iteration 19820 => Loss: 6.71248812416042106577\n",
      "Iteration 19821 => Loss: 6.71248619585391637088\n",
      "Iteration 19822 => Loss: 6.71248426780725893082\n",
      "Iteration 19823 => Loss: 6.71248234002040167212\n",
      "Iteration 19824 => Loss: 6.71248041249331883762\n",
      "Iteration 19825 => Loss: 6.71247848522596335386\n",
      "Iteration 19826 => Loss: 6.71247655821831212819\n",
      "Iteration 19827 => Loss: 6.71247463147032075170\n",
      "Iteration 19828 => Loss: 6.71247270498196080268\n",
      "Iteration 19829 => Loss: 6.71247077875319231310\n",
      "Iteration 19830 => Loss: 6.71246885278399130215\n",
      "Iteration 19831 => Loss: 6.71246692707431158453\n",
      "Iteration 19832 => Loss: 6.71246500162412296220\n",
      "Iteration 19833 => Loss: 6.71246307643338369076\n",
      "Iteration 19834 => Loss: 6.71246115150206623667\n",
      "Iteration 19835 => Loss: 6.71245922683012974375\n",
      "Iteration 19836 => Loss: 6.71245730241755111933\n",
      "Iteration 19837 => Loss: 6.71245537826428062544\n",
      "Iteration 19838 => Loss: 6.71245345437029339308\n",
      "Iteration 19839 => Loss: 6.71245153073555123058\n",
      "Iteration 19840 => Loss: 6.71244960736001949897\n",
      "Iteration 19841 => Loss: 6.71244768424365823023\n",
      "Iteration 19842 => Loss: 6.71244576138644344354\n",
      "Iteration 19843 => Loss: 6.71244383878832984180\n",
      "Iteration 19844 => Loss: 6.71244191644928367424\n",
      "Iteration 19845 => Loss: 6.71243999436928273639\n",
      "Iteration 19846 => Loss: 6.71243807254826840847\n",
      "Iteration 19847 => Loss: 6.71243615098622559145\n",
      "Iteration 19848 => Loss: 6.71243422968311787002\n",
      "Iteration 19849 => Loss: 6.71243230863890527615\n",
      "Iteration 19850 => Loss: 6.71243038785354606546\n",
      "Iteration 19851 => Loss: 6.71242846732701892165\n",
      "Iteration 19852 => Loss: 6.71242654705928387671\n",
      "Iteration 19853 => Loss: 6.71242462705029829806\n",
      "Iteration 19854 => Loss: 6.71242270730003820489\n",
      "Iteration 19855 => Loss: 6.71242078780847073460\n",
      "Iteration 19856 => Loss: 6.71241886857554259649\n",
      "Iteration 19857 => Loss: 6.71241694960124046787\n",
      "Iteration 19858 => Loss: 6.71241503088551993983\n",
      "Iteration 19859 => Loss: 6.71241311242834814976\n",
      "Iteration 19860 => Loss: 6.71241119422967980057\n",
      "Iteration 19861 => Loss: 6.71240927628949446415\n",
      "Iteration 19862 => Loss: 6.71240735860775217247\n",
      "Iteration 19863 => Loss: 6.71240544118441473387\n",
      "Iteration 19864 => Loss: 6.71240352401945639116\n",
      "Iteration 19865 => Loss: 6.71240160711282829453\n",
      "Iteration 19866 => Loss: 6.71239969046450557499\n",
      "Iteration 19867 => Loss: 6.71239777407445448176\n",
      "Iteration 19868 => Loss: 6.71239585794263504681\n",
      "Iteration 19869 => Loss: 6.71239394206901351936\n",
      "Iteration 19870 => Loss: 6.71239202645355881316\n",
      "Iteration 19871 => Loss: 6.71239011109623540108\n",
      "Iteration 19872 => Loss: 6.71238819599700331509\n",
      "Iteration 19873 => Loss: 6.71238628115583146894\n",
      "Iteration 19874 => Loss: 6.71238436657268167096\n",
      "Iteration 19875 => Loss: 6.71238245224752372309\n",
      "Iteration 19876 => Loss: 6.71238053818032121001\n",
      "Iteration 19877 => Loss: 6.71237862437103949276\n",
      "Iteration 19878 => Loss: 6.71237671081964926145\n",
      "Iteration 19879 => Loss: 6.71237479752610344264\n",
      "Iteration 19880 => Loss: 6.71237288449037627913\n",
      "Iteration 19881 => Loss: 6.71237097171242780291\n",
      "Iteration 19882 => Loss: 6.71236905919223669770\n",
      "Iteration 19883 => Loss: 6.71236714692974878460\n",
      "Iteration 19884 => Loss: 6.71236523492493741827\n",
      "Iteration 19885 => Loss: 6.71236332317776529521\n",
      "Iteration 19886 => Loss: 6.71236141168820932279\n",
      "Iteration 19887 => Loss: 6.71235950045621976301\n",
      "Iteration 19888 => Loss: 6.71235758948177352323\n",
      "Iteration 19889 => Loss: 6.71235567876483329997\n",
      "Iteration 19890 => Loss: 6.71235376830536356607\n",
      "Iteration 19891 => Loss: 6.71235185810332612988\n",
      "Iteration 19892 => Loss: 6.71234994815868279971\n",
      "Iteration 19893 => Loss: 6.71234803847141492383\n",
      "Iteration 19894 => Loss: 6.71234612904147631696\n",
      "Iteration 19895 => Loss: 6.71234421986882878741\n",
      "Iteration 19896 => Loss: 6.71234231095343769624\n",
      "Iteration 19897 => Loss: 6.71234040229527906263\n",
      "Iteration 19898 => Loss: 6.71233849389431114218\n",
      "Iteration 19899 => Loss: 6.71233658575050284867\n",
      "Iteration 19900 => Loss: 6.71233467786381599041\n",
      "Iteration 19901 => Loss: 6.71233277023421237573\n",
      "Iteration 19902 => Loss: 6.71233086286167246470\n",
      "Iteration 19903 => Loss: 6.71232895574614030210\n",
      "Iteration 19904 => Loss: 6.71232704888759368345\n",
      "Iteration 19905 => Loss: 6.71232514228600241069\n",
      "Iteration 19906 => Loss: 6.71232323594132207489\n",
      "Iteration 19907 => Loss: 6.71232132985352514254\n",
      "Iteration 19908 => Loss: 6.71231942402256809288\n",
      "Iteration 19909 => Loss: 6.71231751844842516874\n",
      "Iteration 19910 => Loss: 6.71231561313105640210\n",
      "Iteration 19911 => Loss: 6.71231370807043337123\n",
      "Iteration 19912 => Loss: 6.71231180326651433177\n",
      "Iteration 19913 => Loss: 6.71230989871926642110\n",
      "Iteration 19914 => Loss: 6.71230799442865855298\n",
      "Iteration 19915 => Loss: 6.71230609039465253574\n",
      "Iteration 19916 => Loss: 6.71230418661721728313\n",
      "Iteration 19917 => Loss: 6.71230228309631726802\n",
      "Iteration 19918 => Loss: 6.71230037983191341056\n",
      "Iteration 19919 => Loss: 6.71229847682397284814\n",
      "Iteration 19920 => Loss: 6.71229657407246982359\n",
      "Iteration 19921 => Loss: 6.71229467157735815164\n",
      "Iteration 19922 => Loss: 6.71229276933860852239\n",
      "Iteration 19923 => Loss: 6.71229086735618718507\n",
      "Iteration 19924 => Loss: 6.71228896563006038889\n",
      "Iteration 19925 => Loss: 6.71228706416018283676\n",
      "Iteration 19926 => Loss: 6.71228516294653321239\n",
      "Iteration 19927 => Loss: 6.71228326198907421229\n",
      "Iteration 19928 => Loss: 6.71228136128776675662\n",
      "Iteration 19929 => Loss: 6.71227946084257887094\n",
      "Iteration 19930 => Loss: 6.71227756065347325176\n",
      "Iteration 19931 => Loss: 6.71227566072042236556\n",
      "Iteration 19932 => Loss: 6.71227376104339246154\n",
      "Iteration 19933 => Loss: 6.71227186162233735445\n",
      "Iteration 19934 => Loss: 6.71226996245723395162\n",
      "Iteration 19935 => Loss: 6.71226806354803784416\n",
      "Iteration 19936 => Loss: 6.71226616489471972216\n",
      "Iteration 19937 => Loss: 6.71226426649725205209\n",
      "Iteration 19938 => Loss: 6.71226236835558598415\n",
      "Iteration 19939 => Loss: 6.71226047046970109022\n",
      "Iteration 19940 => Loss: 6.71225857283954852051\n",
      "Iteration 19941 => Loss: 6.71225667546511139960\n",
      "Iteration 19942 => Loss: 6.71225477834633998953\n",
      "Iteration 19943 => Loss: 6.71225288148320764492\n",
      "Iteration 19944 => Loss: 6.71225098487567528593\n",
      "Iteration 19945 => Loss: 6.71224908852371182633\n",
      "Iteration 19946 => Loss: 6.71224719242728262714\n",
      "Iteration 19947 => Loss: 6.71224529658635127305\n",
      "Iteration 19948 => Loss: 6.71224340100088934236\n",
      "Iteration 19949 => Loss: 6.71224150567085064978\n",
      "Iteration 19950 => Loss: 6.71223961059621476721\n",
      "Iteration 19951 => Loss: 6.71223771577692573942\n",
      "Iteration 19952 => Loss: 6.71223582121298356640\n",
      "Iteration 19953 => Loss: 6.71223392690432341112\n",
      "Iteration 19954 => Loss: 6.71223203285092484549\n",
      "Iteration 19955 => Loss: 6.71223013905274701330\n",
      "Iteration 19956 => Loss: 6.71222824550976326918\n",
      "Iteration 19957 => Loss: 6.71222635222192920423\n",
      "Iteration 19958 => Loss: 6.71222445918922083763\n",
      "Iteration 19959 => Loss: 6.71222256641158931956\n",
      "Iteration 19960 => Loss: 6.71222067388901866281\n",
      "Iteration 19961 => Loss: 6.71221878162146179392\n",
      "Iteration 19962 => Loss: 6.71221688960889473208\n",
      "Iteration 19963 => Loss: 6.71221499785127395654\n",
      "Iteration 19964 => Loss: 6.71221310634855949928\n",
      "Iteration 19965 => Loss: 6.71221121510072915584\n",
      "Iteration 19966 => Loss: 6.71220932410774384635\n",
      "Iteration 19967 => Loss: 6.71220743336957603731\n",
      "Iteration 19968 => Loss: 6.71220554288618220795\n",
      "Iteration 19969 => Loss: 6.71220365265753216022\n",
      "Iteration 19970 => Loss: 6.71220176268359214333\n",
      "Iteration 19971 => Loss: 6.71219987296432130108\n",
      "Iteration 19972 => Loss: 6.71219798349969565265\n",
      "Iteration 19973 => Loss: 6.71219609428966546005\n",
      "Iteration 19974 => Loss: 6.71219420533421029518\n",
      "Iteration 19975 => Loss: 6.71219231663330262450\n",
      "Iteration 19976 => Loss: 6.71219042818688471641\n",
      "Iteration 19977 => Loss: 6.71218853999494413642\n",
      "Iteration 19978 => Loss: 6.71218665205742848201\n",
      "Iteration 19979 => Loss: 6.71218476437432265413\n",
      "Iteration 19980 => Loss: 6.71218287694557869116\n",
      "Iteration 19981 => Loss: 6.71218098977116106596\n",
      "Iteration 19982 => Loss: 6.71217910285104402135\n",
      "Iteration 19983 => Loss: 6.71217721618518847748\n",
      "Iteration 19984 => Loss: 6.71217532977356068358\n",
      "Iteration 19985 => Loss: 6.71217344361613577064\n",
      "Iteration 19986 => Loss: 6.71217155771286133614\n",
      "Iteration 19987 => Loss: 6.71216967206371251109\n",
      "Iteration 19988 => Loss: 6.71216778666865643288\n",
      "Iteration 19989 => Loss: 6.71216590152766023891\n",
      "Iteration 19990 => Loss: 6.71216401664068396116\n",
      "Iteration 19991 => Loss: 6.71216213200769562519\n",
      "Iteration 19992 => Loss: 6.71216024762866858566\n",
      "Iteration 19993 => Loss: 6.71215836350355310458\n",
      "Iteration 19994 => Loss: 6.71215647963232520112\n",
      "Iteration 19995 => Loss: 6.71215459601495378905\n",
      "Iteration 19996 => Loss: 6.71215271265139179491\n",
      "Iteration 19997 => Loss: 6.71215082954161701423\n",
      "Iteration 19998 => Loss: 6.71214894668559747259\n",
      "Iteration 19999 => Loss: 6.71214706408329053744\n",
      "Iteration 20000 => Loss: 6.71214518173466512252\n",
      "Iteration 20001 => Loss: 6.71214329963967593073\n",
      "Iteration 20002 => Loss: 6.71214141779831319212\n",
      "Iteration 20003 => Loss: 6.71213953621052272780\n",
      "Iteration 20004 => Loss: 6.71213765487627700423\n",
      "Iteration 20005 => Loss: 6.71213577379553871793\n",
      "Iteration 20006 => Loss: 6.71213389296827855901\n",
      "Iteration 20007 => Loss: 6.71213201239446011215\n",
      "Iteration 20008 => Loss: 6.71213013207404340932\n",
      "Iteration 20009 => Loss: 6.71212825200700358153\n",
      "Iteration 20010 => Loss: 6.71212637219330510163\n",
      "Iteration 20011 => Loss: 6.71212449263291333068\n",
      "Iteration 20012 => Loss: 6.71212261332578918882\n",
      "Iteration 20013 => Loss: 6.71212073427190247799\n",
      "Iteration 20014 => Loss: 6.71211885547121678286\n",
      "Iteration 20015 => Loss: 6.71211697692370190538\n",
      "Iteration 20016 => Loss: 6.71211509862932231840\n",
      "Iteration 20017 => Loss: 6.71211322058803983026\n",
      "Iteration 20018 => Loss: 6.71211134279982513107\n",
      "Iteration 20019 => Loss: 6.71210946526464447004\n",
      "Iteration 20020 => Loss: 6.71210758798245787915\n",
      "Iteration 20021 => Loss: 6.71210571095323871305\n",
      "Iteration 20022 => Loss: 6.71210383417694789188\n",
      "Iteration 20023 => Loss: 6.71210195765355432940\n",
      "Iteration 20024 => Loss: 6.71210008138301539304\n",
      "Iteration 20025 => Loss: 6.71209820536531687196\n",
      "Iteration 20026 => Loss: 6.71209632960040192273\n",
      "Iteration 20027 => Loss: 6.71209445408824745272\n",
      "Iteration 20028 => Loss: 6.71209257882881882296\n",
      "Iteration 20029 => Loss: 6.71209070382208494721\n",
      "Iteration 20030 => Loss: 6.71208882906800763379\n",
      "Iteration 20031 => Loss: 6.71208695456655313194\n",
      "Iteration 20032 => Loss: 6.71208508031768413815\n",
      "Iteration 20033 => Loss: 6.71208320632137578343\n",
      "Iteration 20034 => Loss: 6.71208133257758632340\n",
      "Iteration 20035 => Loss: 6.71207945908628289544\n",
      "Iteration 20036 => Loss: 6.71207758584743441332\n",
      "Iteration 20037 => Loss: 6.71207571286099913266\n",
      "Iteration 20038 => Loss: 6.71207384012695396081\n",
      "Iteration 20039 => Loss: 6.71207196764525981791\n",
      "Iteration 20040 => Loss: 6.71207009541588384138\n",
      "Iteration 20041 => Loss: 6.71206822343879050408\n",
      "Iteration 20042 => Loss: 6.71206635171394072614\n",
      "Iteration 20043 => Loss: 6.71206448024131052676\n",
      "Iteration 20044 => Loss: 6.71206260902086793152\n",
      "Iteration 20045 => Loss: 6.71206073805256231424\n",
      "Iteration 20046 => Loss: 6.71205886733636969410\n",
      "Iteration 20047 => Loss: 6.71205699687226875483\n",
      "Iteration 20048 => Loss: 6.71205512666019821211\n",
      "Iteration 20049 => Loss: 6.71205325670014651962\n",
      "Iteration 20050 => Loss: 6.71205138699207282116\n",
      "Iteration 20051 => Loss: 6.71204951753593714869\n",
      "Iteration 20052 => Loss: 6.71204764833171729776\n",
      "Iteration 20053 => Loss: 6.71204577937936885945\n",
      "Iteration 20054 => Loss: 6.71204391067886163569\n",
      "Iteration 20055 => Loss: 6.71204204223015654662\n",
      "Iteration 20056 => Loss: 6.71204017403323671687\n",
      "Iteration 20057 => Loss: 6.71203830608805507296\n",
      "Iteration 20058 => Loss: 6.71203643839457253506\n",
      "Iteration 20059 => Loss: 6.71203457095276867506\n",
      "Iteration 20060 => Loss: 6.71203270376259020225\n",
      "Iteration 20061 => Loss: 6.71203083682402734667\n",
      "Iteration 20062 => Loss: 6.71202897013703303486\n",
      "Iteration 20063 => Loss: 6.71202710370156907516\n",
      "Iteration 20064 => Loss: 6.71202523751761592763\n",
      "Iteration 20065 => Loss: 6.71202337158511763704\n",
      "Iteration 20066 => Loss: 6.71202150590406798614\n",
      "Iteration 20067 => Loss: 6.71201964047441368422\n",
      "Iteration 20068 => Loss: 6.71201777529612897411\n",
      "Iteration 20069 => Loss: 6.71201591036917211142\n",
      "Iteration 20070 => Loss: 6.71201404569352089169\n",
      "Iteration 20071 => Loss: 6.71201218126913179418\n",
      "Iteration 20072 => Loss: 6.71201031709596929176\n",
      "Iteration 20073 => Loss: 6.71200845317401206813\n",
      "Iteration 20074 => Loss: 6.71200658950321216167\n",
      "Iteration 20075 => Loss: 6.71200472608354914428\n",
      "Iteration 20076 => Loss: 6.71200286291497416613\n",
      "Iteration 20077 => Loss: 6.71200099999746502277\n",
      "Iteration 20078 => Loss: 6.71199913733097908164\n",
      "Iteration 20079 => Loss: 6.71199727491549946734\n",
      "Iteration 20080 => Loss: 6.71199541275097022464\n",
      "Iteration 20081 => Loss: 6.71199355083737358996\n",
      "Iteration 20082 => Loss: 6.71199168917466515438\n",
      "Iteration 20083 => Loss: 6.71198982776281827256\n",
      "Iteration 20084 => Loss: 6.71198796660179741735\n",
      "Iteration 20085 => Loss: 6.71198610569156706163\n",
      "Iteration 20086 => Loss: 6.71198424503209523095\n",
      "Iteration 20087 => Loss: 6.71198238462334551002\n",
      "Iteration 20088 => Loss: 6.71198052446528947712\n",
      "Iteration 20089 => Loss: 6.71197866455788449969\n",
      "Iteration 20090 => Loss: 6.71197680490111014961\n",
      "Iteration 20091 => Loss: 6.71197494549491757709\n",
      "Iteration 20092 => Loss: 6.71197308633927836041\n",
      "Iteration 20093 => Loss: 6.71197122743416851876\n",
      "Iteration 20094 => Loss: 6.71196936877953920231\n",
      "Iteration 20095 => Loss: 6.71196751037537087115\n",
      "Iteration 20096 => Loss: 6.71196565222162089270\n",
      "Iteration 20097 => Loss: 6.71196379431825107531\n",
      "Iteration 20098 => Loss: 6.71196193666524454358\n",
      "Iteration 20099 => Loss: 6.71196007926254090137\n",
      "Iteration 20100 => Loss: 6.71195822211013659597\n",
      "Iteration 20101 => Loss: 6.71195636520797744851\n",
      "Iteration 20102 => Loss: 6.71195450855604214269\n",
      "Iteration 20103 => Loss: 6.71195265215428893413\n",
      "Iteration 20104 => Loss: 6.71195079600267607844\n",
      "Iteration 20105 => Loss: 6.71194894010118492389\n",
      "Iteration 20106 => Loss: 6.71194708444978083151\n",
      "Iteration 20107 => Loss: 6.71194522904843093869\n",
      "Iteration 20108 => Loss: 6.71194337389708728381\n",
      "Iteration 20109 => Loss: 6.71194151899572499786\n",
      "Iteration 20110 => Loss: 6.71193966434431388279\n",
      "Iteration 20111 => Loss: 6.71193780994281663510\n",
      "Iteration 20112 => Loss: 6.71193595579119861583\n",
      "Iteration 20113 => Loss: 6.71193410188942429784\n",
      "Iteration 20114 => Loss: 6.71193224823747058849\n",
      "Iteration 20115 => Loss: 6.71193039483530107248\n",
      "Iteration 20116 => Loss: 6.71192854168286512362\n",
      "Iteration 20117 => Loss: 6.71192668878015030742\n",
      "Iteration 20118 => Loss: 6.71192483612711221497\n",
      "Iteration 20119 => Loss: 6.71192298372371443094\n",
      "Iteration 20120 => Loss: 6.71192113156993297451\n",
      "Iteration 20121 => Loss: 6.71191927966573054221\n",
      "Iteration 20122 => Loss: 6.71191742801106805416\n",
      "Iteration 20123 => Loss: 6.71191557660592330592\n",
      "Iteration 20124 => Loss: 6.71191372545024567131\n",
      "Iteration 20125 => Loss: 6.71191187454401649859\n",
      "Iteration 20126 => Loss: 6.71191002388719404337\n",
      "Iteration 20127 => Loss: 6.71190817347975521301\n",
      "Iteration 20128 => Loss: 6.71190632332165115770\n",
      "Iteration 20129 => Loss: 6.71190447341286589023\n",
      "Iteration 20130 => Loss: 6.71190262375334878442\n",
      "Iteration 20131 => Loss: 6.71190077434307319493\n",
      "Iteration 20132 => Loss: 6.71189892518200714733\n",
      "Iteration 20133 => Loss: 6.71189707627011511448\n",
      "Iteration 20134 => Loss: 6.71189522760735890472\n",
      "Iteration 20135 => Loss: 6.71189337919372253083\n",
      "Iteration 20136 => Loss: 6.71189153102915181393\n",
      "Iteration 20137 => Loss: 6.71188968311362099683\n",
      "Iteration 20138 => Loss: 6.71188783544709632878\n",
      "Iteration 20139 => Loss: 6.71188598802954849987\n",
      "Iteration 20140 => Loss: 6.71188414086094020661\n",
      "Iteration 20141 => Loss: 6.71188229394123858640\n",
      "Iteration 20142 => Loss: 6.71188044727040367121\n",
      "Iteration 20143 => Loss: 6.71187860084841236841\n",
      "Iteration 20144 => Loss: 6.71187675467522470996\n",
      "Iteration 20145 => Loss: 6.71187490875081227415\n",
      "Iteration 20146 => Loss: 6.71187306307513242842\n",
      "Iteration 20147 => Loss: 6.71187121764816652103\n",
      "Iteration 20148 => Loss: 6.71186937246986659034\n",
      "Iteration 20149 => Loss: 6.71186752754020421463\n",
      "Iteration 20150 => Loss: 6.71186568285914564314\n",
      "Iteration 20151 => Loss: 6.71186383842666245414\n",
      "Iteration 20152 => Loss: 6.71186199424271112690\n",
      "Iteration 20153 => Loss: 6.71186015030726501607\n",
      "Iteration 20154 => Loss: 6.71185830662029037086\n",
      "Iteration 20155 => Loss: 6.71185646318175610503\n",
      "Iteration 20156 => Loss: 6.71185461999161869784\n",
      "Iteration 20157 => Loss: 6.71185277704985683300\n",
      "Iteration 20158 => Loss: 6.71185093435643231885\n",
      "Iteration 20159 => Loss: 6.71184909191130429917\n",
      "Iteration 20160 => Loss: 6.71184724971444790498\n",
      "Iteration 20161 => Loss: 6.71184540776583293820\n",
      "Iteration 20162 => Loss: 6.71184356606541943080\n",
      "Iteration 20163 => Loss: 6.71184172461316741476\n",
      "Iteration 20164 => Loss: 6.71183988340906001469\n",
      "Iteration 20165 => Loss: 6.71183804245304749259\n",
      "Iteration 20166 => Loss: 6.71183620174511119671\n",
      "Iteration 20167 => Loss: 6.71183436128520938269\n",
      "Iteration 20168 => Loss: 6.71183252107330385883\n",
      "Iteration 20169 => Loss: 6.71183068110937330886\n",
      "Iteration 20170 => Loss: 6.71182884139337598839\n",
      "Iteration 20171 => Loss: 6.71182700192527725847\n",
      "Iteration 20172 => Loss: 6.71182516270505225009\n",
      "Iteration 20173 => Loss: 6.71182332373265744252\n",
      "Iteration 20174 => Loss: 6.71182148500807063130\n",
      "Iteration 20175 => Loss: 6.71181964653124474296\n",
      "Iteration 20176 => Loss: 6.71181780830215579670\n",
      "Iteration 20177 => Loss: 6.71181597032077537079\n",
      "Iteration 20178 => Loss: 6.71181413258705461544\n",
      "Iteration 20179 => Loss: 6.71181229510097132618\n",
      "Iteration 20180 => Loss: 6.71181045786248908769\n",
      "Iteration 20181 => Loss: 6.71180862087157592555\n",
      "Iteration 20182 => Loss: 6.71180678412819720080\n",
      "Iteration 20183 => Loss: 6.71180494763231916266\n",
      "Iteration 20184 => Loss: 6.71180311138390628400\n",
      "Iteration 20185 => Loss: 6.71180127538293547218\n",
      "Iteration 20186 => Loss: 6.71179943962935876556\n",
      "Iteration 20187 => Loss: 6.71179760412315307150\n",
      "Iteration 20188 => Loss: 6.71179576886428552740\n",
      "Iteration 20189 => Loss: 6.71179393385271172434\n",
      "Iteration 20190 => Loss: 6.71179209908840856968\n",
      "Iteration 20191 => Loss: 6.71179026457134320083\n",
      "Iteration 20192 => Loss: 6.71178843030147653792\n",
      "Iteration 20193 => Loss: 6.71178659627878193561\n",
      "Iteration 20194 => Loss: 6.71178476250321853769\n",
      "Iteration 20195 => Loss: 6.71178292897475525791\n",
      "Iteration 20196 => Loss: 6.71178109569335745732\n",
      "Iteration 20197 => Loss: 6.71177926265899760239\n",
      "Iteration 20198 => Loss: 6.71177742987164105415\n",
      "Iteration 20199 => Loss: 6.71177559733125672636\n",
      "Iteration 20200 => Loss: 6.71177376503779665740\n",
      "Iteration 20201 => Loss: 6.71177193299124574821\n",
      "Iteration 20202 => Loss: 6.71177010119155870171\n",
      "Iteration 20203 => Loss: 6.71176826963870976073\n",
      "Iteration 20204 => Loss: 6.71176643833266428629\n",
      "Iteration 20205 => Loss: 6.71176460727339208034\n",
      "Iteration 20206 => Loss: 6.71176277646084340489\n",
      "Iteration 20207 => Loss: 6.71176094589500671361\n",
      "Iteration 20208 => Loss: 6.71175911557583138034\n",
      "Iteration 20209 => Loss: 6.71175728550329520061\n",
      "Iteration 20210 => Loss: 6.71175545567735820640\n",
      "Iteration 20211 => Loss: 6.71175362609799730507\n",
      "Iteration 20212 => Loss: 6.71175179676516453497\n",
      "Iteration 20213 => Loss: 6.71174996767884302074\n",
      "Iteration 20214 => Loss: 6.71174813883898213618\n",
      "Iteration 20215 => Loss: 6.71174631024556145320\n",
      "Iteration 20216 => Loss: 6.71174448189854366831\n",
      "Iteration 20217 => Loss: 6.71174265379789858343\n",
      "Iteration 20218 => Loss: 6.71174082594358711873\n",
      "Iteration 20219 => Loss: 6.71173899833558174066\n",
      "Iteration 20220 => Loss: 6.71173717097384248120\n",
      "Iteration 20221 => Loss: 6.71173534385834091864\n",
      "Iteration 20222 => Loss: 6.71173351698904685492\n",
      "Iteration 20223 => Loss: 6.71173169036592387471\n",
      "Iteration 20224 => Loss: 6.71172986398893556270\n",
      "Iteration 20225 => Loss: 6.71172803785805260901\n",
      "Iteration 20226 => Loss: 6.71172621197323771014\n",
      "Iteration 20227 => Loss: 6.71172438633446688527\n",
      "Iteration 20228 => Loss: 6.71172256094169927820\n",
      "Iteration 20229 => Loss: 6.71172073579490202633\n",
      "Iteration 20230 => Loss: 6.71171891089404848429\n",
      "Iteration 20231 => Loss: 6.71171708623909246683\n",
      "Iteration 20232 => Loss: 6.71171526183000910493\n",
      "Iteration 20233 => Loss: 6.71171343766676908871\n",
      "Iteration 20234 => Loss: 6.71171161374933511468\n",
      "Iteration 20235 => Loss: 6.71170979007767520841\n",
      "Iteration 20236 => Loss: 6.71170796665175295459\n",
      "Iteration 20237 => Loss: 6.71170614347153904333\n",
      "Iteration 20238 => Loss: 6.71170432053699528296\n",
      "Iteration 20239 => Loss: 6.71170249784809147542\n",
      "Iteration 20240 => Loss: 6.71170067540480186352\n",
      "Iteration 20241 => Loss: 6.71169885320708203835\n",
      "Iteration 20242 => Loss: 6.71169703125490446638\n",
      "Iteration 20243 => Loss: 6.71169520954823628500\n",
      "Iteration 20244 => Loss: 6.71169338808704640797\n",
      "Iteration 20245 => Loss: 6.71169156687128687366\n",
      "Iteration 20246 => Loss: 6.71168974590094435939\n",
      "Iteration 20247 => Loss: 6.71168792517597712077\n",
      "Iteration 20248 => Loss: 6.71168610469635584792\n",
      "Iteration 20249 => Loss: 6.71168428446204412552\n",
      "Iteration 20250 => Loss: 6.71168246447300287372\n",
      "Iteration 20251 => Loss: 6.71168064472920988806\n",
      "Iteration 20252 => Loss: 6.71167882523062075961\n",
      "Iteration 20253 => Loss: 6.71167700597722038935\n",
      "Iteration 20254 => Loss: 6.71167518696895193386\n",
      "Iteration 20255 => Loss: 6.71167336820580384682\n",
      "Iteration 20256 => Loss: 6.71167154968772994295\n",
      "Iteration 20257 => Loss: 6.71166973141471068232\n",
      "Iteration 20258 => Loss: 6.71166791338669366240\n",
      "Iteration 20259 => Loss: 6.71166609560365934328\n",
      "Iteration 20260 => Loss: 6.71166427806557308600\n",
      "Iteration 20261 => Loss: 6.71166246077239581069\n",
      "Iteration 20262 => Loss: 6.71166064372410264838\n",
      "Iteration 20263 => Loss: 6.71165882692065540738\n",
      "Iteration 20264 => Loss: 6.71165701036202211327\n",
      "Iteration 20265 => Loss: 6.71165519404817079163\n",
      "Iteration 20266 => Loss: 6.71165337797906946804\n",
      "Iteration 20267 => Loss: 6.71165156215467995082\n",
      "Iteration 20268 => Loss: 6.71164974657497914734\n",
      "Iteration 20269 => Loss: 6.71164793123992264867\n",
      "Iteration 20270 => Loss: 6.71164611614948469764\n",
      "Iteration 20271 => Loss: 6.71164430130362443805\n",
      "Iteration 20272 => Loss: 6.71164248670232321814\n",
      "Iteration 20273 => Loss: 6.71164067234554106989\n",
      "Iteration 20274 => Loss: 6.71163885823323536073\n",
      "Iteration 20275 => Loss: 6.71163704436538743892\n",
      "Iteration 20276 => Loss: 6.71163523074195644824\n",
      "Iteration 20277 => Loss: 6.71163341736290952610\n",
      "Iteration 20278 => Loss: 6.71163160422821558626\n",
      "Iteration 20279 => Loss: 6.71162979133784620700\n",
      "Iteration 20280 => Loss: 6.71162797869175431487\n",
      "Iteration 20281 => Loss: 6.71162616628992658718\n",
      "Iteration 20282 => Loss: 6.71162435413231506232\n",
      "Iteration 20283 => Loss: 6.71162254221889220673\n",
      "Iteration 20284 => Loss: 6.71162073054963226326\n",
      "Iteration 20285 => Loss: 6.71161891912448727027\n",
      "Iteration 20286 => Loss: 6.71161710794342791786\n",
      "Iteration 20287 => Loss: 6.71161529700643288976\n",
      "Iteration 20288 => Loss: 6.71161348631346488247\n",
      "Iteration 20289 => Loss: 6.71161167586448126343\n",
      "Iteration 20290 => Loss: 6.71160986565945716364\n",
      "Iteration 20291 => Loss: 6.71160805569836060869\n",
      "Iteration 20292 => Loss: 6.71160624598115251871\n",
      "Iteration 20293 => Loss: 6.71160443650780447200\n",
      "Iteration 20294 => Loss: 6.71160262727828449414\n",
      "Iteration 20295 => Loss: 6.71160081829255350527\n",
      "Iteration 20296 => Loss: 6.71159900955059196548\n",
      "Iteration 20297 => Loss: 6.71159720105235546583\n",
      "Iteration 20298 => Loss: 6.71159539279781647281\n",
      "Iteration 20299 => Loss: 6.71159358478693413019\n",
      "Iteration 20300 => Loss: 6.71159177701968534535\n",
      "Iteration 20301 => Loss: 6.71158996949603103843\n",
      "Iteration 20302 => Loss: 6.71158816221594545226\n",
      "Iteration 20303 => Loss: 6.71158635517938773063\n",
      "Iteration 20304 => Loss: 6.71158454838632412276\n",
      "Iteration 20305 => Loss: 6.71158274183672798330\n",
      "Iteration 20306 => Loss: 6.71158093553057266689\n",
      "Iteration 20307 => Loss: 6.71157912946781110008\n",
      "Iteration 20308 => Loss: 6.71157732364841486117\n",
      "Iteration 20309 => Loss: 6.71157551807235641661\n",
      "Iteration 20310 => Loss: 6.71157371273959224567\n",
      "Iteration 20311 => Loss: 6.71157190765010724931\n",
      "Iteration 20312 => Loss: 6.71157010280385168954\n",
      "Iteration 20313 => Loss: 6.71156829820080158555\n",
      "Iteration 20314 => Loss: 6.71156649384091785748\n",
      "Iteration 20315 => Loss: 6.71156468972417563634\n",
      "Iteration 20316 => Loss: 6.71156288585053939499\n",
      "Iteration 20317 => Loss: 6.71156108221996916541\n",
      "Iteration 20318 => Loss: 6.71155927883244718402\n",
      "Iteration 20319 => Loss: 6.71155747568792637736\n",
      "Iteration 20320 => Loss: 6.71155567278638276463\n",
      "Iteration 20321 => Loss: 6.71155387012777548961\n",
      "Iteration 20322 => Loss: 6.71155206771207524241\n",
      "Iteration 20323 => Loss: 6.71155026553926159494\n",
      "Iteration 20324 => Loss: 6.71154846360927859195\n",
      "Iteration 20325 => Loss: 6.71154666192211024622\n",
      "Iteration 20326 => Loss: 6.71154486047772369517\n",
      "Iteration 20327 => Loss: 6.71154305927607719440\n",
      "Iteration 20328 => Loss: 6.71154125831714587491\n",
      "Iteration 20329 => Loss: 6.71153945760088799233\n",
      "Iteration 20330 => Loss: 6.71153765712728400672\n",
      "Iteration 20331 => Loss: 6.71153585689629128552\n",
      "Iteration 20332 => Loss: 6.71153405690787696614\n",
      "Iteration 20333 => Loss: 6.71153225716201706774\n",
      "Iteration 20334 => Loss: 6.71153045765866718142\n",
      "Iteration 20335 => Loss: 6.71152865839780599089\n",
      "Iteration 20336 => Loss: 6.71152685937938731087\n",
      "Iteration 20337 => Loss: 6.71152506060339071325\n",
      "Iteration 20338 => Loss: 6.71152326206978422363\n",
      "Iteration 20339 => Loss: 6.71152146377852432124\n",
      "Iteration 20340 => Loss: 6.71151966572959146617\n",
      "Iteration 20341 => Loss: 6.71151786792293414408\n",
      "Iteration 20342 => Loss: 6.71151607035853903227\n",
      "Iteration 20343 => Loss: 6.71151427303636882726\n",
      "Iteration 20344 => Loss: 6.71151247595637467924\n",
      "Iteration 20345 => Loss: 6.71151067911855125914\n",
      "Iteration 20346 => Loss: 6.71150888252284083535\n",
      "Iteration 20347 => Loss: 6.71150708616922919703\n",
      "Iteration 20348 => Loss: 6.71150529005767815249\n",
      "Iteration 20349 => Loss: 6.71150349418815217462\n",
      "Iteration 20350 => Loss: 6.71150169856061840079\n",
      "Iteration 20351 => Loss: 6.71149990317504308024\n",
      "Iteration 20352 => Loss: 6.71149810803139690307\n",
      "Iteration 20353 => Loss: 6.71149631312964523033\n",
      "Iteration 20354 => Loss: 6.71149451846975697578\n",
      "Iteration 20355 => Loss: 6.71149272405170105316\n",
      "Iteration 20356 => Loss: 6.71149092987544282352\n",
      "Iteration 20357 => Loss: 6.71148913594095297697\n",
      "Iteration 20358 => Loss: 6.71148734224819687455\n",
      "Iteration 20359 => Loss: 6.71148554879713898913\n",
      "Iteration 20360 => Loss: 6.71148375558774379357\n",
      "Iteration 20361 => Loss: 6.71148196261998553069\n",
      "Iteration 20362 => Loss: 6.71148016989383133790\n",
      "Iteration 20363 => Loss: 6.71147837740924924077\n",
      "Iteration 20364 => Loss: 6.71147658516620460034\n",
      "Iteration 20365 => Loss: 6.71147479316465922494\n",
      "Iteration 20366 => Loss: 6.71147300140459091011\n",
      "Iteration 20367 => Loss: 6.71147120988595791147\n",
      "Iteration 20368 => Loss: 6.71146941860873980090\n",
      "Iteration 20369 => Loss: 6.71146762757289305767\n",
      "Iteration 20370 => Loss: 6.71146583677838393101\n",
      "Iteration 20371 => Loss: 6.71146404622519021643\n",
      "Iteration 20372 => Loss: 6.71146225591327016957\n",
      "Iteration 20373 => Loss: 6.71146046584259980960\n",
      "Iteration 20374 => Loss: 6.71145867601313472761\n",
      "Iteration 20375 => Loss: 6.71145688642485271913\n",
      "Iteration 20376 => Loss: 6.71145509707771825703\n",
      "Iteration 20377 => Loss: 6.71145330797170114323\n",
      "Iteration 20378 => Loss: 6.71145151910676052154\n",
      "Iteration 20379 => Loss: 6.71144973048287152295\n",
      "Iteration 20380 => Loss: 6.71144794210000306123\n",
      "Iteration 20381 => Loss: 6.71144615395812049741\n",
      "Iteration 20382 => Loss: 6.71144436605718386346\n",
      "Iteration 20383 => Loss: 6.71144257839716917857\n",
      "Iteration 20384 => Loss: 6.71144079097803913925\n",
      "Iteration 20385 => Loss: 6.71143900379976887649\n",
      "Iteration 20386 => Loss: 6.71143721686231931045\n",
      "Iteration 20387 => Loss: 6.71143543016565669035\n",
      "Iteration 20388 => Loss: 6.71143364370975348265\n",
      "Iteration 20389 => Loss: 6.71143185749457860112\n",
      "Iteration 20390 => Loss: 6.71143007152009474225\n",
      "Iteration 20391 => Loss: 6.71142828578626904346\n",
      "Iteration 20392 => Loss: 6.71142650029307308301\n",
      "Iteration 20393 => Loss: 6.71142471504047399833\n",
      "Iteration 20394 => Loss: 6.71142293002843270955\n",
      "Iteration 20395 => Loss: 6.71142114525692612403\n",
      "Iteration 20396 => Loss: 6.71141936072591249740\n",
      "Iteration 20397 => Loss: 6.71141757643537584244\n",
      "Iteration 20398 => Loss: 6.71141579238525931572\n",
      "Iteration 20399 => Loss: 6.71141400857555492365\n",
      "Iteration 20400 => Loss: 6.71141222500621026370\n",
      "Iteration 20401 => Loss: 6.71141044167720401958\n",
      "Iteration 20402 => Loss: 6.71140865858850421688\n",
      "Iteration 20403 => Loss: 6.71140687574007355209\n",
      "Iteration 20404 => Loss: 6.71140509313187383356\n",
      "Iteration 20405 => Loss: 6.71140331076389617948\n",
      "Iteration 20406 => Loss: 6.71140152863607930556\n",
      "Iteration 20407 => Loss: 6.71139974674842054725\n",
      "Iteration 20408 => Loss: 6.71139796510085773207\n",
      "Iteration 20409 => Loss: 6.71139618369338020187\n",
      "Iteration 20410 => Loss: 6.71139440252594265957\n",
      "Iteration 20411 => Loss: 6.71139262159851490708\n",
      "Iteration 20412 => Loss: 6.71139084091107029906\n",
      "Iteration 20413 => Loss: 6.71138906046357330837\n",
      "Iteration 20414 => Loss: 6.71138728025599551330\n",
      "Iteration 20415 => Loss: 6.71138550028829605765\n",
      "Iteration 20416 => Loss: 6.71138372056044918423\n",
      "Iteration 20417 => Loss: 6.71138194107242114228\n",
      "Iteration 20418 => Loss: 6.71138016182417818101\n",
      "Iteration 20419 => Loss: 6.71137838281568743781\n",
      "Iteration 20420 => Loss: 6.71137660404692049099\n",
      "Iteration 20421 => Loss: 6.71137482551784447793\n",
      "Iteration 20422 => Loss: 6.71137304722842209515\n",
      "Iteration 20423 => Loss: 6.71137126917862580910\n",
      "Iteration 20424 => Loss: 6.71136949136841742813\n",
      "Iteration 20425 => Loss: 6.71136771379777208324\n",
      "Iteration 20426 => Loss: 6.71136593646665780000\n",
      "Iteration 20427 => Loss: 6.71136415937503993945\n",
      "Iteration 20428 => Loss: 6.71136238252288386263\n",
      "Iteration 20429 => Loss: 6.71136060591015404242\n",
      "Iteration 20430 => Loss: 6.71135882953682472163\n",
      "Iteration 20431 => Loss: 6.71135705340286659037\n",
      "Iteration 20432 => Loss: 6.71135527750824145699\n",
      "Iteration 20433 => Loss: 6.71135350185291201797\n",
      "Iteration 20434 => Loss: 6.71135172643685606886\n",
      "Iteration 20435 => Loss: 6.71134995126003719434\n",
      "Iteration 20436 => Loss: 6.71134817632242342000\n",
      "Iteration 20437 => Loss: 6.71134640162398632413\n",
      "Iteration 20438 => Loss: 6.71134462716468416232\n",
      "Iteration 20439 => Loss: 6.71134285294449384196\n",
      "Iteration 20440 => Loss: 6.71134107896337717136\n",
      "Iteration 20441 => Loss: 6.71133930522130484064\n",
      "Iteration 20442 => Loss: 6.71133753171824665174\n",
      "Iteration 20443 => Loss: 6.71133575845417151839\n",
      "Iteration 20444 => Loss: 6.71133398542904036077\n",
      "Iteration 20445 => Loss: 6.71133221264282475715\n",
      "Iteration 20446 => Loss: 6.71133044009548918041\n",
      "Iteration 20447 => Loss: 6.71132866778700432064\n",
      "Iteration 20448 => Loss: 6.71132689571734619705\n",
      "Iteration 20449 => Loss: 6.71132512388646684798\n",
      "Iteration 20450 => Loss: 6.71132335229434140444\n",
      "Iteration 20451 => Loss: 6.71132158094094499745\n",
      "Iteration 20452 => Loss: 6.71131980982623499443\n",
      "Iteration 20453 => Loss: 6.71131803895018119732\n",
      "Iteration 20454 => Loss: 6.71131626831275962530\n",
      "Iteration 20455 => Loss: 6.71131449791392320492\n",
      "Iteration 20456 => Loss: 6.71131272775365239625\n",
      "Iteration 20457 => Loss: 6.71131095783190367854\n",
      "Iteration 20458 => Loss: 6.71130918814866550548\n",
      "Iteration 20459 => Loss: 6.71130741870388281001\n",
      "Iteration 20460 => Loss: 6.71130564949753072312\n",
      "Iteration 20461 => Loss: 6.71130388052958792855\n",
      "Iteration 20462 => Loss: 6.71130211180001001736\n",
      "Iteration 20463 => Loss: 6.71130034330876323878\n",
      "Iteration 20464 => Loss: 6.71129857505583160560\n",
      "Iteration 20465 => Loss: 6.71129680704116449164\n",
      "Iteration 20466 => Loss: 6.71129503926473791608\n",
      "Iteration 20467 => Loss: 6.71129327172652168088\n",
      "Iteration 20468 => Loss: 6.71129150442648292341\n",
      "Iteration 20469 => Loss: 6.71128973736459055743\n",
      "Iteration 20470 => Loss: 6.71128797054079573314\n",
      "Iteration 20471 => Loss: 6.71128620395508868057\n",
      "Iteration 20472 => Loss: 6.71128443760743209623\n",
      "Iteration 20473 => Loss: 6.71128267149778601208\n",
      "Iteration 20474 => Loss: 6.71128090562612644732\n",
      "Iteration 20475 => Loss: 6.71127913999242142751\n",
      "Iteration 20476 => Loss: 6.71127737459663009645\n",
      "Iteration 20477 => Loss: 6.71127560943872492061\n",
      "Iteration 20478 => Loss: 6.71127384451868280735\n",
      "Iteration 20479 => Loss: 6.71127207983645757139\n",
      "Iteration 20480 => Loss: 6.71127031539202345556\n",
      "Iteration 20481 => Loss: 6.71126855118534759725\n",
      "Iteration 20482 => Loss: 6.71126678721640423930\n",
      "Iteration 20483 => Loss: 6.71126502348515074914\n",
      "Iteration 20484 => Loss: 6.71126325999156669866\n",
      "Iteration 20485 => Loss: 6.71126149673560057352\n",
      "Iteration 20486 => Loss: 6.71125973371724704464\n",
      "Iteration 20487 => Loss: 6.71125797093645548586\n",
      "Iteration 20488 => Loss: 6.71125620839319481092\n",
      "Iteration 20489 => Loss: 6.71125444608744103903\n",
      "Iteration 20490 => Loss: 6.71125268401915686667\n",
      "Iteration 20491 => Loss: 6.71125092218831476032\n",
      "Iteration 20492 => Loss: 6.71124916059488008102\n",
      "Iteration 20493 => Loss: 6.71124739923881374892\n",
      "Iteration 20494 => Loss: 6.71124563812009267139\n",
      "Iteration 20495 => Loss: 6.71124387723868309763\n",
      "Iteration 20496 => Loss: 6.71124211659455571777\n",
      "Iteration 20497 => Loss: 6.71124035618767056377\n",
      "Iteration 20498 => Loss: 6.71123859601800187846\n",
      "Iteration 20499 => Loss: 6.71123683608551324653\n",
      "Iteration 20500 => Loss: 6.71123507639018423987\n",
      "Iteration 20501 => Loss: 6.71123331693196423231\n",
      "Iteration 20502 => Loss: 6.71123155771084078935\n",
      "Iteration 20503 => Loss: 6.71122979872676861390\n",
      "Iteration 20504 => Loss: 6.71122803997972017243\n",
      "Iteration 20505 => Loss: 6.71122628146965904961\n",
      "Iteration 20506 => Loss: 6.71122452319656037645\n",
      "Iteration 20507 => Loss: 6.71122276516038684946\n",
      "Iteration 20508 => Loss: 6.71122100736111182329\n",
      "Iteration 20509 => Loss: 6.71121924979869799444\n",
      "Iteration 20510 => Loss: 6.71121749247312138209\n",
      "Iteration 20511 => Loss: 6.71121573538433757733\n",
      "Iteration 20512 => Loss: 6.71121397853232526387\n",
      "Iteration 20513 => Loss: 6.71121222191704625004\n",
      "Iteration 20514 => Loss: 6.71121046553846944960\n",
      "Iteration 20515 => Loss: 6.71120870939657265808\n",
      "Iteration 20516 => Loss: 6.71120695349130791385\n",
      "Iteration 20517 => Loss: 6.71120519782265567699\n",
      "Iteration 20518 => Loss: 6.71120344239058130853\n",
      "Iteration 20519 => Loss: 6.71120168719505194588\n",
      "Iteration 20520 => Loss: 6.71119993223603117372\n",
      "Iteration 20521 => Loss: 6.71119817751349412305\n",
      "Iteration 20522 => Loss: 6.71119642302739993767\n",
      "Iteration 20523 => Loss: 6.71119466877772730129\n",
      "Iteration 20524 => Loss: 6.71119291476444779221\n",
      "Iteration 20525 => Loss: 6.71119116098751611332\n",
      "Iteration 20526 => Loss: 6.71118940744689940203\n",
      "Iteration 20527 => Loss: 6.71118765414257634205\n",
      "Iteration 20528 => Loss: 6.71118590107451229443\n",
      "Iteration 20529 => Loss: 6.71118414824268239016\n",
      "Iteration 20530 => Loss: 6.71118239564703245037\n",
      "Iteration 20531 => Loss: 6.71118064328755625780\n",
      "Iteration 20532 => Loss: 6.71117889116420762718\n",
      "Iteration 20533 => Loss: 6.71117713927695369591\n",
      "Iteration 20534 => Loss: 6.71117538762576426592\n",
      "Iteration 20535 => Loss: 6.71117363621061535639\n",
      "Iteration 20536 => Loss: 6.71117188503147232836\n",
      "Iteration 20537 => Loss: 6.71117013408829787835\n",
      "Iteration 20538 => Loss: 6.71116838338106003192\n",
      "Iteration 20539 => Loss: 6.71116663290973480827\n",
      "Iteration 20540 => Loss: 6.71116488267428490389\n",
      "Iteration 20541 => Loss: 6.71116313267467923254\n",
      "Iteration 20542 => Loss: 6.71116138291088404344\n",
      "Iteration 20543 => Loss: 6.71115963338286469764\n",
      "Iteration 20544 => Loss: 6.71115788409060254338\n",
      "Iteration 20545 => Loss: 6.71115613503405850082\n",
      "Iteration 20546 => Loss: 6.71115438621319437829\n",
      "Iteration 20547 => Loss: 6.71115263762798530678\n",
      "Iteration 20548 => Loss: 6.71115088927840375277\n",
      "Iteration 20549 => Loss: 6.71114914116440441916\n",
      "Iteration 20550 => Loss: 6.71114739328596865420\n",
      "Iteration 20551 => Loss: 6.71114564564305648986\n",
      "Iteration 20552 => Loss: 6.71114389823564749804\n",
      "Iteration 20553 => Loss: 6.71114215106369282893\n",
      "Iteration 20554 => Loss: 6.71114040412717027806\n",
      "Iteration 20555 => Loss: 6.71113865742605142373\n",
      "Iteration 20556 => Loss: 6.71113691096029985061\n",
      "Iteration 20557 => Loss: 6.71113516472988358430\n",
      "Iteration 20558 => Loss: 6.71113341873477331490\n",
      "Iteration 20559 => Loss: 6.71113167297493529162\n",
      "Iteration 20560 => Loss: 6.71112992745033576369\n",
      "Iteration 20561 => Loss: 6.71112818216095075030\n",
      "Iteration 20562 => Loss: 6.71112643710674294795\n",
      "Iteration 20563 => Loss: 6.71112469228767327678\n",
      "Iteration 20564 => Loss: 6.71112294770372663777\n",
      "Iteration 20565 => Loss: 6.71112120335485951017\n",
      "Iteration 20566 => Loss: 6.71111945924105146588\n",
      "Iteration 20567 => Loss: 6.71111771536224654966\n",
      "Iteration 20568 => Loss: 6.71111597171844120879\n",
      "Iteration 20569 => Loss: 6.71111422830958836983\n",
      "Iteration 20570 => Loss: 6.71111248513566049922\n",
      "Iteration 20571 => Loss: 6.71111074219662917528\n",
      "Iteration 20572 => Loss: 6.71110899949245531815\n",
      "Iteration 20573 => Loss: 6.71110725702310961793\n",
      "Iteration 20574 => Loss: 6.71110551478856987018\n",
      "Iteration 20575 => Loss: 6.71110377278879077778\n",
      "Iteration 20576 => Loss: 6.71110203102374480721\n",
      "Iteration 20577 => Loss: 6.71110028949339998405\n",
      "Iteration 20578 => Loss: 6.71109854819773055112\n",
      "Iteration 20579 => Loss: 6.71109680713670186947\n",
      "Iteration 20580 => Loss: 6.71109506631027752377\n",
      "Iteration 20581 => Loss: 6.71109332571842731596\n",
      "Iteration 20582 => Loss: 6.71109158536113259430\n",
      "Iteration 20583 => Loss: 6.71108984523834450897\n",
      "Iteration 20584 => Loss: 6.71108810535003730280\n",
      "Iteration 20585 => Loss: 6.71108636569617811318\n",
      "Iteration 20586 => Loss: 6.71108462627674384748\n",
      "Iteration 20587 => Loss: 6.71108288709169098496\n",
      "Iteration 20588 => Loss: 6.71108114814099376844\n",
      "Iteration 20589 => Loss: 6.71107940942462199985\n",
      "Iteration 20590 => Loss: 6.71107767094254814566\n",
      "Iteration 20591 => Loss: 6.71107593269472779696\n",
      "Iteration 20592 => Loss: 6.71107419468113519656\n",
      "Iteration 20593 => Loss: 6.71107245690174192276\n",
      "Iteration 20594 => Loss: 6.71107071935651777750\n",
      "Iteration 20595 => Loss: 6.71106898204542456909\n",
      "Iteration 20596 => Loss: 6.71106724496843565220\n",
      "Iteration 20597 => Loss: 6.71106550812551727603\n",
      "Iteration 20598 => Loss: 6.71106377151663480163\n",
      "Iteration 20599 => Loss: 6.71106203514176691272\n",
      "Iteration 20600 => Loss: 6.71106029900087719398\n",
      "Iteration 20601 => Loss: 6.71105856309392478920\n",
      "Iteration 20602 => Loss: 6.71105682742088927029\n",
      "Iteration 20603 => Loss: 6.71105509198173866281\n",
      "Iteration 20604 => Loss: 6.71105335677643743963\n",
      "Iteration 20605 => Loss: 6.71105162180495540269\n",
      "Iteration 20606 => Loss: 6.71104988706725258396\n",
      "Iteration 20607 => Loss: 6.71104815256331566076\n",
      "Iteration 20608 => Loss: 6.71104641829310022416\n",
      "Iteration 20609 => Loss: 6.71104468425658229336\n",
      "Iteration 20610 => Loss: 6.71104295045372012396\n",
      "Iteration 20611 => Loss: 6.71104121688449151151\n",
      "Iteration 20612 => Loss: 6.71103948354885737615\n",
      "Iteration 20613 => Loss: 6.71103775044679107253\n",
      "Iteration 20614 => Loss: 6.71103601757826506713\n",
      "Iteration 20615 => Loss: 6.71103428494323761555\n",
      "Iteration 20616 => Loss: 6.71103255254168562516\n",
      "Iteration 20617 => Loss: 6.71103082037356912792\n",
      "Iteration 20618 => Loss: 6.71102908843887657753\n",
      "Iteration 20619 => Loss: 6.71102735673754757784\n",
      "Iteration 20620 => Loss: 6.71102562526957413525\n",
      "Iteration 20621 => Loss: 6.71102389403491450537\n",
      "Iteration 20622 => Loss: 6.71102216303353671378\n",
      "Iteration 20623 => Loss: 6.71102043226541500331\n",
      "Iteration 20624 => Loss: 6.71101870173051207047\n",
      "Iteration 20625 => Loss: 6.71101697142879416447\n",
      "Iteration 20626 => Loss: 6.71101524136023996903\n",
      "Iteration 20627 => Loss: 6.71101351152481484519\n",
      "Iteration 20628 => Loss: 6.71101178192248148946\n",
      "Iteration 20629 => Loss: 6.71101005255321414467\n",
      "Iteration 20630 => Loss: 6.71100832341697994821\n",
      "Iteration 20631 => Loss: 6.71100659451374514930\n",
      "Iteration 20632 => Loss: 6.71100486584347688535\n",
      "Iteration 20633 => Loss: 6.71100313740615384006\n",
      "Iteration 20634 => Loss: 6.71100140920173426906\n",
      "Iteration 20635 => Loss: 6.71099968123018886246\n",
      "Iteration 20636 => Loss: 6.71099795349149363943\n",
      "Iteration 20637 => Loss: 6.71099622598560152653\n",
      "Iteration 20638 => Loss: 6.71099449871250097743\n",
      "Iteration 20639 => Loss: 6.71099277167214847140\n",
      "Iteration 20640 => Loss: 6.71099104486451025764\n",
      "Iteration 20641 => Loss: 6.71098931828956413170\n",
      "Iteration 20642 => Loss: 6.71098759194727190192\n",
      "Iteration 20643 => Loss: 6.71098586583760248203\n",
      "Iteration 20644 => Loss: 6.71098413996053100306\n",
      "Iteration 20645 => Loss: 6.71098241431601483242\n",
      "Iteration 20646 => Loss: 6.71098068890403354203\n",
      "Iteration 20647 => Loss: 6.71097896372454538749\n",
      "Iteration 20648 => Loss: 6.71097723877754415156\n",
      "Iteration 20649 => Loss: 6.71097551406295700360\n",
      "Iteration 20650 => Loss: 6.71097378958078927269\n",
      "Iteration 20651 => Loss: 6.71097206533099210901\n",
      "Iteration 20652 => Loss: 6.71097034131354153175\n",
      "Iteration 20653 => Loss: 6.71096861752839579651\n",
      "Iteration 20654 => Loss: 6.71096689397553003431\n",
      "Iteration 20655 => Loss: 6.71096517065492026433\n",
      "Iteration 20656 => Loss: 6.71096344756652474217\n",
      "Iteration 20657 => Loss: 6.71096172471031326978\n",
      "Iteration 20658 => Loss: 6.71096000208625831362\n",
      "Iteration 20659 => Loss: 6.71095827969432612292\n",
      "Iteration 20660 => Loss: 6.71095655753448649961\n",
      "Iteration 20661 => Loss: 6.71095483560671013379\n",
      "Iteration 20662 => Loss: 6.71095311391096149833\n",
      "Iteration 20663 => Loss: 6.71095139244721039518\n",
      "Iteration 20664 => Loss: 6.71094967121542929078\n",
      "Iteration 20665 => Loss: 6.71094795021558532255\n",
      "Iteration 20666 => Loss: 6.71094622944764296335\n",
      "Iteration 20667 => Loss: 6.71094450891158089689\n",
      "Iteration 20668 => Loss: 6.71094278860734849701\n",
      "Iteration 20669 => Loss: 6.71094106853493421738\n",
      "Iteration 20670 => Loss: 6.71093934869430075452\n",
      "Iteration 20671 => Loss: 6.71093762908541791035\n",
      "Iteration 20672 => Loss: 6.71093590970825637498\n",
      "Iteration 20673 => Loss: 6.71093419056276285772\n",
      "Iteration 20674 => Loss: 6.71093247164894179946\n",
      "Iteration 20675 => Loss: 6.71093075296674079766\n",
      "Iteration 20676 => Loss: 6.71092903451612876609\n",
      "Iteration 20677 => Loss: 6.71092731629707994756\n",
      "Iteration 20678 => Loss: 6.71092559830955792677\n",
      "Iteration 20679 => Loss: 6.71092388055354494014\n",
      "Iteration 20680 => Loss: 6.71092216302899302605\n",
      "Iteration 20681 => Loss: 6.71092044573587642731\n",
      "Iteration 20682 => Loss: 6.71091872867416405768\n",
      "Iteration 20683 => Loss: 6.71091701184382927181\n",
      "Iteration 20684 => Loss: 6.71091529524483298985\n",
      "Iteration 20685 => Loss: 6.71091357887715300734\n",
      "Iteration 20686 => Loss: 6.71091186274074846807\n",
      "Iteration 20687 => Loss: 6.71091014683560249665\n",
      "Iteration 20688 => Loss: 6.71090843116166890781\n",
      "Iteration 20689 => Loss: 6.71090671571892460889\n",
      "Iteration 20690 => Loss: 6.71090500050733318460\n",
      "Iteration 20691 => Loss: 6.71090328552687243047\n",
      "Iteration 20692 => Loss: 6.71090157077749882575\n",
      "Iteration 20693 => Loss: 6.71089985625918394874\n",
      "Iteration 20694 => Loss: 6.71089814197191270040\n",
      "Iteration 20695 => Loss: 6.71089642791564244817\n",
      "Iteration 20696 => Loss: 6.71089471409033233584\n",
      "Iteration 20697 => Loss: 6.71089300049596104714\n",
      "Iteration 20698 => Loss: 6.71089128713250282487\n",
      "Iteration 20699 => Loss: 6.71088957399991681285\n",
      "Iteration 20700 => Loss: 6.71088786109817192482\n",
      "Iteration 20701 => Loss: 6.71088614842724684451\n",
      "Iteration 20702 => Loss: 6.71088443598710249205\n",
      "Iteration 20703 => Loss: 6.71088272377770955757\n",
      "Iteration 20704 => Loss: 6.71088101179903517846\n",
      "Iteration 20705 => Loss: 6.71087930005105270936\n",
      "Iteration 20706 => Loss: 6.71087758853372395862\n",
      "Iteration 20707 => Loss: 6.71087587724702316905\n",
      "Iteration 20708 => Loss: 6.71087416619092458347\n",
      "Iteration 20709 => Loss: 6.71087245536538823387\n",
      "Iteration 20710 => Loss: 6.71087074477038125764\n",
      "Iteration 20711 => Loss: 6.71086903440587700942\n",
      "Iteration 20712 => Loss: 6.71086732427184795569\n",
      "Iteration 20713 => Loss: 6.71086561436826567473\n",
      "Iteration 20714 => Loss: 6.71086390469508220491\n",
      "Iteration 20715 => Loss: 6.71086219525228422356\n",
      "Iteration 20716 => Loss: 6.71086048603982998628\n",
      "Iteration 20717 => Loss: 6.71085877705769373591\n",
      "Iteration 20718 => Loss: 6.71085706830583816895\n",
      "Iteration 20719 => Loss: 6.71085535978424729819\n",
      "Iteration 20720 => Loss: 6.71085365149287049746\n",
      "Iteration 20721 => Loss: 6.71085194343169444409\n",
      "Iteration 20722 => Loss: 6.71085023560067295278\n",
      "Iteration 20723 => Loss: 6.71084852799978293092\n",
      "Iteration 20724 => Loss: 6.71084682062899329225\n",
      "Iteration 20725 => Loss: 6.71084511348827383870\n",
      "Iteration 20726 => Loss: 6.71084340657758549042\n",
      "Iteration 20727 => Loss: 6.71084169989690781932\n",
      "Iteration 20728 => Loss: 6.71083999344620618643\n",
      "Iteration 20729 => Loss: 6.71083828722545305823\n",
      "Iteration 20730 => Loss: 6.71083658123460757849\n",
      "Iteration 20731 => Loss: 6.71083487547364221371\n",
      "Iteration 20732 => Loss: 6.71083316994253742394\n",
      "Iteration 20733 => Loss: 6.71083146464124169484\n",
      "Iteration 20734 => Loss: 6.71082975956974348009\n",
      "Iteration 20735 => Loss: 6.71082805472800014712\n",
      "Iteration 20736 => Loss: 6.71082635011599215602\n",
      "Iteration 20737 => Loss: 6.71082464573366710425\n",
      "Iteration 20738 => Loss: 6.71082294158101522186\n",
      "Iteration 20739 => Loss: 6.71082123765800009352\n",
      "Iteration 20740 => Loss: 6.71081953396458441574\n",
      "Iteration 20741 => Loss: 6.71081783050074243135\n",
      "Iteration 20742 => Loss: 6.71081612726644394229\n",
      "Iteration 20743 => Loss: 6.71081442426165519777\n",
      "Iteration 20744 => Loss: 6.71081272148634688790\n",
      "Iteration 20745 => Loss: 6.71081101894048792644\n",
      "Iteration 20746 => Loss: 6.71080931662404900351\n",
      "Iteration 20747 => Loss: 6.71080761453699192742\n",
      "Iteration 20748 => Loss: 6.71080591267929715826\n",
      "Iteration 20749 => Loss: 6.71080421105092472800\n",
      "Iteration 20750 => Loss: 6.71080250965184976764\n",
      "Iteration 20751 => Loss: 6.71080080848203053279\n",
      "Iteration 20752 => Loss: 6.71079910754145281260\n",
      "Iteration 20753 => Loss: 6.71079740683007397450\n",
      "Iteration 20754 => Loss: 6.71079570634786204408\n",
      "Iteration 20755 => Loss: 6.71079400609480014595\n",
      "Iteration 20756 => Loss: 6.71079230607084031845\n",
      "Iteration 20757 => Loss: 6.71079060627596035715\n",
      "Iteration 20758 => Loss: 6.71078890671012651126\n",
      "Iteration 20759 => Loss: 6.71078720737331213542\n",
      "Iteration 20760 => Loss: 6.71078550826548259067\n",
      "Iteration 20761 => Loss: 6.71078380938660767896\n",
      "Iteration 20762 => Loss: 6.71078211073665276132\n",
      "Iteration 20763 => Loss: 6.71078041231559652147\n",
      "Iteration 20764 => Loss: 6.71077871412340076773\n",
      "Iteration 20765 => Loss: 6.71077701616003352569\n",
      "Iteration 20766 => Loss: 6.71077531842547081453\n",
      "Iteration 20767 => Loss: 6.71077362091967710711\n",
      "Iteration 20768 => Loss: 6.71077192364262220536\n",
      "Iteration 20769 => Loss: 6.71077022659427679940\n",
      "Iteration 20770 => Loss: 6.71076852977460802663\n",
      "Iteration 20771 => Loss: 6.71076683318358480079\n",
      "Iteration 20772 => Loss: 6.71076513682117870019\n",
      "Iteration 20773 => Loss: 6.71076344068735419768\n",
      "Iteration 20774 => Loss: 6.71076174478209086516\n",
      "Iteration 20775 => Loss: 6.71076004910533807646\n",
      "Iteration 20776 => Loss: 6.71075835365709494340\n",
      "Iteration 20777 => Loss: 6.71075665843730195803\n",
      "Iteration 20778 => Loss: 6.71075496344594579767\n",
      "Iteration 20779 => Loss: 6.71075326868298738248\n",
      "Iteration 20780 => Loss: 6.71075157414839917891\n",
      "Iteration 20781 => Loss: 6.71074987984215098891\n",
      "Iteration 20782 => Loss: 6.71074818576420373262\n",
      "Iteration 20783 => Loss: 6.71074649191453964647\n",
      "Iteration 20784 => Loss: 6.71074479829311965062\n",
      "Iteration 20785 => Loss: 6.71074310489990910611\n",
      "Iteration 20786 => Loss: 6.71074141173489824297\n",
      "Iteration 20787 => Loss: 6.71073971879802844143\n",
      "Iteration 20788 => Loss: 6.71073802608928993152\n",
      "Iteration 20789 => Loss: 6.71073633360864008068\n",
      "Iteration 20790 => Loss: 6.71073464135605401992\n",
      "Iteration 20791 => Loss: 6.71073294933150510388\n",
      "Iteration 20792 => Loss: 6.71073125753494537094\n",
      "Iteration 20793 => Loss: 6.71072956596636061022\n",
      "Iteration 20794 => Loss: 6.71072787462570907735\n",
      "Iteration 20795 => Loss: 6.71072618351297567330\n",
      "Iteration 20796 => Loss: 6.71072449262811598913\n",
      "Iteration 20797 => Loss: 6.71072280197110337951\n",
      "Iteration 20798 => Loss: 6.71072111154189965276\n",
      "Iteration 20799 => Loss: 6.71071942134049148621\n",
      "Iteration 20800 => Loss: 6.71071773136683180638\n",
      "Iteration 20801 => Loss: 6.71071604162090373791\n",
      "Iteration 20802 => Loss: 6.71071435210266198368\n",
      "Iteration 20803 => Loss: 6.71071266281208522742\n",
      "Iteration 20804 => Loss: 6.71071097374914238287\n",
      "Iteration 20805 => Loss: 6.71070928491379969927\n",
      "Iteration 20806 => Loss: 6.71070759630601898493\n",
      "Iteration 20807 => Loss: 6.71070590792579313444\n",
      "Iteration 20808 => Loss: 6.71070421977306974526\n",
      "Iteration 20809 => Loss: 6.71070253184781773115\n",
      "Iteration 20810 => Loss: 6.71070084415002732214\n",
      "Iteration 20811 => Loss: 6.71069915667964966843\n",
      "Iteration 20812 => Loss: 6.71069746943665368377\n",
      "Iteration 20813 => Loss: 6.71069578242101716370\n",
      "Iteration 20814 => Loss: 6.71069409563270546926\n",
      "Iteration 20815 => Loss: 6.71069240907168840238\n",
      "Iteration 20816 => Loss: 6.71069072273794109407\n",
      "Iteration 20817 => Loss: 6.71068903663141913540\n",
      "Iteration 20818 => Loss: 6.71068735075210653918\n",
      "Iteration 20819 => Loss: 6.71068566509995978464\n",
      "Iteration 20820 => Loss: 6.71068397967495666734\n",
      "Iteration 20821 => Loss: 6.71068229447707231827\n",
      "Iteration 20822 => Loss: 6.71068060950626055217\n",
      "Iteration 20823 => Loss: 6.71067892476250360545\n",
      "Iteration 20824 => Loss: 6.71067724024576062192\n",
      "Iteration 20825 => Loss: 6.71067555595600762075\n",
      "Iteration 20826 => Loss: 6.71067387189321795660\n",
      "Iteration 20827 => Loss: 6.71067218805734722054\n",
      "Iteration 20828 => Loss: 6.71067050444838208989\n",
      "Iteration 20829 => Loss: 6.71066882106628348481\n",
      "Iteration 20830 => Loss: 6.71066713791101232545\n",
      "Iteration 20831 => Loss: 6.71066545498255706548\n",
      "Iteration 20832 => Loss: 6.71066377228086885509\n",
      "Iteration 20833 => Loss: 6.71066208980592371347\n",
      "Iteration 20834 => Loss: 6.71066040755769499526\n",
      "Iteration 20835 => Loss: 6.71065872553615339058\n",
      "Iteration 20836 => Loss: 6.71065704374125981957\n",
      "Iteration 20837 => Loss: 6.71065536217298763688\n",
      "Iteration 20838 => Loss: 6.71065368083131108534\n",
      "Iteration 20839 => Loss: 6.71065199971618664421\n",
      "Iteration 20840 => Loss: 6.71065031882760099080\n",
      "Iteration 20841 => Loss: 6.71064863816551948617\n",
      "Iteration 20842 => Loss: 6.71064695772989594502\n",
      "Iteration 20843 => Loss: 6.71064527752071793287\n",
      "Iteration 20844 => Loss: 6.71064359753794192898\n",
      "Iteration 20845 => Loss: 6.71064191778155194612\n",
      "Iteration 20846 => Loss: 6.71064023825150535174\n",
      "Iteration 20847 => Loss: 6.71063855894777638866\n",
      "Iteration 20848 => Loss: 6.71063687987033308247\n",
      "Iteration 20849 => Loss: 6.71063520101914345872\n",
      "Iteration 20850 => Loss: 6.71063352239418087208\n",
      "Iteration 20851 => Loss: 6.71063184399541334813\n",
      "Iteration 20852 => Loss: 6.71063016582281424149\n",
      "Iteration 20853 => Loss: 6.71062848787633736691\n",
      "Iteration 20854 => Loss: 6.71062681015597295442\n",
      "Iteration 20855 => Loss: 6.71062513266168192416\n",
      "Iteration 20856 => Loss: 6.71062345539342963718\n",
      "Iteration 20857 => Loss: 6.71062177835119300084\n",
      "Iteration 20858 => Loss: 6.71062010153493382347\n",
      "Iteration 20859 => Loss: 6.71061842494462812425\n",
      "Iteration 20860 => Loss: 6.71061674858024748147\n",
      "Iteration 20861 => Loss: 6.71061507244175015074\n",
      "Iteration 20862 => Loss: 6.71061339652911570397\n",
      "Iteration 20863 => Loss: 6.71061172084231216672\n",
      "Iteration 20864 => Loss: 6.71061004538129690644\n",
      "Iteration 20865 => Loss: 6.71060837014606725859\n",
      "Iteration 20866 => Loss: 6.71060669513656726792\n",
      "Iteration 20867 => Loss: 6.71060502035277295363\n",
      "Iteration 20868 => Loss: 6.71060334579466122307\n",
      "Iteration 20869 => Loss: 6.71060167146219033185\n",
      "Iteration 20870 => Loss: 6.71059999735533541099\n",
      "Iteration 20871 => Loss: 6.71059832347407247966\n",
      "Iteration 20872 => Loss: 6.71059664981836068165\n",
      "Iteration 20873 => Loss: 6.71059497638817870069\n",
      "Iteration 20874 => Loss: 6.71059330318348568056\n",
      "Iteration 20875 => Loss: 6.71059163020426385771\n",
      "Iteration 20876 => Loss: 6.71058995745047059955\n",
      "Iteration 20877 => Loss: 6.71058828492208370164\n",
      "Iteration 20878 => Loss: 6.71058661261906941320\n",
      "Iteration 20879 => Loss: 6.71058494054139487162\n",
      "Iteration 20880 => Loss: 6.71058326868903609608\n",
      "Iteration 20881 => Loss: 6.71058159706196555305\n",
      "Iteration 20882 => Loss: 6.71057992566013084001\n",
      "Iteration 20883 => Loss: 6.71057825448353195696\n",
      "Iteration 20884 => Loss: 6.71057658353212538316\n",
      "Iteration 20885 => Loss: 6.71057491280586759785\n",
      "Iteration 20886 => Loss: 6.71057324230474350202\n",
      "Iteration 20887 => Loss: 6.71057157202872645030\n",
      "Iteration 20888 => Loss: 6.71056990197777469831\n",
      "Iteration 20889 => Loss: 6.71056823215186248888\n",
      "Iteration 20890 => Loss: 6.71056656255096140029\n",
      "Iteration 20891 => Loss: 6.71056489317503856995\n",
      "Iteration 20892 => Loss: 6.71056322402406557615\n",
      "Iteration 20893 => Loss: 6.71056155509800511538\n",
      "Iteration 20894 => Loss: 6.71055988639684208863\n",
      "Iteration 20895 => Loss: 6.71055821792052231700\n",
      "Iteration 20896 => Loss: 6.71055654966904668868\n",
      "Iteration 20897 => Loss: 6.71055488164236191295\n",
      "Iteration 20898 => Loss: 6.71055321384043512722\n",
      "Iteration 20899 => Loss: 6.71055154626325922607\n",
      "Iteration 20900 => Loss: 6.71054987891077647788\n",
      "Iteration 20901 => Loss: 6.71054821178297444817\n",
      "Iteration 20902 => Loss: 6.71054654487981760980\n",
      "Iteration 20903 => Loss: 6.71054487820127310016\n",
      "Iteration 20904 => Loss: 6.71054321174732049116\n",
      "Iteration 20905 => Loss: 6.71054154551792514383\n",
      "Iteration 20906 => Loss: 6.71053987951304709014\n",
      "Iteration 20907 => Loss: 6.71053821373266590200\n",
      "Iteration 20908 => Loss: 6.71053654817674782862\n",
      "Iteration 20909 => Loss: 6.71053488284526089558\n",
      "Iteration 20910 => Loss: 6.71053321773818289842\n",
      "Iteration 20911 => Loss: 6.71053155285547653364\n",
      "Iteration 20912 => Loss: 6.71052988819710893864\n",
      "Iteration 20913 => Loss: 6.71052822376305879715\n",
      "Iteration 20914 => Loss: 6.71052655955329591109\n",
      "Iteration 20915 => Loss: 6.71052489556777498336\n",
      "Iteration 20916 => Loss: 6.71052323180648091494\n",
      "Iteration 20917 => Loss: 6.71052156826937462597\n",
      "Iteration 20918 => Loss: 6.71051990495643835288\n",
      "Iteration 20919 => Loss: 6.71051824186762768676\n",
      "Iteration 20920 => Loss: 6.71051657900292308767\n",
      "Iteration 20921 => Loss: 6.71051491636228458759\n",
      "Iteration 20922 => Loss: 6.71051325394568820570\n",
      "Iteration 20923 => Loss: 6.71051159175310552030\n",
      "Iteration 20924 => Loss: 6.71050992978450189241\n",
      "Iteration 20925 => Loss: 6.71050826803985067670\n",
      "Iteration 20926 => Loss: 6.71050660651911368149\n",
      "Iteration 20927 => Loss: 6.71050494522227047867\n",
      "Iteration 20928 => Loss: 6.71050328414928465293\n",
      "Iteration 20929 => Loss: 6.71050162330013133527\n",
      "Iteration 20930 => Loss: 6.71049996267477411038\n",
      "Iteration 20931 => Loss: 6.71049830227319255016\n",
      "Iteration 20932 => Loss: 6.71049664209534846293\n",
      "Iteration 20933 => Loss: 6.71049498214121431516\n",
      "Iteration 20934 => Loss: 6.71049332241075369154\n",
      "Iteration 20935 => Loss: 6.71049166290394438761\n",
      "Iteration 20936 => Loss: 6.71049000362075176440\n",
      "Iteration 20937 => Loss: 6.71048834456115894653\n",
      "Iteration 20938 => Loss: 6.71048668572511353148\n",
      "Iteration 20939 => Loss: 6.71048502711259686748\n",
      "Iteration 20940 => Loss: 6.71048336872357342742\n",
      "Iteration 20941 => Loss: 6.71048171055802900042\n",
      "Iteration 20942 => Loss: 6.71048005261592361848\n",
      "Iteration 20943 => Loss: 6.71047839489722264261\n",
      "Iteration 20944 => Loss: 6.71047673740188788116\n",
      "Iteration 20945 => Loss: 6.71047508012991578141\n",
      "Iteration 20946 => Loss: 6.71047342308125482901\n",
      "Iteration 20947 => Loss: 6.71047176625588459586\n",
      "Iteration 20948 => Loss: 6.71047010965376866665\n",
      "Iteration 20949 => Loss: 6.71046845327487861965\n",
      "Iteration 20950 => Loss: 6.71046679711919225042\n",
      "Iteration 20951 => Loss: 6.71046514118666248550\n",
      "Iteration 20952 => Loss: 6.71046348547727689038\n",
      "Iteration 20953 => Loss: 6.71046182999100260247\n",
      "Iteration 20954 => Loss: 6.71046017472779432467\n",
      "Iteration 20955 => Loss: 6.71045851968764317519\n",
      "Iteration 20956 => Loss: 6.71045686487049763969\n",
      "Iteration 20957 => Loss: 6.71045521027634528366\n",
      "Iteration 20958 => Loss: 6.71045355590515057997\n",
      "Iteration 20959 => Loss: 6.71045190175688421874\n",
      "Iteration 20960 => Loss: 6.71045024783151067282\n",
      "Iteration 20961 => Loss: 6.71044859412900862594\n",
      "Iteration 20962 => Loss: 6.71044694064933455735\n",
      "Iteration 20963 => Loss: 6.71044528739247247984\n",
      "Iteration 20964 => Loss: 6.71044363435838420173\n",
      "Iteration 20965 => Loss: 6.71044198154704574222\n",
      "Iteration 20966 => Loss: 6.71044032895842335051\n",
      "Iteration 20967 => Loss: 6.71043867659249126945\n",
      "Iteration 20968 => Loss: 6.71043702444920686645\n",
      "Iteration 20969 => Loss: 6.71043537252854793707\n",
      "Iteration 20970 => Loss: 6.71043372083049138865\n",
      "Iteration 20971 => Loss: 6.71043206935500347043\n",
      "Iteration 20972 => Loss: 6.71043041810205131981\n",
      "Iteration 20973 => Loss: 6.71042876707160118599\n",
      "Iteration 20974 => Loss: 6.71042711626362642363\n",
      "Iteration 20975 => Loss: 6.71042546567810394009\n",
      "Iteration 20976 => Loss: 6.71042381531499554370\n",
      "Iteration 20977 => Loss: 6.71042216517426748368\n",
      "Iteration 20978 => Loss: 6.71042051525590732552\n",
      "Iteration 20979 => Loss: 6.71041886555987421303\n",
      "Iteration 20980 => Loss: 6.71041721608612729000\n",
      "Iteration 20981 => Loss: 6.71041556683465234556\n",
      "Iteration 20982 => Loss: 6.71041391780541474077\n",
      "Iteration 20983 => Loss: 6.71041226899838072484\n",
      "Iteration 20984 => Loss: 6.71041062041353075784\n",
      "Iteration 20985 => Loss: 6.71040897205081598997\n",
      "Iteration 20986 => Loss: 6.71040732391022309855\n",
      "Iteration 20987 => Loss: 6.71040567599172366187\n",
      "Iteration 20988 => Loss: 6.71040402829527860007\n",
      "Iteration 20989 => Loss: 6.71040238082085416238\n",
      "Iteration 20990 => Loss: 6.71040073356843613794\n",
      "Iteration 20991 => Loss: 6.71039908653797922966\n",
      "Iteration 20992 => Loss: 6.71039743972945945671\n",
      "Iteration 20993 => Loss: 6.71039579314285283829\n",
      "Iteration 20994 => Loss: 6.71039414677811674181\n",
      "Iteration 20995 => Loss: 6.71039250063523695644\n",
      "Iteration 20996 => Loss: 6.71039085471416729689\n",
      "Iteration 20997 => Loss: 6.71038920901488733506\n",
      "Iteration 20998 => Loss: 6.71038756353736509652\n",
      "Iteration 20999 => Loss: 6.71038591828157660046\n",
      "Iteration 21000 => Loss: 6.71038427324748099068\n",
      "Iteration 21001 => Loss: 6.71038262843505606270\n",
      "Iteration 21002 => Loss: 6.71038098384426806575\n",
      "Iteration 21003 => Loss: 6.71037933947509124266\n",
      "Iteration 21004 => Loss: 6.71037769532749450718\n",
      "Iteration 21005 => Loss: 6.71037605140144144400\n",
      "Iteration 21006 => Loss: 6.71037440769690984865\n",
      "Iteration 21007 => Loss: 6.71037276421386774672\n",
      "Iteration 21008 => Loss: 6.71037112095228316377\n",
      "Iteration 21009 => Loss: 6.71036947791213744807\n",
      "Iteration 21010 => Loss: 6.71036783509337642073\n",
      "Iteration 21011 => Loss: 6.71036619249599919357\n",
      "Iteration 21012 => Loss: 6.71036455011995158770\n",
      "Iteration 21013 => Loss: 6.71036290796522116864\n",
      "Iteration 21014 => Loss: 6.71036126603176885652\n",
      "Iteration 21015 => Loss: 6.71035962431957155871\n",
      "Iteration 21016 => Loss: 6.71035798282858841901\n",
      "Iteration 21017 => Loss: 6.71035634155880345020\n",
      "Iteration 21018 => Loss: 6.71035470051017135518\n",
      "Iteration 21019 => Loss: 6.71035305968267614674\n",
      "Iteration 21020 => Loss: 6.71035141907628052138\n",
      "Iteration 21021 => Loss: 6.71034977869095428105\n",
      "Iteration 21022 => Loss: 6.71034813852667344491\n",
      "Iteration 21023 => Loss: 6.71034649858340781492\n",
      "Iteration 21024 => Loss: 6.71034485886111919939\n",
      "Iteration 21025 => Loss: 6.71034321935978361751\n",
      "Iteration 21026 => Loss: 6.71034158007937620027\n",
      "Iteration 21027 => Loss: 6.71033994101985520331\n",
      "Iteration 21028 => Loss: 6.71033830218120108668\n",
      "Iteration 21029 => Loss: 6.71033666356338365233\n",
      "Iteration 21030 => Loss: 6.71033502516636293223\n",
      "Iteration 21031 => Loss: 6.71033338699012027462\n",
      "Iteration 21032 => Loss: 6.71033174903462281691\n",
      "Iteration 21033 => Loss: 6.71033011129984569010\n",
      "Iteration 21034 => Loss: 6.71032847378574004438\n",
      "Iteration 21035 => Loss: 6.71032683649229433342\n",
      "Iteration 21036 => Loss: 6.71032519941948013553\n",
      "Iteration 21037 => Loss: 6.71032356256726014720\n",
      "Iteration 21038 => Loss: 6.71032192593560150584\n",
      "Iteration 21039 => Loss: 6.71032028952448023063\n",
      "Iteration 21040 => Loss: 6.71031865333386789985\n",
      "Iteration 21041 => Loss: 6.71031701736372632183\n",
      "Iteration 21042 => Loss: 6.71031538161404039755\n",
      "Iteration 21043 => Loss: 6.71031374608476838262\n",
      "Iteration 21044 => Loss: 6.71031211077587830260\n",
      "Iteration 21045 => Loss: 6.71031047568735505848\n",
      "Iteration 21046 => Loss: 6.71030884081915335315\n",
      "Iteration 21047 => Loss: 6.71030720617125542304\n",
      "Iteration 21048 => Loss: 6.71030557174362929374\n",
      "Iteration 21049 => Loss: 6.71030393753623588537\n",
      "Iteration 21050 => Loss: 6.71030230354905210532\n",
      "Iteration 21051 => Loss: 6.71030066978204953188\n",
      "Iteration 21052 => Loss: 6.71029903623519707878\n",
      "Iteration 21053 => Loss: 6.71029740290846010708\n",
      "Iteration 21054 => Loss: 6.71029576980181996504\n",
      "Iteration 21055 => Loss: 6.71029413691524823093\n",
      "Iteration 21056 => Loss: 6.71029250424869871949\n",
      "Iteration 21057 => Loss: 6.71029087180215544350\n",
      "Iteration 21058 => Loss: 6.71028923957557754676\n",
      "Iteration 21059 => Loss: 6.71028760756894815387\n",
      "Iteration 21060 => Loss: 6.71028597578222907316\n",
      "Iteration 21061 => Loss: 6.71028434421539632382\n",
      "Iteration 21062 => Loss: 6.71028271286841881960\n",
      "Iteration 21063 => Loss: 6.71028108174125925700\n",
      "Iteration 21064 => Loss: 6.71027945083389631975\n",
      "Iteration 21065 => Loss: 6.71027782014630336249\n",
      "Iteration 21066 => Loss: 6.71027618967843775266\n",
      "Iteration 21067 => Loss: 6.71027455943028527940\n",
      "Iteration 21068 => Loss: 6.71027292940180064562\n",
      "Iteration 21069 => Loss: 6.71027129959297052864\n",
      "Iteration 21070 => Loss: 6.71026967000375496042\n",
      "Iteration 21071 => Loss: 6.71026804063412551926\n",
      "Iteration 21072 => Loss: 6.71026641148405023074\n",
      "Iteration 21073 => Loss: 6.71026478255350866675\n",
      "Iteration 21074 => Loss: 6.71026315384246352380\n",
      "Iteration 21075 => Loss: 6.71026152535088726836\n",
      "Iteration 21076 => Loss: 6.71025989707875147872\n",
      "Iteration 21077 => Loss: 6.71025826902602240409\n",
      "Iteration 21078 => Loss: 6.71025664119267251095\n",
      "Iteration 21079 => Loss: 6.71025501357868225938\n",
      "Iteration 21080 => Loss: 6.71025338618400368773\n",
      "Iteration 21081 => Loss: 6.71025175900862347333\n",
      "Iteration 21082 => Loss: 6.71025013205249720727\n",
      "Iteration 21083 => Loss: 6.71024850531560979050\n",
      "Iteration 21084 => Loss: 6.71024687879792036682\n",
      "Iteration 21085 => Loss: 6.71024525249941117266\n",
      "Iteration 21086 => Loss: 6.71024362642004046364\n",
      "Iteration 21087 => Loss: 6.71024200055978781165\n",
      "Iteration 21088 => Loss: 6.71024037491861857774\n",
      "Iteration 21089 => Loss: 6.71023874949650167565\n",
      "Iteration 21090 => Loss: 6.71023712429341401275\n",
      "Iteration 21091 => Loss: 6.71023549930932272645\n",
      "Iteration 21092 => Loss: 6.71023387454419584230\n",
      "Iteration 21093 => Loss: 6.71023224999801204405\n",
      "Iteration 21094 => Loss: 6.71023062567072425821\n",
      "Iteration 21095 => Loss: 6.71022900156232893210\n",
      "Iteration 21096 => Loss: 6.71022737767276922227\n",
      "Iteration 21097 => Loss: 6.71022575400203713514\n",
      "Iteration 21098 => Loss: 6.71022413055008914995\n",
      "Iteration 21099 => Loss: 6.71022250731691016767\n",
      "Iteration 21100 => Loss: 6.71022088430245400303\n",
      "Iteration 21101 => Loss: 6.71021926150670466882\n",
      "Iteration 21102 => Loss: 6.71021763892961953246\n",
      "Iteration 21103 => Loss: 6.71021601657118438311\n",
      "Iteration 21104 => Loss: 6.71021439443136102909\n",
      "Iteration 21105 => Loss: 6.71021277251011571963\n",
      "Iteration 21106 => Loss: 6.71021115080742447390\n",
      "Iteration 21107 => Loss: 6.71020952932327130469\n",
      "Iteration 21108 => Loss: 6.71020790805759936859\n",
      "Iteration 21109 => Loss: 6.71020628701039623110\n",
      "Iteration 21110 => Loss: 6.71020466618162902961\n",
      "Iteration 21111 => Loss: 6.71020304557127023060\n",
      "Iteration 21112 => Loss: 6.71020142517928697146\n",
      "Iteration 21113 => Loss: 6.71019980500565349502\n",
      "Iteration 21114 => Loss: 6.71019818505034226774\n",
      "Iteration 21115 => Loss: 6.71019656531330888072\n",
      "Iteration 21116 => Loss: 6.71019494579454889305\n",
      "Iteration 21117 => Loss: 6.71019332649400634949\n",
      "Iteration 21118 => Loss: 6.71019170741167503280\n",
      "Iteration 21119 => Loss: 6.71019008854750964588\n",
      "Iteration 21120 => Loss: 6.71018846990148709608\n",
      "Iteration 21121 => Loss: 6.71018685147357984988\n",
      "Iteration 21122 => Loss: 6.71018523326375504467\n",
      "Iteration 21123 => Loss: 6.71018361527197715333\n",
      "Iteration 21124 => Loss: 6.71018199749823374134\n",
      "Iteration 21125 => Loss: 6.71018037994247684708\n",
      "Iteration 21126 => Loss: 6.71017876260469314786\n",
      "Iteration 21127 => Loss: 6.71017714548483912296\n",
      "Iteration 21128 => Loss: 6.71017552858289878515\n",
      "Iteration 21129 => Loss: 6.71017391189883305458\n",
      "Iteration 21130 => Loss: 6.71017229543261972680\n",
      "Iteration 21131 => Loss: 6.71017067918421705741\n",
      "Iteration 21132 => Loss: 6.71016906315360461832\n",
      "Iteration 21133 => Loss: 6.71016744734076286960\n",
      "Iteration 21134 => Loss: 6.71016583174564118508\n",
      "Iteration 21135 => Loss: 6.71016421636822890662\n",
      "Iteration 21136 => Loss: 6.71016260120848162529\n",
      "Iteration 21137 => Loss: 6.71016098626637891300\n",
      "Iteration 21138 => Loss: 6.71015937154189501257\n",
      "Iteration 21139 => Loss: 6.71015775703499350868\n",
      "Iteration 21140 => Loss: 6.71015614274564065056\n",
      "Iteration 21141 => Loss: 6.71015452867382133917\n",
      "Iteration 21142 => Loss: 6.71015291481949827102\n",
      "Iteration 21143 => Loss: 6.71015130118263591896\n",
      "Iteration 21144 => Loss: 6.71014968776321563126\n",
      "Iteration 21145 => Loss: 6.71014807456120099260\n",
      "Iteration 21146 => Loss: 6.71014646157656891035\n",
      "Iteration 21147 => Loss: 6.71014484880928385735\n",
      "Iteration 21148 => Loss: 6.71014323625931918826\n",
      "Iteration 21149 => Loss: 6.71014162392664648138\n",
      "Iteration 21150 => Loss: 6.71014001181123465045\n",
      "Iteration 21151 => Loss: 6.71013839991306149102\n",
      "Iteration 21152 => Loss: 6.71013678823208081781\n",
      "Iteration 21153 => Loss: 6.71013517676828552538\n",
      "Iteration 21154 => Loss: 6.71013356552162942847\n",
      "Iteration 21155 => Loss: 6.71013195449208943444\n",
      "Iteration 21156 => Loss: 6.71013034367963268068\n",
      "Iteration 21157 => Loss: 6.71012873308423518637\n",
      "Iteration 21158 => Loss: 6.71012712270587297070\n",
      "Iteration 21159 => Loss: 6.71012551254450251292\n",
      "Iteration 21160 => Loss: 6.71012390260010249676\n",
      "Iteration 21161 => Loss: 6.71012229287264094779\n",
      "Iteration 21162 => Loss: 6.71012068336208855612\n",
      "Iteration 21163 => Loss: 6.71011907406842489365\n",
      "Iteration 21164 => Loss: 6.71011746499160643964\n",
      "Iteration 21165 => Loss: 6.71011585613161631869\n",
      "Iteration 21166 => Loss: 6.71011424748841811549\n",
      "Iteration 21167 => Loss: 6.71011263906198607287\n",
      "Iteration 21168 => Loss: 6.71011103085228288734\n",
      "Iteration 21169 => Loss: 6.71010942285929434803\n",
      "Iteration 21170 => Loss: 6.71010781508297426967\n",
      "Iteration 21171 => Loss: 6.71010620752331110594\n",
      "Iteration 21172 => Loss: 6.71010460018026755336\n",
      "Iteration 21173 => Loss: 6.71010299305380630841\n",
      "Iteration 21174 => Loss: 6.71010138614391316025\n",
      "Iteration 21175 => Loss: 6.71009977945054281179\n",
      "Iteration 21176 => Loss: 6.71009817297367661126\n",
      "Iteration 21177 => Loss: 6.71009656671329057787\n",
      "Iteration 21178 => Loss: 6.71009496066934296721\n",
      "Iteration 21179 => Loss: 6.71009335484180891029\n",
      "Iteration 21180 => Loss: 6.71009174923065732088\n",
      "Iteration 21181 => Loss: 6.71009014383586688268\n",
      "Iteration 21182 => Loss: 6.71008853865740473310\n",
      "Iteration 21183 => Loss: 6.71008693369523623318\n",
      "Iteration 21184 => Loss: 6.71008532894934273116\n",
      "Iteration 21185 => Loss: 6.71008372441968514721\n",
      "Iteration 21186 => Loss: 6.71008212010623328325\n",
      "Iteration 21187 => Loss: 6.71008051600896315847\n",
      "Iteration 21188 => Loss: 6.71007891212785434476\n",
      "Iteration 21189 => Loss: 6.71007730846286865045\n",
      "Iteration 21190 => Loss: 6.71007570501396877205\n",
      "Iteration 21191 => Loss: 6.71007410178113428145\n",
      "Iteration 21192 => Loss: 6.71007249876434208602\n",
      "Iteration 21193 => Loss: 6.71007089596355665861\n",
      "Iteration 21194 => Loss: 6.71006929337873980757\n",
      "Iteration 21195 => Loss: 6.71006769100987199295\n",
      "Iteration 21196 => Loss: 6.71006608885693278665\n",
      "Iteration 21197 => Loss: 6.71006448691987689159\n",
      "Iteration 21198 => Loss: 6.71006288519868387965\n",
      "Iteration 21199 => Loss: 6.71006128369331911188\n",
      "Iteration 21200 => Loss: 6.71005968240376926559\n",
      "Iteration 21201 => Loss: 6.71005808132998105009\n",
      "Iteration 21202 => Loss: 6.71005648047193670180\n",
      "Iteration 21203 => Loss: 6.71005487982961756899\n",
      "Iteration 21204 => Loss: 6.71005327940297657818\n",
      "Iteration 21205 => Loss: 6.71005167919199951854\n",
      "Iteration 21206 => Loss: 6.71005007919665441563\n",
      "Iteration 21207 => Loss: 6.71004847941690041324\n",
      "Iteration 21208 => Loss: 6.71004687985271708328\n",
      "Iteration 21209 => Loss: 6.71004528050408133311\n",
      "Iteration 21210 => Loss: 6.71004368137095230651\n",
      "Iteration 21211 => Loss: 6.71004208245331312810\n",
      "Iteration 21212 => Loss: 6.71004048375111938896\n",
      "Iteration 21213 => Loss: 6.71003888526435421369\n",
      "Iteration 21214 => Loss: 6.71003728699299095695\n",
      "Iteration 21215 => Loss: 6.71003568893699231523\n",
      "Iteration 21216 => Loss: 6.71003409109632276142\n",
      "Iteration 21217 => Loss: 6.71003249347097252553\n",
      "Iteration 21218 => Loss: 6.71003089606090252772\n",
      "Iteration 21219 => Loss: 6.71002929886607812904\n",
      "Iteration 21220 => Loss: 6.71002770188647712502\n",
      "Iteration 21221 => Loss: 6.71002610512207287030\n",
      "Iteration 21222 => Loss: 6.71002450857283427865\n",
      "Iteration 21223 => Loss: 6.71002291223872227022\n",
      "Iteration 21224 => Loss: 6.71002131611972085778\n",
      "Iteration 21225 => Loss: 6.71001972021579806693\n",
      "Iteration 21226 => Loss: 6.71001812452692281141\n",
      "Iteration 21227 => Loss: 6.71001652905306222863\n",
      "Iteration 21228 => Loss: 6.71001493379419766683\n",
      "Iteration 21229 => Loss: 6.71001333875028915799\n",
      "Iteration 21230 => Loss: 6.71001174392131805035\n",
      "Iteration 21231 => Loss: 6.71001014930724970498\n",
      "Iteration 21232 => Loss: 6.71000855490805392378\n",
      "Iteration 21233 => Loss: 6.71000696072370317324\n",
      "Iteration 21234 => Loss: 6.71000536675417169619\n",
      "Iteration 21235 => Loss: 6.71000377299942574183\n",
      "Iteration 21236 => Loss: 6.71000217945943955300\n",
      "Iteration 21237 => Loss: 6.71000058613418115527\n",
      "Iteration 21238 => Loss: 6.70999899302362212694\n",
      "Iteration 21239 => Loss: 6.70999740012773404629\n",
      "Iteration 21240 => Loss: 6.70999580744649470887\n",
      "Iteration 21241 => Loss: 6.70999421497986947571\n",
      "Iteration 21242 => Loss: 6.70999262272782281968\n",
      "Iteration 21243 => Loss: 6.70999103069033431268\n",
      "Iteration 21244 => Loss: 6.70998943886737198028\n",
      "Iteration 21245 => Loss: 6.70998784725891184166\n",
      "Iteration 21246 => Loss: 6.70998625586491925787\n",
      "Iteration 21247 => Loss: 6.70998466468536935992\n",
      "Iteration 21248 => Loss: 6.70998307372022395612\n",
      "Iteration 21249 => Loss: 6.70998148296946972380\n",
      "Iteration 21250 => Loss: 6.70997989243306491858\n",
      "Iteration 21251 => Loss: 6.70997830211098200692\n",
      "Iteration 21252 => Loss: 6.70997671200320588980\n",
      "Iteration 21253 => Loss: 6.70997512210969038193\n",
      "Iteration 21254 => Loss: 6.70997353243040883797\n",
      "Iteration 21255 => Loss: 6.70997194296533994162\n",
      "Iteration 21256 => Loss: 6.70997035371445083030\n",
      "Iteration 21257 => Loss: 6.70996876467771929953\n",
      "Iteration 21258 => Loss: 6.70996717585510449311\n",
      "Iteration 21259 => Loss: 6.70996558724658243023\n",
      "Iteration 21260 => Loss: 6.70996399885213268277\n",
      "Iteration 21261 => Loss: 6.70996241067170995365\n",
      "Iteration 21262 => Loss: 6.70996082270530358471\n",
      "Iteration 21263 => Loss: 6.70995923495287005522\n",
      "Iteration 21264 => Loss: 6.70995764741438804890\n",
      "Iteration 21265 => Loss: 6.70995606008982203861\n",
      "Iteration 21266 => Loss: 6.70995447297915692531\n",
      "Iteration 21267 => Loss: 6.70995288608235185279\n",
      "Iteration 21268 => Loss: 6.70995129939938461661\n",
      "Iteration 21269 => Loss: 6.70994971293021436054\n",
      "Iteration 21270 => Loss: 6.70994812667482865010\n",
      "Iteration 21271 => Loss: 6.70994654063318662907\n",
      "Iteration 21272 => Loss: 6.70994495480526431663\n",
      "Iteration 21273 => Loss: 6.70994336919103329109\n",
      "Iteration 21274 => Loss: 6.70994178379046424254\n",
      "Iteration 21275 => Loss: 6.70994019860352697293\n",
      "Iteration 21276 => Loss: 6.70993861363019217237\n",
      "Iteration 21277 => Loss: 6.70993702887043941274\n",
      "Iteration 21278 => Loss: 6.70993544432422872603\n",
      "Iteration 21279 => Loss: 6.70993385999153080235\n",
      "Iteration 21280 => Loss: 6.70993227587233409537\n",
      "Iteration 21281 => Loss: 6.70993069196658620257\n",
      "Iteration 21282 => Loss: 6.70992910827427113674\n",
      "Iteration 21283 => Loss: 6.70992752479536136434\n",
      "Iteration 21284 => Loss: 6.70992594152983112821\n",
      "Iteration 21285 => Loss: 6.70992435847763601942\n",
      "Iteration 21286 => Loss: 6.70992277563875738622\n",
      "Iteration 21287 => Loss: 6.70992119301317302416\n",
      "Iteration 21288 => Loss: 6.70991961060084918245\n",
      "Iteration 21289 => Loss: 6.70991802840175033396\n",
      "Iteration 21290 => Loss: 6.70991644641585516240\n",
      "Iteration 21291 => Loss: 6.70991486464313346971\n",
      "Iteration 21292 => Loss: 6.70991328308355416965\n",
      "Iteration 21293 => Loss: 6.70991170173708795232\n",
      "Iteration 21294 => Loss: 6.70991012060371083692\n",
      "Iteration 21295 => Loss: 6.70990853968339440172\n",
      "Iteration 21296 => Loss: 6.70990695897610223142\n",
      "Iteration 21297 => Loss: 6.70990537848181123337\n",
      "Iteration 21298 => Loss: 6.70990379820048943316\n",
      "Iteration 21299 => Loss: 6.70990221813211462631\n",
      "Iteration 21300 => Loss: 6.70990063827665572660\n",
      "Iteration 21301 => Loss: 6.70989905863408342412\n",
      "Iteration 21302 => Loss: 6.70989747920436041539\n",
      "Iteration 21303 => Loss: 6.70989589998746716049\n",
      "Iteration 21304 => Loss: 6.70989432098337967858\n",
      "Iteration 21305 => Loss: 6.70989274219206244254\n",
      "Iteration 21306 => Loss: 6.70989116361348347795\n",
      "Iteration 21307 => Loss: 6.70988958524761969215\n",
      "Iteration 21308 => Loss: 6.70988800709444266346\n",
      "Iteration 21309 => Loss: 6.70988642915391686472\n",
      "Iteration 21310 => Loss: 6.70988485142602364419\n",
      "Iteration 21311 => Loss: 6.70988327391072747474\n",
      "Iteration 21312 => Loss: 6.70988169660800526373\n",
      "Iteration 21313 => Loss: 6.70988011951782059583\n",
      "Iteration 21314 => Loss: 6.70987854264014504935\n",
      "Iteration 21315 => Loss: 6.70987696597495997253\n",
      "Iteration 21316 => Loss: 6.70987538952222983824\n",
      "Iteration 21317 => Loss: 6.70987381328192444840\n",
      "Iteration 21318 => Loss: 6.70987223725402071040\n",
      "Iteration 21319 => Loss: 6.70987066143848398525\n",
      "Iteration 21320 => Loss: 6.70986908583528762762\n",
      "Iteration 21321 => Loss: 6.70986751044440588032\n",
      "Iteration 21322 => Loss: 6.70986593526580676894\n",
      "Iteration 21323 => Loss: 6.70986436029946897719\n",
      "Iteration 21324 => Loss: 6.70986278554535342522\n",
      "Iteration 21325 => Loss: 6.70986121100343257950\n",
      "Iteration 21326 => Loss: 6.70985963667368956465\n",
      "Iteration 21327 => Loss: 6.70985806255608441262\n",
      "Iteration 21328 => Loss: 6.70985648865058958989\n",
      "Iteration 21329 => Loss: 6.70985491495717933930\n",
      "Iteration 21330 => Loss: 6.70985334147582701547\n",
      "Iteration 21331 => Loss: 6.70985176820650153218\n",
      "Iteration 21332 => Loss: 6.70985019514916825045\n",
      "Iteration 21333 => Loss: 6.70984862230380585402\n",
      "Iteration 21334 => Loss: 6.70984704967038769752\n",
      "Iteration 21335 => Loss: 6.70984547724888091835\n",
      "Iteration 21336 => Loss: 6.70984390503925975935\n",
      "Iteration 21337 => Loss: 6.70984233304149313426\n",
      "Iteration 21338 => Loss: 6.70984076125555084502\n",
      "Iteration 21339 => Loss: 6.70983918968140979899\n",
      "Iteration 21340 => Loss: 6.70983761831903979811\n",
      "Iteration 21341 => Loss: 6.70983604716840709159\n",
      "Iteration 21342 => Loss: 6.70983447622948592226\n",
      "Iteration 21343 => Loss: 6.70983290550224875659\n",
      "Iteration 21344 => Loss: 6.70983133498667516648\n",
      "Iteration 21345 => Loss: 6.70982976468272074300\n",
      "Iteration 21346 => Loss: 6.70982819459036861076\n",
      "Iteration 21347 => Loss: 6.70982662470959123624\n",
      "Iteration 21348 => Loss: 6.70982505504034865140\n",
      "Iteration 21349 => Loss: 6.70982348558261953997\n",
      "Iteration 21350 => Loss: 6.70982191633637281569\n",
      "Iteration 21351 => Loss: 6.70982034730158893865\n",
      "Iteration 21352 => Loss: 6.70981877847822794081\n",
      "Iteration 21353 => Loss: 6.70981720986626939407\n",
      "Iteration 21354 => Loss: 6.70981564146567510676\n",
      "Iteration 21355 => Loss: 6.70981407327642731531\n",
      "Iteration 21356 => Loss: 6.70981250529849670983\n",
      "Iteration 21357 => Loss: 6.70981093753184154593\n",
      "Iteration 21358 => Loss: 6.70980936997644494824\n",
      "Iteration 21359 => Loss: 6.70980780263228115956\n",
      "Iteration 21360 => Loss: 6.70980623549931642913\n",
      "Iteration 21361 => Loss: 6.70980466857752411158\n",
      "Iteration 21362 => Loss: 6.70980310186687312068\n",
      "Iteration 21363 => Loss: 6.70980153536733414654\n",
      "Iteration 21364 => Loss: 6.70979996907888409652\n",
      "Iteration 21365 => Loss: 6.70979840300149632526\n",
      "Iteration 21366 => Loss: 6.70979683713513175292\n",
      "Iteration 21367 => Loss: 6.70979527147976639867\n",
      "Iteration 21368 => Loss: 6.70979370603537095263\n",
      "Iteration 21369 => Loss: 6.70979214080192676306\n",
      "Iteration 21370 => Loss: 6.70979057577939297374\n",
      "Iteration 21371 => Loss: 6.70978901096775004476\n",
      "Iteration 21372 => Loss: 6.70978744636696244896\n",
      "Iteration 21373 => Loss: 6.70978588197700176465\n",
      "Iteration 21374 => Loss: 6.70978431779785022826\n",
      "Iteration 21375 => Loss: 6.70978275382946875993\n",
      "Iteration 21376 => Loss: 6.70978119007182893796\n",
      "Iteration 21377 => Loss: 6.70977962652491033424\n",
      "Iteration 21378 => Loss: 6.70977806318867919799\n",
      "Iteration 21379 => Loss: 6.70977650006310355479\n",
      "Iteration 21380 => Loss: 6.70977493714816564108\n",
      "Iteration 21381 => Loss: 6.70977337444382992970\n",
      "Iteration 21382 => Loss: 6.70977181195006000536\n",
      "Iteration 21383 => Loss: 6.70977024966684520990\n",
      "Iteration 21384 => Loss: 6.70976868759414291077\n",
      "Iteration 21385 => Loss: 6.70976712573193356803\n",
      "Iteration 21386 => Loss: 6.70976556408018609545\n",
      "Iteration 21387 => Loss: 6.70976400263886496589\n",
      "Iteration 21388 => Loss: 6.70976244140795152759\n",
      "Iteration 21389 => Loss: 6.70976088038741824704\n",
      "Iteration 21390 => Loss: 6.70975931957722693255\n",
      "Iteration 21391 => Loss: 6.70975775897735804421\n",
      "Iteration 21392 => Loss: 6.70975619858778493665\n",
      "Iteration 21393 => Loss: 6.70975463840846586550\n",
      "Iteration 21394 => Loss: 6.70975307843938573171\n",
      "Iteration 21395 => Loss: 6.70975151868050545545\n",
      "Iteration 21396 => Loss: 6.70974995913180904950\n",
      "Iteration 21397 => Loss: 6.70974839979326009853\n",
      "Iteration 21398 => Loss: 6.70974684066483639810\n",
      "Iteration 21399 => Loss: 6.70974528174649620382\n",
      "Iteration 21400 => Loss: 6.70974372303822352848\n",
      "Iteration 21401 => Loss: 6.70974216453998817400\n",
      "Iteration 21402 => Loss: 6.70974060625176438322\n",
      "Iteration 21403 => Loss: 6.70973904817351840535\n",
      "Iteration 21404 => Loss: 6.70973749030522093051\n",
      "Iteration 21405 => Loss: 6.70973593264684797788\n",
      "Iteration 21406 => Loss: 6.70973437519836846121\n",
      "Iteration 21407 => Loss: 6.70973281795975751152\n",
      "Iteration 21408 => Loss: 6.70973126093098226619\n",
      "Iteration 21409 => Loss: 6.70972970411201874441\n",
      "Iteration 21410 => Loss: 6.70972814750282875451\n",
      "Iteration 21411 => Loss: 6.70972659110340252653\n",
      "Iteration 21412 => Loss: 6.70972503491370098061\n",
      "Iteration 21413 => Loss: 6.70972347893368858962\n",
      "Iteration 21414 => Loss: 6.70972192316335203088\n",
      "Iteration 21415 => Loss: 6.70972036760264689548\n",
      "Iteration 21416 => Loss: 6.70971881225155719619\n",
      "Iteration 21417 => Loss: 6.70971725711005628767\n",
      "Iteration 21418 => Loss: 6.70971570217810509007\n",
      "Iteration 21419 => Loss: 6.70971414745568583982\n",
      "Iteration 21420 => Loss: 6.70971259294276567431\n",
      "Iteration 21421 => Loss: 6.70971103863931261913\n",
      "Iteration 21422 => Loss: 6.70970948454530002891\n",
      "Iteration 21423 => Loss: 6.70970793066070658739\n",
      "Iteration 21424 => Loss: 6.70970637698550298467\n",
      "Iteration 21425 => Loss: 6.70970482351964570000\n",
      "Iteration 21426 => Loss: 6.70970327026312762797\n",
      "Iteration 21427 => Loss: 6.70970171721590258329\n",
      "Iteration 21428 => Loss: 6.70970016437795901965\n",
      "Iteration 21429 => Loss: 6.70969861174925519265\n",
      "Iteration 21430 => Loss: 6.70969705932977156237\n",
      "Iteration 21431 => Loss: 6.70969550711947704258\n",
      "Iteration 21432 => Loss: 6.70969395511834054702\n",
      "Iteration 21433 => Loss: 6.70969240332633543034\n",
      "Iteration 21434 => Loss: 6.70969085174343593536\n",
      "Iteration 21435 => Loss: 6.70968930036961008767\n",
      "Iteration 21436 => Loss: 6.70968774920483568280\n",
      "Iteration 21437 => Loss: 6.70968619824907896998\n",
      "Iteration 21438 => Loss: 6.70968464750231063931\n",
      "Iteration 21439 => Loss: 6.70968309696451203905\n",
      "Iteration 21440 => Loss: 6.70968154663565030660\n",
      "Iteration 21441 => Loss: 6.70967999651568636210\n",
      "Iteration 21442 => Loss: 6.70967844660460510653\n",
      "Iteration 21443 => Loss: 6.70967689690237456546\n",
      "Iteration 21444 => Loss: 6.70967534740896986989\n",
      "Iteration 21445 => Loss: 6.70967379812435282815\n",
      "Iteration 21446 => Loss: 6.70967224904849857126\n",
      "Iteration 21447 => Loss: 6.70967070018139910559\n",
      "Iteration 21448 => Loss: 6.70966915152299669955\n",
      "Iteration 21449 => Loss: 6.70966760307328069501\n",
      "Iteration 21450 => Loss: 6.70966605483221734119\n",
      "Iteration 21451 => Loss: 6.70966450679978176908\n",
      "Iteration 21452 => Loss: 6.70966295897594200426\n",
      "Iteration 21453 => Loss: 6.70966141136067051320\n",
      "Iteration 21454 => Loss: 6.70965986395393887420\n",
      "Iteration 21455 => Loss: 6.70965831675572310644\n",
      "Iteration 21456 => Loss: 6.70965676976598768277\n",
      "Iteration 21457 => Loss: 6.70965522298471839235\n",
      "Iteration 21458 => Loss: 6.70965367641186993808\n",
      "Iteration 21459 => Loss: 6.70965213004742722092\n",
      "Iteration 21460 => Loss: 6.70965058389134938466\n",
      "Iteration 21461 => Loss: 6.70964903794362399481\n",
      "Iteration 21462 => Loss: 6.70964749220421641240\n",
      "Iteration 21463 => Loss: 6.70964594667309377485\n",
      "Iteration 21464 => Loss: 6.70964440135023032497\n",
      "Iteration 21465 => Loss: 6.70964285623560208194\n",
      "Iteration 21466 => Loss: 6.70964131132917529499\n",
      "Iteration 21467 => Loss: 6.70963976663092775965\n",
      "Iteration 21468 => Loss: 6.70963822214082306061\n",
      "Iteration 21469 => Loss: 6.70963667785884254613\n",
      "Iteration 21470 => Loss: 6.70963513378495779449\n",
      "Iteration 21471 => Loss: 6.70963358991913239038\n",
      "Iteration 21472 => Loss: 6.70963204626134324116\n",
      "Iteration 21473 => Loss: 6.70963050281156458965\n",
      "Iteration 21474 => Loss: 6.70962895956976446143\n",
      "Iteration 21475 => Loss: 6.70962741653591709934\n",
      "Iteration 21476 => Loss: 6.70962587370998786440\n",
      "Iteration 21477 => Loss: 6.70962433109196343395\n",
      "Iteration 21478 => Loss: 6.70962278868179762270\n",
      "Iteration 21479 => Loss: 6.70962124647947710798\n",
      "Iteration 21480 => Loss: 6.70961970448496813901\n",
      "Iteration 21481 => Loss: 6.70961816269824495862\n",
      "Iteration 21482 => Loss: 6.70961662111927736873\n",
      "Iteration 21483 => Loss: 6.70961507974803339494\n",
      "Iteration 21484 => Loss: 6.70961353858449260912\n",
      "Iteration 21485 => Loss: 6.70961199762862747775\n",
      "Iteration 21486 => Loss: 6.70961045688039803281\n",
      "Iteration 21487 => Loss: 6.70960891633978651072\n",
      "Iteration 21488 => Loss: 6.70960737600677337156\n",
      "Iteration 21489 => Loss: 6.70960583588130532462\n",
      "Iteration 21490 => Loss: 6.70960429596337704083\n",
      "Iteration 21491 => Loss: 6.70960275625294766400\n",
      "Iteration 21492 => Loss: 6.70960121675000298325\n",
      "Iteration 21493 => Loss: 6.70959967745450303056\n",
      "Iteration 21494 => Loss: 6.70959813836641583151\n",
      "Iteration 21495 => Loss: 6.70959659948573250432\n",
      "Iteration 21496 => Loss: 6.70959506081240508735\n",
      "Iteration 21497 => Loss: 6.70959352234641759338\n",
      "Iteration 21498 => Loss: 6.70959198408773893618\n",
      "Iteration 21499 => Loss: 6.70959044603633891768\n",
      "Iteration 21500 => Loss: 6.70958890819219178070\n",
      "Iteration 21501 => Loss: 6.70958737055526821536\n",
      "Iteration 21502 => Loss: 6.70958583312553891176\n",
      "Iteration 21503 => Loss: 6.70958429590298610634\n",
      "Iteration 21504 => Loss: 6.70958275888756983107\n",
      "Iteration 21505 => Loss: 6.70958122207926788150\n",
      "Iteration 21506 => Loss: 6.70957968547804561865\n",
      "Iteration 21507 => Loss: 6.70957814908389060804\n",
      "Iteration 21508 => Loss: 6.70957661289675844074\n",
      "Iteration 21509 => Loss: 6.70957507691662513594\n",
      "Iteration 21510 => Loss: 6.70957354114346937735\n",
      "Iteration 21511 => Loss: 6.70957200557726185508\n",
      "Iteration 21512 => Loss: 6.70957047021796704200\n",
      "Iteration 21513 => Loss: 6.70956893506556095730\n",
      "Iteration 21514 => Loss: 6.70956740012001962015\n",
      "Iteration 21515 => Loss: 6.70956586538131816155\n",
      "Iteration 21516 => Loss: 6.70956433084941483713\n",
      "Iteration 21517 => Loss: 6.70956279652429188332\n",
      "Iteration 21518 => Loss: 6.70956126240591288479\n",
      "Iteration 21519 => Loss: 6.70955972849426363069\n",
      "Iteration 21520 => Loss: 6.70955819478931214661\n",
      "Iteration 21521 => Loss: 6.70955666129102112905\n",
      "Iteration 21522 => Loss: 6.70955512799936748536\n",
      "Iteration 21523 => Loss: 6.70955359491433878105\n",
      "Iteration 21524 => Loss: 6.70955206203587817271\n",
      "Iteration 21525 => Loss: 6.70955052936397677854\n",
      "Iteration 21526 => Loss: 6.70954899689860351231\n",
      "Iteration 21527 => Loss: 6.70954746463973261683\n",
      "Iteration 21528 => Loss: 6.70954593258733034133\n",
      "Iteration 21529 => Loss: 6.70954440074137803407\n",
      "Iteration 21530 => Loss: 6.70954286910183839154\n",
      "Iteration 21531 => Loss: 6.70954133766868654476\n",
      "Iteration 21532 => Loss: 6.70953980644189496019\n",
      "Iteration 21533 => Loss: 6.70953827542143965701\n",
      "Iteration 21534 => Loss: 6.70953674460728510809\n",
      "Iteration 21535 => Loss: 6.70953521399941266168\n",
      "Iteration 21536 => Loss: 6.70953368359778501429\n",
      "Iteration 21537 => Loss: 6.70953215340237729691\n",
      "Iteration 21538 => Loss: 6.70953062341316996964\n",
      "Iteration 21539 => Loss: 6.70952909363012572896\n",
      "Iteration 21540 => Loss: 6.70952756405322325861\n",
      "Iteration 21541 => Loss: 6.70952603468242703144\n",
      "Iteration 21542 => Loss: 6.70952450551771839571\n",
      "Iteration 21543 => Loss: 6.70952297655906182428\n",
      "Iteration 21544 => Loss: 6.70952144780642800725\n",
      "Iteration 21545 => Loss: 6.70951991925980006926\n",
      "Iteration 21546 => Loss: 6.70951839091913893043\n",
      "Iteration 21547 => Loss: 6.70951686278442593903\n",
      "Iteration 21548 => Loss: 6.70951533485562823245\n",
      "Iteration 21549 => Loss: 6.70951380713271738898\n",
      "Iteration 21550 => Loss: 6.70951227961567564506\n",
      "Iteration 21551 => Loss: 6.70951075230445770359\n",
      "Iteration 21552 => Loss: 6.70950922519904580099\n",
      "Iteration 21553 => Loss: 6.70950769829941506828\n",
      "Iteration 21554 => Loss: 6.70950617160552909013\n",
      "Iteration 21555 => Loss: 6.70950464511736921480\n",
      "Iteration 21556 => Loss: 6.70950311883490435605\n",
      "Iteration 21557 => Loss: 6.70950159275810342763\n",
      "Iteration 21558 => Loss: 6.70950006688694067236\n",
      "Iteration 21559 => Loss: 6.70949854122139122126\n",
      "Iteration 21560 => Loss: 6.70949701576142931714\n",
      "Iteration 21561 => Loss: 6.70949549050702032105\n",
      "Iteration 21562 => Loss: 6.70949396545813758763\n",
      "Iteration 21563 => Loss: 6.70949244061475624790\n",
      "Iteration 21564 => Loss: 6.70949091597685143284\n",
      "Iteration 21565 => Loss: 6.70948939154438406263\n",
      "Iteration 21566 => Loss: 6.70948786731733992639\n",
      "Iteration 21567 => Loss: 6.70948634329567905610\n",
      "Iteration 21568 => Loss: 6.70948481947938368819\n",
      "Iteration 21569 => Loss: 6.70948329586842362460\n",
      "Iteration 21570 => Loss: 6.70948177246277044361\n",
      "Iteration 21571 => Loss: 6.70948024926240016441\n",
      "Iteration 21572 => Loss: 6.70947872626727548351\n",
      "Iteration 21573 => Loss: 6.70947720347737863733\n",
      "Iteration 21574 => Loss: 6.70947568089267498692\n",
      "Iteration 21575 => Loss: 6.70947415851314055146\n",
      "Iteration 21576 => Loss: 6.70947263633874335653\n",
      "Iteration 21577 => Loss: 6.70947111436946119767\n",
      "Iteration 21578 => Loss: 6.70946959260526387681\n",
      "Iteration 21579 => Loss: 6.70946807104613185402\n",
      "Iteration 21580 => Loss: 6.70946654969202338492\n",
      "Iteration 21581 => Loss: 6.70946502854292159412\n",
      "Iteration 21582 => Loss: 6.70946350759878740178\n",
      "Iteration 21583 => Loss: 6.70946198685960482067\n",
      "Iteration 21584 => Loss: 6.70946046632534276455\n",
      "Iteration 21585 => Loss: 6.70945894599597458807\n",
      "Iteration 21586 => Loss: 6.70945742587146920499\n",
      "Iteration 21587 => Loss: 6.70945590595179641724\n",
      "Iteration 21588 => Loss: 6.70945438623693668490\n",
      "Iteration 21589 => Loss: 6.70945286672685980989\n",
      "Iteration 21590 => Loss: 6.70945134742153470597\n",
      "Iteration 21591 => Loss: 6.70944982832093383962\n",
      "Iteration 21592 => Loss: 6.70944830942503589455\n",
      "Iteration 21593 => Loss: 6.70944679073381067269\n",
      "Iteration 21594 => Loss: 6.70944527224722708780\n",
      "Iteration 21595 => Loss: 6.70944375396526115907\n",
      "Iteration 21596 => Loss: 6.70944223588788268842\n",
      "Iteration 21597 => Loss: 6.70944071801506325414\n",
      "Iteration 21598 => Loss: 6.70943920034677887543\n",
      "Iteration 21599 => Loss: 6.70943768288300290692\n",
      "Iteration 21600 => Loss: 6.70943616562370337419\n",
      "Iteration 21601 => Loss: 6.70943464856885274372\n",
      "Iteration 21602 => Loss: 6.70943313171842614651\n",
      "Iteration 21603 => Loss: 6.70943161507239782537\n",
      "Iteration 21604 => Loss: 6.70943009863073758225\n",
      "Iteration 21605 => Loss: 6.70942858239341344273\n",
      "Iteration 21606 => Loss: 6.70942706636040231416\n",
      "Iteration 21607 => Loss: 6.70942555053167932755\n",
      "Iteration 21608 => Loss: 6.70942403490721694936\n",
      "Iteration 21609 => Loss: 6.70942251948698231701\n",
      "Iteration 21610 => Loss: 6.70942100427095056148\n",
      "Iteration 21611 => Loss: 6.70941948925909681378\n",
      "Iteration 21612 => Loss: 6.70941797445138909950\n",
      "Iteration 21613 => Loss: 6.70941645984780077328\n",
      "Iteration 21614 => Loss: 6.70941494544830518976\n",
      "Iteration 21615 => Loss: 6.70941343125287836813\n",
      "Iteration 21616 => Loss: 6.70941191726148744578\n",
      "Iteration 21617 => Loss: 6.70941040347410755373\n",
      "Iteration 21618 => Loss: 6.70940888989070494119\n",
      "Iteration 21619 => Loss: 6.70940737651126983820\n",
      "Iteration 21620 => Loss: 6.70940586333575428313\n",
      "Iteration 21621 => Loss: 6.70940435036413962422\n",
      "Iteration 21622 => Loss: 6.70940283759640010430\n",
      "Iteration 21623 => Loss: 6.70940132503250641349\n",
      "Iteration 21624 => Loss: 6.70939981267242835372\n",
      "Iteration 21625 => Loss: 6.70939830051614638506\n",
      "Iteration 21626 => Loss: 6.70939678856361698678\n",
      "Iteration 21627 => Loss: 6.70939527681482861254\n",
      "Iteration 21628 => Loss: 6.70939376526975195247\n",
      "Iteration 21629 => Loss: 6.70939225392835947304\n",
      "Iteration 21630 => Loss: 6.70939074279060942985\n",
      "Iteration 21631 => Loss: 6.70938923185648938841\n",
      "Iteration 21632 => Loss: 6.70938772112597359154\n",
      "Iteration 21633 => Loss: 6.70938621059902384758\n",
      "Iteration 21634 => Loss: 6.70938470027561528752\n",
      "Iteration 21635 => Loss: 6.70938319015572837145\n",
      "Iteration 21636 => Loss: 6.70938168023932579587\n",
      "Iteration 21637 => Loss: 6.70938017052638624449\n",
      "Iteration 21638 => Loss: 6.70937866101688129561\n",
      "Iteration 21639 => Loss: 6.70937715171078608023\n",
      "Iteration 21640 => Loss: 6.70937564260806595939\n",
      "Iteration 21641 => Loss: 6.70937413370869695228\n",
      "Iteration 21642 => Loss: 6.70937262501265152537\n",
      "Iteration 21643 => Loss: 6.70937111651990569783\n",
      "Iteration 21644 => Loss: 6.70936960823042749524\n",
      "Iteration 21645 => Loss: 6.70936810014419116044\n",
      "Iteration 21646 => Loss: 6.70936659226117271260\n",
      "Iteration 21647 => Loss: 6.70936508458133662458\n",
      "Iteration 21648 => Loss: 6.70936357710466602100\n",
      "Iteration 21649 => Loss: 6.70936206983112626290\n",
      "Iteration 21650 => Loss: 6.70936056276068448767\n",
      "Iteration 21651 => Loss: 6.70935905589333358989\n",
      "Iteration 21652 => Loss: 6.70935754922902560793\n",
      "Iteration 21653 => Loss: 6.70935604276773567278\n",
      "Iteration 21654 => Loss: 6.70935453650945223814\n",
      "Iteration 21655 => Loss: 6.70935303045413533596\n",
      "Iteration 21656 => Loss: 6.70935152460175121547\n",
      "Iteration 21657 => Loss: 6.70935001895228744218\n",
      "Iteration 21658 => Loss: 6.70934851350570493622\n",
      "Iteration 21659 => Loss: 6.70934700826199126311\n",
      "Iteration 21660 => Loss: 6.70934550322110201392\n",
      "Iteration 21661 => Loss: 6.70934399838301942509\n",
      "Iteration 21662 => Loss: 6.70934249374771329855\n",
      "Iteration 21663 => Loss: 6.70934098931515698894\n",
      "Iteration 21664 => Loss: 6.70933948508532385091\n",
      "Iteration 21665 => Loss: 6.70933798105818546276\n",
      "Iteration 21666 => Loss: 6.70933647723370629734\n",
      "Iteration 21667 => Loss: 6.70933497361188369013\n",
      "Iteration 21668 => Loss: 6.70933347019265902134\n",
      "Iteration 21669 => Loss: 6.70933196697603229097\n",
      "Iteration 21670 => Loss: 6.70933046396195909011\n",
      "Iteration 21671 => Loss: 6.70932896115042076701\n",
      "Iteration 21672 => Loss: 6.70932745854137824182\n",
      "Iteration 21673 => Loss: 6.70932595613481641550\n",
      "Iteration 21674 => Loss: 6.70932445393070242545\n",
      "Iteration 21675 => Loss: 6.70932295192901229086\n",
      "Iteration 21676 => Loss: 6.70932145012972114273\n",
      "Iteration 21677 => Loss: 6.70931994853279434210\n",
      "Iteration 21678 => Loss: 6.70931844713820701998\n",
      "Iteration 21679 => Loss: 6.70931694594593963643\n",
      "Iteration 21680 => Loss: 6.70931544495594600619\n",
      "Iteration 21681 => Loss: 6.70931394416821369475\n",
      "Iteration 21682 => Loss: 6.70931244358271694495\n",
      "Iteration 21683 => Loss: 6.70931094319942111781\n",
      "Iteration 21684 => Loss: 6.70930944301830489707\n",
      "Iteration 21685 => Loss: 6.70930794303933630829\n",
      "Iteration 21686 => Loss: 6.70930644326248870613\n",
      "Iteration 21687 => Loss: 6.70930494368774343883\n",
      "Iteration 21688 => Loss: 6.70930344431505520930\n",
      "Iteration 21689 => Loss: 6.70930194514441247122\n",
      "Iteration 21690 => Loss: 6.70930044617578413835\n",
      "Iteration 21691 => Loss: 6.70929894740913645990\n",
      "Iteration 21692 => Loss: 6.70929744884445788955\n",
      "Iteration 21693 => Loss: 6.70929595048170401839\n",
      "Iteration 21694 => Loss: 6.70929445232085797102\n",
      "Iteration 21695 => Loss: 6.70929295436188510848\n",
      "Iteration 21696 => Loss: 6.70929145660476589086\n",
      "Iteration 21697 => Loss: 6.70928995904946479101\n",
      "Iteration 21698 => Loss: 6.70928846169596404536\n",
      "Iteration 21699 => Loss: 6.70928696454423256768\n",
      "Iteration 21700 => Loss: 6.70928546759424371260\n",
      "Iteration 21701 => Loss: 6.70928397084596195299\n",
      "Iteration 21702 => Loss: 6.70928247429936774893\n",
      "Iteration 21703 => Loss: 6.70928097795444511320\n",
      "Iteration 21704 => Loss: 6.70927948181113986692\n",
      "Iteration 21705 => Loss: 6.70927798586944934556\n",
      "Iteration 21706 => Loss: 6.70927649012933180472\n",
      "Iteration 21707 => Loss: 6.70927499459077036903\n",
      "Iteration 21708 => Loss: 6.70927349925372951134\n",
      "Iteration 21709 => Loss: 6.70927200411819324444\n",
      "Iteration 21710 => Loss: 6.70927050918411449487\n",
      "Iteration 21711 => Loss: 6.70926901445148526903\n",
      "Iteration 21712 => Loss: 6.70926751992026382254\n",
      "Iteration 21713 => Loss: 6.70926602559044127361\n",
      "Iteration 21714 => Loss: 6.70926453146197410149\n",
      "Iteration 21715 => Loss: 6.70926303753484720716\n",
      "Iteration 21716 => Loss: 6.70926154380901529350\n",
      "Iteration 21717 => Loss: 6.70926005028446681422\n",
      "Iteration 21718 => Loss: 6.70925855696117956484\n",
      "Iteration 21719 => Loss: 6.70925706383910558372\n",
      "Iteration 21720 => Loss: 6.70925557091824042999\n",
      "Iteration 21721 => Loss: 6.70925407819853969471\n",
      "Iteration 21722 => Loss: 6.70925258567998294978\n",
      "Iteration 21723 => Loss: 6.70925109336254976711\n",
      "Iteration 21724 => Loss: 6.70924960124620106683\n",
      "Iteration 21725 => Loss: 6.70924810933092263809\n",
      "Iteration 21726 => Loss: 6.70924661761666829563\n",
      "Iteration 21727 => Loss: 6.70924512610342915764\n",
      "Iteration 21728 => Loss: 6.70924363479116703246\n",
      "Iteration 21729 => Loss: 6.70924214367986238017\n",
      "Iteration 21730 => Loss: 6.70924065276948677905\n",
      "Iteration 21731 => Loss: 6.70923916206000559015\n",
      "Iteration 21732 => Loss: 6.70923767155140815532\n",
      "Iteration 21733 => Loss: 6.70923618124365006565\n",
      "Iteration 21734 => Loss: 6.70923469113670822850\n",
      "Iteration 21735 => Loss: 6.70923320123055955122\n",
      "Iteration 21736 => Loss: 6.70923171152517916482\n",
      "Iteration 21737 => Loss: 6.70923022202054131213\n",
      "Iteration 21738 => Loss: 6.70922873271660691330\n",
      "Iteration 21739 => Loss: 6.70922724361335376386\n",
      "Iteration 21740 => Loss: 6.70922575471076676479\n",
      "Iteration 21741 => Loss: 6.70922426600880417169\n",
      "Iteration 21742 => Loss: 6.70922277750744378011\n",
      "Iteration 21743 => Loss: 6.70922128920665805651\n",
      "Iteration 21744 => Loss: 6.70921980110642302009\n",
      "Iteration 21745 => Loss: 6.70921831320671113730\n",
      "Iteration 21746 => Loss: 6.70921682550749132190\n",
      "Iteration 21747 => Loss: 6.70921533800873692854\n",
      "Iteration 21748 => Loss: 6.70921385071043197001\n",
      "Iteration 21749 => Loss: 6.70921236361253203739\n",
      "Iteration 21750 => Loss: 6.70921087671502469618\n",
      "Iteration 21751 => Loss: 6.70920939001787264289\n",
      "Iteration 21752 => Loss: 6.70920790352105278487\n",
      "Iteration 21753 => Loss: 6.70920641722453758860\n",
      "Iteration 21754 => Loss: 6.70920493112830751414\n",
      "Iteration 21755 => Loss: 6.70920344523231992895\n",
      "Iteration 21756 => Loss: 6.70920195953656683940\n",
      "Iteration 21757 => Loss: 6.70920047404100206023\n",
      "Iteration 21758 => Loss: 6.70919898874561493329\n",
      "Iteration 21759 => Loss: 6.70919750365036193784\n",
      "Iteration 21760 => Loss: 6.70919601875523330392\n",
      "Iteration 21761 => Loss: 6.70919453406019705710\n",
      "Iteration 21762 => Loss: 6.70919304956521767025\n",
      "Iteration 21763 => Loss: 6.70919156527028004433\n",
      "Iteration 21764 => Loss: 6.70919008117534243496\n",
      "Iteration 21765 => Loss: 6.70918859728039418400\n",
      "Iteration 21766 => Loss: 6.70918711358539976430\n",
      "Iteration 21767 => Loss: 6.70918563009032897781\n",
      "Iteration 21768 => Loss: 6.70918414679516583732\n",
      "Iteration 21769 => Loss: 6.70918266369986948661\n",
      "Iteration 21770 => Loss: 6.70918118080442660300\n",
      "Iteration 21771 => Loss: 6.70917969810880610027\n",
      "Iteration 21772 => Loss: 6.70917821561296978672\n",
      "Iteration 21773 => Loss: 6.70917673331690611604\n",
      "Iteration 21774 => Loss: 6.70917525122058311382\n",
      "Iteration 21775 => Loss: 6.70917376932396880562\n",
      "Iteration 21776 => Loss: 6.70917228762704365153\n",
      "Iteration 21777 => Loss: 6.70917080612977123621\n",
      "Iteration 21778 => Loss: 6.70916932483213468430\n",
      "Iteration 21779 => Loss: 6.70916784373410379771\n",
      "Iteration 21780 => Loss: 6.70916636283565637200\n",
      "Iteration 21781 => Loss: 6.70916488213675243912\n",
      "Iteration 21782 => Loss: 6.70916340163736890645\n",
      "Iteration 21783 => Loss: 6.70916192133749245130\n",
      "Iteration 21784 => Loss: 6.70916044123707955293\n",
      "Iteration 21785 => Loss: 6.70915896133611422414\n",
      "Iteration 21786 => Loss: 6.70915748163456537867\n",
      "Iteration 21787 => Loss: 6.70915600213240459482\n",
      "Iteration 21788 => Loss: 6.70915452282960789177\n",
      "Iteration 21789 => Loss: 6.70915304372614773598\n",
      "Iteration 21790 => Loss: 6.70915156482198948851\n",
      "Iteration 21791 => Loss: 6.70915008611712515574\n",
      "Iteration 21792 => Loss: 6.70914860761151121693\n",
      "Iteration 21793 => Loss: 6.70914712930512724398\n",
      "Iteration 21794 => Loss: 6.70914565119794659154\n",
      "Iteration 21795 => Loss: 6.70914417328993994971\n",
      "Iteration 21796 => Loss: 6.70914269558107800862\n",
      "Iteration 21797 => Loss: 6.70914121807134300468\n",
      "Iteration 21798 => Loss: 6.70913974076070118713\n",
      "Iteration 21799 => Loss: 6.70913826364912502243\n",
      "Iteration 21800 => Loss: 6.70913678673658697704\n",
      "Iteration 21801 => Loss: 6.70913531002306573470\n",
      "Iteration 21802 => Loss: 6.70913383350853553821\n",
      "Iteration 21803 => Loss: 6.70913235719295730775\n",
      "Iteration 21804 => Loss: 6.70913088107631683243\n",
      "Iteration 21805 => Loss: 6.70912940515858746693\n",
      "Iteration 21806 => Loss: 6.70912792943973546045\n",
      "Iteration 21807 => Loss: 6.70912645391973949671\n",
      "Iteration 21808 => Loss: 6.70912497859856582494\n",
      "Iteration 21809 => Loss: 6.70912350347618602342\n",
      "Iteration 21810 => Loss: 6.70912202855258854584\n",
      "Iteration 21811 => Loss: 6.70912055382773964141\n",
      "Iteration 21812 => Loss: 6.70911907930160111846\n",
      "Iteration 21813 => Loss: 6.70911760497415787796\n",
      "Iteration 21814 => Loss: 6.70911613084538149820\n",
      "Iteration 21815 => Loss: 6.70911465691524178112\n",
      "Iteration 21816 => Loss: 6.70911318318372185132\n",
      "Iteration 21817 => Loss: 6.70911170965078085260\n",
      "Iteration 21818 => Loss: 6.70911023631639658049\n",
      "Iteration 21819 => Loss: 6.70910876318054150147\n",
      "Iteration 21820 => Loss: 6.70910729024319607561\n",
      "Iteration 21821 => Loss: 6.70910581750433365755\n",
      "Iteration 21822 => Loss: 6.70910434496392493742\n",
      "Iteration 21823 => Loss: 6.70910287262193438806\n",
      "Iteration 21824 => Loss: 6.70910140047834424593\n",
      "Iteration 21825 => Loss: 6.70909992853311987204\n",
      "Iteration 21826 => Loss: 6.70909845678624616738\n",
      "Iteration 21827 => Loss: 6.70909698523769293388\n",
      "Iteration 21828 => Loss: 6.70909551388742464439\n",
      "Iteration 21829 => Loss: 6.70909404273541820629\n",
      "Iteration 21830 => Loss: 6.70909257178166207325\n",
      "Iteration 21831 => Loss: 6.70909110102610739546\n",
      "Iteration 21832 => Loss: 6.70908963046873818570\n",
      "Iteration 21833 => Loss: 6.70908816010953046316\n",
      "Iteration 21834 => Loss: 6.70908668994845047706\n",
      "Iteration 21835 => Loss: 6.70908521998547691112\n",
      "Iteration 21836 => Loss: 6.70908375022057601456\n",
      "Iteration 21837 => Loss: 6.70908228065373002380\n",
      "Iteration 21838 => Loss: 6.70908081128490874079\n",
      "Iteration 21839 => Loss: 6.70907934211409084924\n",
      "Iteration 21840 => Loss: 6.70907787314123549294\n",
      "Iteration 21841 => Loss: 6.70907640436632224379\n",
      "Iteration 21842 => Loss: 6.70907493578933333822\n",
      "Iteration 21843 => Loss: 6.70907346741023502545\n",
      "Iteration 21844 => Loss: 6.70907199922899799560\n",
      "Iteration 21845 => Loss: 6.70907053124559826784\n",
      "Iteration 21846 => Loss: 6.70906906346000830865\n",
      "Iteration 21847 => Loss: 6.70906759587220413721\n",
      "Iteration 21848 => Loss: 6.70906612848215910816\n",
      "Iteration 21849 => Loss: 6.70906466128984391162\n",
      "Iteration 21850 => Loss: 6.70906319429523190223\n",
      "Iteration 21851 => Loss: 6.70906172749829909918\n",
      "Iteration 21852 => Loss: 6.70906026089901352805\n",
      "Iteration 21853 => Loss: 6.70905879449735387254\n",
      "Iteration 21854 => Loss: 6.70905732829329704003\n",
      "Iteration 21855 => Loss: 6.70905586228680395067\n",
      "Iteration 21856 => Loss: 6.70905439647786128177\n",
      "Iteration 21857 => Loss: 6.70905293086643528255\n",
      "Iteration 21858 => Loss: 6.70905146545250197221\n",
      "Iteration 21859 => Loss: 6.70905000023602493542\n",
      "Iteration 21860 => Loss: 6.70904853521699262586\n",
      "Iteration 21861 => Loss: 6.70904707039537040458\n",
      "Iteration 21862 => Loss: 6.70904560577113606712\n",
      "Iteration 21863 => Loss: 6.70904414134426207994\n",
      "Iteration 21864 => Loss: 6.70904267711471380409\n",
      "Iteration 21865 => Loss: 6.70904121308246637057\n",
      "Iteration 21866 => Loss: 6.70903974924750379216\n",
      "Iteration 21867 => Loss: 6.70903828560980208806\n",
      "Iteration 21868 => Loss: 6.70903682216931063209\n",
      "Iteration 21869 => Loss: 6.70903535892602054247\n",
      "Iteration 21870 => Loss: 6.70903389587990428566\n",
      "Iteration 21871 => Loss: 6.70903243303093699268\n",
      "Iteration 21872 => Loss: 6.70903097037908136002\n",
      "Iteration 21873 => Loss: 6.70902950792432761773\n",
      "Iteration 21874 => Loss: 6.70902804566662958052\n",
      "Iteration 21875 => Loss: 6.70902658360597925480\n",
      "Iteration 21876 => Loss: 6.70902512174234111342\n",
      "Iteration 21877 => Loss: 6.70902366007568673467\n",
      "Iteration 21878 => Loss: 6.70902219860599213774\n",
      "Iteration 21879 => Loss: 6.70902073733323511817\n",
      "Iteration 21880 => Loss: 6.70901927625737748428\n",
      "Iteration 21881 => Loss: 6.70901781537840236069\n",
      "Iteration 21882 => Loss: 6.70901635469628221387\n",
      "Iteration 21883 => Loss: 6.70901489421098506938\n",
      "Iteration 21884 => Loss: 6.70901343392249849273\n",
      "Iteration 21885 => Loss: 6.70901197383077718683\n",
      "Iteration 21886 => Loss: 6.70901051393580249993\n",
      "Iteration 21887 => Loss: 6.70900905423755311574\n",
      "Iteration 21888 => Loss: 6.70900759473599794802\n",
      "Iteration 21889 => Loss: 6.70900613543110502235\n",
      "Iteration 21890 => Loss: 6.70900467632286456876\n",
      "Iteration 21891 => Loss: 6.70900321741123040198\n",
      "Iteration 21892 => Loss: 6.70900175869618564661\n",
      "Iteration 21893 => Loss: 6.70900030017770188095\n",
      "Iteration 21894 => Loss: 6.70899884185575157147\n",
      "Iteration 21895 => Loss: 6.70899738373031961913\n",
      "Iteration 21896 => Loss: 6.70899592580135983866\n",
      "Iteration 21897 => Loss: 6.70899446806886068373\n",
      "Iteration 21898 => Loss: 6.70899301053279906171\n",
      "Iteration 21899 => Loss: 6.70899155319313234003\n",
      "Iteration 21900 => Loss: 6.70899009604983653787\n",
      "Iteration 21901 => Loss: 6.70898863910289833257\n",
      "Iteration 21902 => Loss: 6.70898718235228486151\n",
      "Iteration 21903 => Loss: 6.70898572579797125570\n",
      "Iteration 21904 => Loss: 6.70898426943992021165\n",
      "Iteration 21905 => Loss: 6.70898281327811396579\n",
      "Iteration 21906 => Loss: 6.70898135731252676095\n",
      "Iteration 21907 => Loss: 6.70897990154314172173\n",
      "Iteration 21908 => Loss: 6.70897844596990733379\n",
      "Iteration 21909 => Loss: 6.70897699059282093259\n",
      "Iteration 21910 => Loss: 6.70897553541183633286\n",
      "Iteration 21911 => Loss: 6.70897408042694642916\n",
      "Iteration 21912 => Loss: 6.70897262563811747071\n",
      "Iteration 21913 => Loss: 6.70897117104531748311\n",
      "Iteration 21914 => Loss: 6.70896971664852603823\n",
      "Iteration 21915 => Loss: 6.70896826244770938530\n",
      "Iteration 21916 => Loss: 6.70896680844284798440\n",
      "Iteration 21917 => Loss: 6.70896535463392051923\n",
      "Iteration 21918 => Loss: 6.70896390102088879814\n",
      "Iteration 21919 => Loss: 6.70896244760372972848\n",
      "Iteration 21920 => Loss: 6.70896099438241844126\n",
      "Iteration 21921 => Loss: 6.70895954135693006748\n",
      "Iteration 21922 => Loss: 6.70895808852723707361\n",
      "Iteration 21923 => Loss: 6.70895663589331725518\n",
      "Iteration 21924 => Loss: 6.70895518345513153236\n",
      "Iteration 21925 => Loss: 6.70895373121267279970\n",
      "Iteration 21926 => Loss: 6.70895227916589576012\n",
      "Iteration 21927 => Loss: 6.70895082731478176186\n",
      "Iteration 21928 => Loss: 6.70894937565931659407\n",
      "Iteration 21929 => Loss: 6.70894792419944607786\n",
      "Iteration 21930 => Loss: 6.70894647293517021325\n",
      "Iteration 21931 => Loss: 6.70894502186644725583\n",
      "Iteration 21932 => Loss: 6.70894357099326210658\n",
      "Iteration 21933 => Loss: 6.70894212031557390929\n",
      "Iteration 21934 => Loss: 6.70894066983337022947\n",
      "Iteration 21935 => Loss: 6.70893921954662086904\n",
      "Iteration 21936 => Loss: 6.70893776945528763633\n",
      "Iteration 21937 => Loss: 6.70893631955936164957\n",
      "Iteration 21938 => Loss: 6.70893486985881004614\n",
      "Iteration 21939 => Loss: 6.70893342035361150977\n",
      "Iteration 21940 => Loss: 6.70893197104372429607\n",
      "Iteration 21941 => Loss: 6.70893052192912975329\n",
      "Iteration 21942 => Loss: 6.70892907300981367058\n",
      "Iteration 21943 => Loss: 6.70892762428572542177\n",
      "Iteration 21944 => Loss: 6.70892617575686678322\n",
      "Iteration 21945 => Loss: 6.70892472742318979329\n",
      "Iteration 21946 => Loss: 6.70892327928467935294\n",
      "Iteration 21947 => Loss: 6.70892183134130437594\n",
      "Iteration 21948 => Loss: 6.70892038359303821693\n",
      "Iteration 21949 => Loss: 6.70891893603985867145\n",
      "Iteration 21950 => Loss: 6.70891748868173820597\n",
      "Iteration 21951 => Loss: 6.70891604151864573424\n",
      "Iteration 21952 => Loss: 6.70891459455056349270\n",
      "Iteration 21953 => Loss: 6.70891314777745328968\n",
      "Iteration 21954 => Loss: 6.70891170119930091431\n",
      "Iteration 21955 => Loss: 6.70891025481607439218\n",
      "Iteration 21956 => Loss: 6.70890880862774796611\n",
      "Iteration 21957 => Loss: 6.70890736263428966168\n",
      "Iteration 21958 => Loss: 6.70890591683569059711\n",
      "Iteration 21959 => Loss: 6.70890447123190103440\n",
      "Iteration 21960 => Loss: 6.70890302582291919720\n",
      "Iteration 21961 => Loss: 6.70890158060869357115\n",
      "Iteration 21962 => Loss: 6.70890013558922060355\n",
      "Iteration 21963 => Loss: 6.70889869076446032636\n",
      "Iteration 21964 => Loss: 6.70889724613439497602\n",
      "Iteration 21965 => Loss: 6.70889580169898280815\n",
      "Iteration 21966 => Loss: 6.70889435745821405277\n",
      "Iteration 21967 => Loss: 6.70889291341206206454\n",
      "Iteration 21968 => Loss: 6.70889146956048776360\n",
      "Iteration 21969 => Loss: 6.70889002590347516275\n",
      "Iteration 21970 => Loss: 6.70888858244099939299\n",
      "Iteration 21971 => Loss: 6.70888713917302581535\n",
      "Iteration 21972 => Loss: 6.70888569609952778450\n",
      "Iteration 21973 => Loss: 6.70888425322049819499\n",
      "Iteration 21974 => Loss: 6.70888281053588642067\n",
      "Iteration 21975 => Loss: 6.70888136804567647431\n",
      "Iteration 21976 => Loss: 6.70887992574984171057\n",
      "Iteration 21977 => Loss: 6.70887848364836258952\n",
      "Iteration 21978 => Loss: 6.70887704174120447220\n",
      "Iteration 21979 => Loss: 6.70887560002834071327\n",
      "Iteration 21980 => Loss: 6.70887415850974733189\n",
      "Iteration 21981 => Loss: 6.70887271718539679455\n",
      "Iteration 21982 => Loss: 6.70887127605527311403\n",
      "Iteration 21983 => Loss: 6.70886983511933809865\n",
      "Iteration 21984 => Loss: 6.70886839437756332671\n",
      "Iteration 21985 => Loss: 6.70886695382993725190\n",
      "Iteration 21986 => Loss: 6.70886551347642168253\n",
      "Iteration 21987 => Loss: 6.70886407331699974321\n",
      "Iteration 21988 => Loss: 6.70886263335163057775\n",
      "Iteration 21989 => Loss: 6.70886119358029908710\n",
      "Iteration 21990 => Loss: 6.70885975400297684956\n",
      "Iteration 21991 => Loss: 6.70885831461964166067\n",
      "Iteration 21992 => Loss: 6.70885687543025799329\n",
      "Iteration 21993 => Loss: 6.70885543643480719567\n",
      "Iteration 21994 => Loss: 6.70885399763326795153\n",
      "Iteration 21995 => Loss: 6.70885255902559141106\n",
      "Iteration 21996 => Loss: 6.70885112061177846243\n",
      "Iteration 21997 => Loss: 6.70884968239179269034\n",
      "Iteration 21998 => Loss: 6.70884824436560034400\n",
      "Iteration 21999 => Loss: 6.70884680653318810073\n",
      "Iteration 22000 => Loss: 6.70884536889452576247\n",
      "Iteration 22001 => Loss: 6.70884393144958224298\n",
      "Iteration 22002 => Loss: 6.70884249419833533779\n",
      "Iteration 22003 => Loss: 6.70884105714075396065\n",
      "Iteration 22004 => Loss: 6.70883962027681857165\n",
      "Iteration 22005 => Loss: 6.70883818360650430179\n",
      "Iteration 22006 => Loss: 6.70883674712977917665\n",
      "Iteration 22007 => Loss: 6.70883531084661299815\n",
      "Iteration 22008 => Loss: 6.70883387475699244362\n",
      "Iteration 22009 => Loss: 6.70883243886089264407\n",
      "Iteration 22010 => Loss: 6.70883100315826652604\n",
      "Iteration 22011 => Loss: 6.70882956764910698411\n",
      "Iteration 22012 => Loss: 6.70882813233338204384\n",
      "Iteration 22013 => Loss: 6.70882669721106772442\n",
      "Iteration 22014 => Loss: 6.70882526228213116326\n",
      "Iteration 22015 => Loss: 6.70882382754655726131\n",
      "Iteration 22016 => Loss: 6.70882239300431493234\n",
      "Iteration 22017 => Loss: 6.70882095865536953738\n",
      "Iteration 22018 => Loss: 6.70881952449971130648\n",
      "Iteration 22019 => Loss: 6.70881809053729760706\n",
      "Iteration 22020 => Loss: 6.70881665676811600463\n",
      "Iteration 22021 => Loss: 6.70881522319213630112\n",
      "Iteration 22022 => Loss: 6.70881378980932741030\n",
      "Iteration 22023 => Loss: 6.70881235661966979222\n",
      "Iteration 22024 => Loss: 6.70881092362313236066\n",
      "Iteration 22025 => Loss: 6.70880949081968935843\n",
      "Iteration 22026 => Loss: 6.70880805820932213379\n",
      "Iteration 22027 => Loss: 6.70880662579199515960\n",
      "Iteration 22028 => Loss: 6.70880519356768356687\n",
      "Iteration 22029 => Loss: 6.70880376153637048020\n",
      "Iteration 22030 => Loss: 6.70880232969802037246\n",
      "Iteration 22031 => Loss: 6.70880089805261370373\n",
      "Iteration 22032 => Loss: 6.70879946660011761139\n",
      "Iteration 22033 => Loss: 6.70879803534050989100\n",
      "Iteration 22034 => Loss: 6.70879660427377011445\n",
      "Iteration 22035 => Loss: 6.70879517339985742552\n",
      "Iteration 22036 => Loss: 6.70879374271876738334\n",
      "Iteration 22037 => Loss: 6.70879231223045380261\n",
      "Iteration 22038 => Loss: 6.70879088193489891978\n",
      "Iteration 22039 => Loss: 6.70878945183207608949\n",
      "Iteration 22040 => Loss: 6.70878802192196310727\n",
      "Iteration 22041 => Loss: 6.70878659220452711054\n",
      "Iteration 22042 => Loss: 6.70878516267974589482\n",
      "Iteration 22043 => Loss: 6.70878373334759103841\n",
      "Iteration 22044 => Loss: 6.70878230420804655409\n",
      "Iteration 22045 => Loss: 6.70878087526107513838\n",
      "Iteration 22046 => Loss: 6.70877944650665458681\n",
      "Iteration 22047 => Loss: 6.70877801794475292496\n",
      "Iteration 22048 => Loss: 6.70877658957535860651\n",
      "Iteration 22049 => Loss: 6.70877516139842633436\n",
      "Iteration 22050 => Loss: 6.70877373341395166761\n",
      "Iteration 22051 => Loss: 6.70877230562189286189\n",
      "Iteration 22052 => Loss: 6.70877087802223215363\n",
      "Iteration 22053 => Loss: 6.70876945061493934475\n",
      "Iteration 22054 => Loss: 6.70876802339998334901\n",
      "Iteration 22055 => Loss: 6.70876659637735706099\n",
      "Iteration 22056 => Loss: 6.70876516954701784812\n",
      "Iteration 22057 => Loss: 6.70876374290893906505\n",
      "Iteration 22058 => Loss: 6.70876231646311005363\n",
      "Iteration 22059 => Loss: 6.70876089020948462860\n",
      "Iteration 22060 => Loss: 6.70875946414804946727\n",
      "Iteration 22061 => Loss: 6.70875803827877970065\n",
      "Iteration 22062 => Loss: 6.70875661260164068977\n",
      "Iteration 22063 => Loss: 6.70875518711661200655\n",
      "Iteration 22064 => Loss: 6.70875376182366967015\n",
      "Iteration 22065 => Loss: 6.70875233672278792341\n",
      "Iteration 22066 => Loss: 6.70875091181393479189\n",
      "Iteration 22067 => Loss: 6.70874948709709340022\n",
      "Iteration 22068 => Loss: 6.70874806257222644490\n",
      "Iteration 22069 => Loss: 6.70874663823931705053\n",
      "Iteration 22070 => Loss: 6.70874521409833857177\n",
      "Iteration 22071 => Loss: 6.70874379014926081055\n",
      "Iteration 22072 => Loss: 6.70874236639205978605\n",
      "Iteration 22073 => Loss: 6.70874094282670974110\n",
      "Iteration 22074 => Loss: 6.70873951945318847123\n",
      "Iteration 22075 => Loss: 6.70873809627147110746\n",
      "Iteration 22076 => Loss: 6.70873667328151856992\n",
      "Iteration 22077 => Loss: 6.70873525048332020049\n",
      "Iteration 22078 => Loss: 6.70873382787683514294\n",
      "Iteration 22079 => Loss: 6.70873240546205895640\n",
      "Iteration 22080 => Loss: 6.70873098323894101469\n",
      "Iteration 22081 => Loss: 6.70872956120747243602\n",
      "Iteration 22082 => Loss: 6.70872813936762746323\n",
      "Iteration 22083 => Loss: 6.70872671771937234553\n",
      "Iteration 22084 => Loss: 6.70872529626267866121\n",
      "Iteration 22085 => Loss: 6.70872387499753131124\n",
      "Iteration 22086 => Loss: 6.70872245392389920937\n",
      "Iteration 22087 => Loss: 6.70872103304175926297\n",
      "Iteration 22088 => Loss: 6.70871961235108038579\n",
      "Iteration 22089 => Loss: 6.70871819185184481427\n",
      "Iteration 22090 => Loss: 6.70871677154401258036\n",
      "Iteration 22091 => Loss: 6.70871535142757213777\n",
      "Iteration 22092 => Loss: 6.70871393150249506476\n",
      "Iteration 22093 => Loss: 6.70871251176875116329\n",
      "Iteration 22094 => Loss: 6.70871109222630757074\n",
      "Iteration 22095 => Loss: 6.70870967287516251076\n",
      "Iteration 22096 => Loss: 6.70870825371527068626\n",
      "Iteration 22097 => Loss: 6.70870683474660722823\n",
      "Iteration 22098 => Loss: 6.70870541596914726767\n",
      "Iteration 22099 => Loss: 6.70870399738286860014\n",
      "Iteration 22100 => Loss: 6.70870257898775079752\n",
      "Iteration 22101 => Loss: 6.70870116078375744451\n",
      "Iteration 22102 => Loss: 6.70869974277086900116\n",
      "Iteration 22103 => Loss: 6.70869832494905171671\n",
      "Iteration 22104 => Loss: 6.70869690731828871577\n",
      "Iteration 22105 => Loss: 6.70869548987855868205\n",
      "Iteration 22106 => Loss: 6.70869407262982253570\n",
      "Iteration 22107 => Loss: 6.70869265557206517769\n",
      "Iteration 22108 => Loss: 6.70869123870525196907\n",
      "Iteration 22109 => Loss: 6.70868982202935981718\n",
      "Iteration 22110 => Loss: 6.70868840554437628754\n",
      "Iteration 22111 => Loss: 6.70868698925024986579\n",
      "Iteration 22112 => Loss: 6.70868557314697788740\n",
      "Iteration 22113 => Loss: 6.70868415723452482524\n",
      "Iteration 22114 => Loss: 6.70868274151286314577\n",
      "Iteration 22115 => Loss: 6.70868132598196886818\n",
      "Iteration 22116 => Loss: 6.70867991064182334071\n",
      "Iteration 22117 => Loss: 6.70867849549239192442\n",
      "Iteration 22118 => Loss: 6.70867708053365152665\n",
      "Iteration 22119 => Loss: 6.70867566576557639024\n",
      "Iteration 22120 => Loss: 6.70867425118814431073\n",
      "Iteration 22121 => Loss: 6.70867283680132509005\n",
      "Iteration 22122 => Loss: 6.70867142260509741192\n",
      "Iteration 22123 => Loss: 6.70867000859942574920\n",
      "Iteration 22124 => Loss: 6.70866859478429589103\n",
      "Iteration 22125 => Loss: 6.70866718115967675118\n",
      "Iteration 22126 => Loss: 6.70866576772554612518\n",
      "Iteration 22127 => Loss: 6.70866435448187292678\n",
      "Iteration 22128 => Loss: 6.70866294142863406336\n",
      "Iteration 22129 => Loss: 6.70866152856580644226\n",
      "Iteration 22130 => Loss: 6.70866011589336430632\n",
      "Iteration 22131 => Loss: 6.70865870341127212839\n",
      "Iteration 22132 => Loss: 6.70865729111951836217\n",
      "Iteration 22133 => Loss: 6.70865587901806925686\n",
      "Iteration 22134 => Loss: 6.70865446710690260801\n",
      "Iteration 22135 => Loss: 6.70865305538598910573\n",
      "Iteration 22136 => Loss: 6.70865164385530654556\n",
      "Iteration 22137 => Loss: 6.70865023251482206490\n",
      "Iteration 22138 => Loss: 6.70864882136452322925\n",
      "Iteration 22139 => Loss: 6.70864741040437628783\n",
      "Iteration 22140 => Loss: 6.70864599963435459529\n",
      "Iteration 22141 => Loss: 6.70864458905443239445\n",
      "Iteration 22142 => Loss: 6.70864317866458481632\n",
      "Iteration 22143 => Loss: 6.70864176846479676186\n",
      "Iteration 22144 => Loss: 6.70864035845502648669\n",
      "Iteration 22145 => Loss: 6.70863894863525001000\n",
      "Iteration 22146 => Loss: 6.70863753900545578546\n",
      "Iteration 22147 => Loss: 6.70863612956560473322\n",
      "Iteration 22148 => Loss: 6.70863472031567642517\n",
      "Iteration 22149 => Loss: 6.70863331125564776869\n",
      "Iteration 22150 => Loss: 6.70863190238548323663\n",
      "Iteration 22151 => Loss: 6.70863049370517483538\n",
      "Iteration 22152 => Loss: 6.70862908521467637968\n",
      "Iteration 22153 => Loss: 6.70862767691397454684\n",
      "Iteration 22154 => Loss: 6.70862626880304535604\n",
      "Iteration 22155 => Loss: 6.70862486088185061561\n",
      "Iteration 22156 => Loss: 6.70862345315037966742\n",
      "Iteration 22157 => Loss: 6.70862204560860231339\n",
      "Iteration 22158 => Loss: 6.70862063825648569093\n",
      "Iteration 22159 => Loss: 6.70861923109401647736\n",
      "Iteration 22160 => Loss: 6.70861782412115381646\n",
      "Iteration 22161 => Loss: 6.70861641733788438557\n",
      "Iteration 22162 => Loss: 6.70861501074418153934\n",
      "Iteration 22163 => Loss: 6.70861360434001685604\n",
      "Iteration 22164 => Loss: 6.70861219812536724305\n",
      "Iteration 22165 => Loss: 6.70861079210020072594\n",
      "Iteration 22166 => Loss: 6.70860938626450131750\n",
      "Iteration 22167 => Loss: 6.70860798061823526695\n",
      "Iteration 22168 => Loss: 6.70860657516138392253\n",
      "Iteration 22169 => Loss: 6.70860516989391175713\n",
      "Iteration 22170 => Loss: 6.70860376481580189534\n",
      "Iteration 22171 => Loss: 6.70860235992703035635\n",
      "Iteration 22172 => Loss: 6.70860095522756338937\n",
      "Iteration 22173 => Loss: 6.70859955071737878995\n",
      "Iteration 22174 => Loss: 6.70859814639645080092\n",
      "Iteration 22175 => Loss: 6.70859674226475544145\n",
      "Iteration 22176 => Loss: 6.70859533832227050709\n",
      "Iteration 22177 => Loss: 6.70859393456896579977\n",
      "Iteration 22178 => Loss: 6.70859253100482000320\n",
      "Iteration 22179 => Loss: 6.70859112762980114297\n",
      "Iteration 22180 => Loss: 6.70858972444388879097\n",
      "Iteration 22181 => Loss: 6.70858832144705274914\n",
      "Iteration 22182 => Loss: 6.70858691863927258936\n",
      "Iteration 22183 => Loss: 6.70858551602052255447\n",
      "Iteration 22184 => Loss: 6.70858411359077067004\n",
      "Iteration 22185 => Loss: 6.70858271135000094887\n",
      "Iteration 22186 => Loss: 6.70858130929818319288\n",
      "Iteration 22187 => Loss: 6.70857990743528986854\n",
      "Iteration 22188 => Loss: 6.70857850576129699505\n",
      "Iteration 22189 => Loss: 6.70857710427618325610\n",
      "Iteration 22190 => Loss: 6.70857570297991578911\n",
      "Iteration 22191 => Loss: 6.70857430187247505415\n",
      "Iteration 22192 => Loss: 6.70857290095383440587\n",
      "Iteration 22193 => Loss: 6.70857150022396808708\n",
      "Iteration 22194 => Loss: 6.70857009968284589974\n",
      "Iteration 22195 => Loss: 6.70856869933045363297\n",
      "Iteration 22196 => Loss: 6.70856729916675309511\n",
      "Iteration 22197 => Loss: 6.70856589919172385805\n",
      "Iteration 22198 => Loss: 6.70856449940534727006\n",
      "Iteration 22199 => Loss: 6.70856309980758869216\n",
      "Iteration 22200 => Loss: 6.70856170039842503172\n",
      "Iteration 22201 => Loss: 6.70856030117783763700\n",
      "Iteration 22202 => Loss: 6.70855890214579453357\n",
      "Iteration 22203 => Loss: 6.70855750330225930611\n",
      "Iteration 22204 => Loss: 6.70855610464722751374\n",
      "Iteration 22205 => Loss: 6.70855470618066451749\n",
      "Iteration 22206 => Loss: 6.70855330790254367201\n",
      "Iteration 22207 => Loss: 6.70855190981284099649\n",
      "Iteration 22208 => Loss: 6.70855051191153428647\n",
      "Iteration 22209 => Loss: 6.70854911419859156751\n",
      "Iteration 22210 => Loss: 6.70854771667398974699\n",
      "Iteration 22211 => Loss: 6.70854631933770306773\n",
      "Iteration 22212 => Loss: 6.70854492218971731887\n",
      "Iteration 22213 => Loss: 6.70854352522998720332\n",
      "Iteration 22214 => Loss: 6.70854212845850117475\n",
      "Iteration 22215 => Loss: 6.70854073187522725874\n",
      "Iteration 22216 => Loss: 6.70853933548014857990\n",
      "Iteration 22217 => Loss: 6.70853793927323138746\n",
      "Iteration 22218 => Loss: 6.70853654325445081241\n",
      "Iteration 22219 => Loss: 6.70853514742378553848\n",
      "Iteration 22220 => Loss: 6.70853375178121336120\n",
      "Iteration 22221 => Loss: 6.70853235632669431254\n",
      "Iteration 22222 => Loss: 6.70853096106022306344\n",
      "Iteration 22223 => Loss: 6.70852956598176053404\n",
      "Iteration 22224 => Loss: 6.70852817109128185535\n",
      "Iteration 22225 => Loss: 6.70852677638876748745\n",
      "Iteration 22226 => Loss: 6.70852538187418989679\n",
      "Iteration 22227 => Loss: 6.70852398754751622079\n",
      "Iteration 22228 => Loss: 6.70852259340874024218\n",
      "Iteration 22229 => Loss: 6.70852119945781222299\n",
      "Iteration 22230 => Loss: 6.70851980569473038685\n",
      "Iteration 22231 => Loss: 6.70851841211945032484\n",
      "Iteration 22232 => Loss: 6.70851701873195338521\n",
      "Iteration 22233 => Loss: 6.70851562553222535712\n",
      "Iteration 22234 => Loss: 6.70851423252022627253\n",
      "Iteration 22235 => Loss: 6.70851283969593392698\n",
      "Iteration 22236 => Loss: 6.70851144705932345147\n",
      "Iteration 22237 => Loss: 6.70851005461037264155\n",
      "Iteration 22238 => Loss: 6.70850866234905840457\n",
      "Iteration 22239 => Loss: 6.70850727027534876612\n",
      "Iteration 22240 => Loss: 6.70850587838922240991\n",
      "Iteration 22241 => Loss: 6.70850448669064647333\n",
      "Iteration 22242 => Loss: 6.70850309517960585737\n",
      "Iteration 22243 => Loss: 6.70850170385606947576\n",
      "Iteration 22244 => Loss: 6.70850031272002400584\n",
      "Iteration 22245 => Loss: 6.70849892177142326233\n",
      "Iteration 22246 => Loss: 6.70849753101025303437\n",
      "Iteration 22247 => Loss: 6.70849614043649200568\n",
      "Iteration 22248 => Loss: 6.70849475005011353090\n",
      "Iteration 22249 => Loss: 6.70849335985108474745\n",
      "Iteration 22250 => Loss: 6.70849196983938522720\n",
      "Iteration 22251 => Loss: 6.70849058001499098935\n",
      "Iteration 22252 => Loss: 6.70848919037787805308\n",
      "Iteration 22253 => Loss: 6.70848780092802066122\n",
      "Iteration 22254 => Loss: 6.70848641166538595115\n",
      "Iteration 22255 => Loss: 6.70848502258995704750\n",
      "Iteration 22256 => Loss: 6.70848363370170286402\n",
      "Iteration 22257 => Loss: 6.70848224500060474895\n",
      "Iteration 22258 => Loss: 6.70848085648663250424\n",
      "Iteration 22259 => Loss: 6.70847946815976214907\n",
      "Iteration 22260 => Loss: 6.70847808001997591987\n",
      "Iteration 22261 => Loss: 6.70847669206723651314\n",
      "Iteration 22262 => Loss: 6.70847530430152172443\n",
      "Iteration 22263 => Loss: 6.70847391672281201380\n",
      "Iteration 22264 => Loss: 6.70847252933107895956\n",
      "Iteration 22265 => Loss: 6.70847114212629147545\n",
      "Iteration 22266 => Loss: 6.70846975510843623880\n",
      "Iteration 22267 => Loss: 6.70846836827748393972\n",
      "Iteration 22268 => Loss: 6.70846698163339905108\n",
      "Iteration 22269 => Loss: 6.70846559517616913837\n",
      "Iteration 22270 => Loss: 6.70846420890576400353\n",
      "Iteration 22271 => Loss: 6.70846282282215966575\n",
      "Iteration 22272 => Loss: 6.70846143692533303238\n",
      "Iteration 22273 => Loss: 6.70846005121524235904\n",
      "Iteration 22274 => Loss: 6.70845866569189297479\n",
      "Iteration 22275 => Loss: 6.70845728035523247712\n",
      "Iteration 22276 => Loss: 6.70845589520525198424\n",
      "Iteration 22277 => Loss: 6.70845451024191596900\n",
      "Iteration 22278 => Loss: 6.70845312546520577968\n",
      "Iteration 22279 => Loss: 6.70845174087509654726\n",
      "Iteration 22280 => Loss: 6.70845035647155718550\n",
      "Iteration 22281 => Loss: 6.70844897225456815448\n",
      "Iteration 22282 => Loss: 6.70844758822410369703\n",
      "Iteration 22283 => Loss: 6.70844620438013894415\n",
      "Iteration 22284 => Loss: 6.70844482072264192141\n",
      "Iteration 22285 => Loss: 6.70844343725159752978\n",
      "Iteration 22286 => Loss: 6.70844205396697201849\n",
      "Iteration 22287 => Loss: 6.70844067086874495942\n",
      "Iteration 22288 => Loss: 6.70843928795689503630\n",
      "Iteration 22289 => Loss: 6.70843790523139027471\n",
      "Iteration 22290 => Loss: 6.70843652269220758200\n",
      "Iteration 22291 => Loss: 6.70843514033931409557\n",
      "Iteration 22292 => Loss: 6.70843375817270182182\n",
      "Iteration 22293 => Loss: 6.70843237619233700997\n",
      "Iteration 22294 => Loss: 6.70843099439819035013\n",
      "Iteration 22295 => Loss: 6.70842961279024230237\n",
      "Iteration 22296 => Loss: 6.70842823136846710952\n",
      "Iteration 22297 => Loss: 6.70842685013283634987\n",
      "Iteration 22298 => Loss: 6.70842546908333137168\n",
      "Iteration 22299 => Loss: 6.70842408821991753598\n",
      "Iteration 22300 => Loss: 6.70842270754257796739\n",
      "Iteration 22301 => Loss: 6.70842132705128335601\n",
      "Iteration 22302 => Loss: 6.70841994674600616833\n",
      "Iteration 22303 => Loss: 6.70841856662673929890\n",
      "Iteration 22304 => Loss: 6.70841718669343034520\n",
      "Iteration 22305 => Loss: 6.70841580694607220181\n",
      "Iteration 22306 => Loss: 6.70841442738463911155\n",
      "Iteration 22307 => Loss: 6.70841304800909288275\n",
      "Iteration 22308 => Loss: 6.70841166881941664002\n",
      "Iteration 22309 => Loss: 6.70841028981560150157\n",
      "Iteration 22310 => Loss: 6.70840891099759506488\n",
      "Iteration 22311 => Loss: 6.70840753236538667181\n",
      "Iteration 22312 => Loss: 6.70840615391894701247\n",
      "Iteration 22313 => Loss: 6.70840477565825654693\n",
      "Iteration 22314 => Loss: 6.70840339758328685349\n",
      "Iteration 22315 => Loss: 6.70840201969401217497\n",
      "Iteration 22316 => Loss: 6.70840064199040586601\n",
      "Iteration 22317 => Loss: 6.70839926447244661034\n",
      "Iteration 22318 => Loss: 6.70839788714011042714\n",
      "Iteration 22319 => Loss: 6.70839650999337155923\n",
      "Iteration 22320 => Loss: 6.70839513303220069673\n",
      "Iteration 22321 => Loss: 6.70839375625656497704\n",
      "Iteration 22322 => Loss: 6.70839237966646262379\n",
      "Iteration 22323 => Loss: 6.70839100326185011625\n",
      "Iteration 22324 => Loss: 6.70838962704271768445\n",
      "Iteration 22325 => Loss: 6.70838825100901825493\n",
      "Iteration 22326 => Loss: 6.70838687516074916317\n",
      "Iteration 22327 => Loss: 6.70838549949786955295\n",
      "Iteration 22328 => Loss: 6.70838412402036787796\n",
      "Iteration 22329 => Loss: 6.70838274872820150563\n",
      "Iteration 22330 => Loss: 6.70838137362135977781\n",
      "Iteration 22331 => Loss: 6.70837999869981693735\n",
      "Iteration 22332 => Loss: 6.70837862396354633887\n",
      "Iteration 22333 => Loss: 6.70837724941251867250\n",
      "Iteration 22334 => Loss: 6.70837587504670640470\n",
      "Iteration 22335 => Loss: 6.70837450086609976552\n",
      "Iteration 22336 => Loss: 6.70837312687066233963\n",
      "Iteration 22337 => Loss: 6.70837175306036392897\n",
      "Iteration 22338 => Loss: 6.70837037943519032268\n",
      "Iteration 22339 => Loss: 6.70836900599511309906\n",
      "Iteration 22340 => Loss: 6.70836763274010650093\n",
      "Iteration 22341 => Loss: 6.70836625967014921201\n",
      "Iteration 22342 => Loss: 6.70836488678520925788\n",
      "Iteration 22343 => Loss: 6.70836351408526887496\n",
      "Iteration 22344 => Loss: 6.70836214157030230609\n",
      "Iteration 22345 => Loss: 6.70836076924027313595\n",
      "Iteration 22346 => Loss: 6.70835939709516981821\n",
      "Iteration 22347 => Loss: 6.70835802513496659572\n",
      "Iteration 22348 => Loss: 6.70835665335963149403\n",
      "Iteration 22349 => Loss: 6.70835528176914763776\n",
      "Iteration 22350 => Loss: 6.70835391036347594707\n",
      "Iteration 22351 => Loss: 6.70835253914261020469\n",
      "Iteration 22352 => Loss: 6.70835116810651577168\n",
      "Iteration 22353 => Loss: 6.70834979725516511451\n",
      "Iteration 22354 => Loss: 6.70834842658854135777\n",
      "Iteration 22355 => Loss: 6.70834705610660630981\n",
      "Iteration 22356 => Loss: 6.70834568580935197701\n",
      "Iteration 22357 => Loss: 6.70834431569674105589\n",
      "Iteration 22358 => Loss: 6.70834294576875489469\n",
      "Iteration 22359 => Loss: 6.70834157602537040077\n",
      "Iteration 22360 => Loss: 6.70834020646655293518\n",
      "Iteration 22361 => Loss: 6.70833883709229006342\n",
      "Iteration 22362 => Loss: 6.70833746790254448200\n",
      "Iteration 22363 => Loss: 6.70833609889729132192\n",
      "Iteration 22364 => Loss: 6.70833473007652525411\n",
      "Iteration 22365 => Loss: 6.70833336144020186964\n",
      "Iteration 22366 => Loss: 6.70833199298830340496\n",
      "Iteration 22367 => Loss: 6.70833062472080321470\n",
      "Iteration 22368 => Loss: 6.70832925663767376534\n",
      "Iteration 22369 => Loss: 6.70832788873890173420\n",
      "Iteration 22370 => Loss: 6.70832652102444715325\n",
      "Iteration 22371 => Loss: 6.70832515349428870621\n",
      "Iteration 22372 => Loss: 6.70832378614841662312\n",
      "Iteration 22373 => Loss: 6.70832241898678383052\n",
      "Iteration 22374 => Loss: 6.70832105200938499934\n",
      "Iteration 22375 => Loss: 6.70831968521618193790\n",
      "Iteration 22376 => Loss: 6.70831831860715688265\n",
      "Iteration 22377 => Loss: 6.70831695218228052369\n",
      "Iteration 22378 => Loss: 6.70831558594152888020\n",
      "Iteration 22379 => Loss: 6.70831421988487974772\n",
      "Iteration 22380 => Loss: 6.70831285401230559273\n",
      "Iteration 22381 => Loss: 6.70831148832377799351\n",
      "Iteration 22382 => Loss: 6.70831012281928540375\n",
      "Iteration 22383 => Loss: 6.70830875749879496084\n",
      "Iteration 22384 => Loss: 6.70830739236227469036\n",
      "Iteration 22385 => Loss: 6.70830602740971393416\n",
      "Iteration 22386 => Loss: 6.70830466264107450058\n",
      "Iteration 22387 => Loss: 6.70830329805634661966\n",
      "Iteration 22388 => Loss: 6.70830193365549209972\n",
      "Iteration 22389 => Loss: 6.70830056943848695994\n",
      "Iteration 22390 => Loss: 6.70829920540530988404\n",
      "Iteration 22391 => Loss: 6.70829784155593777939\n",
      "Iteration 22392 => Loss: 6.70829647789034133609\n",
      "Iteration 22393 => Loss: 6.70829511440851256054\n",
      "Iteration 22394 => Loss: 6.70829375111039816204\n",
      "Iteration 22395 => Loss: 6.70829238799599814058\n",
      "Iteration 22396 => Loss: 6.70829102506527252814\n",
      "Iteration 22397 => Loss: 6.70828966231820356114\n",
      "Iteration 22398 => Loss: 6.70828829975476459424\n",
      "Iteration 22399 => Loss: 6.70828693737492542937\n",
      "Iteration 22400 => Loss: 6.70828557517867718474\n",
      "Iteration 22401 => Loss: 6.70828421316597989232\n",
      "Iteration 22402 => Loss: 6.70828285133681490038\n",
      "Iteration 22403 => Loss: 6.70828148969115467537\n",
      "Iteration 22404 => Loss: 6.70828012822897523648\n",
      "Iteration 22405 => Loss: 6.70827876695026148468\n",
      "Iteration 22406 => Loss: 6.70827740585497167558\n",
      "Iteration 22407 => Loss: 6.70827604494308715744\n",
      "Iteration 22408 => Loss: 6.70827468421459460757\n",
      "Iteration 22409 => Loss: 6.70827332366945583431\n",
      "Iteration 22410 => Loss: 6.70827196330764863319\n",
      "Iteration 22411 => Loss: 6.70827060312915257612\n",
      "Iteration 22412 => Loss: 6.70826924313394101773\n",
      "Iteration 22413 => Loss: 6.70826788332199352993\n",
      "Iteration 22414 => Loss: 6.70826652369327103287\n",
      "Iteration 22415 => Loss: 6.70826516424776642111\n",
      "Iteration 22416 => Loss: 6.70826380498543439757\n",
      "Iteration 22417 => Loss: 6.70826244590628295583\n",
      "Iteration 22418 => Loss: 6.70826108701025525249\n",
      "Iteration 22419 => Loss: 6.70825972829733796488\n",
      "Iteration 22420 => Loss: 6.70825836976751155305\n",
      "Iteration 22421 => Loss: 6.70825701142074581895\n",
      "Iteration 22422 => Loss: 6.70825565325702033448\n",
      "Iteration 22423 => Loss: 6.70825429527630134885\n",
      "Iteration 22424 => Loss: 6.70825293747857820392\n",
      "Iteration 22425 => Loss: 6.70825157986380737896\n",
      "Iteration 22426 => Loss: 6.70825022243198088034\n",
      "Iteration 22427 => Loss: 6.70824886518307206273\n",
      "Iteration 22428 => Loss: 6.70824750811704983988\n",
      "Iteration 22429 => Loss: 6.70824615123389822458\n",
      "Iteration 22430 => Loss: 6.70824479453357813696\n",
      "Iteration 22431 => Loss: 6.70824343801607536619\n",
      "Iteration 22432 => Loss: 6.70824208168136237873\n",
      "Iteration 22433 => Loss: 6.70824072552941785830\n",
      "Iteration 22434 => Loss: 6.70823936956021515954\n",
      "Iteration 22435 => Loss: 6.70823801377373118981\n",
      "Iteration 22436 => Loss: 6.70823665816993486288\n",
      "Iteration 22437 => Loss: 6.70823530274880752700\n",
      "Iteration 22438 => Loss: 6.70823394751032342498\n",
      "Iteration 22439 => Loss: 6.70823259245446124055\n",
      "Iteration 22440 => Loss: 6.70823123758119255200\n",
      "Iteration 22441 => Loss: 6.70822988289048716126\n",
      "Iteration 22442 => Loss: 6.70822852838233529837\n",
      "Iteration 22443 => Loss: 6.70822717405669965984\n",
      "Iteration 22444 => Loss: 6.70822581991355626485\n",
      "Iteration 22445 => Loss: 6.70822446595288912619\n",
      "Iteration 22446 => Loss: 6.70822311217465916400\n",
      "Iteration 22447 => Loss: 6.70822175857885927286\n",
      "Iteration 22448 => Loss: 6.70822040516545303745\n",
      "Iteration 22449 => Loss: 6.70821905193442180604\n",
      "Iteration 22450 => Loss: 6.70821769888573715690\n",
      "Iteration 22451 => Loss: 6.70821634601937422104\n",
      "Iteration 22452 => Loss: 6.70821499333531789944\n",
      "Iteration 22453 => Loss: 6.70821364083352555951\n",
      "Iteration 22454 => Loss: 6.70821228851398831949\n",
      "Iteration 22455 => Loss: 6.70821093637667598131\n",
      "Iteration 22456 => Loss: 6.70820958442156101142\n",
      "Iteration 22457 => Loss: 6.70820823264862475810\n",
      "Iteration 22458 => Loss: 6.70820688105783613508\n",
      "Iteration 22459 => Loss: 6.70820552964918181971\n",
      "Iteration 22460 => Loss: 6.70820417842262894936\n",
      "Iteration 22461 => Loss: 6.70820282737814999052\n",
      "Iteration 22462 => Loss: 6.70820147651572273872\n",
      "Iteration 22463 => Loss: 6.70820012583533209494\n",
      "Iteration 22464 => Loss: 6.70819877533693809113\n",
      "Iteration 22465 => Loss: 6.70819742502053006916\n",
      "Iteration 22466 => Loss: 6.70819607488607161372\n",
      "Iteration 22467 => Loss: 6.70819472493354584941\n",
      "Iteration 22468 => Loss: 6.70819337516292346635\n",
      "Iteration 22469 => Loss: 6.70819202557419114186\n",
      "Iteration 22470 => Loss: 6.70819067616730890791\n",
      "Iteration 22471 => Loss: 6.70818932694225988911\n",
      "Iteration 22472 => Loss: 6.70818797789902010464\n",
      "Iteration 22473 => Loss: 6.70818662903757179095\n",
      "Iteration 22474 => Loss: 6.70818528035787231545\n",
      "Iteration 22475 => Loss: 6.70818393185990657912\n",
      "Iteration 22476 => Loss: 6.70818258354365504204\n",
      "Iteration 22477 => Loss: 6.70818123540908839431\n",
      "Iteration 22478 => Loss: 6.70817988745618620783\n",
      "Iteration 22479 => Loss: 6.70817853968491473182\n",
      "Iteration 22480 => Loss: 6.70817719209526153179\n",
      "Iteration 22481 => Loss: 6.70817584468719108060\n",
      "Iteration 22482 => Loss: 6.70817449746068650285\n",
      "Iteration 22483 => Loss: 6.70817315041572292955\n",
      "Iteration 22484 => Loss: 6.70817180355226749811\n",
      "Iteration 22485 => Loss: 6.70817045687030777401\n",
      "Iteration 22486 => Loss: 6.70816911036981711192\n",
      "Iteration 22487 => Loss: 6.70816776405076087286\n",
      "Iteration 22488 => Loss: 6.70816641791312306964\n",
      "Iteration 22489 => Loss: 6.70816507195687883325\n",
      "Iteration 22490 => Loss: 6.70816372618199530109\n",
      "Iteration 22491 => Loss: 6.70816238058846003867\n",
      "Iteration 22492 => Loss: 6.70816103517624551245\n",
      "Iteration 22493 => Loss: 6.70815968994532330072\n",
      "Iteration 22494 => Loss: 6.70815834489567386356\n",
      "Iteration 22495 => Loss: 6.70815700002726877926\n",
      "Iteration 22496 => Loss: 6.70815565534008051429\n",
      "Iteration 22497 => Loss: 6.70815431083408952873\n",
      "Iteration 22498 => Loss: 6.70815296650927450628\n",
      "Iteration 22499 => Loss: 6.70815162236560613707\n",
      "Iteration 22500 => Loss: 6.70815027840306399298\n",
      "Iteration 22501 => Loss: 6.70814893462161965232\n",
      "Iteration 22502 => Loss: 6.70814759102124558154\n",
      "Iteration 22503 => Loss: 6.70814624760192668163\n",
      "Iteration 22504 => Loss: 6.70814490436363008996\n",
      "Iteration 22505 => Loss: 6.70814356130633804298\n",
      "Iteration 22506 => Loss: 6.70814221843001767809\n",
      "Iteration 22507 => Loss: 6.70814087573465211989\n",
      "Iteration 22508 => Loss: 6.70813953322022449299\n",
      "Iteration 22509 => Loss: 6.70813819088668861212\n",
      "Iteration 22510 => Loss: 6.70813684873403648368\n",
      "Iteration 22511 => Loss: 6.70813550676223879776\n",
      "Iteration 22512 => Loss: 6.70813416497127423810\n",
      "Iteration 22513 => Loss: 6.70813282336111438298\n",
      "Iteration 22514 => Loss: 6.70813148193173525158\n",
      "Iteration 22515 => Loss: 6.70813014068312263305\n",
      "Iteration 22516 => Loss: 6.70812879961523389483\n",
      "Iteration 22517 => Loss: 6.70812745872805926695\n",
      "Iteration 22518 => Loss: 6.70812611802156144591\n",
      "Iteration 22519 => Loss: 6.70812477749572710906\n",
      "Iteration 22520 => Loss: 6.70812343715053138737\n",
      "Iteration 22521 => Loss: 6.70812209698594319462\n",
      "Iteration 22522 => Loss: 6.70812075700195098449\n",
      "Iteration 22523 => Loss: 6.70811941719851567711\n",
      "Iteration 22524 => Loss: 6.70811807757561773258\n",
      "Iteration 22525 => Loss: 6.70811673813323761095\n",
      "Iteration 22526 => Loss: 6.70811539887134156146\n",
      "Iteration 22527 => Loss: 6.70811405978991537324\n",
      "Iteration 22528 => Loss: 6.70811272088893328913\n",
      "Iteration 22529 => Loss: 6.70811138216836511106\n",
      "Iteration 22530 => Loss: 6.70811004362819218727\n",
      "Iteration 22531 => Loss: 6.70810870526838698424\n",
      "Iteration 22532 => Loss: 6.70810736708892285662\n",
      "Iteration 22533 => Loss: 6.70810602908977759995\n",
      "Iteration 22534 => Loss: 6.70810469127093078612\n",
      "Iteration 22535 => Loss: 6.70810335363235843431\n",
      "Iteration 22536 => Loss: 6.70810201617402501739\n",
      "Iteration 22537 => Loss: 6.70810067889591810086\n",
      "Iteration 22538 => Loss: 6.70809934179801370391\n",
      "Iteration 22539 => Loss: 6.70809800488028251664\n",
      "Iteration 22540 => Loss: 6.70809666814269522916\n",
      "Iteration 22541 => Loss: 6.70809533158522874885\n",
      "Iteration 22542 => Loss: 6.70809399520787508209\n",
      "Iteration 22543 => Loss: 6.70809265901059781356\n",
      "Iteration 22544 => Loss: 6.70809132299336408067\n",
      "Iteration 22545 => Loss: 6.70808998715616855435\n",
      "Iteration 22546 => Loss: 6.70808865149897037838\n",
      "Iteration 22547 => Loss: 6.70808731602175534192\n",
      "Iteration 22548 => Loss: 6.70808598072449857597\n",
      "Iteration 22549 => Loss: 6.70808464560716632974\n",
      "Iteration 22550 => Loss: 6.70808331066974616874\n",
      "Iteration 22551 => Loss: 6.70808197591220611855\n",
      "Iteration 22552 => Loss: 6.70808064133452486288\n",
      "Iteration 22553 => Loss: 6.70807930693668197364\n",
      "Iteration 22554 => Loss: 6.70807797271864636457\n",
      "Iteration 22555 => Loss: 6.70807663868039405486\n",
      "Iteration 22556 => Loss: 6.70807530482190905730\n",
      "Iteration 22557 => Loss: 6.70807397114316028564\n",
      "Iteration 22558 => Loss: 6.70807263764412287088\n",
      "Iteration 22559 => Loss: 6.70807130432477549675\n",
      "Iteration 22560 => Loss: 6.70806997118509240607\n",
      "Iteration 22561 => Loss: 6.70806863822505583528\n",
      "Iteration 22562 => Loss: 6.70806730544462581634\n",
      "Iteration 22563 => Loss: 6.70806597284379435564\n",
      "Iteration 22564 => Loss: 6.70806464042252414970\n",
      "Iteration 22565 => Loss: 6.70806330818080187584\n",
      "Iteration 22566 => Loss: 6.70806197611860177688\n",
      "Iteration 22567 => Loss: 6.70806064423589631929\n",
      "Iteration 22568 => Loss: 6.70805931253266773950\n",
      "Iteration 22569 => Loss: 6.70805798100887784585\n",
      "Iteration 22570 => Loss: 6.70805664966451509201\n",
      "Iteration 22571 => Loss: 6.70805531849954306267\n",
      "Iteration 22572 => Loss: 6.70805398751395465240\n",
      "Iteration 22573 => Loss: 6.70805265670771078135\n",
      "Iteration 22574 => Loss: 6.70805132608079990320\n",
      "Iteration 22575 => Loss: 6.70804999563318826716\n",
      "Iteration 22576 => Loss: 6.70804866536484833972\n",
      "Iteration 22577 => Loss: 6.70804733527577123908\n",
      "Iteration 22578 => Loss: 6.70804600536591699722\n",
      "Iteration 22579 => Loss: 6.70804467563527317964\n",
      "Iteration 22580 => Loss: 6.70804334608380603555\n",
      "Iteration 22581 => Loss: 6.70804201671150224229\n",
      "Iteration 22582 => Loss: 6.70804068751832272000\n",
      "Iteration 22583 => Loss: 6.70803935850426213960\n",
      "Iteration 22584 => Loss: 6.70803802966928230944\n",
      "Iteration 22585 => Loss: 6.70803670101335836051\n",
      "Iteration 22586 => Loss: 6.70803537253647697014\n",
      "Iteration 22587 => Loss: 6.70803404423860527572\n",
      "Iteration 22588 => Loss: 6.70803271611972373734\n",
      "Iteration 22589 => Loss: 6.70803138817980038056\n",
      "Iteration 22590 => Loss: 6.70803006041882365906\n",
      "Iteration 22591 => Loss: 6.70802873283675715754\n",
      "Iteration 22592 => Loss: 6.70802740543358755332\n",
      "Iteration 22593 => Loss: 6.70802607820928553650\n",
      "Iteration 22594 => Loss: 6.70802475116382357356\n",
      "Iteration 22595 => Loss: 6.70802342429718567729\n",
      "Iteration 22596 => Loss: 6.70802209760934164962\n",
      "Iteration 22597 => Loss: 6.70802077110026928608\n",
      "Iteration 22598 => Loss: 6.70801944476993927680\n",
      "Iteration 22599 => Loss: 6.70801811861833297002\n",
      "Iteration 22600 => Loss: 6.70801679264543171399\n",
      "Iteration 22601 => Loss: 6.70801546685120353430\n",
      "Iteration 22602 => Loss: 6.70801414123562356195\n",
      "Iteration 22603 => Loss: 6.70801281579867048066\n",
      "Iteration 22604 => Loss: 6.70801149054032030961\n",
      "Iteration 22605 => Loss: 6.70801016546054906797\n",
      "Iteration 22606 => Loss: 6.70800884055933455130\n",
      "Iteration 22607 => Loss: 6.70800751583665455513\n",
      "Iteration 22608 => Loss: 6.70800619129247444050\n",
      "Iteration 22609 => Loss: 6.70800486692677555567\n",
      "Iteration 22610 => Loss: 6.70800354273953569617\n",
      "Iteration 22611 => Loss: 6.70800221873073798662\n",
      "Iteration 22612 => Loss: 6.70800089490033979445\n",
      "Iteration 22613 => Loss: 6.70799957124833401423\n",
      "Iteration 22614 => Loss: 6.70799824777469044790\n",
      "Iteration 22615 => Loss: 6.70799692447937623285\n",
      "Iteration 22616 => Loss: 6.70799560136238781638\n",
      "Iteration 22617 => Loss: 6.70799427842369233588\n",
      "Iteration 22618 => Loss: 6.70799295566324893514\n",
      "Iteration 22619 => Loss: 6.70799163308105494963\n",
      "Iteration 22620 => Loss: 6.70799031067707929310\n",
      "Iteration 22621 => Loss: 6.70798898845129887292\n",
      "Iteration 22622 => Loss: 6.70798766640368437919\n",
      "Iteration 22623 => Loss: 6.70798634453422160107\n",
      "Iteration 22624 => Loss: 6.70798502284287589958\n",
      "Iteration 22625 => Loss: 6.70798370132962595846\n",
      "Iteration 22626 => Loss: 6.70798237999445756685\n",
      "Iteration 22627 => Loss: 6.70798105883733697397\n",
      "Iteration 22628 => Loss: 6.70797973785824108717\n",
      "Iteration 22629 => Loss: 6.70797841705714770200\n",
      "Iteration 22630 => Loss: 6.70797709643403106128\n",
      "Iteration 22631 => Loss: 6.70797577598886896055\n",
      "Iteration 22632 => Loss: 6.70797445572163208993\n",
      "Iteration 22633 => Loss: 6.70797313563231334399\n",
      "Iteration 22634 => Loss: 6.70797181572086742563\n",
      "Iteration 22635 => Loss: 6.70797049598727923581\n",
      "Iteration 22636 => Loss: 6.70796917643152479371\n",
      "Iteration 22637 => Loss: 6.70796785705358722396\n",
      "Iteration 22638 => Loss: 6.70796653785343277576\n",
      "Iteration 22639 => Loss: 6.70796521883104102102\n",
      "Iteration 22640 => Loss: 6.70796389998638531438\n",
      "Iteration 22641 => Loss: 6.70796258131944611591\n",
      "Iteration 22642 => Loss: 6.70796126283019766845\n",
      "Iteration 22643 => Loss: 6.70795994451861954389\n",
      "Iteration 22644 => Loss: 6.70795862638467621508\n",
      "Iteration 22645 => Loss: 6.70795730842835080665\n",
      "Iteration 22646 => Loss: 6.70795599064962644320\n",
      "Iteration 22647 => Loss: 6.70795467304847115031\n",
      "Iteration 22648 => Loss: 6.70795335562486627623\n",
      "Iteration 22649 => Loss: 6.70795203837877362929\n",
      "Iteration 22650 => Loss: 6.70795072131019232131\n",
      "Iteration 22651 => Loss: 6.70794940441907705519\n",
      "Iteration 22652 => Loss: 6.70794808770541628462\n",
      "Iteration 22653 => Loss: 6.70794677116918425241\n",
      "Iteration 22654 => Loss: 6.70794545481035431322\n",
      "Iteration 22655 => Loss: 6.70794413862890337441\n",
      "Iteration 22656 => Loss: 6.70794282262481011969\n",
      "Iteration 22657 => Loss: 6.70794150679804701554\n",
      "Iteration 22658 => Loss: 6.70794019114858652841\n",
      "Iteration 22659 => Loss: 6.70793887567641711200\n",
      "Iteration 22660 => Loss: 6.70793756038150323917\n",
      "Iteration 22661 => Loss: 6.70793624526382625817\n",
      "Iteration 22662 => Loss: 6.70793493032336307635\n",
      "Iteration 22663 => Loss: 6.70793361556008882474\n",
      "Iteration 22664 => Loss: 6.70793230097397863432\n",
      "Iteration 22665 => Loss: 6.70793098656500852428\n",
      "Iteration 22666 => Loss: 6.70792967233315717834\n",
      "Iteration 22667 => Loss: 6.70792835827839528662\n",
      "Iteration 22668 => Loss: 6.70792704440070330918\n",
      "Iteration 22669 => Loss: 6.70792573070005548885\n",
      "Iteration 22670 => Loss: 6.70792441717642962118\n",
      "Iteration 22671 => Loss: 6.70792310382980083716\n",
      "Iteration 22672 => Loss: 6.70792179066015137323\n",
      "Iteration 22673 => Loss: 6.70792047766744481407\n",
      "Iteration 22674 => Loss: 6.70791916485166428430\n",
      "Iteration 22675 => Loss: 6.70791785221279113216\n",
      "Iteration 22676 => Loss: 6.70791653975078894234\n",
      "Iteration 22677 => Loss: 6.70791522746564350399\n",
      "Iteration 22678 => Loss: 6.70791391535733261264\n",
      "Iteration 22679 => Loss: 6.70791260342582518206\n",
      "Iteration 22680 => Loss: 6.70791129167110433684\n",
      "Iteration 22681 => Loss: 6.70790998009313366168\n",
      "Iteration 22682 => Loss: 6.70790866869190516297\n",
      "Iteration 22683 => Loss: 6.70790735746738864265\n",
      "Iteration 22684 => Loss: 6.70790604641955656717\n",
      "Iteration 22685 => Loss: 6.70790473554839294934\n",
      "Iteration 22686 => Loss: 6.70790342485386226201\n",
      "Iteration 22687 => Loss: 6.70790211433595295887\n",
      "Iteration 22688 => Loss: 6.70790080399463395366\n",
      "Iteration 22689 => Loss: 6.70789949382988481830\n",
      "Iteration 22690 => Loss: 6.70789818384167890741\n",
      "Iteration 22691 => Loss: 6.70789687402999756927\n",
      "Iteration 22692 => Loss: 6.70789556439480971761\n",
      "Iteration 22693 => Loss: 6.70789425493609936524\n",
      "Iteration 22694 => Loss: 6.70789294565383364954\n",
      "Iteration 22695 => Loss: 6.70789163654799658332\n",
      "Iteration 22696 => Loss: 6.70789032761855885667\n",
      "Iteration 22697 => Loss: 6.70788901886550181786\n",
      "Iteration 22698 => Loss: 6.70788771028880059788\n",
      "Iteration 22699 => Loss: 6.70788640188842411050\n",
      "Iteration 22700 => Loss: 6.70788509366436080938\n",
      "Iteration 22701 => Loss: 6.70788378561657694377\n",
      "Iteration 22702 => Loss: 6.70788247774505830279\n",
      "Iteration 22703 => Loss: 6.70788117004977468838\n",
      "Iteration 22704 => Loss: 6.70787986253069501430\n",
      "Iteration 22705 => Loss: 6.70787855518780951058\n",
      "Iteration 22706 => Loss: 6.70787724802108975553\n",
      "Iteration 22707 => Loss: 6.70787594103050821559\n",
      "Iteration 22708 => Loss: 6.70787463421604979175\n",
      "Iteration 22709 => Loss: 6.70787332757767629232\n",
      "Iteration 22710 => Loss: 6.70787202111537528282\n",
      "Iteration 22711 => Loss: 6.70787071482911656517\n",
      "Iteration 22712 => Loss: 6.70786940871888859306\n",
      "Iteration 22713 => Loss: 6.70786810278465495117\n",
      "Iteration 22714 => Loss: 6.70786679702639787592\n",
      "Iteration 22715 => Loss: 6.70786549144408539291\n",
      "Iteration 22716 => Loss: 6.70786418603770862035\n",
      "Iteration 22717 => Loss: 6.70786288080723114291\n",
      "Iteration 22718 => Loss: 6.70786157575263519703\n",
      "Iteration 22719 => Loss: 6.70786027087389058465\n",
      "Iteration 22720 => Loss: 6.70785896617098043038\n",
      "Iteration 22721 => Loss: 6.70785766164388164157\n",
      "Iteration 22722 => Loss: 6.70785635729256579651\n",
      "Iteration 22723 => Loss: 6.70785505311701779618\n",
      "Iteration 22724 => Loss: 6.70785374911719678437\n",
      "Iteration 22725 => Loss: 6.70785244529310009653\n",
      "Iteration 22726 => Loss: 6.70785114164469309372\n",
      "Iteration 22727 => Loss: 6.70784983817194735423\n",
      "Iteration 22728 => Loss: 6.70784853487484244994\n",
      "Iteration 22729 => Loss: 6.70784723175336150547\n",
      "Iteration 22730 => Loss: 6.70784592880747254640\n",
      "Iteration 22731 => Loss: 6.70784462603716580276\n",
      "Iteration 22732 => Loss: 6.70784332344239153656\n",
      "Iteration 22733 => Loss: 6.70784202102314974780\n",
      "Iteration 22734 => Loss: 6.70784071877941379114\n",
      "Iteration 22735 => Loss: 6.70783941671114902761\n",
      "Iteration 22736 => Loss: 6.70783811481834124635\n",
      "Iteration 22737 => Loss: 6.70783681310095758477\n",
      "Iteration 22738 => Loss: 6.70783551155898916107\n",
      "Iteration 22739 => Loss: 6.70783421019240400085\n",
      "Iteration 22740 => Loss: 6.70783290900117279421\n",
      "Iteration 22741 => Loss: 6.70783160798528133029\n",
      "Iteration 22742 => Loss: 6.70783030714469585831\n",
      "Iteration 22743 => Loss: 6.70782900647940394379\n",
      "Iteration 22744 => Loss: 6.70782770598937450046\n",
      "Iteration 22745 => Loss: 6.70782640567458621206\n",
      "Iteration 22746 => Loss: 6.70782510553501865047\n",
      "Iteration 22747 => Loss: 6.70782380557063984128\n",
      "Iteration 22748 => Loss: 6.70782250578143912634\n",
      "Iteration 22749 => Loss: 6.70782120616737298491\n",
      "Iteration 22750 => Loss: 6.70781990672844141699\n",
      "Iteration 22751 => Loss: 6.70781860746459734912\n",
      "Iteration 22752 => Loss: 6.70781730837584166949\n",
      "Iteration 22753 => Loss: 6.70781600946212730463\n",
      "Iteration 22754 => Loss: 6.70781471072345603091\n",
      "Iteration 22755 => Loss: 6.70781341215977100489\n",
      "Iteration 22756 => Loss: 6.70781211377108199656\n",
      "Iteration 22757 => Loss: 6.70781081555734903787\n",
      "Iteration 22758 => Loss: 6.70780951751855081255\n",
      "Iteration 22759 => Loss: 6.70780821965466067525\n",
      "Iteration 22760 => Loss: 6.70780692196565997421\n",
      "Iteration 22761 => Loss: 6.70780562445151584683\n",
      "Iteration 22762 => Loss: 6.70780432711221408226\n",
      "Iteration 22763 => Loss: 6.70780302994773069969\n",
      "Iteration 22764 => Loss: 6.70780173295804171829\n",
      "Iteration 22765 => Loss: 6.70780043614312493361\n",
      "Iteration 22766 => Loss: 6.70779913950295370029\n",
      "Iteration 22767 => Loss: 6.70779784303749959662\n",
      "Iteration 22768 => Loss: 6.70779654674674752357\n",
      "Iteration 22769 => Loss: 6.70779525063066905943\n",
      "Iteration 22770 => Loss: 6.70779395468924732882\n",
      "Iteration 22771 => Loss: 6.70779265892244680458\n",
      "Iteration 22772 => Loss: 6.70779136333025327588\n",
      "Iteration 22773 => Loss: 6.70779006791264009735\n",
      "Iteration 22774 => Loss: 6.70778877266958861725\n",
      "Iteration 22775 => Loss: 6.70778747760106242026\n",
      "Iteration 22776 => Loss: 6.70778618270705795368\n",
      "Iteration 22777 => Loss: 6.70778488798753436129\n",
      "Iteration 22778 => Loss: 6.70778359344247743223\n",
      "Iteration 22779 => Loss: 6.70778229907185696845\n",
      "Iteration 22780 => Loss: 6.70778100487565343002\n",
      "Iteration 22781 => Loss: 6.70777971085384638883\n",
      "Iteration 22782 => Loss: 6.70777841700641097589\n",
      "Iteration 22783 => Loss: 6.70777712333331432859\n",
      "Iteration 22784 => Loss: 6.70777582983454223609\n",
      "Iteration 22785 => Loss: 6.70777453651006805302\n",
      "Iteration 22786 => Loss: 6.70777324335987668036\n",
      "Iteration 22787 => Loss: 6.70777195038393081461\n",
      "Iteration 22788 => Loss: 6.70777065758221446856\n",
      "Iteration 22789 => Loss: 6.70776936495470010868\n",
      "Iteration 22790 => Loss: 6.70776807250137618865\n",
      "Iteration 22791 => Loss: 6.70776678022220274045\n",
      "Iteration 22792 => Loss: 6.70776548811716732956\n",
      "Iteration 22793 => Loss: 6.70776419618624419883\n",
      "Iteration 22794 => Loss: 6.70776290442940759107\n",
      "Iteration 22795 => Loss: 6.70776161284663263729\n",
      "Iteration 22796 => Loss: 6.70776032143790512663\n",
      "Iteration 22797 => Loss: 6.70775903020318953196\n",
      "Iteration 22798 => Loss: 6.70775773914246897789\n",
      "Iteration 22799 => Loss: 6.70775644825571681906\n",
      "Iteration 22800 => Loss: 6.70775515754291884463\n",
      "Iteration 22801 => Loss: 6.70775386700404041562\n",
      "Iteration 22802 => Loss: 6.70775257663905843941\n",
      "Iteration 22803 => Loss: 6.70775128644795071153\n",
      "Iteration 22804 => Loss: 6.70774999643070835020\n",
      "Iteration 22805 => Loss: 6.70774870658728605832\n",
      "Iteration 22806 => Loss: 6.70774741691767495411\n",
      "Iteration 22807 => Loss: 6.70774612742184750402\n",
      "Iteration 22808 => Loss: 6.70774483809978061544\n",
      "Iteration 22809 => Loss: 6.70774354895144497846\n",
      "Iteration 22810 => Loss: 6.70774225997682282951\n",
      "Iteration 22811 => Loss: 6.70774097117589196415\n",
      "Iteration 22812 => Loss: 6.70773968254862928973\n",
      "Iteration 22813 => Loss: 6.70773839409500549635\n",
      "Iteration 22814 => Loss: 6.70773710581500015593\n",
      "Iteration 22815 => Loss: 6.70773581770859195217\n",
      "Iteration 22816 => Loss: 6.70773452977576489786\n",
      "Iteration 22817 => Loss: 6.70773324201647813680\n",
      "Iteration 22818 => Loss: 6.70773195443071745814\n",
      "Iteration 22819 => Loss: 6.70773066701845621651\n",
      "Iteration 22820 => Loss: 6.70772937977967487200\n",
      "Iteration 22821 => Loss: 6.70772809271435122014\n",
      "Iteration 22822 => Loss: 6.70772680582245861558\n",
      "Iteration 22823 => Loss: 6.70772551910397485386\n",
      "Iteration 22824 => Loss: 6.70772423255888394777\n",
      "Iteration 22825 => Loss: 6.70772294618714592929\n",
      "Iteration 22826 => Loss: 6.70772165998875280479\n",
      "Iteration 22827 => Loss: 6.70772037396366638262\n",
      "Iteration 22828 => Loss: 6.70771908811187778099\n",
      "Iteration 22829 => Loss: 6.70771780243336213090\n",
      "Iteration 22830 => Loss: 6.70771651692808568157\n",
      "Iteration 22831 => Loss: 6.70771523159602978126\n",
      "Iteration 22832 => Loss: 6.70771394643717933093\n",
      "Iteration 22833 => Loss: 6.70771266145149613891\n",
      "Iteration 22834 => Loss: 6.70771137663897487613\n",
      "Iteration 22835 => Loss: 6.70771009199957468638\n",
      "Iteration 22836 => Loss: 6.70770880753327958246\n",
      "Iteration 22837 => Loss: 6.70770752324006824807\n",
      "Iteration 22838 => Loss: 6.70770623911991314969\n",
      "Iteration 22839 => Loss: 6.70770495517279563558\n",
      "Iteration 22840 => Loss: 6.70770367139869172490\n",
      "Iteration 22841 => Loss: 6.70770238779757654868\n",
      "Iteration 22842 => Loss: 6.70770110436942079701\n",
      "Iteration 22843 => Loss: 6.70769982111421292359\n",
      "Iteration 22844 => Loss: 6.70769853803192095398\n",
      "Iteration 22845 => Loss: 6.70769725512252534827\n",
      "Iteration 22846 => Loss: 6.70769597238600212563\n",
      "Iteration 22847 => Loss: 6.70769468982232464072\n",
      "Iteration 22848 => Loss: 6.70769340743148045902\n",
      "Iteration 22849 => Loss: 6.70769212521342961253\n",
      "Iteration 22850 => Loss: 6.70769084316816055491\n",
      "Iteration 22851 => Loss: 6.70768956129565108171\n",
      "Iteration 22852 => Loss: 6.70768827959587010668\n",
      "Iteration 22853 => Loss: 6.70768699806880341896\n",
      "Iteration 22854 => Loss: 6.70768571671442170867\n",
      "Iteration 22855 => Loss: 6.70768443553269388957\n",
      "Iteration 22856 => Loss: 6.70768315452361196805\n",
      "Iteration 22857 => Loss: 6.70768187368714219332\n",
      "Iteration 22858 => Loss: 6.70768059302327035454\n",
      "Iteration 22859 => Loss: 6.70767931253196625363\n",
      "Iteration 22860 => Loss: 6.70767803221320857432\n",
      "Iteration 22861 => Loss: 6.70767675206697333579\n",
      "Iteration 22862 => Loss: 6.70767547209324099811\n",
      "Iteration 22863 => Loss: 6.70767419229198047503\n",
      "Iteration 22864 => Loss: 6.70767291266317400300\n",
      "Iteration 22865 => Loss: 6.70767163320680204208\n",
      "Iteration 22866 => Loss: 6.70767035392283084150\n",
      "Iteration 22867 => Loss: 6.70766907481124619039\n",
      "Iteration 22868 => Loss: 6.70766779587202766066\n",
      "Iteration 22869 => Loss: 6.70766651710513794882\n",
      "Iteration 22870 => Loss: 6.70766523851056373218\n",
      "Iteration 22871 => Loss: 6.70766396008828635900\n",
      "Iteration 22872 => Loss: 6.70766268183826941396\n",
      "Iteration 22873 => Loss: 6.70766140376049513350\n",
      "Iteration 22874 => Loss: 6.70766012585494664222\n",
      "Iteration 22875 => Loss: 6.70765884812159374206\n",
      "Iteration 22876 => Loss: 6.70765757056042044582\n",
      "Iteration 22877 => Loss: 6.70765629317139655541\n",
      "Iteration 22878 => Loss: 6.70765501595449720185\n",
      "Iteration 22879 => Loss: 6.70765373890970284521\n",
      "Iteration 22880 => Loss: 6.70765246203699394556\n",
      "Iteration 22881 => Loss: 6.70765118533634385756\n",
      "Iteration 22882 => Loss: 6.70764990880772948856\n",
      "Iteration 22883 => Loss: 6.70764863245111886414\n",
      "Iteration 22884 => Loss: 6.70764735626650310252\n",
      "Iteration 22885 => Loss: 6.70764608025385733470\n",
      "Iteration 22886 => Loss: 6.70764480441315313897\n",
      "Iteration 22887 => Loss: 6.70764352874436831087\n",
      "Iteration 22888 => Loss: 6.70764225324747620505\n",
      "Iteration 22889 => Loss: 6.70764097792245728158\n",
      "Iteration 22890 => Loss: 6.70763970276929466507\n",
      "Iteration 22891 => Loss: 6.70763842778795993382\n",
      "Iteration 22892 => Loss: 6.70763715297842022522\n",
      "Iteration 22893 => Loss: 6.70763587834066488114\n",
      "Iteration 22894 => Loss: 6.70763460387466636803\n",
      "Iteration 22895 => Loss: 6.70763332958040692233\n",
      "Iteration 22896 => Loss: 6.70763205545785545780\n",
      "Iteration 22897 => Loss: 6.70763078150698799362\n",
      "Iteration 22898 => Loss: 6.70762950772779298347\n",
      "Iteration 22899 => Loss: 6.70762823412023490022\n",
      "Iteration 22900 => Loss: 6.70762696068429775664\n",
      "Iteration 22901 => Loss: 6.70762568741995401922\n",
      "Iteration 22902 => Loss: 6.70762441432718237166\n",
      "Iteration 22903 => Loss: 6.70762314140596593859\n",
      "Iteration 22904 => Loss: 6.70762186865627185739\n",
      "Iteration 22905 => Loss: 6.70762059607807525907\n",
      "Iteration 22906 => Loss: 6.70761932367136548550\n",
      "Iteration 22907 => Loss: 6.70761805143611322677\n",
      "Iteration 22908 => Loss: 6.70761677937229539026\n",
      "Iteration 22909 => Loss: 6.70761550747988266608\n",
      "Iteration 22910 => Loss: 6.70761423575885729065\n",
      "Iteration 22911 => Loss: 6.70761296420920416494\n",
      "Iteration 22912 => Loss: 6.70761169283088598547\n",
      "Iteration 22913 => Loss: 6.70761042162389120591\n",
      "Iteration 22914 => Loss: 6.70760915058818429912\n",
      "Iteration 22915 => Loss: 6.70760787972375371879\n",
      "Iteration 22916 => Loss: 6.70760660903057459592\n",
      "Iteration 22917 => Loss: 6.70760533850861939698\n",
      "Iteration 22918 => Loss: 6.70760406815786591750\n",
      "Iteration 22919 => Loss: 6.70760279797829284121\n",
      "Iteration 22920 => Loss: 6.70760152796987707546\n",
      "Iteration 22921 => Loss: 6.70760025813259552763\n",
      "Iteration 22922 => Loss: 6.70759898846642066417\n",
      "Iteration 22923 => Loss: 6.70759771897134271512\n",
      "Iteration 22924 => Loss: 6.70759644964732348882\n",
      "Iteration 22925 => Loss: 6.70759518049434255715\n",
      "Iteration 22926 => Loss: 6.70759391151239015016\n",
      "Iteration 22927 => Loss: 6.70759264270142274711\n",
      "Iteration 22928 => Loss: 6.70759137406143324256\n",
      "Iteration 22929 => Loss: 6.70759010559238877391\n",
      "Iteration 22930 => Loss: 6.70758883729426802489\n",
      "Iteration 22931 => Loss: 6.70758756916705767281\n",
      "Iteration 22932 => Loss: 6.70758630121072485508\n",
      "Iteration 22933 => Loss: 6.70758503342524470270\n",
      "Iteration 22934 => Loss: 6.70758376581061188659\n",
      "Iteration 22935 => Loss: 6.70758249836677933331\n",
      "Iteration 22936 => Loss: 6.70758123109374526649\n",
      "Iteration 22937 => Loss: 6.70757996399146172450\n",
      "Iteration 22938 => Loss: 6.70757869705992604281\n",
      "Iteration 22939 => Loss: 6.70757743029911512878\n",
      "Iteration 22940 => Loss: 6.70757616370899878433\n",
      "Iteration 22941 => Loss: 6.70757489728954681141\n",
      "Iteration 22942 => Loss: 6.70757363104074944005\n",
      "Iteration 22943 => Loss: 6.70757236496258535396\n",
      "Iteration 22944 => Loss: 6.70757109905501636149\n",
      "Iteration 22945 => Loss: 6.70756983331803358084\n",
      "Iteration 22946 => Loss: 6.70756856775161214301\n",
      "Iteration 22947 => Loss: 6.70756730235571652088\n",
      "Iteration 22948 => Loss: 6.70756603713034227354\n",
      "Iteration 22949 => Loss: 6.70756477207545387387\n",
      "Iteration 22950 => Loss: 6.70756350719103000557\n",
      "Iteration 22951 => Loss: 6.70756224247705290509\n",
      "Iteration 22952 => Loss: 6.70756097793349503888\n",
      "Iteration 22953 => Loss: 6.70755971356033064978\n",
      "Iteration 22954 => Loss: 6.70755844935754375058\n",
      "Iteration 22955 => Loss: 6.70755718532510591956\n",
      "Iteration 22956 => Loss: 6.70755592146300383405\n",
      "Iteration 22957 => Loss: 6.70755465777120107873\n",
      "Iteration 22958 => Loss: 6.70755339424967722550\n",
      "Iteration 22959 => Loss: 6.70755213089841895169\n",
      "Iteration 22960 => Loss: 6.70755086771739694740\n",
      "Iteration 22961 => Loss: 6.70754960470658456728\n",
      "Iteration 22962 => Loss: 6.70754834186596760048\n",
      "Iteration 22963 => Loss: 6.70754707919552206619\n",
      "Iteration 22964 => Loss: 6.70754581669521598997\n",
      "Iteration 22965 => Loss: 6.70754455436503249643\n",
      "Iteration 22966 => Loss: 6.70754329220495382202\n",
      "Iteration 22967 => Loss: 6.70754203021493999870\n",
      "Iteration 22968 => Loss: 6.70754076839498569740\n",
      "Iteration 22969 => Loss: 6.70753950674506516094\n",
      "Iteration 22970 => Loss: 6.70753824526513930948\n",
      "Iteration 22971 => Loss: 6.70753698395521080755\n",
      "Iteration 22972 => Loss: 6.70753572281523613441\n",
      "Iteration 22973 => Loss: 6.70753446184520729645\n",
      "Iteration 22974 => Loss: 6.70753320104508432564\n",
      "Iteration 22975 => Loss: 6.70753194041486722199\n",
      "Iteration 22976 => Loss: 6.70753067995451512928\n",
      "Iteration 22977 => Loss: 6.70752941966400761942\n",
      "Iteration 22978 => Loss: 6.70752815954332426429\n",
      "Iteration 22979 => Loss: 6.70752689959244108309\n",
      "Iteration 22980 => Loss: 6.70752563981134208859\n",
      "Iteration 22981 => Loss: 6.70752438019998908914\n",
      "Iteration 22982 => Loss: 6.70752312075837586747\n",
      "Iteration 22983 => Loss: 6.70752186148647400188\n",
      "Iteration 22984 => Loss: 6.70752060238425773520\n",
      "Iteration 22985 => Loss: 6.70751934345170131024\n",
      "Iteration 22986 => Loss: 6.70751808468879229252\n",
      "Iteration 22987 => Loss: 6.70751682609549870762\n",
      "Iteration 22988 => Loss: 6.70751556767180101559\n",
      "Iteration 22989 => Loss: 6.70751430941767434746\n",
      "Iteration 22990 => Loss: 6.70751305133309916329\n",
      "Iteration 22991 => Loss: 6.70751179341805148226\n",
      "Iteration 22992 => Loss: 6.70751053567250821175\n",
      "Iteration 22993 => Loss: 6.70750927809644359456\n",
      "Iteration 22994 => Loss: 6.70750802068984164350\n",
      "Iteration 22995 => Loss: 6.70750676345267216050\n",
      "Iteration 22996 => Loss: 6.70750550638491827016\n",
      "Iteration 22997 => Loss: 6.70750424948655066260\n",
      "Iteration 22998 => Loss: 6.70750299275755246242\n",
      "Iteration 22999 => Loss: 6.70750173619789702428\n",
      "Iteration 23000 => Loss: 6.70750047980756836097\n",
      "Iteration 23001 => Loss: 6.70749922358653272170\n",
      "Iteration 23002 => Loss: 6.70749796753476790201\n",
      "Iteration 23003 => Loss: 6.70749671165226679648\n",
      "Iteration 23004 => Loss: 6.70749545593899210161\n",
      "Iteration 23005 => Loss: 6.70749420039492338930\n",
      "Iteration 23006 => Loss: 6.70749294502004111962\n",
      "Iteration 23007 => Loss: 6.70749168981432042358\n",
      "Iteration 23008 => Loss: 6.70749043477773909672\n",
      "Iteration 23009 => Loss: 6.70748917991027227004\n",
      "Iteration 23010 => Loss: 6.70748792521189773908\n",
      "Iteration 23011 => Loss: 6.70748667068259685209\n",
      "Iteration 23012 => Loss: 6.70748541632234562826\n",
      "Iteration 23013 => Loss: 6.70748416213111386952\n",
      "Iteration 23014 => Loss: 6.70748290810889091773\n",
      "Iteration 23015 => Loss: 6.70748165425565101572\n",
      "Iteration 23016 => Loss: 6.70748040057135330727\n",
      "Iteration 23017 => Loss: 6.70747914705599690421\n",
      "Iteration 23018 => Loss: 6.70747789370955338484\n",
      "Iteration 23019 => Loss: 6.70747664053199610379\n",
      "Iteration 23020 => Loss: 6.70747538752330640932\n",
      "Iteration 23021 => Loss: 6.70747413468346209697\n",
      "Iteration 23022 => Loss: 6.70747288201243030414\n",
      "Iteration 23023 => Loss: 6.70747162951019948451\n",
      "Iteration 23024 => Loss: 6.70747037717675009816\n",
      "Iteration 23025 => Loss: 6.70746912501204928247\n",
      "Iteration 23026 => Loss: 6.70746787301607305665\n",
      "Iteration 23027 => Loss: 6.70746662118880099257\n",
      "Iteration 23028 => Loss: 6.70746536953022332028\n",
      "Iteration 23029 => Loss: 6.70746411804030184811\n",
      "Iteration 23030 => Loss: 6.70746286671901792431\n",
      "Iteration 23031 => Loss: 6.70746161556635112078\n",
      "Iteration 23032 => Loss: 6.70746036458228012123\n",
      "Iteration 23033 => Loss: 6.70745911376676851035\n",
      "Iteration 23034 => Loss: 6.70745786311981895267\n",
      "Iteration 23035 => Loss: 6.70745661264138348656\n",
      "Iteration 23036 => Loss: 6.70745536233145323024\n",
      "Iteration 23037 => Loss: 6.70745411219000242653\n",
      "Iteration 23038 => Loss: 6.70745286221700531826\n",
      "Iteration 23039 => Loss: 6.70745161241244414185\n",
      "Iteration 23040 => Loss: 6.70745036277630024557\n",
      "Iteration 23041 => Loss: 6.70744911330853366138\n",
      "Iteration 23042 => Loss: 6.70744786400914083657\n",
      "Iteration 23043 => Loss: 6.70744661487808979672\n",
      "Iteration 23044 => Loss: 6.70744536591536189007\n",
      "Iteration 23045 => Loss: 6.70744411712092869493\n",
      "Iteration 23046 => Loss: 6.70744286849476534229\n",
      "Iteration 23047 => Loss: 6.70744162003686028584\n",
      "Iteration 23048 => Loss: 6.70744037174719043293\n",
      "Iteration 23049 => Loss: 6.70743912362571936825\n",
      "Iteration 23050 => Loss: 6.70743787567243465730\n",
      "Iteration 23051 => Loss: 6.70743662788731409563\n",
      "Iteration 23052 => Loss: 6.70743538027032570881\n",
      "Iteration 23053 => Loss: 6.70743413282146416776\n",
      "Iteration 23054 => Loss: 6.70743288554069483354\n",
      "Iteration 23055 => Loss: 6.70743163842798928442\n",
      "Iteration 23056 => Loss: 6.70743039148333686228\n",
      "Iteration 23057 => Loss: 6.70742914470670736904\n",
      "Iteration 23058 => Loss: 6.70742789809808659385\n",
      "Iteration 23059 => Loss: 6.70742665165743989775\n",
      "Iteration 23060 => Loss: 6.70742540538476195167\n",
      "Iteration 23061 => Loss: 6.70742415928000479397\n",
      "Iteration 23062 => Loss: 6.70742291334316842466\n",
      "Iteration 23063 => Loss: 6.70742166757422708656\n",
      "Iteration 23064 => Loss: 6.70742042197314258800\n",
      "Iteration 23065 => Loss: 6.70741917653990338266\n",
      "Iteration 23066 => Loss: 6.70741793127449703604\n",
      "Iteration 23067 => Loss: 6.70741668617688269194\n",
      "Iteration 23068 => Loss: 6.70741544124704702767\n",
      "Iteration 23069 => Loss: 6.70741419648496250971\n",
      "Iteration 23070 => Loss: 6.70741295189061492721\n",
      "Iteration 23071 => Loss: 6.70741170746397319391\n",
      "Iteration 23072 => Loss: 6.70741046320501688172\n",
      "Iteration 23073 => Loss: 6.70740921911373177977\n",
      "Iteration 23074 => Loss: 6.70740797519008147276\n",
      "Iteration 23075 => Loss: 6.70740673143405174983\n",
      "Iteration 23076 => Loss: 6.70740548784561863016\n",
      "Iteration 23077 => Loss: 6.70740424442475458022\n",
      "Iteration 23078 => Loss: 6.70740300117144538916\n",
      "Iteration 23079 => Loss: 6.70740175808566618798\n",
      "Iteration 23080 => Loss: 6.70740051516739033133\n",
      "Iteration 23081 => Loss: 6.70739927241659916746\n",
      "Iteration 23082 => Loss: 6.70739802983327138008\n",
      "Iteration 23083 => Loss: 6.70739678741738121204\n",
      "Iteration 23084 => Loss: 6.70739554516890112978\n",
      "Iteration 23085 => Loss: 6.70739430308781958701\n",
      "Iteration 23086 => Loss: 6.70739306117410549746\n",
      "Iteration 23087 => Loss: 6.70739181942774287393\n",
      "Iteration 23088 => Loss: 6.70739057784870240653\n",
      "Iteration 23089 => Loss: 6.70738933643696011444\n",
      "Iteration 23090 => Loss: 6.70738809519250889224\n",
      "Iteration 23091 => Loss: 6.70738685411531143643\n",
      "Iteration 23092 => Loss: 6.70738561320534820709\n",
      "Iteration 23093 => Loss: 6.70738437246259699975\n",
      "Iteration 23094 => Loss: 6.70738313188703294543\n",
      "Iteration 23095 => Loss: 6.70738189147864272144\n",
      "Iteration 23096 => Loss: 6.70738065123738902429\n",
      "Iteration 23097 => Loss: 6.70737941116326563673\n",
      "Iteration 23098 => Loss: 6.70737817125624147252\n",
      "Iteration 23099 => Loss: 6.70737693151629521537\n",
      "Iteration 23100 => Loss: 6.70737569194340288448\n",
      "Iteration 23101 => Loss: 6.70737445253754760444\n",
      "Iteration 23102 => Loss: 6.70737321329869384812\n",
      "Iteration 23103 => Loss: 6.70737197422683273373\n",
      "Iteration 23104 => Loss: 6.70737073532193583958\n",
      "Iteration 23105 => Loss: 6.70736949658397918483\n",
      "Iteration 23106 => Loss: 6.70736825801294500593\n",
      "Iteration 23107 => Loss: 6.70736701960880843387\n",
      "Iteration 23108 => Loss: 6.70736578137154459966\n",
      "Iteration 23109 => Loss: 6.70736454330113573974\n",
      "Iteration 23110 => Loss: 6.70736330539756053781\n",
      "Iteration 23111 => Loss: 6.70736206766078613128\n",
      "Iteration 23112 => Loss: 6.70736083009080008566\n",
      "Iteration 23113 => Loss: 6.70735959268757131468\n",
      "Iteration 23114 => Loss: 6.70735835545109093658\n",
      "Iteration 23115 => Loss: 6.70735711838132164786\n",
      "Iteration 23116 => Loss: 6.70735588147825012584\n",
      "Iteration 23117 => Loss: 6.70735464474185860695\n",
      "Iteration 23118 => Loss: 6.70735340817211067588\n",
      "Iteration 23119 => Loss: 6.70735217176898679270\n",
      "Iteration 23120 => Loss: 6.70735093553247363474\n",
      "Iteration 23121 => Loss: 6.70734969946254100392\n",
      "Iteration 23122 => Loss: 6.70734846355916847216\n",
      "Iteration 23123 => Loss: 6.70734722782234094041\n",
      "Iteration 23124 => Loss: 6.70734599225201932882\n",
      "Iteration 23125 => Loss: 6.70734475684819209107\n",
      "Iteration 23126 => Loss: 6.70734352161084235178\n",
      "Iteration 23127 => Loss: 6.70734228653993636016\n",
      "Iteration 23128 => Loss: 6.70734105163545812900\n",
      "Iteration 23129 => Loss: 6.70733981689738190113\n",
      "Iteration 23130 => Loss: 6.70733858232568991298\n",
      "Iteration 23131 => Loss: 6.70733734792035374284\n",
      "Iteration 23132 => Loss: 6.70733611368135029807\n",
      "Iteration 23133 => Loss: 6.70733487960866625599\n",
      "Iteration 23134 => Loss: 6.70733364570227141854\n",
      "Iteration 23135 => Loss: 6.70733241196214180491\n",
      "Iteration 23136 => Loss: 6.70733117838826675694\n",
      "Iteration 23137 => Loss: 6.70732994498060985933\n",
      "Iteration 23138 => Loss: 6.70732871173915956575\n",
      "Iteration 23139 => Loss: 6.70732747866388567815\n",
      "Iteration 23140 => Loss: 6.70732624575476954476\n",
      "Iteration 23141 => Loss: 6.70732501301178896114\n",
      "Iteration 23142 => Loss: 6.70732378043491639374\n",
      "Iteration 23143 => Loss: 6.70732254802413763173\n",
      "Iteration 23144 => Loss: 6.70732131577942425338\n",
      "Iteration 23145 => Loss: 6.70732008370075849513\n",
      "Iteration 23146 => Loss: 6.70731885178810571801\n",
      "Iteration 23147 => Loss: 6.70731762004146503386\n",
      "Iteration 23148 => Loss: 6.70731638846079469829\n",
      "Iteration 23149 => Loss: 6.70731515704608849404\n",
      "Iteration 23150 => Loss: 6.70731392579730822945\n",
      "Iteration 23151 => Loss: 6.70731269471444679908\n",
      "Iteration 23152 => Loss: 6.70731146379746867581\n",
      "Iteration 23153 => Loss: 6.70731023304635609605\n",
      "Iteration 23154 => Loss: 6.70730900246108330265\n",
      "Iteration 23155 => Loss: 6.70730777204163874927\n",
      "Iteration 23156 => Loss: 6.70730654178798779697\n",
      "Iteration 23157 => Loss: 6.70730531170011623487\n",
      "Iteration 23158 => Loss: 6.70730408177800541125\n",
      "Iteration 23159 => Loss: 6.70730285202162335167\n",
      "Iteration 23160 => Loss: 6.70730162243094785168\n",
      "Iteration 23161 => Loss: 6.70730039300595315410\n",
      "Iteration 23162 => Loss: 6.70729916374663392986\n",
      "Iteration 23163 => Loss: 6.70729793465295287547\n",
      "Iteration 23164 => Loss: 6.70729670572489578007\n",
      "Iteration 23165 => Loss: 6.70729547696243866284\n",
      "Iteration 23166 => Loss: 6.70729424836554866118\n",
      "Iteration 23167 => Loss: 6.70729301993421778150\n",
      "Iteration 23168 => Loss: 6.70729179166841937842\n",
      "Iteration 23169 => Loss: 6.70729056356812858297\n",
      "Iteration 23170 => Loss: 6.70728933563332052614\n",
      "Iteration 23171 => Loss: 6.70728810786398010890\n",
      "Iteration 23172 => Loss: 6.70728688026008601497\n",
      "Iteration 23173 => Loss: 6.70728565282160893446\n",
      "Iteration 23174 => Loss: 6.70728442554852399837\n",
      "Iteration 23175 => Loss: 6.70728319844081699586\n",
      "Iteration 23176 => Loss: 6.70728197149846394609\n",
      "Iteration 23177 => Loss: 6.70728074472144353280\n",
      "Iteration 23178 => Loss: 6.70727951810972911062\n",
      "Iteration 23179 => Loss: 6.70727829166329669874\n",
      "Iteration 23180 => Loss: 6.70727706538213563903\n",
      "Iteration 23181 => Loss: 6.70727583926621129251\n",
      "Iteration 23182 => Loss: 6.70727461331551033652\n",
      "Iteration 23183 => Loss: 6.70727338752999546756\n",
      "Iteration 23184 => Loss: 6.70727216190967023834\n",
      "Iteration 23185 => Loss: 6.70727093645449112813\n",
      "Iteration 23186 => Loss: 6.70726971116444214971\n",
      "Iteration 23187 => Loss: 6.70726848603949665772\n",
      "Iteration 23188 => Loss: 6.70726726107964132950\n",
      "Iteration 23189 => Loss: 6.70726603628485040787\n",
      "Iteration 23190 => Loss: 6.70726481165509458293\n",
      "Iteration 23191 => Loss: 6.70726358719036497291\n",
      "Iteration 23192 => Loss: 6.70726236289062782703\n",
      "Iteration 23193 => Loss: 6.70726113875586804625\n",
      "Iteration 23194 => Loss: 6.70725991478606076157\n",
      "Iteration 23195 => Loss: 6.70725869098117932765\n",
      "Iteration 23196 => Loss: 6.70725746734121042181\n",
      "Iteration 23197 => Loss: 6.70725624386612562233\n",
      "Iteration 23198 => Loss: 6.70725502055590450112\n",
      "Iteration 23199 => Loss: 6.70725379741052041283\n",
      "Iteration 23200 => Loss: 6.70725257442996269930\n",
      "Iteration 23201 => Loss: 6.70725135161419583341\n",
      "Iteration 23202 => Loss: 6.70725012896320116340\n",
      "Iteration 23203 => Loss: 6.70724890647696181389\n",
      "Iteration 23204 => Loss: 6.70724768415546002132\n",
      "Iteration 23205 => Loss: 6.70724646199865670582\n",
      "Iteration 23206 => Loss: 6.70724524000654387379\n",
      "Iteration 23207 => Loss: 6.70724401817908866263\n",
      "Iteration 23208 => Loss: 6.70724279651627863785\n",
      "Iteration 23209 => Loss: 6.70724157501809070681\n",
      "Iteration 23210 => Loss: 6.70724035368449467143\n",
      "Iteration 23211 => Loss: 6.70723913251547454450\n",
      "Iteration 23212 => Loss: 6.70723791151100989794\n",
      "Iteration 23213 => Loss: 6.70723669067106875730\n",
      "Iteration 23214 => Loss: 6.70723546999564934623\n",
      "Iteration 23215 => Loss: 6.70723424948470459128\n",
      "Iteration 23216 => Loss: 6.70723302913822827520\n",
      "Iteration 23217 => Loss: 6.70723180895619108810\n",
      "Iteration 23218 => Loss: 6.70723058893857526641\n",
      "Iteration 23219 => Loss: 6.70722936908535594114\n",
      "Iteration 23220 => Loss: 6.70722814939651268418\n",
      "Iteration 23221 => Loss: 6.70722692987202684378\n",
      "Iteration 23222 => Loss: 6.70722571051186378099\n",
      "Iteration 23223 => Loss: 6.70722449131601639039\n",
      "Iteration 23224 => Loss: 6.70722327228444914482\n",
      "Iteration 23225 => Loss: 6.70722205341715760341\n",
      "Iteration 23226 => Loss: 6.70722083471409380451\n",
      "Iteration 23227 => Loss: 6.70721961617526041266\n",
      "Iteration 23228 => Loss: 6.70721839780062278891\n",
      "Iteration 23229 => Loss: 6.70721717959016316968\n",
      "Iteration 23230 => Loss: 6.70721596154385135691\n",
      "Iteration 23231 => Loss: 6.70721474366167491610\n",
      "Iteration 23232 => Loss: 6.70721352594360897825\n",
      "Iteration 23233 => Loss: 6.70721230838962867438\n",
      "Iteration 23234 => Loss: 6.70721109099971801726\n",
      "Iteration 23235 => Loss: 6.70720987377384858519\n",
      "Iteration 23236 => Loss: 6.70720865671199906188\n",
      "Iteration 23237 => Loss: 6.70720743981414990742\n",
      "Iteration 23238 => Loss: 6.70720622308027447644\n",
      "Iteration 23239 => Loss: 6.70720500651035322903\n",
      "Iteration 23240 => Loss: 6.70720379010437195433\n",
      "Iteration 23241 => Loss: 6.70720257386230578334\n",
      "Iteration 23242 => Loss: 6.70720135778411563621\n",
      "Iteration 23243 => Loss: 6.70720014186979973658\n",
      "Iteration 23244 => Loss: 6.70719892611932433368\n",
      "Iteration 23245 => Loss: 6.70719771053267166394\n",
      "Iteration 23246 => Loss: 6.70719649510982218743\n",
      "Iteration 23247 => Loss: 6.70719527985075014698\n",
      "Iteration 23248 => Loss: 6.70719406475543156176\n",
      "Iteration 23249 => Loss: 6.70719284982385044458\n",
      "Iteration 23250 => Loss: 6.70719163505598370278\n",
      "Iteration 23251 => Loss: 6.70719042045179936196\n",
      "Iteration 23252 => Loss: 6.70718920601129031667\n",
      "Iteration 23253 => Loss: 6.70718799173442370432\n",
      "Iteration 23254 => Loss: 6.70718677762117998498\n",
      "Iteration 23255 => Loss: 6.70718556367153873055\n",
      "Iteration 23256 => Loss: 6.70718434988547951292\n",
      "Iteration 23257 => Loss: 6.70718313626297835128\n",
      "Iteration 23258 => Loss: 6.70718192280401126482\n",
      "Iteration 23259 => Loss: 6.70718070950855871359\n",
      "Iteration 23260 => Loss: 6.70717949637659494044\n",
      "Iteration 23261 => Loss: 6.70717828340810395815\n",
      "Iteration 23262 => Loss: 6.70717707060306178590\n",
      "Iteration 23263 => Loss: 6.70717585796143556109\n",
      "Iteration 23264 => Loss: 6.70717464548322617190\n",
      "Iteration 23265 => Loss: 6.70717343316839098577\n",
      "Iteration 23266 => Loss: 6.70717222101691934455\n",
      "Iteration 23267 => Loss: 6.70717100902878016200\n",
      "Iteration 23268 => Loss: 6.70716979720395656273\n",
      "Iteration 23269 => Loss: 6.70716858554243255952\n",
      "Iteration 23270 => Loss: 6.70716737404417795432\n",
      "Iteration 23271 => Loss: 6.70716616270917054266\n",
      "Iteration 23272 => Loss: 6.70716495153739256097\n",
      "Iteration 23273 => Loss: 6.70716374052881558754\n",
      "Iteration 23274 => Loss: 6.70716252968342541152\n",
      "Iteration 23275 => Loss: 6.70716131900119716391\n",
      "Iteration 23276 => Loss: 6.70716010848210775208\n",
      "Iteration 23277 => Loss: 6.70715889812613852428\n",
      "Iteration 23278 => Loss: 6.70715768793325839425\n",
      "Iteration 23279 => Loss: 6.70715647790345581569\n",
      "Iteration 23280 => Loss: 6.70715526803670325506\n",
      "Iteration 23281 => Loss: 6.70715405833299094240\n",
      "Iteration 23282 => Loss: 6.70715284879227269244\n",
      "Iteration 23283 => Loss: 6.70715163941453962337\n",
      "Iteration 23284 => Loss: 6.70715043019977485983\n",
      "Iteration 23285 => Loss: 6.70714922114795886188\n",
      "Iteration 23286 => Loss: 6.70714801225905699056\n",
      "Iteration 23287 => Loss: 6.70714680353305325866\n",
      "Iteration 23288 => Loss: 6.70714559496992368537\n",
      "Iteration 23289 => Loss: 6.70714438656964961893\n",
      "Iteration 23290 => Loss: 6.70714317833220441400\n",
      "Iteration 23291 => Loss: 6.70714197025757030701\n",
      "Iteration 23292 => Loss: 6.70714076234572953439\n",
      "Iteration 23293 => Loss: 6.70713955459664834535\n",
      "Iteration 23294 => Loss: 6.70713834701031608176\n",
      "Iteration 23295 => Loss: 6.70713713958670254556\n",
      "Iteration 23296 => Loss: 6.70713593232579263770\n",
      "Iteration 23297 => Loss: 6.70713472522755971283\n",
      "Iteration 23298 => Loss: 6.70713351829198423104\n",
      "Iteration 23299 => Loss: 6.70713231151904132332\n",
      "Iteration 23300 => Loss: 6.70713110490870967340\n",
      "Iteration 23301 => Loss: 6.70712989846097507041\n",
      "Iteration 23302 => Loss: 6.70712869217580820447\n",
      "Iteration 23303 => Loss: 6.70712748605318331840\n",
      "Iteration 23304 => Loss: 6.70712628009308886590\n",
      "Iteration 23305 => Loss: 6.70712507429549464888\n",
      "Iteration 23306 => Loss: 6.70712386866038468014\n",
      "Iteration 23307 => Loss: 6.70712266318772964979\n",
      "Iteration 23308 => Loss: 6.70712145787751179427\n",
      "Iteration 23309 => Loss: 6.70712025272971068546\n",
      "Iteration 23310 => Loss: 6.70711904774430944798\n",
      "Iteration 23311 => Loss: 6.70711784292127344287\n",
      "Iteration 23312 => Loss: 6.70711663826058934745\n",
      "Iteration 23313 => Loss: 6.70711543376222962820\n",
      "Iteration 23314 => Loss: 6.70711422942618451515\n",
      "Iteration 23315 => Loss: 6.70711302525242114569\n",
      "Iteration 23316 => Loss: 6.70711182124092086809\n",
      "Iteration 23317 => Loss: 6.70711061739165081974\n",
      "Iteration 23318 => Loss: 6.70710941370460833610\n",
      "Iteration 23319 => Loss: 6.70710821017976410729\n",
      "Iteration 23320 => Loss: 6.70710700681709592885\n",
      "Iteration 23321 => Loss: 6.70710580361657537907\n",
      "Iteration 23322 => Loss: 6.70710460057819002344\n",
      "Iteration 23323 => Loss: 6.70710339770190877573\n",
      "Iteration 23324 => Loss: 6.70710219498771653690\n",
      "Iteration 23325 => Loss: 6.70710099243559554338\n",
      "Iteration 23326 => Loss: 6.70709979004552003801\n",
      "Iteration 23327 => Loss: 6.70709858781745982270\n",
      "Iteration 23328 => Loss: 6.70709738575140601569\n",
      "Iteration 23329 => Loss: 6.70709618384732308982\n",
      "Iteration 23330 => Loss: 6.70709498210520393968\n",
      "Iteration 23331 => Loss: 6.70709378052500948542\n",
      "Iteration 23332 => Loss: 6.70709257910674683245\n",
      "Iteration 23333 => Loss: 6.70709137785035558466\n",
      "Iteration 23334 => Loss: 6.70709017675584462381\n",
      "Iteration 23335 => Loss: 6.70708897582317664643\n",
      "Iteration 23336 => Loss: 6.70708777505233832983\n",
      "Iteration 23337 => Loss: 6.70708657444329947594\n",
      "Iteration 23338 => Loss: 6.70708537399604587392\n",
      "Iteration 23339 => Loss: 6.70708417371055087841\n",
      "Iteration 23340 => Loss: 6.70708297358679761402\n",
      "Iteration 23341 => Loss: 6.70708177362476387628\n",
      "Iteration 23342 => Loss: 6.70708057382441857897\n",
      "Iteration 23343 => Loss: 6.70707937418574662303\n",
      "Iteration 23344 => Loss: 6.70707817470872758037\n",
      "Iteration 23345 => Loss: 6.70707697539333480563\n",
      "Iteration 23346 => Loss: 6.70707577623955764068\n",
      "Iteration 23347 => Loss: 6.70707457724735967020\n",
      "Iteration 23348 => Loss: 6.70707337841673201240\n",
      "Iteration 23349 => Loss: 6.70707217974764269286\n",
      "Iteration 23350 => Loss: 6.70707098124007305984\n",
      "Iteration 23351 => Loss: 6.70706978289400446158\n",
      "Iteration 23352 => Loss: 6.70706858470940847639\n",
      "Iteration 23353 => Loss: 6.70706738668628066335\n",
      "Iteration 23354 => Loss: 6.70706618882457128450\n",
      "Iteration 23355 => Loss: 6.70706499112428478071\n",
      "Iteration 23356 => Loss: 6.70706379358538740121\n",
      "Iteration 23357 => Loss: 6.70706259620784983611\n",
      "Iteration 23358 => Loss: 6.70706139899167119722\n",
      "Iteration 23359 => Loss: 6.70706020193680885200\n",
      "Iteration 23360 => Loss: 6.70705900504324858957\n",
      "Iteration 23361 => Loss: 6.70705780831097975181\n",
      "Iteration 23362 => Loss: 6.70705661173995970614\n",
      "Iteration 23363 => Loss: 6.70705541533018045897\n",
      "Iteration 23364 => Loss: 6.70705421908162158218\n",
      "Iteration 23365 => Loss: 6.70705302299425554224\n",
      "Iteration 23366 => Loss: 6.70705182706806013471\n",
      "Iteration 23367 => Loss: 6.70705063130301581964\n",
      "Iteration 23368 => Loss: 6.70704943569910216894\n",
      "Iteration 23369 => Loss: 6.70704824025629875450\n",
      "Iteration 23370 => Loss: 6.70704704497457804280\n",
      "Iteration 23371 => Loss: 6.70704584985392404661\n",
      "Iteration 23372 => Loss: 6.70704465489430923242\n",
      "Iteration 23373 => Loss: 6.70704346009571938936\n",
      "Iteration 23374 => Loss: 6.70704226545812609572\n",
      "Iteration 23375 => Loss: 6.70704107098151247612\n",
      "Iteration 23376 => Loss: 6.70703987666585099703\n",
      "Iteration 23377 => Loss: 6.70703868251112389487\n",
      "Iteration 23378 => Loss: 6.70703748851731695879\n",
      "Iteration 23379 => Loss: 6.70703629468439110894\n",
      "Iteration 23380 => Loss: 6.70703510101234190444\n",
      "Iteration 23381 => Loss: 6.70703390750113825902\n",
      "Iteration 23382 => Loss: 6.70703271415076063278\n",
      "Iteration 23383 => Loss: 6.70703152096118326853\n",
      "Iteration 23384 => Loss: 6.70703032793238929088\n",
      "Iteration 23385 => Loss: 6.70702913506435560720\n",
      "Iteration 23386 => Loss: 6.70702794235705734849\n",
      "Iteration 23387 => Loss: 6.70702674981048474478\n",
      "Iteration 23388 => Loss: 6.70702555742460315713\n",
      "Iteration 23389 => Loss: 6.70702436519939571014\n",
      "Iteration 23390 => Loss: 6.70702317313484375205\n",
      "Iteration 23391 => Loss: 6.70702198123092330206\n",
      "Iteration 23392 => Loss: 6.70702078948761037935\n",
      "Iteration 23393 => Loss: 6.70701959790488366764\n",
      "Iteration 23394 => Loss: 6.70701840648272007428\n",
      "Iteration 23395 => Loss: 6.70701721522110538842\n",
      "Iteration 23396 => Loss: 6.70701602412001740561\n",
      "Iteration 23397 => Loss: 6.70701483317941438145\n",
      "Iteration 23398 => Loss: 6.70701364239930608591\n",
      "Iteration 23399 => Loss: 6.70701245177964988642\n",
      "Iteration 23400 => Loss: 6.70701126132042890760\n",
      "Iteration 23401 => Loss: 6.70701007102163071494\n",
      "Iteration 23402 => Loss: 6.70700888088321001135\n",
      "Iteration 23403 => Loss: 6.70700769090517301407\n",
      "Iteration 23404 => Loss: 6.70700650108748597233\n",
      "Iteration 23405 => Loss: 6.70700531143012312896\n",
      "Iteration 23406 => Loss: 6.70700412193306227948\n",
      "Iteration 23407 => Loss: 6.70700293259628743670\n",
      "Iteration 23408 => Loss: 6.70700174341978083703\n",
      "Iteration 23409 => Loss: 6.70700055440351228242\n",
      "Iteration 23410 => Loss: 6.70699936554746578565\n",
      "Iteration 23411 => Loss: 6.70699817685161647773\n",
      "Iteration 23412 => Loss: 6.70699698831594393056\n",
      "Iteration 23413 => Loss: 6.70699579994042860420\n",
      "Iteration 23414 => Loss: 6.70699461172504562967\n",
      "Iteration 23415 => Loss: 6.70699342366977457885\n",
      "Iteration 23416 => Loss: 6.70699223577459413548\n",
      "Iteration 23417 => Loss: 6.70699104803948920051\n",
      "Iteration 23418 => Loss: 6.70698986046441980591\n",
      "Iteration 23419 => Loss: 6.70698867304938239897\n",
      "Iteration 23420 => Loss: 6.70698748579434500527\n",
      "Iteration 23421 => Loss: 6.70698629869929963121\n",
      "Iteration 23422 => Loss: 6.70698511176421252600\n",
      "Iteration 23423 => Loss: 6.70698392498905615611\n",
      "Iteration 23424 => Loss: 6.70698273837382341611\n",
      "Iteration 23425 => Loss: 6.70698155191848766066\n",
      "Iteration 23426 => Loss: 6.70698036562302668528\n",
      "Iteration 23427 => Loss: 6.70697917948741562100\n",
      "Iteration 23428 => Loss: 6.70697799351164825055\n",
      "Iteration 23429 => Loss: 6.70697680769568105319\n",
      "Iteration 23430 => Loss: 6.70697562203950514714\n",
      "Iteration 23431 => Loss: 6.70697443654309477523\n",
      "Iteration 23432 => Loss: 6.70697325120643128571\n",
      "Iteration 23433 => Loss: 6.70697206602949513865\n",
      "Iteration 23434 => Loss: 6.70697088101225613599\n",
      "Iteration 23435 => Loss: 6.70696969615469740233\n",
      "Iteration 23436 => Loss: 6.70696851145681005590\n",
      "Iteration 23437 => Loss: 6.70696732691854791142\n",
      "Iteration 23438 => Loss: 6.70696614253990563981\n",
      "Iteration 23439 => Loss: 6.70696495832086370115\n",
      "Iteration 23440 => Loss: 6.70696377426138656830\n",
      "Iteration 23441 => Loss: 6.70696259036146891219\n",
      "Iteration 23442 => Loss: 6.70696140662108142294\n",
      "Iteration 23443 => Loss: 6.70696022304020100790\n",
      "Iteration 23444 => Loss: 6.70695903961880635080\n",
      "Iteration 23445 => Loss: 6.70695785635688235260\n",
      "Iteration 23446 => Loss: 6.70695667325439970341\n",
      "Iteration 23447 => Loss: 6.70695549031133797513\n",
      "Iteration 23448 => Loss: 6.70695430752768029237\n",
      "Iteration 23449 => Loss: 6.70695312490340711520\n",
      "Iteration 23450 => Loss: 6.70695194243848735738\n",
      "Iteration 23451 => Loss: 6.70695076013290947259\n",
      "Iteration 23452 => Loss: 6.70694957798663882187\n",
      "Iteration 23453 => Loss: 6.70694839599967096433\n",
      "Iteration 23454 => Loss: 6.70694721417197037283\n",
      "Iteration 23455 => Loss: 6.70694603250352372470\n",
      "Iteration 23456 => Loss: 6.70694485099430615094\n",
      "Iteration 23457 => Loss: 6.70694366964429988798\n",
      "Iteration 23458 => Loss: 6.70694248845348006682\n",
      "Iteration 23459 => Loss: 6.70694130742182714755\n",
      "Iteration 23460 => Loss: 6.70694012654931359663\n",
      "Iteration 23461 => Loss: 6.70693894583592076231\n",
      "Iteration 23462 => Loss: 6.70693776528163354556\n",
      "Iteration 23463 => Loss: 6.70693658488642885374\n",
      "Iteration 23464 => Loss: 6.70693540465027648878\n",
      "Iteration 23465 => Loss: 6.70693422457315779894\n",
      "Iteration 23466 => Loss: 6.70693304465506301426\n",
      "Iteration 23467 => Loss: 6.70693186489596460120\n",
      "Iteration 23468 => Loss: 6.70693068529583324988\n",
      "Iteration 23469 => Loss: 6.70692950585465386126\n",
      "Iteration 23470 => Loss: 6.70692832657240423089\n",
      "Iteration 23471 => Loss: 6.70692714744906215429\n",
      "Iteration 23472 => Loss: 6.70692596848461253245\n",
      "Iteration 23473 => Loss: 6.70692478967902072640\n",
      "Iteration 23474 => Loss: 6.70692361103227785435\n",
      "Iteration 23475 => Loss: 6.70692243254435815913\n",
      "Iteration 23476 => Loss: 6.70692125421523055451\n",
      "Iteration 23477 => Loss: 6.70692007604488704686\n",
      "Iteration 23478 => Loss: 6.70691889803330631992\n",
      "Iteration 23479 => Loss: 6.70691772018046616921\n",
      "Iteration 23480 => Loss: 6.70691654248633373214\n",
      "Iteration 23481 => Loss: 6.70691536495089835057\n",
      "Iteration 23482 => Loss: 6.70691418757413870821\n",
      "Iteration 23483 => Loss: 6.70691301035602993608\n",
      "Iteration 23484 => Loss: 6.70691183329654982970\n",
      "Iteration 23485 => Loss: 6.70691065639568062551\n",
      "Iteration 23486 => Loss: 6.70690947965339567816\n",
      "Iteration 23487 => Loss: 6.70690830306968077679\n",
      "Iteration 23488 => Loss: 6.70690712664450661151\n",
      "Iteration 23489 => Loss: 6.70690595037785097787\n",
      "Iteration 23490 => Loss: 6.70690477426970588226\n",
      "Iteration 23491 => Loss: 6.70690359832004379115\n",
      "Iteration 23492 => Loss: 6.70690242252883628282\n",
      "Iteration 23493 => Loss: 6.70690124689606381736\n",
      "Iteration 23494 => Loss: 6.70690007142171307208\n",
      "Iteration 23495 => Loss: 6.70689889610576006618\n",
      "Iteration 23496 => Loss: 6.70689772094817371340\n",
      "Iteration 23497 => Loss: 6.70689654594894424378\n",
      "Iteration 23498 => Loss: 6.70689537110804057107\n",
      "Iteration 23499 => Loss: 6.70689419642545381350\n",
      "Iteration 23500 => Loss: 6.70689302190115022029\n",
      "Iteration 23501 => Loss: 6.70689184753511380421\n",
      "Iteration 23502 => Loss: 6.70689067332732768989\n",
      "Iteration 23503 => Loss: 6.70688949927775990290\n",
      "Iteration 23504 => Loss: 6.70688832538640067327\n",
      "Iteration 23505 => Loss: 6.70688715165322602019\n",
      "Iteration 23506 => Loss: 6.70688597807821107466\n",
      "Iteration 23507 => Loss: 6.70688480466132830315\n",
      "Iteration 23508 => Loss: 6.70688363140257060024\n",
      "Iteration 23509 => Loss: 6.70688245830190421515\n",
      "Iteration 23510 => Loss: 6.70688128535931937790\n",
      "Iteration 23511 => Loss: 6.70688011257478144955\n",
      "Iteration 23512 => Loss: 6.70687893994828243649\n",
      "Iteration 23513 => Loss: 6.70687776747978681158\n",
      "Iteration 23514 => Loss: 6.70687659516929102210\n",
      "Iteration 23515 => Loss: 6.70687542301675865275\n",
      "Iteration 23516 => Loss: 6.70687425102217638084\n",
      "Iteration 23517 => Loss: 6.70687307918552111374\n",
      "Iteration 23518 => Loss: 6.70687190750676620610\n",
      "Iteration 23519 => Loss: 6.70687073598590188794\n",
      "Iteration 23520 => Loss: 6.70686956462289796121\n",
      "Iteration 23521 => Loss: 6.70686839341773399781\n",
      "Iteration 23522 => Loss: 6.70686722237038956962\n",
      "Iteration 23523 => Loss: 6.70686605148084602490\n",
      "Iteration 23524 => Loss: 6.70686488074907583012\n",
      "Iteration 23525 => Loss: 6.70686371017506743897\n",
      "Iteration 23526 => Loss: 6.70686253975879065337\n",
      "Iteration 23527 => Loss: 6.70686136950023126246\n",
      "Iteration 23528 => Loss: 6.70686019939935285095\n",
      "Iteration 23529 => Loss: 6.70685902945615541881\n",
      "Iteration 23530 => Loss: 6.70685785967060432711\n",
      "Iteration 23531 => Loss: 6.70685669004268625315\n",
      "Iteration 23532 => Loss: 6.70685552057237366341\n",
      "Iteration 23533 => Loss: 6.70685435125964701797\n",
      "Iteration 23534 => Loss: 6.70685318210448411236\n",
      "Iteration 23535 => Loss: 6.70685201310686451848\n",
      "Iteration 23536 => Loss: 6.70685084426676692004\n",
      "Iteration 23537 => Loss: 6.70684967558417621802\n",
      "Iteration 23538 => Loss: 6.70684850705905777346\n",
      "Iteration 23539 => Loss: 6.70684733869140448093\n",
      "Iteration 23540 => Loss: 6.70684617048118703053\n",
      "Iteration 23541 => Loss: 6.70684500242838410600\n",
      "Iteration 23542 => Loss: 6.70684383453298416100\n",
      "Iteration 23543 => Loss: 6.70684266679494545116\n",
      "Iteration 23544 => Loss: 6.70684149921426442376\n",
      "Iteration 23545 => Loss: 6.70684033179091976251\n",
      "Iteration 23546 => Loss: 6.70683916452488038118\n",
      "Iteration 23547 => Loss: 6.70683799741613118073\n",
      "Iteration 23548 => Loss: 6.70683683046465084487\n",
      "Iteration 23549 => Loss: 6.70683566367041983369\n",
      "Iteration 23550 => Loss: 6.70683449703341505455\n",
      "Iteration 23551 => Loss: 6.70683333055360986208\n",
      "Iteration 23552 => Loss: 6.70683216423099270997\n",
      "Iteration 23553 => Loss: 6.70683099806552629474\n",
      "Iteration 23554 => Loss: 6.70682983205720972819\n",
      "Iteration 23555 => Loss: 6.70682866620601636498\n",
      "Iteration 23556 => Loss: 6.70682750051191600704\n",
      "Iteration 23557 => Loss: 6.70682633497489355534\n",
      "Iteration 23558 => Loss: 6.70682516959493035813\n",
      "Iteration 23559 => Loss: 6.70682400437199799370\n",
      "Iteration 23560 => Loss: 6.70682283930608669209\n",
      "Iteration 23561 => Loss: 6.70682167439716181434\n",
      "Iteration 23562 => Loss: 6.70682050964520204417\n",
      "Iteration 23563 => Loss: 6.70681934505020560522\n",
      "Iteration 23564 => Loss: 6.70681818061212986493\n",
      "Iteration 23565 => Loss: 6.70681701633096682968\n",
      "Iteration 23566 => Loss: 6.70681585220668896596\n",
      "Iteration 23567 => Loss: 6.70681468823927495748\n",
      "Iteration 23568 => Loss: 6.70681352442870704067\n",
      "Iteration 23569 => Loss: 6.70681236077496656378\n",
      "Iteration 23570 => Loss: 6.70681119727802421693\n",
      "Iteration 23571 => Loss: 6.70681003393786667743\n",
      "Iteration 23572 => Loss: 6.70680887075446729995\n",
      "Iteration 23573 => Loss: 6.70680770772780476818\n",
      "Iteration 23574 => Loss: 6.70680654485786487129\n",
      "Iteration 23575 => Loss: 6.70680538214461741120\n",
      "Iteration 23576 => Loss: 6.70680421958804995342\n",
      "Iteration 23577 => Loss: 6.70680305718812874716\n",
      "Iteration 23578 => Loss: 6.70680189494485023971\n",
      "Iteration 23579 => Loss: 6.70680073285818068030\n",
      "Iteration 23580 => Loss: 6.70679957092810585806\n",
      "Iteration 23581 => Loss: 6.70679840915459113404\n",
      "Iteration 23582 => Loss: 6.70679724753763384371\n",
      "Iteration 23583 => Loss: 6.70679608607720378899\n",
      "Iteration 23584 => Loss: 6.70679492477328231814\n",
      "Iteration 23585 => Loss: 6.70679376362584545035\n",
      "Iteration 23586 => Loss: 6.70679260263486920479\n",
      "Iteration 23587 => Loss: 6.70679144180033848244\n",
      "Iteration 23588 => Loss: 6.70679028112223196700\n",
      "Iteration 23589 => Loss: 6.70678912060052656585\n",
      "Iteration 23590 => Loss: 6.70678796023520096270\n",
      "Iteration 23591 => Loss: 6.70678680002623384127\n",
      "Iteration 23592 => Loss: 6.70678563997360566162\n",
      "Iteration 23593 => Loss: 6.70678448007729155478\n",
      "Iteration 23594 => Loss: 6.70678332033727997441\n",
      "Iteration 23595 => Loss: 6.70678216075354338699\n",
      "Iteration 23596 => Loss: 6.70678100132605425898\n",
      "Iteration 23597 => Loss: 6.70677984205479926771\n",
      "Iteration 23598 => Loss: 6.70677868293975620873\n",
      "Iteration 23599 => Loss: 6.70677752398091264752\n",
      "Iteration 23600 => Loss: 6.70677636517823483331\n",
      "Iteration 23601 => Loss: 6.70677520653169256803\n",
      "Iteration 23602 => Loss: 6.70677404804129384530\n",
      "Iteration 23603 => Loss: 6.70677288970699514437\n",
      "Iteration 23604 => Loss: 6.70677173152878758344\n",
      "Iteration 23605 => Loss: 6.70677057350663652358\n",
      "Iteration 23606 => Loss: 6.70676941564053308298\n",
      "Iteration 23607 => Loss: 6.70676825793045328084\n",
      "Iteration 23608 => Loss: 6.70676710037637846540\n",
      "Iteration 23609 => Loss: 6.70676594297827488589\n",
      "Iteration 23610 => Loss: 6.70676478573613898959\n",
      "Iteration 23611 => Loss: 6.70676362864993613755\n",
      "Iteration 23612 => Loss: 6.70676247171965389526\n",
      "Iteration 23613 => Loss: 6.70676131494526739374\n",
      "Iteration 23614 => Loss: 6.70676015832675354034\n",
      "Iteration 23615 => Loss: 6.70675900186409723602\n",
      "Iteration 23616 => Loss: 6.70675784555727805269\n",
      "Iteration 23617 => Loss: 6.70675668940626401593\n",
      "Iteration 23618 => Loss: 6.70675553341104357941\n",
      "Iteration 23619 => Loss: 6.70675437757159631502\n",
      "Iteration 23620 => Loss: 6.70675322188789735378\n",
      "Iteration 23621 => Loss: 6.70675206635992182669\n",
      "Iteration 23622 => Loss: 6.70675091098765552289\n",
      "Iteration 23623 => Loss: 6.70674975577108511970\n",
      "Iteration 23624 => Loss: 6.70674860071017064911\n",
      "Iteration 23625 => Loss: 6.70674744580490145296\n",
      "Iteration 23626 => Loss: 6.70674629105526332040\n",
      "Iteration 23627 => Loss: 6.70674513646121628341\n",
      "Iteration 23628 => Loss: 6.70674398202276300651\n",
      "Iteration 23629 => Loss: 6.70674282773986707440\n",
      "Iteration 23630 => Loss: 6.70674167361250450625\n",
      "Iteration 23631 => Loss: 6.70674051964065753850\n",
      "Iteration 23632 => Loss: 6.70673936582431728937\n",
      "Iteration 23633 => Loss: 6.70673821216344911988\n",
      "Iteration 23634 => Loss: 6.70673705865804592463\n",
      "Iteration 23635 => Loss: 6.70673590530806951193\n",
      "Iteration 23636 => Loss: 6.70673475211350744729\n",
      "Iteration 23637 => Loss: 6.70673359907434196714\n",
      "Iteration 23638 => Loss: 6.70673244619054287341\n",
      "Iteration 23639 => Loss: 6.70673129346209595525\n",
      "Iteration 23640 => Loss: 6.70673014088899055452\n",
      "Iteration 23641 => Loss: 6.70672898847117782140\n",
      "Iteration 23642 => Loss: 6.70672783620866219678\n",
      "Iteration 23643 => Loss: 6.70672668410141703532\n",
      "Iteration 23644 => Loss: 6.70672553214941480348\n",
      "Iteration 23645 => Loss: 6.70672438035263596134\n",
      "Iteration 23646 => Loss: 6.70672322871106452169\n",
      "Iteration 23647 => Loss: 6.70672207722467650370\n",
      "Iteration 23648 => Loss: 6.70672092589344970293\n",
      "Iteration 23649 => Loss: 6.70671977471736280307\n",
      "Iteration 23650 => Loss: 6.70671862369640248147\n",
      "Iteration 23651 => Loss: 6.70671747283054120459\n",
      "Iteration 23652 => Loss: 6.70671632211976387339\n",
      "Iteration 23653 => Loss: 6.70671517156403140802\n",
      "Iteration 23654 => Loss: 6.70671402116334647303\n",
      "Iteration 23655 => Loss: 6.70671287091767354127\n",
      "Iteration 23656 => Loss: 6.70671172082700284278\n",
      "Iteration 23657 => Loss: 6.70671057089129885043\n",
      "Iteration 23658 => Loss: 6.70670942111055534696\n",
      "Iteration 23659 => Loss: 6.70670827148473769341\n",
      "Iteration 23660 => Loss: 6.70670712201383967255\n",
      "Iteration 23661 => Loss: 6.70670597269782842176\n",
      "Iteration 23662 => Loss: 6.70670482353669150655\n",
      "Iteration 23663 => Loss: 6.70670367453040228156\n",
      "Iteration 23664 => Loss: 6.70670252567893943052\n",
      "Iteration 23665 => Loss: 6.70670137698228341350\n",
      "Iteration 23666 => Loss: 6.70670022844042090782\n",
      "Iteration 23667 => Loss: 6.70669908005331549816\n",
      "Iteration 23668 => Loss: 6.70669793182096807271\n",
      "Iteration 23669 => Loss: 6.70669678374333155801\n",
      "Iteration 23670 => Loss: 6.70669563582040950678\n",
      "Iteration 23671 => Loss: 6.70669448805216639187\n",
      "Iteration 23672 => Loss: 6.70669334043858444971\n",
      "Iteration 23673 => Loss: 6.70669219297963969950\n",
      "Iteration 23674 => Loss: 6.70669104567532237127\n",
      "Iteration 23675 => Loss: 6.70668989852559960241\n",
      "Iteration 23676 => Loss: 6.70668875153044918846\n",
      "Iteration 23677 => Loss: 6.70668760468986935308\n",
      "Iteration 23678 => Loss: 6.70668645800381479916\n",
      "Iteration 23679 => Loss: 6.70668531147228552669\n",
      "Iteration 23680 => Loss: 6.70668416509525044944\n",
      "Iteration 23681 => Loss: 6.70668301887268647477\n",
      "Iteration 23682 => Loss: 6.70668187280457406274\n",
      "Iteration 23683 => Loss: 6.70668072689090255523\n",
      "Iteration 23684 => Loss: 6.70667958113163464873\n",
      "Iteration 23685 => Loss: 6.70667843552676590235\n",
      "Iteration 23686 => Loss: 6.70667729007625723625\n",
      "Iteration 23687 => Loss: 6.70667614478010243317\n",
      "Iteration 23688 => Loss: 6.70667499963828106502\n",
      "Iteration 23689 => Loss: 6.70667385465076559825\n",
      "Iteration 23690 => Loss: 6.70667270981753471659\n",
      "Iteration 23691 => Loss: 6.70667156513856710376\n",
      "Iteration 23692 => Loss: 6.70667042061385121343\n",
      "Iteration 23693 => Loss: 6.70666927624335595937\n",
      "Iteration 23694 => Loss: 6.70666813202707068342\n",
      "Iteration 23695 => Loss: 6.70666698796496429935\n",
      "Iteration 23696 => Loss: 6.70666584405701904359\n",
      "Iteration 23697 => Loss: 6.70666470030321271167\n",
      "Iteration 23698 => Loss: 6.70666355670353464546\n",
      "Iteration 23699 => Loss: 6.70666241325795819961\n",
      "Iteration 23700 => Loss: 6.70666126996645495240\n",
      "Iteration 23701 => Loss: 6.70666012682901158115\n",
      "Iteration 23702 => Loss: 6.70665898384560499323\n",
      "Iteration 23703 => Loss: 6.70665784101622186597\n",
      "Iteration 23704 => Loss: 6.70665669834082489587\n",
      "Iteration 23705 => Loss: 6.70665555581940964203\n",
      "Iteration 23706 => Loss: 6.70665441345195123546\n",
      "Iteration 23707 => Loss: 6.70665327123842480717\n",
      "Iteration 23708 => Loss: 6.70665212917880992904\n",
      "Iteration 23709 => Loss: 6.70665098727309061388\n",
      "Iteration 23710 => Loss: 6.70664984552124288086\n",
      "Iteration 23711 => Loss: 6.70664870392324541370\n",
      "Iteration 23712 => Loss: 6.70664756247907511977\n",
      "Iteration 23713 => Loss: 6.70664642118871423548\n",
      "Iteration 23714 => Loss: 6.70664528005214233275\n",
      "Iteration 23715 => Loss: 6.70664413906935230614\n",
      "Iteration 23716 => Loss: 6.70664299824029885855\n",
      "Iteration 23717 => Loss: 6.70664185756496955548\n",
      "Iteration 23718 => Loss: 6.70664071704335107427\n",
      "Iteration 23719 => Loss: 6.70663957667541410501\n",
      "Iteration 23720 => Loss: 6.70663843646114443686\n",
      "Iteration 23721 => Loss: 6.70663729640051808900\n",
      "Iteration 23722 => Loss: 6.70663615649351818604\n",
      "Iteration 23723 => Loss: 6.70663501674011808262\n",
      "Iteration 23724 => Loss: 6.70663387714029912701\n",
      "Iteration 23725 => Loss: 6.70663273769403822655\n",
      "Iteration 23726 => Loss: 6.70663159840132383493\n",
      "Iteration 23727 => Loss: 6.70663045926212664227\n",
      "Iteration 23728 => Loss: 6.70662932027643154953\n",
      "Iteration 23729 => Loss: 6.70662818144420480593\n",
      "Iteration 23730 => Loss: 6.70662704276544996418\n",
      "Iteration 23731 => Loss: 6.70662590424011817447\n",
      "Iteration 23732 => Loss: 6.70662476586821210134\n",
      "Iteration 23733 => Loss: 6.70662362764970243489\n",
      "Iteration 23734 => Loss: 6.70662248958455986525\n",
      "Iteration 23735 => Loss: 6.70662135167277995151\n",
      "Iteration 23736 => Loss: 6.70662021391432805473\n",
      "Iteration 23737 => Loss: 6.70661907630919884582\n",
      "Iteration 23738 => Loss: 6.70661793885735058041\n",
      "Iteration 23739 => Loss: 6.70661680155877526488\n",
      "Iteration 23740 => Loss: 6.70661566441346046474\n",
      "Iteration 23741 => Loss: 6.70661452742136177108\n",
      "Iteration 23742 => Loss: 6.70661339058248362477\n",
      "Iteration 23743 => Loss: 6.70661225389679760411\n",
      "Iteration 23744 => Loss: 6.70661111736426907015\n",
      "Iteration 23745 => Loss: 6.70660998098489535835\n",
      "Iteration 23746 => Loss: 6.70660884475864360610\n",
      "Iteration 23747 => Loss: 6.70660770868549960255\n",
      "Iteration 23748 => Loss: 6.70660657276544824867\n",
      "Iteration 23749 => Loss: 6.70660543699845579368\n",
      "Iteration 23750 => Loss: 6.70660430138451246762\n",
      "Iteration 23751 => Loss: 6.70660316592358718424\n",
      "Iteration 23752 => Loss: 6.70660203061567372629\n",
      "Iteration 23753 => Loss: 6.70660089546073212574\n",
      "Iteration 23754 => Loss: 6.70659976045876060624\n",
      "Iteration 23755 => Loss: 6.70659862560972719336\n",
      "Iteration 23756 => Loss: 6.70659749091361856443\n",
      "Iteration 23757 => Loss: 6.70659635637040629774\n",
      "Iteration 23758 => Loss: 6.70659522198007884697\n",
      "Iteration 23759 => Loss: 6.70659408774260334951\n",
      "Iteration 23760 => Loss: 6.70659295365797003541\n",
      "Iteration 23761 => Loss: 6.70659181972615758838\n",
      "Iteration 23762 => Loss: 6.70659068594714469214\n",
      "Iteration 23763 => Loss: 6.70658955232089759591\n",
      "Iteration 23764 => Loss: 6.70658841884741274697\n",
      "Iteration 23765 => Loss: 6.70658728552666438816\n",
      "Iteration 23766 => Loss: 6.70658615235863386772\n",
      "Iteration 23767 => Loss: 6.70658501934329454031\n",
      "Iteration 23768 => Loss: 6.70658388648062864235\n",
      "Iteration 23769 => Loss: 6.70658275377061841027\n",
      "Iteration 23770 => Loss: 6.70658162121323986327\n",
      "Iteration 23771 => Loss: 6.70658048880847434958\n",
      "Iteration 23772 => Loss: 6.70657935655630144112\n",
      "Iteration 23773 => Loss: 6.70657822445669182798\n",
      "Iteration 23774 => Loss: 6.70657709250963929293\n",
      "Iteration 23775 => Loss: 6.70657596071511274971\n",
      "Iteration 23776 => Loss: 6.70657482907310065201\n",
      "Iteration 23777 => Loss: 6.70657369758357546630\n",
      "Iteration 23778 => Loss: 6.70657256624651143539\n",
      "Iteration 23779 => Loss: 6.70657143506190767113\n",
      "Iteration 23780 => Loss: 6.70657030402972331729\n",
      "Iteration 23781 => Loss: 6.70656917314995393298\n",
      "Iteration 23782 => Loss: 6.70656804242255777382\n",
      "Iteration 23783 => Loss: 6.70656691184753572799\n",
      "Iteration 23784 => Loss: 6.70656578142485404470\n",
      "Iteration 23785 => Loss: 6.70656465115450206582\n",
      "Iteration 23786 => Loss: 6.70656352103644870510\n",
      "Iteration 23787 => Loss: 6.70656239107068063987\n",
      "Iteration 23788 => Loss: 6.70656126125718010655\n",
      "Iteration 23789 => Loss: 6.70656013159592045980\n",
      "Iteration 23790 => Loss: 6.70655900208687949515\n",
      "Iteration 23791 => Loss: 6.70655787273004033722\n",
      "Iteration 23792 => Loss: 6.70655674352538433425\n",
      "Iteration 23793 => Loss: 6.70655561447288839361\n",
      "Iteration 23794 => Loss: 6.70655448557253386355\n",
      "Iteration 23795 => Loss: 6.70655335682429498689\n",
      "Iteration 23796 => Loss: 6.70655222822815755279\n",
      "Iteration 23797 => Loss: 6.70655109978409758043\n",
      "Iteration 23798 => Loss: 6.70654997149209552987\n",
      "Iteration 23799 => Loss: 6.70654884335213274937\n",
      "Iteration 23800 => Loss: 6.70654771536417726452\n",
      "Iteration 23801 => Loss: 6.70654658752822818713\n",
      "Iteration 23802 => Loss: 6.70654545984425265459\n",
      "Iteration 23803 => Loss: 6.70654433231223645606\n",
      "Iteration 23804 => Loss: 6.70654320493215116983\n",
      "Iteration 23805 => Loss: 6.70654207770397992050\n",
      "Iteration 23806 => Loss: 6.70654095062770760904\n",
      "Iteration 23807 => Loss: 6.70653982370329959650\n",
      "Iteration 23808 => Loss: 6.70653869693075410652\n",
      "Iteration 23809 => Loss: 6.70653757031003472378\n",
      "Iteration 23810 => Loss: 6.70653644384113434285\n",
      "Iteration 23811 => Loss: 6.70653531752401743660\n",
      "Iteration 23812 => Loss: 6.70653419135867601142\n",
      "Iteration 23813 => Loss: 6.70653306534508963921\n",
      "Iteration 23814 => Loss: 6.70653193948322723372\n",
      "Iteration 23815 => Loss: 6.70653081377307280775\n",
      "Iteration 23816 => Loss: 6.70652968821461836768\n",
      "Iteration 23817 => Loss: 6.70652856280782927456\n",
      "Iteration 23818 => Loss: 6.70652743755268332393\n",
      "Iteration 23819 => Loss: 6.70652631244917074582\n",
      "Iteration 23820 => Loss: 6.70652518749726223035\n",
      "Iteration 23821 => Loss: 6.70652406269694179031\n",
      "Iteration 23822 => Loss: 6.70652293804819255030\n",
      "Iteration 23823 => Loss: 6.70652181355098164772\n",
      "Iteration 23824 => Loss: 6.70652068920530286533\n",
      "Iteration 23825 => Loss: 6.70651956501112955777\n",
      "Iteration 23826 => Loss: 6.70651844096843952059\n",
      "Iteration 23827 => Loss: 6.70651731707721410203\n",
      "Iteration 23828 => Loss: 6.70651619333744086759\n",
      "Iteration 23829 => Loss: 6.70651506974908162562\n",
      "Iteration 23830 => Loss: 6.70651394631212838249\n",
      "Iteration 23831 => Loss: 6.70651282302656515100\n",
      "Iteration 23832 => Loss: 6.70651169989235551583\n",
      "Iteration 23833 => Loss: 6.70651057690949592427\n",
      "Iteration 23834 => Loss: 6.70650945407794640829\n",
      "Iteration 23835 => Loss: 6.70650833139771140878\n",
      "Iteration 23836 => Loss: 6.70650720886875273408\n",
      "Iteration 23837 => Loss: 6.70650608649105794967\n",
      "Iteration 23838 => Loss: 6.70650496426459330479\n",
      "Iteration 23839 => Loss: 6.70650384218935879943\n",
      "Iteration 23840 => Loss: 6.70650272026532157099\n",
      "Iteration 23841 => Loss: 6.70650159849246385591\n",
      "Iteration 23842 => Loss: 6.70650047687076433789\n",
      "Iteration 23843 => Loss: 6.70649935540020880609\n",
      "Iteration 23844 => Loss: 6.70649823408075551612\n",
      "Iteration 23845 => Loss: 6.70649711291241867883\n",
      "Iteration 23846 => Loss: 6.70649599189515299713\n",
      "Iteration 23847 => Loss: 6.70649487102893715473\n",
      "Iteration 23848 => Loss: 6.70649375031376315803\n",
      "Iteration 23849 => Loss: 6.70649262974960613803\n",
      "Iteration 23850 => Loss: 6.70649150933645366024\n",
      "Iteration 23851 => Loss: 6.70649038907426575662\n",
      "Iteration 23852 => Loss: 6.70648926896303798628\n",
      "Iteration 23853 => Loss: 6.70648814900274548023\n",
      "Iteration 23854 => Loss: 6.70648702919336248129\n",
      "Iteration 23855 => Loss: 6.70648590953488010769\n",
      "Iteration 23856 => Loss: 6.70648479002726993770\n",
      "Iteration 23857 => Loss: 6.70648367067050976686\n",
      "Iteration 23858 => Loss: 6.70648255146459160159\n",
      "Iteration 23859 => Loss: 6.70648143240947458565\n",
      "Iteration 23860 => Loss: 6.70648031350515871907\n",
      "Iteration 23861 => Loss: 6.70647919475161469194\n",
      "Iteration 23862 => Loss: 6.70647807614881763527\n",
      "Iteration 23863 => Loss: 6.70647695769675955546\n",
      "Iteration 23864 => Loss: 6.70647583939540936626\n",
      "Iteration 23865 => Loss: 6.70647472124474663957\n",
      "Iteration 23866 => Loss: 6.70647360324476249360\n",
      "Iteration 23867 => Loss: 6.70647248539542761847\n",
      "Iteration 23868 => Loss: 6.70647136769671892154\n",
      "Iteration 23869 => Loss: 6.70647025014862130377\n",
      "Iteration 23870 => Loss: 6.70646913275111522523\n",
      "Iteration 23871 => Loss: 6.70646801550417404059\n",
      "Iteration 23872 => Loss: 6.70646689840778620351\n",
      "Iteration 23873 => Loss: 6.70646578146192418046\n",
      "Iteration 23874 => Loss: 6.70646466466657376060\n",
      "Iteration 23875 => Loss: 6.70646354802171362763\n",
      "Iteration 23876 => Loss: 6.70646243152731624804\n",
      "Iteration 23877 => Loss: 6.70646131518337185184\n",
      "Iteration 23878 => Loss: 6.70646019898984757646\n",
      "Iteration 23879 => Loss: 6.70645908294673720462\n",
      "Iteration 23880 => Loss: 6.70645796705401053828\n",
      "Iteration 23881 => Loss: 6.70645685131164626114\n",
      "Iteration 23882 => Loss: 6.70645573571963460324\n",
      "Iteration 23883 => Loss: 6.70645462027794803106\n",
      "Iteration 23884 => Loss: 6.70645350498656789284\n",
      "Iteration 23885 => Loss: 6.70645238984547287231\n",
      "Iteration 23886 => Loss: 6.70645127485464076500\n",
      "Iteration 23887 => Loss: 6.70645016001406002459\n",
      "Iteration 23888 => Loss: 6.70644904532369778849\n",
      "Iteration 23889 => Loss: 6.70644793078354428673\n",
      "Iteration 23890 => Loss: 6.70644681639357020941\n",
      "Iteration 23891 => Loss: 6.70644570215376134570\n",
      "Iteration 23892 => Loss: 6.70644458806410259655\n",
      "Iteration 23893 => Loss: 6.70644347412456109936\n",
      "Iteration 23894 => Loss: 6.70644236033512886053\n",
      "Iteration 23895 => Loss: 6.70644124669577301745\n",
      "Iteration 23896 => Loss: 6.70644013320648202381\n",
      "Iteration 23897 => Loss: 6.70643901986723633968\n",
      "Iteration 23898 => Loss: 6.70643790667801020788\n",
      "Iteration 23899 => Loss: 6.70643679363878852939\n",
      "Iteration 23900 => Loss: 6.70643568074955176428\n",
      "Iteration 23901 => Loss: 6.70643456801027237901\n",
      "Iteration 23902 => Loss: 6.70643345542093349820\n",
      "Iteration 23903 => Loss: 6.70643234298152268735\n",
      "Iteration 23904 => Loss: 6.70643123069200797204\n",
      "Iteration 23905 => Loss: 6.70643011855237780594\n",
      "Iteration 23906 => Loss: 6.70642900656260021464\n",
      "Iteration 23907 => Loss: 6.70642789472267608630\n",
      "Iteration 23908 => Loss: 6.70642678303255923566\n",
      "Iteration 23909 => Loss: 6.70642567149225587997\n",
      "Iteration 23910 => Loss: 6.70642456010172072212\n",
      "Iteration 23911 => Loss: 6.70642344886094932122\n",
      "Iteration 23912 => Loss: 6.70642233776992036098\n",
      "Iteration 23913 => Loss: 6.70642122682860719607\n",
      "Iteration 23914 => Loss: 6.70642011603700449740\n",
      "Iteration 23915 => Loss: 6.70641900539506785606\n",
      "Iteration 23916 => Loss: 6.70641789490279371933\n",
      "Iteration 23917 => Loss: 6.70641678456016254728\n",
      "Iteration 23918 => Loss: 6.70641567436714236550\n",
      "Iteration 23919 => Loss: 6.70641456432373139762\n",
      "Iteration 23920 => Loss: 6.70641345442989234016\n",
      "Iteration 23921 => Loss: 6.70641234468561187043\n",
      "Iteration 23922 => Loss: 6.70641123509087133669\n",
      "Iteration 23923 => Loss: 6.70641012564564764631\n",
      "Iteration 23924 => Loss: 6.70640901634992125935\n",
      "Iteration 23925 => Loss: 6.70640790720366997135\n",
      "Iteration 23926 => Loss: 6.70640679820688312418\n",
      "Iteration 23927 => Loss: 6.70640568935953140794\n",
      "Iteration 23928 => Loss: 6.70640458066159350636\n",
      "Iteration 23929 => Loss: 6.70640347211305165587\n",
      "Iteration 23930 => Loss: 6.70640236371388986925\n",
      "Iteration 23931 => Loss: 6.70640125546408771839\n",
      "Iteration 23932 => Loss: 6.70640014736361589343\n",
      "Iteration 23933 => Loss: 6.70639903941246640073\n",
      "Iteration 23934 => Loss: 6.70639793161061170679\n",
      "Iteration 23935 => Loss: 6.70639682395803315984\n",
      "Iteration 23936 => Loss: 6.70639571645471299632\n",
      "Iteration 23937 => Loss: 6.70639460910061746546\n",
      "Iteration 23938 => Loss: 6.70639350189575722538\n",
      "Iteration 23939 => Loss: 6.70639239484007720904\n",
      "Iteration 23940 => Loss: 6.70639128793358363367\n",
      "Iteration 23941 => Loss: 6.70639018117623830761\n",
      "Iteration 23942 => Loss: 6.70638907456803234908\n",
      "Iteration 23943 => Loss: 6.70638796810894177725\n",
      "Iteration 23944 => Loss: 6.70638686179894616401\n",
      "Iteration 23945 => Loss: 6.70638575563802330493\n",
      "Iteration 23946 => Loss: 6.70638464962615543641\n",
      "Iteration 23947 => Loss: 6.70638354376333190032\n",
      "Iteration 23948 => Loss: 6.70638243804951361682\n",
      "Iteration 23949 => Loss: 6.70638133248468637504\n",
      "Iteration 23950 => Loss: 6.70638022706884839863\n",
      "Iteration 23951 => Loss: 6.70637912180195794321\n",
      "Iteration 23952 => Loss: 6.70637801668400435062\n",
      "Iteration 23953 => Loss: 6.70637691171496097553\n",
      "Iteration 23954 => Loss: 6.70637580689481271889\n",
      "Iteration 23955 => Loss: 6.70637470222353915261\n",
      "Iteration 23956 => Loss: 6.70637359770112428947\n",
      "Iteration 23957 => Loss: 6.70637249332754059594\n",
      "Iteration 23958 => Loss: 6.70637138910277652570\n",
      "Iteration 23959 => Loss: 6.70637028502679832798\n",
      "Iteration 23960 => Loss: 6.70636918109960422640\n",
      "Iteration 23961 => Loss: 6.70636807732115602931\n",
      "Iteration 23962 => Loss: 6.70636697369144751946\n",
      "Iteration 23963 => Loss: 6.70636587021044938695\n",
      "Iteration 23964 => Loss: 6.70636476687814742093\n",
      "Iteration 23965 => Loss: 6.70636366369451675240\n",
      "Iteration 23966 => Loss: 6.70636256065954583505\n",
      "Iteration 23967 => Loss: 6.70636145777320091810\n",
      "Iteration 23968 => Loss: 6.70636035503547756065\n",
      "Iteration 23969 => Loss: 6.70635925244634378828\n",
      "Iteration 23970 => Loss: 6.70635815000578450196\n",
      "Iteration 23971 => Loss: 6.70635704771378016176\n",
      "Iteration 23972 => Loss: 6.70635594557031300411\n",
      "Iteration 23973 => Loss: 6.70635484357535549549\n",
      "Iteration 23974 => Loss: 6.70635374172889431321\n",
      "Iteration 23975 => Loss: 6.70635264003090458829\n",
      "Iteration 23976 => Loss: 6.70635153848137033350\n",
      "Iteration 23977 => Loss: 6.70635043708027023257\n",
      "Iteration 23978 => Loss: 6.70634933582758208104\n",
      "Iteration 23979 => Loss: 6.70634823472329077987\n",
      "Iteration 23980 => Loss: 6.70634713376737057189\n",
      "Iteration 23981 => Loss: 6.70634603295979925264\n",
      "Iteration 23982 => Loss: 6.70634493230056971669\n",
      "Iteration 23983 => Loss: 6.70634383178965265415\n",
      "Iteration 23984 => Loss: 6.70634273142702852510\n",
      "Iteration 23985 => Loss: 6.70634163121268400687\n",
      "Iteration 23986 => Loss: 6.70634053114658623684\n",
      "Iteration 23987 => Loss: 6.70633943122872278053\n",
      "Iteration 23988 => Loss: 6.70633833145907942708\n",
      "Iteration 23989 => Loss: 6.70633723183762064934\n",
      "Iteration 23990 => Loss: 6.70633613236434111826\n",
      "Iteration 23991 => Loss: 6.70633503303921418848\n",
      "Iteration 23992 => Loss: 6.70633393386222298460\n",
      "Iteration 23993 => Loss: 6.70633283483334619035\n",
      "Iteration 23994 => Loss: 6.70633173595256781852\n",
      "Iteration 23995 => Loss: 6.70633063721985855921\n",
      "Iteration 23996 => Loss: 6.70632953863520331339\n",
      "Iteration 23997 => Loss: 6.70632844019858431750\n",
      "Iteration 23998 => Loss: 6.70632734190997226165\n",
      "Iteration 23999 => Loss: 6.70632624376936714583\n",
      "Iteration 24000 => Loss: 6.70632514577672989020\n",
      "Iteration 24001 => Loss: 6.70632404793204894844\n",
      "Iteration 24002 => Loss: 6.70632295023530122791\n",
      "Iteration 24003 => Loss: 6.70632185268646896503\n",
      "Iteration 24004 => Loss: 6.70632075528552817900\n",
      "Iteration 24005 => Loss: 6.70631965803247087621\n",
      "Iteration 24006 => Loss: 6.70631856092725620044\n",
      "Iteration 24007 => Loss: 6.70631746396988237535\n",
      "Iteration 24008 => Loss: 6.70631636716032453194\n",
      "Iteration 24009 => Loss: 6.70631527049856757117\n",
      "Iteration 24010 => Loss: 6.70631417398458307133\n",
      "Iteration 24011 => Loss: 6.70631307761834793979\n",
      "Iteration 24012 => Loss: 6.70631198139984707751\n",
      "Iteration 24013 => Loss: 6.70631088532906538546\n",
      "Iteration 24014 => Loss: 6.70630978940598332372\n",
      "Iteration 24015 => Loss: 6.70630869363057247057\n",
      "Iteration 24016 => Loss: 6.70630759800281861516\n",
      "Iteration 24017 => Loss: 6.70630650252269866485\n",
      "Iteration 24018 => Loss: 6.70630540719020107332\n",
      "Iteration 24019 => Loss: 6.70630431200529208979\n",
      "Iteration 24020 => Loss: 6.70630321696796283248\n",
      "Iteration 24021 => Loss: 6.70630212207819020875\n",
      "Iteration 24022 => Loss: 6.70630102733595201414\n",
      "Iteration 24023 => Loss: 6.70629993274123314961\n",
      "Iteration 24024 => Loss: 6.70629883829400963435\n",
      "Iteration 24025 => Loss: 6.70629774399426548115\n",
      "Iteration 24026 => Loss: 6.70629664984197404465\n",
      "Iteration 24027 => Loss: 6.70629555583712111400\n",
      "Iteration 24028 => Loss: 6.70629446197968448473\n",
      "Iteration 24029 => Loss: 6.70629336826964994600\n",
      "Iteration 24030 => Loss: 6.70629227470698818792\n",
      "Iteration 24031 => Loss: 6.70629118129168677598\n",
      "Iteration 24032 => Loss: 6.70629008802371906484\n",
      "Iteration 24033 => Loss: 6.70628899490307173181\n",
      "Iteration 24034 => Loss: 6.70628790192972612516\n",
      "Iteration 24035 => Loss: 6.70628680910364938228\n",
      "Iteration 24036 => Loss: 6.70628571642483972681\n",
      "Iteration 24037 => Loss: 6.70628462389326518434\n",
      "Iteration 24038 => Loss: 6.70628353150891065582\n",
      "Iteration 24039 => Loss: 6.70628243927175748951\n",
      "Iteration 24040 => Loss: 6.70628134718178081641\n",
      "Iteration 24041 => Loss: 6.70628025523896287297\n",
      "Iteration 24042 => Loss: 6.70627916344328411924\n",
      "Iteration 24043 => Loss: 6.70627807179472501531\n",
      "Iteration 24044 => Loss: 6.70627698029326868578\n",
      "Iteration 24045 => Loss: 6.70627588893888759713\n",
      "Iteration 24046 => Loss: 6.70627479773156665033\n",
      "Iteration 24047 => Loss: 6.70627370667129252269\n",
      "Iteration 24048 => Loss: 6.70627261575803057525\n",
      "Iteration 24049 => Loss: 6.70627152499176926170\n",
      "Iteration 24050 => Loss: 6.70627043437249348301\n",
      "Iteration 24051 => Loss: 6.70626934390017925836\n",
      "Iteration 24052 => Loss: 6.70626825357480438328\n",
      "Iteration 24053 => Loss: 6.70626716339634931785\n",
      "Iteration 24054 => Loss: 6.70626607336479718668\n",
      "Iteration 24055 => Loss: 6.70626498348012844986\n",
      "Iteration 24056 => Loss: 6.70626389374231912655\n",
      "Iteration 24057 => Loss: 6.70626280415135944679\n",
      "Iteration 24058 => Loss: 6.70626171470721477164\n",
      "Iteration 24059 => Loss: 6.70626062540987355476\n",
      "Iteration 24060 => Loss: 6.70625953625931447988\n",
      "Iteration 24061 => Loss: 6.70625844725551978343\n",
      "Iteration 24062 => Loss: 6.70625735839846637276\n",
      "Iteration 24063 => Loss: 6.70625626968814625428\n",
      "Iteration 24064 => Loss: 6.70625518112451679542\n",
      "Iteration 24065 => Loss: 6.70625409270758066071\n",
      "Iteration 24066 => Loss: 6.70625300443730054667\n",
      "Iteration 24067 => Loss: 6.70625191631367378875\n",
      "Iteration 24068 => Loss: 6.70625082833666397164\n",
      "Iteration 24069 => Loss: 6.70624974050626576627\n",
      "Iteration 24070 => Loss: 6.70624865282244986275\n",
      "Iteration 24071 => Loss: 6.70624756528519938570\n",
      "Iteration 24072 => Loss: 6.70624647789449745972\n",
      "Iteration 24073 => Loss: 6.70624539065032010399\n",
      "Iteration 24074 => Loss: 6.70624430355264689041\n",
      "Iteration 24075 => Loss: 6.70624321660145916724\n",
      "Iteration 24076 => Loss: 6.70624212979673917090\n",
      "Iteration 24077 => Loss: 6.70624104313847002601\n",
      "Iteration 24078 => Loss: 6.70623995662662775175\n",
      "Iteration 24079 => Loss: 6.70623887026118481458\n",
      "Iteration 24080 => Loss: 6.70623778404213322091\n",
      "Iteration 24081 => Loss: 6.70623669796945343080\n",
      "Iteration 24082 => Loss: 6.70623561204312235162\n",
      "Iteration 24083 => Loss: 6.70623452626311777891\n",
      "Iteration 24084 => Loss: 6.70623344062942106092\n",
      "Iteration 24085 => Loss: 6.70623235514201621044\n",
      "Iteration 24086 => Loss: 6.70623126980088013482\n",
      "Iteration 24087 => Loss: 6.70623018460599418233\n",
      "Iteration 24088 => Loss: 6.70622909955733348397\n",
      "Iteration 24089 => Loss: 6.70622801465488382888\n",
      "Iteration 24090 => Loss: 6.70622692989863278257\n",
      "Iteration 24091 => Loss: 6.70622584528854837060\n",
      "Iteration 24092 => Loss: 6.70622476082461904667\n",
      "Iteration 24093 => Loss: 6.70622367650681194817\n",
      "Iteration 24094 => Loss: 6.70622259233512441057\n",
      "Iteration 24095 => Loss: 6.70622150830952445943\n",
      "Iteration 24096 => Loss: 6.70622042443000054845\n",
      "Iteration 24097 => Loss: 6.70621934069652603227\n",
      "Iteration 24098 => Loss: 6.70621825710909114093\n",
      "Iteration 24099 => Loss: 6.70621717366766212365\n",
      "Iteration 24100 => Loss: 6.70621609037223276317\n",
      "Iteration 24101 => Loss: 6.70621500722277996687\n",
      "Iteration 24102 => Loss: 6.70621392421927176031\n",
      "Iteration 24103 => Loss: 6.70621284136170103807\n",
      "Iteration 24104 => Loss: 6.70621175865004826022\n",
      "Iteration 24105 => Loss: 6.70621067608428855777\n",
      "Iteration 24106 => Loss: 6.70620959366441127258\n",
      "Iteration 24107 => Loss: 6.70620851139038709476\n",
      "Iteration 24108 => Loss: 6.70620742926219470803\n",
      "Iteration 24109 => Loss: 6.70620634727982167789\n",
      "Iteration 24110 => Loss: 6.70620526544324935259\n",
      "Iteration 24111 => Loss: 6.70620418375244931042\n",
      "Iteration 24112 => Loss: 6.70620310220741000506\n",
      "Iteration 24113 => Loss: 6.70620202080811012024\n",
      "Iteration 24114 => Loss: 6.70620093955452389878\n",
      "Iteration 24115 => Loss: 6.70619985844664068253\n",
      "Iteration 24116 => Loss: 6.70619877748443649068\n",
      "Iteration 24117 => Loss: 6.70619769666789355966\n",
      "Iteration 24118 => Loss: 6.70619661599699057319\n",
      "Iteration 24119 => Loss: 6.70619553547170710317\n",
      "Iteration 24120 => Loss: 6.70619445509201650424\n",
      "Iteration 24121 => Loss: 6.70619337485791877640\n",
      "Iteration 24122 => Loss: 6.70619229476938105705\n",
      "Iteration 24123 => Loss: 6.70619121482637758902\n",
      "Iteration 24124 => Loss: 6.70619013502890481959\n",
      "Iteration 24125 => Loss: 6.70618905537693432706\n",
      "Iteration 24126 => Loss: 6.70618797587044124242\n",
      "Iteration 24127 => Loss: 6.70618689650941846025\n",
      "Iteration 24128 => Loss: 6.70618581729383311796\n",
      "Iteration 24129 => Loss: 6.70618473822367278103\n",
      "Iteration 24130 => Loss: 6.70618365929891790955\n",
      "Iteration 24131 => Loss: 6.70618258051955073995\n",
      "Iteration 24132 => Loss: 6.70618150188554995594\n",
      "Iteration 24133 => Loss: 6.70618042339689512943\n",
      "Iteration 24134 => Loss: 6.70617934505356405595\n",
      "Iteration 24135 => Loss: 6.70617826685554163646\n",
      "Iteration 24136 => Loss: 6.70617718880280833105\n",
      "Iteration 24137 => Loss: 6.70617611089534015889\n",
      "Iteration 24138 => Loss: 6.70617503313312024460\n",
      "Iteration 24139 => Loss: 6.70617395551612283100\n",
      "Iteration 24140 => Loss: 6.70617287804434436538\n",
      "Iteration 24141 => Loss: 6.70617180071775109695\n",
      "Iteration 24142 => Loss: 6.70617072353633147941\n",
      "Iteration 24143 => Loss: 6.70616964650006153192\n",
      "Iteration 24144 => Loss: 6.70616856960891460915\n",
      "Iteration 24145 => Loss: 6.70616749286287650023\n",
      "Iteration 24146 => Loss: 6.70616641626194187609\n",
      "Iteration 24147 => Loss: 6.70616533980607698595\n",
      "Iteration 24148 => Loss: 6.70616426349525784900\n",
      "Iteration 24149 => Loss: 6.70616318732947558345\n",
      "Iteration 24150 => Loss: 6.70616211130870443213\n",
      "Iteration 24151 => Loss: 6.70616103543293551326\n",
      "Iteration 24152 => Loss: 6.70615995970213685240\n",
      "Iteration 24153 => Loss: 6.70615888411628802146\n",
      "Iteration 24154 => Loss: 6.70615780867538191501\n",
      "Iteration 24155 => Loss: 6.70615673337938567045\n",
      "Iteration 24156 => Loss: 6.70615565822827974785\n",
      "Iteration 24157 => Loss: 6.70615458322206414721\n",
      "Iteration 24158 => Loss: 6.70615350836069978868\n",
      "Iteration 24159 => Loss: 6.70615243364417423777\n",
      "Iteration 24160 => Loss: 6.70615135907246173730\n",
      "Iteration 24161 => Loss: 6.70615028464555429366\n",
      "Iteration 24162 => Loss: 6.70614921036342703786\n",
      "Iteration 24163 => Loss: 6.70614813622605332455\n",
      "Iteration 24164 => Loss: 6.70614706223342427194\n",
      "Iteration 24165 => Loss: 6.70614598838551412285\n",
      "Iteration 24166 => Loss: 6.70614491468229978466\n",
      "Iteration 24167 => Loss: 6.70614384112377326375\n",
      "Iteration 24168 => Loss: 6.70614276770990702659\n",
      "Iteration 24169 => Loss: 6.70614169444068419779\n",
      "Iteration 24170 => Loss: 6.70614062131608879014\n",
      "Iteration 24171 => Loss: 6.70613954833608794104\n",
      "Iteration 24172 => Loss: 6.70613847550067543324\n",
      "Iteration 24173 => Loss: 6.70613740280983083863\n",
      "Iteration 24174 => Loss: 6.70613633026352218280\n",
      "Iteration 24175 => Loss: 6.70613525786175035392\n",
      "Iteration 24176 => Loss: 6.70613418560448071304\n",
      "Iteration 24177 => Loss: 6.70613311349169283204\n",
      "Iteration 24178 => Loss: 6.70613204152338138186\n",
      "Iteration 24179 => Loss: 6.70613096969950728266\n",
      "Iteration 24180 => Loss: 6.70612989802006609352\n",
      "Iteration 24181 => Loss: 6.70612882648504182725\n",
      "Iteration 24182 => Loss: 6.70612775509439451582\n",
      "Iteration 24183 => Loss: 6.70612668384812327105\n",
      "Iteration 24184 => Loss: 6.70612561274620144758\n",
      "Iteration 24185 => Loss: 6.70612454178861305820\n",
      "Iteration 24186 => Loss: 6.70612347097533412210\n",
      "Iteration 24187 => Loss: 6.70612240030634776389\n",
      "Iteration 24188 => Loss: 6.70612132978163355546\n",
      "Iteration 24189 => Loss: 6.70612025940117284506\n",
      "Iteration 24190 => Loss: 6.70611918916494520460\n",
      "Iteration 24191 => Loss: 6.70611811907293109414\n",
      "Iteration 24192 => Loss: 6.70611704912511275012\n",
      "Iteration 24193 => Loss: 6.70611597932147152079\n",
      "Iteration 24194 => Loss: 6.70611490966198520169\n",
      "Iteration 24195 => Loss: 6.70611384014663691744\n",
      "Iteration 24196 => Loss: 6.70611277077540623992\n",
      "Iteration 24197 => Loss: 6.70611170154827274104\n",
      "Iteration 24198 => Loss: 6.70611063246521421632\n",
      "Iteration 24199 => Loss: 6.70610956352621734311\n",
      "Iteration 24200 => Loss: 6.70610849473126258147\n",
      "Iteration 24201 => Loss: 6.70610742608032417422\n",
      "Iteration 24202 => Loss: 6.70610635757339057506\n",
      "Iteration 24203 => Loss: 6.70610528921043247408\n",
      "Iteration 24204 => Loss: 6.70610422099144365404\n",
      "Iteration 24205 => Loss: 6.70610315291639569324\n",
      "Iteration 24206 => Loss: 6.70610208498526638721\n",
      "Iteration 24207 => Loss: 6.70610101719804152509\n",
      "Iteration 24208 => Loss: 6.70609994955470423150\n",
      "Iteration 24209 => Loss: 6.70609888205523319016\n",
      "Iteration 24210 => Loss: 6.70609781469960530842\n",
      "Iteration 24211 => Loss: 6.70609674748780282272\n",
      "Iteration 24212 => Loss: 6.70609568041980885766\n",
      "Iteration 24213 => Loss: 6.70609461349560209698\n",
      "Iteration 24214 => Loss: 6.70609354671516211255\n",
      "Iteration 24215 => Loss: 6.70609248007847202899\n",
      "Iteration 24216 => Loss: 6.70609141358551497092\n",
      "Iteration 24217 => Loss: 6.70609034723625718755\n",
      "Iteration 24218 => Loss: 6.70608928103069690252\n",
      "Iteration 24219 => Loss: 6.70608821496881102320\n",
      "Iteration 24220 => Loss: 6.70608714905057645694\n",
      "Iteration 24221 => Loss: 6.70608608327596655840\n",
      "Iteration 24222 => Loss: 6.70608501764497777486\n",
      "Iteration 24223 => Loss: 6.70608395215758523733\n",
      "Iteration 24224 => Loss: 6.70608288681375608320\n",
      "Iteration 24225 => Loss: 6.70608182161349031247\n",
      "Iteration 24226 => Loss: 6.70608075655675950344\n",
      "Iteration 24227 => Loss: 6.70607969164353967528\n",
      "Iteration 24228 => Loss: 6.70607862687382372258\n",
      "Iteration 24229 => Loss: 6.70607756224758233543\n",
      "Iteration 24230 => Loss: 6.70607649776480574388\n",
      "Iteration 24231 => Loss: 6.70607543342546197351\n",
      "Iteration 24232 => Loss: 6.70607436922953681346\n",
      "Iteration 24233 => Loss: 6.70607330517701338835\n",
      "Iteration 24234 => Loss: 6.70607224126787393459\n",
      "Iteration 24235 => Loss: 6.70607117750208825413\n",
      "Iteration 24236 => Loss: 6.70607011387965723515\n",
      "Iteration 24237 => Loss: 6.70606905040054179779\n",
      "Iteration 24238 => Loss: 6.70606798706472950755\n",
      "Iteration 24239 => Loss: 6.70606692387220970630\n",
      "Iteration 24240 => Loss: 6.70606586082294864326\n",
      "Iteration 24241 => Loss: 6.70606479791693210757\n",
      "Iteration 24242 => Loss: 6.70606373515414144748\n",
      "Iteration 24243 => Loss: 6.70606267253456156396\n",
      "Iteration 24244 => Loss: 6.70606161005817202891\n",
      "Iteration 24245 => Loss: 6.70606054772495419058\n",
      "Iteration 24246 => Loss: 6.70605948553486896913\n",
      "Iteration 24247 => Loss: 6.70605842348793323993\n",
      "Iteration 24248 => Loss: 6.70605736158409992953\n",
      "Iteration 24249 => Loss: 6.70605629982335837980\n",
      "Iteration 24250 => Loss: 6.70605523820568905080\n",
      "Iteration 24251 => Loss: 6.70605417673107151444\n",
      "Iteration 24252 => Loss: 6.70605311539949244803\n",
      "Iteration 24253 => Loss: 6.70605205421092342988\n",
      "Iteration 24254 => Loss: 6.70605099316535380183\n",
      "Iteration 24255 => Loss: 6.70604993226275958307\n",
      "Iteration 24256 => Loss: 6.70604887150311590460\n",
      "Iteration 24257 => Loss: 6.70604781088641832554\n",
      "Iteration 24258 => Loss: 6.70604675041263398327\n",
      "Iteration 24259 => Loss: 6.70604569008174156153\n",
      "Iteration 24260 => Loss: 6.70604462989374106030\n",
      "Iteration 24261 => Loss: 6.70604356984859428792\n",
      "Iteration 24262 => Loss: 6.70604250994629502713\n",
      "Iteration 24263 => Loss: 6.70604145018681396806\n",
      "Iteration 24264 => Loss: 6.70604039057012979441\n",
      "Iteration 24265 => Loss: 6.70603933109623540076\n",
      "Iteration 24266 => Loss: 6.70603827176510414176\n",
      "Iteration 24267 => Loss: 6.70603721257671825384\n",
      "Iteration 24268 => Loss: 6.70603615353105109165\n",
      "Iteration 24269 => Loss: 6.70603509462810443154\n",
      "Iteration 24270 => Loss: 6.70603403586783564094\n",
      "Iteration 24271 => Loss: 6.70603297725023761444\n",
      "Iteration 24272 => Loss: 6.70603191877529258846\n",
      "Iteration 24273 => Loss: 6.70603086044296858859\n",
      "Iteration 24274 => Loss: 6.70602980225325318031\n",
      "Iteration 24275 => Loss: 6.70602874420613659368\n",
      "Iteration 24276 => Loss: 6.70602768630159307151\n",
      "Iteration 24277 => Loss: 6.70602662853959063938\n",
      "Iteration 24278 => Loss: 6.70602557092013018547\n",
      "Iteration 24279 => Loss: 6.70602451344318772897\n",
      "Iteration 24280 => Loss: 6.70602345610873395998\n",
      "Iteration 24281 => Loss: 6.70602239891675910854\n",
      "Iteration 24282 => Loss: 6.70602134186724541109\n",
      "Iteration 24283 => Loss: 6.70602028496016089321\n",
      "Iteration 24284 => Loss: 6.70601922819549667310\n",
      "Iteration 24285 => Loss: 6.70601817157323232266\n",
      "Iteration 24286 => Loss: 6.70601711509334741379\n",
      "Iteration 24287 => Loss: 6.70601605875582773564\n",
      "Iteration 24288 => Loss: 6.70601500256064664285\n",
      "Iteration 24289 => Loss: 6.70601394650778104278\n",
      "Iteration 24290 => Loss: 6.70601289059722827091\n",
      "Iteration 24291 => Loss: 6.70601183482895457644\n",
      "Iteration 24292 => Loss: 6.70601077920295463031\n",
      "Iteration 24293 => Loss: 6.70600972371919024084\n",
      "Iteration 24294 => Loss: 6.70600866837765341444\n",
      "Iteration 24295 => Loss: 6.70600761317832549935\n",
      "Iteration 24296 => Loss: 6.70600655812118340293\n",
      "Iteration 24297 => Loss: 6.70600550320621469069\n",
      "Iteration 24298 => Loss: 6.70600444843339094092\n",
      "Iteration 24299 => Loss: 6.70600339380270682454\n",
      "Iteration 24300 => Loss: 6.70600233931412859079\n",
      "Iteration 24301 => Loss: 6.70600128496764469332\n",
      "Iteration 24302 => Loss: 6.70600023076322848681\n",
      "Iteration 24303 => Loss: 6.70599917670087108945\n",
      "Iteration 24304 => Loss: 6.70599812278054674408\n",
      "Iteration 24305 => Loss: 6.70599706900223591077\n",
      "Iteration 24306 => Loss: 6.70599601536592349049\n",
      "Iteration 24307 => Loss: 6.70599496187159527238\n",
      "Iteration 24308 => Loss: 6.70599390851921928203\n",
      "Iteration 24309 => Loss: 6.70599285530878042039\n",
      "Iteration 24310 => Loss: 6.70599180224026802932\n",
      "Iteration 24311 => Loss: 6.70599074931365546348\n",
      "Iteration 24312 => Loss: 6.70598969652892229476\n",
      "Iteration 24313 => Loss: 6.70598864388605431230\n",
      "Iteration 24314 => Loss: 6.70598759138502575894\n",
      "Iteration 24315 => Loss: 6.70598653902582508834\n",
      "Iteration 24316 => Loss: 6.70598548680843009606\n",
      "Iteration 24317 => Loss: 6.70598443473282035399\n",
      "Iteration 24318 => Loss: 6.70598338279897898673\n",
      "Iteration 24319 => Loss: 6.70598233100688378983\n",
      "Iteration 24320 => Loss: 6.70598127935652055243\n",
      "Iteration 24321 => Loss: 6.70598022784786262918\n",
      "Iteration 24322 => Loss: 6.70597917648090469100\n",
      "Iteration 24323 => Loss: 6.70597812525560765806\n",
      "Iteration 24324 => Loss: 6.70597707417196708946\n",
      "Iteration 24325 => Loss: 6.70597602322996255708\n",
      "Iteration 24326 => Loss: 6.70597497242956919195\n",
      "Iteration 24327 => Loss: 6.70597392177077367137\n",
      "Iteration 24328 => Loss: 6.70597287125355467907\n",
      "Iteration 24329 => Loss: 6.70597182087788734606\n",
      "Iteration 24330 => Loss: 6.70597077064376545508\n",
      "Iteration 24331 => Loss: 6.70596972055116502531\n",
      "Iteration 24332 => Loss: 6.70596867060005852323\n",
      "Iteration 24333 => Loss: 6.70596762079043173799\n",
      "Iteration 24334 => Loss: 6.70596657112227312325\n",
      "Iteration 24335 => Loss: 6.70596552159555336914\n",
      "Iteration 24336 => Loss: 6.70596447221025560026\n",
      "Iteration 24337 => Loss: 6.70596342296636471758\n",
      "Iteration 24338 => Loss: 6.70596237386386384571\n",
      "Iteration 24339 => Loss: 6.70596132490271923388\n",
      "Iteration 24340 => Loss: 6.70596027608293532296\n",
      "Iteration 24341 => Loss: 6.70595922740446681587\n",
      "Iteration 24342 => Loss: 6.70595817886731371260\n",
      "Iteration 24343 => Loss: 6.70595713047145469687\n",
      "Iteration 24344 => Loss: 6.70595608221686578787\n",
      "Iteration 24345 => Loss: 6.70595503410352122842\n",
      "Iteration 24346 => Loss: 6.70595398613142812394\n",
      "Iteration 24347 => Loss: 6.70595293830053140738\n",
      "Iteration 24348 => Loss: 6.70595189061083729598\n",
      "Iteration 24349 => Loss: 6.70595084306232003257\n",
      "Iteration 24350 => Loss: 6.70594979565495385998\n",
      "Iteration 24351 => Loss: 6.70594874838872989642\n",
      "Iteration 24352 => Loss: 6.70594770126362327289\n",
      "Iteration 24353 => Loss: 6.70594665427961977855\n",
      "Iteration 24354 => Loss: 6.70594560743670076164\n",
      "Iteration 24355 => Loss: 6.70594456073483602410\n",
      "Iteration 24356 => Loss: 6.70594351417401757232\n",
      "Iteration 24357 => Loss: 6.70594246775422764273\n",
      "Iteration 24358 => Loss: 6.70594142147543781363\n",
      "Iteration 24359 => Loss: 6.70594037533763387415\n",
      "Iteration 24360 => Loss: 6.70593932934080250163\n",
      "Iteration 24361 => Loss: 6.70593828348491260982\n",
      "Iteration 24362 => Loss: 6.70593723776995265240\n",
      "Iteration 24363 => Loss: 6.70593619219590753033\n",
      "Iteration 24364 => Loss: 6.70593514676274971009\n",
      "Iteration 24365 => Loss: 6.70593410147046586900\n",
      "Iteration 24366 => Loss: 6.70593305631903380259\n",
      "Iteration 24367 => Loss: 6.70593201130843663549\n",
      "Iteration 24368 => Loss: 6.70593096643865660411\n",
      "Iteration 24369 => Loss: 6.70592992170966972765\n",
      "Iteration 24370 => Loss: 6.70592887712146268342\n",
      "Iteration 24371 => Loss: 6.70592783267401326697\n",
      "Iteration 24372 => Loss: 6.70592678836730549108\n",
      "Iteration 24373 => Loss: 6.70592574420131448676\n",
      "Iteration 24374 => Loss: 6.70592470017602249044\n",
      "Iteration 24375 => Loss: 6.70592365629141351491\n",
      "Iteration 24376 => Loss: 6.70592261254747246113\n",
      "Iteration 24377 => Loss: 6.70592156894417801283\n",
      "Iteration 24378 => Loss: 6.70592052548150263647\n",
      "Iteration 24379 => Loss: 6.70591948215944011480\n",
      "Iteration 24380 => Loss: 6.70591843897796469065\n",
      "Iteration 24381 => Loss: 6.70591739593705060685\n",
      "Iteration 24382 => Loss: 6.70591635303669519885\n",
      "Iteration 24383 => Loss: 6.70591531027686560407\n",
      "Iteration 24384 => Loss: 6.70591426765754761163\n",
      "Iteration 24385 => Loss: 6.70591322517872789888\n",
      "Iteration 24386 => Loss: 6.70591218284037804409\n",
      "Iteration 24387 => Loss: 6.70591114064248206006\n",
      "Iteration 24388 => Loss: 6.70591009858502751229\n",
      "Iteration 24389 => Loss: 6.70590905666798331453\n",
      "Iteration 24390 => Loss: 6.70590801489134147317\n",
      "Iteration 24391 => Loss: 6.70590697325508156013\n",
      "Iteration 24392 => Loss: 6.70590593175917604185\n",
      "Iteration 24393 => Loss: 6.70590489040361781292\n",
      "Iteration 24394 => Loss: 6.70590384918838378070\n",
      "Iteration 24395 => Loss: 6.70590280811344907619\n",
      "Iteration 24396 => Loss: 6.70590176717879238311\n",
      "Iteration 24397 => Loss: 6.70590072638441725417\n",
      "Iteration 24398 => Loss: 6.70589968573028194498\n",
      "Iteration 24399 => Loss: 6.70589864521636780381\n",
      "Iteration 24400 => Loss: 6.70589760484267305429\n",
      "Iteration 24401 => Loss: 6.70589656460916572200\n",
      "Iteration 24402 => Loss: 6.70589552451583070791\n",
      "Iteration 24403 => Loss: 6.70589448456264580756\n",
      "Iteration 24404 => Loss: 6.70589344474959681008\n",
      "Iteration 24405 => Loss: 6.70589240507666328739\n",
      "Iteration 24406 => Loss: 6.70589136554382392319\n",
      "Iteration 24407 => Loss: 6.70589032615105562485\n",
      "Iteration 24408 => Loss: 6.70588928689835928054\n",
      "Iteration 24409 => Loss: 6.70588824778569758678\n",
      "Iteration 24410 => Loss: 6.70588720881304833910\n",
      "Iteration 24411 => Loss: 6.70588616998040709660\n",
      "Iteration 24412 => Loss: 6.70588513128775165484\n",
      "Iteration 24413 => Loss: 6.70588409273505181574\n",
      "Iteration 24414 => Loss: 6.70588305432230225023\n",
      "Iteration 24415 => Loss: 6.70588201604948341839\n",
      "Iteration 24416 => Loss: 6.70588097791656601032\n",
      "Iteration 24417 => Loss: 6.70587993992353403883\n",
      "Iteration 24418 => Loss: 6.70587890207038395118\n",
      "Iteration 24419 => Loss: 6.70587786435706867394\n",
      "Iteration 24420 => Loss: 6.70587682678359175981\n",
      "Iteration 24421 => Loss: 6.70587578934993633339\n",
      "Iteration 24422 => Loss: 6.70587475205606420303\n",
      "Iteration 24423 => Loss: 6.70587371490196915147\n",
      "Iteration 24424 => Loss: 6.70587267788762986243\n",
      "Iteration 24425 => Loss: 6.70587164101303567776\n",
      "Iteration 24426 => Loss: 6.70587060427815462305\n",
      "Iteration 24427 => Loss: 6.70586956768297781650\n",
      "Iteration 24428 => Loss: 6.70586853122748038913\n",
      "Iteration 24429 => Loss: 6.70586749491164280101\n",
      "Iteration 24430 => Loss: 6.70586645873545084129\n",
      "Iteration 24431 => Loss: 6.70586542269888763457\n",
      "Iteration 24432 => Loss: 6.70586438680192387096\n",
      "Iteration 24433 => Loss: 6.70586335104454356326\n",
      "Iteration 24434 => Loss: 6.70586231542674315875\n",
      "Iteration 24435 => Loss: 6.70586127994848713030\n",
      "Iteration 24436 => Loss: 6.70586024460975771433\n",
      "Iteration 24437 => Loss: 6.70585920941054780542\n",
      "Iteration 24438 => Loss: 6.70585817435081832372\n",
      "Iteration 24439 => Loss: 6.70585713943058081554\n",
      "Iteration 24440 => Loss: 6.70585610464978909562\n",
      "Iteration 24441 => Loss: 6.70585507000843428216\n",
      "Iteration 24442 => Loss: 6.70585403550650305249\n",
      "Iteration 24443 => Loss: 6.70585300114396698490\n",
      "Iteration 24444 => Loss: 6.70585196692080831582\n",
      "Iteration 24445 => Loss: 6.70585093283701816347\n",
      "Iteration 24446 => Loss: 6.70584989889256366524\n",
      "Iteration 24447 => Loss: 6.70584886508743771572\n",
      "Iteration 24448 => Loss: 6.70584783142161278136\n",
      "Iteration 24449 => Loss: 6.70584679789507909220\n",
      "Iteration 24450 => Loss: 6.70584576450781089108\n",
      "Iteration 24451 => Loss: 6.70584473125979219077\n",
      "Iteration 24452 => Loss: 6.70584369815100611589\n",
      "Iteration 24453 => Loss: 6.70584266518143135016\n",
      "Iteration 24454 => Loss: 6.70584163235104746548\n",
      "Iteration 24455 => Loss: 6.70584059965983403373\n",
      "Iteration 24456 => Loss: 6.70583956710778306132\n",
      "Iteration 24457 => Loss: 6.70583853469486523835\n",
      "Iteration 24458 => Loss: 6.70583750242106013673\n",
      "Iteration 24459 => Loss: 6.70583647028635887466\n",
      "Iteration 24460 => Loss: 6.70583543829073569498\n",
      "Iteration 24461 => Loss: 6.70583440643417727500\n",
      "Iteration 24462 => Loss: 6.70583337471666318663\n",
      "Iteration 24463 => Loss: 6.70583234313816856087\n",
      "Iteration 24464 => Loss: 6.70583131169867652233\n",
      "Iteration 24465 => Loss: 6.70583028039817552468\n",
      "Iteration 24466 => Loss: 6.70582924923664602801\n",
      "Iteration 24467 => Loss: 6.70582821821406582785\n",
      "Iteration 24468 => Loss: 6.70582718733040561432\n",
      "Iteration 24469 => Loss: 6.70582615658566361105\n",
      "Iteration 24470 => Loss: 6.70582512597981317271\n",
      "Iteration 24471 => Loss: 6.70582409551283564753\n",
      "Iteration 24472 => Loss: 6.70582306518471593648\n",
      "Iteration 24473 => Loss: 6.70582203499543361147\n",
      "Iteration 24474 => Loss: 6.70582100494497090892\n",
      "Iteration 24475 => Loss: 6.70581997503330473620\n",
      "Iteration 24476 => Loss: 6.70581894526042177063\n",
      "Iteration 24477 => Loss: 6.70581791562630069592\n",
      "Iteration 24478 => Loss: 6.70581688613091930762\n",
      "Iteration 24479 => Loss: 6.70581585677426694758\n",
      "Iteration 24480 => Loss: 6.70581482755631430592\n",
      "Iteration 24481 => Loss: 6.70581379847704983632\n",
      "Iteration 24482 => Loss: 6.70581276953645843975\n",
      "Iteration 24483 => Loss: 6.70581174073451169448\n",
      "Iteration 24484 => Loss: 6.70581071207120071875\n",
      "Iteration 24485 => Loss: 6.70580968354650153174\n",
      "Iteration 24486 => Loss: 6.70580865516039636987\n",
      "Iteration 24487 => Loss: 6.70580762691286569321\n",
      "Iteration 24488 => Loss: 6.70580659880388907368\n",
      "Iteration 24489 => Loss: 6.70580557083345230041\n",
      "Iteration 24490 => Loss: 6.70580454300153405711\n",
      "Iteration 24491 => Loss: 6.70580351530811391569\n",
      "Iteration 24492 => Loss: 6.70580248775318565890\n",
      "Iteration 24493 => Loss: 6.70580146033671375960\n",
      "Iteration 24494 => Loss: 6.70580043305868223058\n",
      "Iteration 24495 => Loss: 6.70579940591907863734\n",
      "Iteration 24496 => Loss: 6.70579837891788699267\n",
      "Iteration 24497 => Loss: 6.70579735205508153939\n",
      "Iteration 24498 => Loss: 6.70579632533064184940\n",
      "Iteration 24499 => Loss: 6.70579529874455904093\n",
      "Iteration 24500 => Loss: 6.70579427229680202771\n",
      "Iteration 24501 => Loss: 6.70579324598736370433\n",
      "Iteration 24502 => Loss: 6.70579221981622364268\n",
      "Iteration 24503 => Loss: 6.70579119378335608559\n",
      "Iteration 24504 => Loss: 6.70579016788874238131\n",
      "Iteration 24505 => Loss: 6.70578914213237453623\n",
      "Iteration 24506 => Loss: 6.70578811651422856954\n",
      "Iteration 24507 => Loss: 6.70578709103427872407\n",
      "Iteration 24508 => Loss: 6.70578606569251967073\n",
      "Iteration 24509 => Loss: 6.70578504048891677058\n",
      "Iteration 24510 => Loss: 6.70578401542347268816\n",
      "Iteration 24511 => Loss: 6.70578299049614212635\n",
      "Iteration 24512 => Loss: 6.70578196570692242062\n",
      "Iteration 24513 => Loss: 6.70578094105579758377\n",
      "Iteration 24514 => Loss: 6.70577991654275074040\n",
      "Iteration 24515 => Loss: 6.70577889216774458703\n",
      "Iteration 24516 => Loss: 6.70577786793078267635\n",
      "Iteration 24517 => Loss: 6.70577684383183303396\n",
      "Iteration 24518 => Loss: 6.70577581987087700810\n",
      "Iteration 24519 => Loss: 6.70577479604790660517\n",
      "Iteration 24520 => Loss: 6.70577377236289340345\n",
      "Iteration 24521 => Loss: 6.70577274881581786303\n",
      "Iteration 24522 => Loss: 6.70577172540667643119\n",
      "Iteration 24523 => Loss: 6.70577070213542736354\n",
      "Iteration 24524 => Loss: 6.70576967900207243645\n",
      "Iteration 24525 => Loss: 6.70576865600658500455\n",
      "Iteration 24526 => Loss: 6.70576763314894019885\n",
      "Iteration 24527 => Loss: 6.70576661042912647304\n",
      "Iteration 24528 => Loss: 6.70576558784713139261\n",
      "Iteration 24529 => Loss: 6.70576456540292031860\n",
      "Iteration 24530 => Loss: 6.70576354309648881014\n",
      "Iteration 24531 => Loss: 6.70576252092781555092\n",
      "Iteration 24532 => Loss: 6.70576149889687389560\n",
      "Iteration 24533 => Loss: 6.70576047700365140969\n",
      "Iteration 24534 => Loss: 6.70575945524812944143\n",
      "Iteration 24535 => Loss: 6.70575843363029200361\n",
      "Iteration 24536 => Loss: 6.70575741215011600360\n",
      "Iteration 24537 => Loss: 6.70575639080758367783\n",
      "Iteration 24538 => Loss: 6.70575536960267548636\n",
      "Iteration 24539 => Loss: 6.70575434853537988289\n",
      "Iteration 24540 => Loss: 6.70575332760566844570\n",
      "Iteration 24541 => Loss: 6.70575230681353229301\n",
      "Iteration 24542 => Loss: 6.70575128615894389128\n",
      "Iteration 24543 => Loss: 6.70575026564189169420\n",
      "Iteration 24544 => Loss: 6.70574924526235527367\n",
      "Iteration 24545 => Loss: 6.70574822502030976068\n",
      "Iteration 24546 => Loss: 6.70574720491574360892\n",
      "Iteration 24547 => Loss: 6.70574618494863816665\n",
      "Iteration 24548 => Loss: 6.70574516511897389393\n",
      "Iteration 24549 => Loss: 6.70574414542673480355\n",
      "Iteration 24550 => Loss: 6.70574312587188892110\n",
      "Iteration 24551 => Loss: 6.70574210645443713474\n",
      "Iteration 24552 => Loss: 6.70574108717435457550\n",
      "Iteration 24553 => Loss: 6.70574006803160749257\n",
      "Iteration 24554 => Loss: 6.70573904902619588597\n",
      "Iteration 24555 => Loss: 6.70573803015809932759\n",
      "Iteration 24556 => Loss: 6.70573701142729206026\n",
      "Iteration 24557 => Loss: 6.70573599283376342584\n",
      "Iteration 24558 => Loss: 6.70573497437748411443\n",
      "Iteration 24559 => Loss: 6.70573395605844169154\n",
      "Iteration 24560 => Loss: 6.70573293787662105814\n",
      "Iteration 24561 => Loss: 6.70573191983200089794\n",
      "Iteration 24562 => Loss: 6.70573090192455989467\n",
      "Iteration 24563 => Loss: 6.70572988415428916653\n",
      "Iteration 24564 => Loss: 6.70572886652115673911\n",
      "Iteration 24565 => Loss: 6.70572784902515017791\n",
      "Iteration 24566 => Loss: 6.70572683166625260753\n",
      "Iteration 24567 => Loss: 6.70572581444444004717\n",
      "Iteration 24568 => Loss: 6.70572479735970539139\n",
      "Iteration 24569 => Loss: 6.70572378041201488941\n",
      "Iteration 24570 => Loss: 6.70572276360136854123\n",
      "Iteration 24571 => Loss: 6.70572174692773081972\n",
      "Iteration 24572 => Loss: 6.70572073039109106674\n",
      "Iteration 24573 => Loss: 6.70571971399142618964\n",
      "Iteration 24574 => Loss: 6.70571869772872641846\n",
      "Iteration 24575 => Loss: 6.70571768160296599603\n",
      "Iteration 24576 => Loss: 6.70571666561412715879\n",
      "Iteration 24577 => Loss: 6.70571564976219924858\n",
      "Iteration 24578 => Loss: 6.70571463404715117917\n",
      "Iteration 24579 => Loss: 6.70571361846897051606\n",
      "Iteration 24580 => Loss: 6.70571260302764482475\n",
      "Iteration 24581 => Loss: 6.70571158772314479535\n",
      "Iteration 24582 => Loss: 6.70571057255545888154\n",
      "Iteration 24583 => Loss: 6.70570955752456310250\n",
      "Iteration 24584 => Loss: 6.70570854263045124100\n",
      "Iteration 24585 => Loss: 6.70570752787308776988\n",
      "Iteration 24586 => Loss: 6.70570651325246824825\n",
      "Iteration 24587 => Loss: 6.70570549876856425442\n",
      "Iteration 24588 => Loss: 6.70570448442136690659\n",
      "Iteration 24589 => Loss: 6.70570347021084867123\n",
      "Iteration 24590 => Loss: 6.70570245613699889020\n",
      "Iteration 24591 => Loss: 6.70570144219979624722\n",
      "Iteration 24592 => Loss: 6.70570042839921676148\n",
      "Iteration 24593 => Loss: 6.70569941473525332754\n",
      "Iteration 24594 => Loss: 6.70569840120787752369\n",
      "Iteration 24595 => Loss: 6.70569738781707336273\n",
      "Iteration 24596 => Loss: 6.70569637456282929833\n",
      "Iteration 24597 => Loss: 6.70569536144511779696\n",
      "Iteration 24598 => Loss: 6.70569434846391843053\n",
      "Iteration 24599 => Loss: 6.70569333561922409359\n",
      "Iteration 24600 => Loss: 6.70569232291101435806\n",
      "Iteration 24601 => Loss: 6.70569131033926346674\n",
      "Iteration 24602 => Loss: 6.70569029790395276791\n",
      "Iteration 24603 => Loss: 6.70568928560507693248\n",
      "Iteration 24604 => Loss: 6.70568827344259865697\n",
      "Iteration 24605 => Loss: 6.70568726141651438866\n",
      "Iteration 24606 => Loss: 6.70568624952680192308\n",
      "Iteration 24607 => Loss: 6.70568523777343461489\n",
      "Iteration 24608 => Loss: 6.70568422615641335227\n",
      "Iteration 24609 => Loss: 6.70568321467569816718\n",
      "Iteration 24610 => Loss: 6.70568220333127573696\n",
      "Iteration 24611 => Loss: 6.70568119212313984434\n",
      "Iteration 24612 => Loss: 6.70568018105126295580\n",
      "Iteration 24613 => Loss: 6.70567917011563086049\n",
      "Iteration 24614 => Loss: 6.70567815931621868941\n",
      "Iteration 24615 => Loss: 6.70567714865301489624\n",
      "Iteration 24616 => Loss: 6.70567613812599372380\n",
      "Iteration 24617 => Loss: 6.70567512773514451396\n",
      "Iteration 24618 => Loss: 6.70567411748044417408\n",
      "Iteration 24619 => Loss: 6.70567310736187760511\n",
      "Iteration 24620 => Loss: 6.70567209737942349079\n",
      "Iteration 24621 => Loss: 6.70567108753306406754\n",
      "Iteration 24622 => Loss: 6.70567007782277624273\n",
      "Iteration 24623 => Loss: 6.70566906824855379909\n",
      "Iteration 24624 => Loss: 6.70566805881037186765\n",
      "Iteration 24625 => Loss: 6.70566704950820646758\n",
      "Iteration 24626 => Loss: 6.70566604034205049345\n",
      "Iteration 24627 => Loss: 6.70566503131188085263\n",
      "Iteration 24628 => Loss: 6.70566402241767356429\n",
      "Iteration 24629 => Loss: 6.70566301365941441759\n",
      "Iteration 24630 => Loss: 6.70566200503708564895\n",
      "Iteration 24631 => Loss: 6.70566099655066860663\n",
      "Iteration 24632 => Loss: 6.70565998820015174431\n",
      "Iteration 24633 => Loss: 6.70565897998550841663\n",
      "Iteration 24634 => Loss: 6.70565797190671286643\n",
      "Iteration 24635 => Loss: 6.70565696396376242916\n",
      "Iteration 24636 => Loss: 6.70565595615663045947\n",
      "Iteration 24637 => Loss: 6.70565494848530008198\n",
      "Iteration 24638 => Loss: 6.70565394094975975037\n",
      "Iteration 24639 => Loss: 6.70565293354998193109\n",
      "Iteration 24640 => Loss: 6.70565192628594974877\n",
      "Iteration 24641 => Loss: 6.70565091915764455166\n",
      "Iteration 24642 => Loss: 6.70564991216505479343\n",
      "Iteration 24643 => Loss: 6.70564890530815294056\n",
      "Iteration 24644 => Loss: 6.70564789858692300584\n",
      "Iteration 24645 => Loss: 6.70564689200135877201\n",
      "Iteration 24646 => Loss: 6.70564588555142737647\n",
      "Iteration 24647 => Loss: 6.70564487923711016748\n",
      "Iteration 24648 => Loss: 6.70564387305840270415\n",
      "Iteration 24649 => Loss: 6.70564286701526768297\n",
      "Iteration 24650 => Loss: 6.70564186110770155125\n",
      "Iteration 24651 => Loss: 6.70564085533568476905\n",
      "Iteration 24652 => Loss: 6.70563984969919246737\n",
      "Iteration 24653 => Loss: 6.70563884419820777083\n",
      "Iteration 24654 => Loss: 6.70563783883271824493\n",
      "Iteration 24655 => Loss: 6.70563683360270434974\n",
      "Iteration 24656 => Loss: 6.70563582850813677538\n",
      "Iteration 24657 => Loss: 6.70563482354901552185\n",
      "Iteration 24658 => Loss: 6.70563381872530683836\n",
      "Iteration 24659 => Loss: 6.70563281403699829042\n",
      "Iteration 24660 => Loss: 6.70563180948407566717\n",
      "Iteration 24661 => Loss: 6.70563080506651054691\n",
      "Iteration 24662 => Loss: 6.70562980078429404784\n",
      "Iteration 24663 => Loss: 6.70562879663740396552\n",
      "Iteration 24664 => Loss: 6.70562779262581987183\n",
      "Iteration 24665 => Loss: 6.70562678874952844410\n",
      "Iteration 24666 => Loss: 6.70562578500851191876\n",
      "Iteration 24667 => Loss: 6.70562478140274986771\n",
      "Iteration 24668 => Loss: 6.70562377793222186284\n",
      "Iteration 24669 => Loss: 6.70562277459690925241\n",
      "Iteration 24670 => Loss: 6.70562177139679871374\n",
      "Iteration 24671 => Loss: 6.70562076833187070690\n",
      "Iteration 24672 => Loss: 6.70561976540210569198\n",
      "Iteration 24673 => Loss: 6.70561876260748679357\n",
      "Iteration 24674 => Loss: 6.70561775994798558997\n",
      "Iteration 24675 => Loss: 6.70561675742360030483\n",
      "Iteration 24676 => Loss: 6.70561575503430784551\n",
      "Iteration 24677 => Loss: 6.70561475278008156664\n",
      "Iteration 24678 => Loss: 6.70561375066091347463\n",
      "Iteration 24679 => Loss: 6.70561274867677958866\n",
      "Iteration 24680 => Loss: 6.70561174682765592792\n",
      "Iteration 24681 => Loss: 6.70561074511353982786\n",
      "Iteration 24682 => Loss: 6.70560974353440908402\n",
      "Iteration 24683 => Loss: 6.70560874209023172199\n",
      "Iteration 24684 => Loss: 6.70560774078099708362\n",
      "Iteration 24685 => Loss: 6.70560673960669539895\n",
      "Iteration 24686 => Loss: 6.70560573856730890441\n",
      "Iteration 24687 => Loss: 6.70560473766280118468\n",
      "Iteration 24688 => Loss: 6.70560373689316868706\n",
      "Iteration 24689 => Loss: 6.70560273625838743072\n",
      "Iteration 24690 => Loss: 6.70560173575844764571\n",
      "Iteration 24691 => Loss: 6.70560073539332091030\n",
      "Iteration 24692 => Loss: 6.70559973516299301366\n",
      "Iteration 24693 => Loss: 6.70559873506745063310\n",
      "Iteration 24694 => Loss: 6.70559773510666978780\n",
      "Iteration 24695 => Loss: 6.70559673528063093784\n",
      "Iteration 24696 => Loss: 6.70559573558931720783\n",
      "Iteration 24697 => Loss: 6.70559473603272149234\n",
      "Iteration 24698 => Loss: 6.70559373661080826423\n",
      "Iteration 24699 => Loss: 6.70559273732356420084\n",
      "Iteration 24700 => Loss: 6.70559173817098308490\n",
      "Iteration 24701 => Loss: 6.70559073915302850111\n",
      "Iteration 24702 => Loss: 6.70558974026969689675\n",
      "Iteration 24703 => Loss: 6.70558874152096784371\n",
      "Iteration 24704 => Loss: 6.70558774290681736119\n",
      "Iteration 24705 => Loss: 6.70558674442722768561\n",
      "Iteration 24706 => Loss: 6.70558574608218638247\n",
      "Iteration 24707 => Loss: 6.70558474787167035913\n",
      "Iteration 24708 => Loss: 6.70558374979566451657\n",
      "Iteration 24709 => Loss: 6.70558275185414931485\n",
      "Iteration 24710 => Loss: 6.70558175404710343770\n",
      "Iteration 24711 => Loss: 6.70558075637451711515\n",
      "Iteration 24712 => Loss: 6.70557975883636370185\n",
      "Iteration 24713 => Loss: 6.70557876143263165147\n",
      "Iteration 24714 => Loss: 6.70557776416329520686\n",
      "Iteration 24715 => Loss: 6.70557676702834815075\n",
      "Iteration 24716 => Loss: 6.70557577002775850872\n",
      "Iteration 24717 => Loss: 6.70557477316151828717\n",
      "Iteration 24718 => Loss: 6.70557377642960794617\n",
      "Iteration 24719 => Loss: 6.70557277983199995219\n",
      "Iteration 24720 => Loss: 6.70557178336869075252\n",
      "Iteration 24721 => Loss: 6.70557078703965281363\n",
      "Iteration 24722 => Loss: 6.70556979084486304288\n",
      "Iteration 24723 => Loss: 6.70556879478432232844\n",
      "Iteration 24724 => Loss: 6.70556779885799336682\n",
      "Iteration 24725 => Loss: 6.70556680306586549989\n",
      "Iteration 24726 => Loss: 6.70556580740792806949\n",
      "Iteration 24727 => Loss: 6.70556481188415087757\n",
      "Iteration 24728 => Loss: 6.70556381649451971327\n",
      "Iteration 24729 => Loss: 6.70556282123901858938\n",
      "Iteration 24730 => Loss: 6.70556182611762441326\n",
      "Iteration 24731 => Loss: 6.70556083113032919130\n",
      "Iteration 24732 => Loss: 6.70555983627710538997\n",
      "Iteration 24733 => Loss: 6.70555884155793169299\n",
      "Iteration 24734 => Loss: 6.70555784697280454765\n",
      "Iteration 24735 => Loss: 6.70555685252170707855\n",
      "Iteration 24736 => Loss: 6.70555585820458599500\n",
      "Iteration 24737 => Loss: 6.70555486402147415959\n",
      "Iteration 24738 => Loss: 6.70555386997231828161\n",
      "Iteration 24739 => Loss: 6.70555287605711214383\n",
      "Iteration 24740 => Loss: 6.70555188227582910088\n",
      "Iteration 24741 => Loss: 6.70555088862847004094\n",
      "Iteration 24742 => Loss: 6.70554989511499677235\n",
      "Iteration 24743 => Loss: 6.70554890173540574239\n",
      "Iteration 24744 => Loss: 6.70554790848966586481\n",
      "Iteration 24745 => Loss: 6.70554691537776825783\n",
      "Iteration 24746 => Loss: 6.70554592239969515788\n",
      "Iteration 24747 => Loss: 6.70554492955541814325\n",
      "Iteration 24748 => Loss: 6.70554393684494076666\n",
      "Iteration 24749 => Loss: 6.70554294426822039554\n",
      "Iteration 24750 => Loss: 6.70554195182525614172\n",
      "Iteration 24751 => Loss: 6.70554095951601603076\n",
      "Iteration 24752 => Loss: 6.70553996734049562178\n",
      "Iteration 24753 => Loss: 6.70553897529866560490\n",
      "Iteration 24754 => Loss: 6.70553798339051798649\n",
      "Iteration 24755 => Loss: 6.70553699161603145029\n",
      "Iteration 24756 => Loss: 6.70553599997518201548\n",
      "Iteration 24757 => Loss: 6.70553500846795635937\n",
      "Iteration 24758 => Loss: 6.70553401709434471201\n",
      "Iteration 24759 => Loss: 6.70553302585431332261\n",
      "Iteration 24760 => Loss: 6.70553203474785153304\n",
      "Iteration 24761 => Loss: 6.70553104377494246791\n",
      "Iteration 24762 => Loss: 6.70553005293556925182\n",
      "Iteration 24763 => Loss: 6.70552906222970879213\n",
      "Iteration 24764 => Loss: 6.70552807165734510164\n",
      "Iteration 24765 => Loss: 6.70552708121846485767\n",
      "Iteration 24766 => Loss: 6.70552609091304585576\n",
      "Iteration 24767 => Loss: 6.70552510074106677962\n",
      "Iteration 24768 => Loss: 6.70552411070252141201\n",
      "Iteration 24769 => Loss: 6.70552312079737955486\n",
      "Iteration 24770 => Loss: 6.70552213102561545099\n",
      "Iteration 24771 => Loss: 6.70552114138723798220\n",
      "Iteration 24772 => Loss: 6.70552015188220984498\n",
      "Iteration 24773 => Loss: 6.70551916251052126938\n",
      "Iteration 24774 => Loss: 6.70551817327214561004\n",
      "Iteration 24775 => Loss: 6.70551718416707309700\n",
      "Iteration 24776 => Loss: 6.70551619519527797308\n",
      "Iteration 24777 => Loss: 6.70551520635675313287\n",
      "Iteration 24778 => Loss: 6.70551421765146571374\n",
      "Iteration 24779 => Loss: 6.70551322907941571572\n",
      "Iteration 24780 => Loss: 6.70551224064056938801\n",
      "Iteration 24781 => Loss: 6.70551125233491962518\n",
      "Iteration 24782 => Loss: 6.70551026416244511097\n",
      "Iteration 24783 => Loss: 6.70550927612312630544\n",
      "Iteration 24784 => Loss: 6.70550828821694100412\n",
      "Iteration 24785 => Loss: 6.70550730044388032525\n",
      "Iteration 24786 => Loss: 6.70550631280392028799\n",
      "Iteration 24787 => Loss: 6.70550532529704579332\n",
      "Iteration 24788 => Loss: 6.70550433792323641313\n",
      "Iteration 24789 => Loss: 6.70550335068247349568\n",
      "Iteration 24790 => Loss: 6.70550236357474638282\n",
      "Iteration 24791 => Loss: 6.70550137660002842921\n",
      "Iteration 24792 => Loss: 6.70550038975831252941\n",
      "Iteration 24793 => Loss: 6.70549940304956582082\n",
      "Iteration 24794 => Loss: 6.70549841647378119802\n",
      "Iteration 24795 => Loss: 6.70549743003093379201\n",
      "Iteration 24796 => Loss: 6.70549644372101738554\n",
      "Iteration 24797 => Loss: 6.70549545754400178055\n",
      "Iteration 24798 => Loss: 6.70549447149987187800\n",
      "Iteration 24799 => Loss: 6.70549348558861879610\n",
      "Iteration 24800 => Loss: 6.70549249981020878408\n",
      "Iteration 24801 => Loss: 6.70549151416463651287\n",
      "Iteration 24802 => Loss: 6.70549052865187800165\n",
      "Iteration 24803 => Loss: 6.70548954327191903957\n",
      "Iteration 24804 => Loss: 6.70548855802474363941\n",
      "Iteration 24805 => Loss: 6.70548757291032426764\n",
      "Iteration 24806 => Loss: 6.70548658792865293066\n",
      "Iteration 24807 => Loss: 6.70548560307970742400\n",
      "Iteration 24808 => Loss: 6.70548461836346465503\n",
      "Iteration 24809 => Loss: 6.70548363377991751832\n",
      "Iteration 24810 => Loss: 6.70548264932904736213\n",
      "Iteration 24811 => Loss: 6.70548166501082221203\n",
      "Iteration 24812 => Loss: 6.70548068082524295619\n",
      "Iteration 24813 => Loss: 6.70547969677228117291\n",
      "Iteration 24814 => Loss: 6.70547871285191821045\n",
      "Iteration 24815 => Loss: 6.70547772906413896976\n",
      "Iteration 24816 => Loss: 6.70547674540892568729\n",
      "Iteration 24817 => Loss: 6.70547576188625704674\n",
      "Iteration 24818 => Loss: 6.70547477849612061362\n",
      "Iteration 24819 => Loss: 6.70547379523849418348\n",
      "Iteration 24820 => Loss: 6.70547281211336265727\n",
      "Iteration 24821 => Loss: 6.70547182912071093597\n",
      "Iteration 24822 => Loss: 6.70547084626051415057\n",
      "Iteration 24823 => Loss: 6.70546986353275897841\n",
      "Iteration 24824 => Loss: 6.70546888093742676773\n",
      "Iteration 24825 => Loss: 6.70546789847449620225\n",
      "Iteration 24826 => Loss: 6.70546691614396195291\n",
      "Iteration 24827 => Loss: 6.70546593394578849257\n",
      "Iteration 24828 => Loss: 6.70546495187996693943\n",
      "Iteration 24829 => Loss: 6.70546396994647952994\n",
      "Iteration 24830 => Loss: 6.70546298814530405963\n",
      "Iteration 24831 => Loss: 6.70546200647643519943\n",
      "Iteration 24832 => Loss: 6.70546102493983919857\n",
      "Iteration 24833 => Loss: 6.70546004353550717525\n",
      "Iteration 24834 => Loss: 6.70545906226342136591\n",
      "Iteration 24835 => Loss: 6.70545808112355867792\n",
      "Iteration 24836 => Loss: 6.70545710011590667676\n",
      "Iteration 24837 => Loss: 6.70545611924044315799\n",
      "Iteration 24838 => Loss: 6.70545513849716012800\n",
      "Iteration 24839 => Loss: 6.70545415788602383600\n",
      "Iteration 24840 => Loss: 6.70545317740702628839\n",
      "Iteration 24841 => Loss: 6.70545219706015327432\n",
      "Iteration 24842 => Loss: 6.70545121684537637208\n",
      "Iteration 24843 => Loss: 6.70545023676268581170\n",
      "Iteration 24844 => Loss: 6.70544925681206560597\n",
      "Iteration 24845 => Loss: 6.70544827699349088590\n",
      "Iteration 24846 => Loss: 6.70544729730694211156\n",
      "Iteration 24847 => Loss: 6.70544631775241484206\n",
      "Iteration 24848 => Loss: 6.70544533832987177391\n",
      "Iteration 24849 => Loss: 6.70544435903931645981\n",
      "Iteration 24850 => Loss: 6.70544337988071514900\n",
      "Iteration 24851 => Loss: 6.70544240085405274243\n",
      "Iteration 24852 => Loss: 6.70544142195932213468\n",
      "Iteration 24853 => Loss: 6.70544044319649135133\n",
      "Iteration 24854 => Loss: 6.70543946456555151059\n",
      "Iteration 24855 => Loss: 6.70543848606648573707\n",
      "Iteration 24856 => Loss: 6.70543750769926738542\n",
      "Iteration 24857 => Loss: 6.70543652946388490932\n",
      "Iteration 24858 => Loss: 6.70543555136032232156\n",
      "Iteration 24859 => Loss: 6.70543457338855564132\n",
      "Iteration 24860 => Loss: 6.70543359554857332228\n",
      "Iteration 24861 => Loss: 6.70543261784035404816\n",
      "Iteration 24862 => Loss: 6.70543164026387650267\n",
      "Iteration 24863 => Loss: 6.70543066281913180404\n",
      "Iteration 24864 => Loss: 6.70542968550610574141\n",
      "Iteration 24865 => Loss: 6.70542870832476367582\n",
      "Iteration 24866 => Loss: 6.70542773127510027820\n",
      "Iteration 24867 => Loss: 6.70542675435709067955\n",
      "Iteration 24868 => Loss: 6.70542577757071978084\n",
      "Iteration 24869 => Loss: 6.70542480091597692393\n",
      "Iteration 24870 => Loss: 6.70542382439283457529\n",
      "Iteration 24871 => Loss: 6.70542284800128296496\n",
      "Iteration 24872 => Loss: 6.70542187174128923033\n",
      "Iteration 24873 => Loss: 6.70542089561285514776\n",
      "Iteration 24874 => Loss: 6.70541991961595318372\n",
      "Iteration 24875 => Loss: 6.70541894375056646282\n",
      "Iteration 24876 => Loss: 6.70541796801668343875\n",
      "Iteration 24877 => Loss: 6.70541699241427480160\n",
      "Iteration 24878 => Loss: 6.70541601694332900507\n",
      "Iteration 24879 => Loss: 6.70541504160382917377\n",
      "Iteration 24880 => Loss: 6.70541406639575487958\n",
      "Iteration 24881 => Loss: 6.70541309131908835894\n",
      "Iteration 24882 => Loss: 6.70541211637381628918\n",
      "Iteration 24883 => Loss: 6.70541114155992001855\n",
      "Iteration 24884 => Loss: 6.70541016687737556623\n",
      "Iteration 24885 => Loss: 6.70540919232617138590\n",
      "Iteration 24886 => Loss: 6.70540821790628527310\n",
      "Iteration 24887 => Loss: 6.70540724361770568152\n",
      "Iteration 24888 => Loss: 6.70540626946041573575\n",
      "Iteration 24889 => Loss: 6.70540529543438879045\n",
      "Iteration 24890 => Loss: 6.70540432153960974659\n",
      "Iteration 24891 => Loss: 6.70540334777606972239\n",
      "Iteration 24892 => Loss: 6.70540237414374029612\n",
      "Iteration 24893 => Loss: 6.70540140064260814512\n",
      "Iteration 24894 => Loss: 6.70540042727265195310\n",
      "Iteration 24895 => Loss: 6.70539945403386639100\n",
      "Iteration 24896 => Loss: 6.70539848092621504350\n",
      "Iteration 24897 => Loss: 6.70539750794969435788\n",
      "Iteration 24898 => Loss: 6.70539653510428390604\n",
      "Iteration 24899 => Loss: 6.70539556238995881898\n",
      "Iteration 24900 => Loss: 6.70539458980671643218\n",
      "Iteration 24901 => Loss: 6.70539361735452654756\n",
      "Iteration 24902 => Loss: 6.70539264503336163159\n",
      "Iteration 24903 => Loss: 6.70539167284323056606\n",
      "Iteration 24904 => Loss: 6.70539070078410226472\n",
      "Iteration 24905 => Loss: 6.70538972885595185858\n",
      "Iteration 24906 => Loss: 6.70538875705877313038\n",
      "Iteration 24907 => Loss: 6.70538778539254387567\n",
      "Iteration 24908 => Loss: 6.70538681385724721906\n",
      "Iteration 24909 => Loss: 6.70538584245286717334\n",
      "Iteration 24910 => Loss: 6.70538487117937975768\n",
      "Iteration 24911 => Loss: 6.70538390003676987305\n",
      "Iteration 24912 => Loss: 6.70538292902502419679\n",
      "Iteration 24913 => Loss: 6.70538195814412762985\n",
      "Iteration 24914 => Loss: 6.70538098739405530324\n",
      "Iteration 24915 => Loss: 6.70538001677478767704\n",
      "Iteration 24916 => Loss: 6.70537904628631409309\n",
      "Iteration 24917 => Loss: 6.70537807592860968242\n",
      "Iteration 24918 => Loss: 6.70537710570166556323\n",
      "Iteration 24919 => Loss: 6.70537613560545420199\n",
      "Iteration 24920 => Loss: 6.70537516563996671692\n",
      "Iteration 24921 => Loss: 6.70537419580518356810\n",
      "Iteration 24922 => Loss: 6.70537322610108788012\n",
      "Iteration 24923 => Loss: 6.70537225652765034312\n",
      "Iteration 24924 => Loss: 6.70537128708487539797\n",
      "Iteration 24925 => Loss: 6.70537031777272130029\n",
      "Iteration 24926 => Loss: 6.70536934859118982644\n",
      "Iteration 24927 => Loss: 6.70536837954025788378\n",
      "Iteration 24928 => Loss: 6.70536741061989793877\n",
      "Iteration 24929 => Loss: 6.70536644183009844511\n",
      "Iteration 24930 => Loss: 6.70536547317085140918\n",
      "Iteration 24931 => Loss: 6.70536450464212485656\n",
      "Iteration 24932 => Loss: 6.70536353624390812911\n",
      "Iteration 24933 => Loss: 6.70536256797619323322\n",
      "Iteration 24934 => Loss: 6.70536159983894286540\n",
      "Iteration 24935 => Loss: 6.70536063183214725569\n",
      "Iteration 24936 => Loss: 6.70535966395579841048\n",
      "Iteration 24937 => Loss: 6.70535869620986790807\n",
      "Iteration 24938 => Loss: 6.70535772859434242577\n",
      "Iteration 24939 => Loss: 6.70535676110920153548\n",
      "Iteration 24940 => Loss: 6.70535579375442214456\n",
      "Iteration 24941 => Loss: 6.70535482653000602937\n",
      "Iteration 24942 => Loss: 6.70535385943591677460\n",
      "Iteration 24943 => Loss: 6.70535289247214461028\n",
      "Iteration 24944 => Loss: 6.70535192563867621374\n",
      "Iteration 24945 => Loss: 6.70535095893548049872\n",
      "Iteration 24946 => Loss: 6.70534999236255302435\n",
      "Iteration 24947 => Loss: 6.70534902591986980980\n",
      "Iteration 24948 => Loss: 6.70534805960741753239\n",
      "Iteration 24949 => Loss: 6.70534709342516954678\n",
      "Iteration 24950 => Loss: 6.70534612737311874753\n",
      "Iteration 24951 => Loss: 6.70534516145124026565\n",
      "Iteration 24952 => Loss: 6.70534419565952433118\n",
      "Iteration 24953 => Loss: 6.70534322999794873965\n",
      "Iteration 24954 => Loss: 6.70534226446649483933\n",
      "Iteration 24955 => Loss: 6.70534129906514753117\n",
      "Iteration 24956 => Loss: 6.70534033379388638707\n",
      "Iteration 24957 => Loss: 6.70533936865269097893\n",
      "Iteration 24958 => Loss: 6.70533840364155864222\n",
      "Iteration 24959 => Loss: 6.70533743876045740251\n",
      "Iteration 24960 => Loss: 6.70533647400937127259\n",
      "Iteration 24961 => Loss: 6.70533550938828604160\n",
      "Iteration 24962 => Loss: 6.70533454489718661051\n",
      "Iteration 24963 => Loss: 6.70533358053604544580\n",
      "Iteration 24964 => Loss: 6.70533261630486432381\n",
      "Iteration 24965 => Loss: 6.70533165220360238834\n",
      "Iteration 24966 => Loss: 6.70533068823226141575\n",
      "Iteration 24967 => Loss: 6.70532972439080943161\n",
      "Iteration 24968 => Loss: 6.70532876067923311325\n",
      "Iteration 24969 => Loss: 6.70532779709752357888\n",
      "Iteration 24970 => Loss: 6.70532683364565507134\n",
      "Iteration 24971 => Loss: 6.70532587032360893886\n",
      "Iteration 24972 => Loss: 6.70532490713136919425\n",
      "Iteration 24973 => Loss: 6.70532394406892251482\n",
      "Iteration 24974 => Loss: 6.70532298113625380154\n",
      "Iteration 24975 => Loss: 6.70532201833333107999\n",
      "Iteration 24976 => Loss: 6.70532105566014990927\n",
      "Iteration 24977 => Loss: 6.70532009311669519036\n",
      "Iteration 24978 => Loss: 6.70531913070293850154\n",
      "Iteration 24979 => Loss: 6.70531816841886740832\n",
      "Iteration 24980 => Loss: 6.70531720626445792988\n",
      "Iteration 24981 => Loss: 6.70531624423970207260\n",
      "Iteration 24982 => Loss: 6.70531528234458207294\n",
      "Iteration 24983 => Loss: 6.70531432057907661459\n",
      "Iteration 24984 => Loss: 6.70531335894317148671\n",
      "Iteration 24985 => Loss: 6.70531239743684270849\n",
      "Iteration 24986 => Loss: 6.70531143606007340452\n",
      "Iteration 24987 => Loss: 6.70531047481286179845\n",
      "Iteration 24988 => Loss: 6.70530951369516881044\n",
      "Iteration 24989 => Loss: 6.70530855270698733506\n",
      "Iteration 24990 => Loss: 6.70530759184830760233\n",
      "Iteration 24991 => Loss: 6.70530663111909497331\n",
      "Iteration 24992 => Loss: 6.70530567051933967804\n",
      "Iteration 24993 => Loss: 6.70530471004903017018\n",
      "Iteration 24994 => Loss: 6.70530374970814690982\n",
      "Iteration 24995 => Loss: 6.70530278949666502797\n",
      "Iteration 24996 => Loss: 6.70530182941456409651\n",
      "Iteration 24997 => Loss: 6.70530086946184145091\n",
      "Iteration 24998 => Loss: 6.70529990963847311036\n",
      "Iteration 24999 => Loss: 6.70529894994444664036\n",
      "Iteration 25000 => Loss: 6.70529799037972740194\n",
      "Iteration 25001 => Loss: 6.70529703094431805965\n",
      "Iteration 25002 => Loss: 6.70529607163818930360\n",
      "Iteration 25003 => Loss: 6.70529511246132692293\n",
      "Iteration 25004 => Loss: 6.70529415341372114767\n",
      "Iteration 25005 => Loss: 6.70529319449533556252\n",
      "Iteration 25006 => Loss: 6.70529223570616927930\n",
      "Iteration 25007 => Loss: 6.70529127704620186989\n",
      "Iteration 25008 => Loss: 6.70529031851541379439\n",
      "Iteration 25009 => Loss: 6.70528936011378107196\n",
      "Iteration 25010 => Loss: 6.70528840184129837354\n",
      "Iteration 25011 => Loss: 6.70528744369794882374\n",
      "Iteration 25012 => Loss: 6.70528648568369689542\n",
      "Iteration 25013 => Loss: 6.70528552779854880583\n",
      "Iteration 25014 => Loss: 6.70528457004247080420\n",
      "Iteration 25015 => Loss: 6.70528361241544246241\n",
      "Iteration 25016 => Loss: 6.70528265491746555682\n",
      "Iteration 25017 => Loss: 6.70528169754850988937\n",
      "Iteration 25018 => Loss: 6.70528074030855858467\n",
      "Iteration 25019 => Loss: 6.70527978319759832004\n",
      "Iteration 25020 => Loss: 6.70527882621560245013\n",
      "Iteration 25021 => Loss: 6.70527786936256298134\n",
      "Iteration 25022 => Loss: 6.70527691263846303826\n",
      "Iteration 25023 => Loss: 6.70527595604327508738\n",
      "Iteration 25024 => Loss: 6.70527499957699113509\n",
      "Iteration 25025 => Loss: 6.70527404323958986510\n",
      "Iteration 25026 => Loss: 6.70527308703106061927\n",
      "Iteration 25027 => Loss: 6.70527213095137053500\n",
      "Iteration 25028 => Loss: 6.70527117500052050048\n",
      "Iteration 25029 => Loss: 6.70527021917848475852\n",
      "Iteration 25030 => Loss: 6.70526926348524110466\n",
      "Iteration 25031 => Loss: 6.70526830792078420984\n",
      "Iteration 25032 => Loss: 6.70526735248508565235\n",
      "Iteration 25033 => Loss: 6.70526639717812766861\n",
      "Iteration 25034 => Loss: 6.70526544199990315320\n",
      "Iteration 25035 => Loss: 6.70526448695039434256\n",
      "Iteration 25036 => Loss: 6.70526353202956393318\n",
      "Iteration 25037 => Loss: 6.70526257723741991867\n",
      "Iteration 25038 => Loss: 6.70526162257393298916\n",
      "Iteration 25039 => Loss: 6.70526066803908626923\n",
      "Iteration 25040 => Loss: 6.70525971363286465987\n",
      "Iteration 25041 => Loss: 6.70525875935524595661\n",
      "Iteration 25042 => Loss: 6.70525780520622038949\n",
      "Iteration 25043 => Loss: 6.70525685118575420773\n",
      "Iteration 25044 => Loss: 6.70525589729385718130\n",
      "Iteration 25045 => Loss: 6.70525494353049467122\n",
      "Iteration 25046 => Loss: 6.70525398989565246666\n",
      "Iteration 25047 => Loss: 6.70525303638930392225\n",
      "Iteration 25048 => Loss: 6.70525208301144637346\n",
      "Iteration 25049 => Loss: 6.70525112976205317494\n",
      "Iteration 25050 => Loss: 6.70525017664111455673\n",
      "Iteration 25051 => Loss: 6.70524922364860920254\n",
      "Iteration 25052 => Loss: 6.70524827078451490792\n",
      "Iteration 25053 => Loss: 6.70524731804882545561\n",
      "Iteration 25054 => Loss: 6.70524636544151508843\n",
      "Iteration 25055 => Loss: 6.70524541296256870737\n",
      "Iteration 25056 => Loss: 6.70524446061196499613\n",
      "Iteration 25057 => Loss: 6.70524350838969507294\n",
      "Iteration 25058 => Loss: 6.70524255629573406878\n",
      "Iteration 25059 => Loss: 6.70524160433006954918\n",
      "Iteration 25060 => Loss: 6.70524065249267842148\n",
      "Iteration 25061 => Loss: 6.70523970078355269209\n",
      "Iteration 25062 => Loss: 6.70523874920266838018\n",
      "Iteration 25063 => Loss: 6.70523779775000683401\n",
      "Iteration 25064 => Loss: 6.70523684642555917179\n",
      "Iteration 25065 => Loss: 6.70523589522929430728\n",
      "Iteration 25066 => Loss: 6.70523494416121934592\n",
      "Iteration 25067 => Loss: 6.70523399322128632605\n",
      "Iteration 25068 => Loss: 6.70523304240949258315\n",
      "Iteration 25069 => Loss: 6.70523209172582834725\n",
      "Iteration 25070 => Loss: 6.70523114117027052572\n",
      "Iteration 25071 => Loss: 6.70523019074278447960\n",
      "Iteration 25072 => Loss: 6.70522924044338264338\n",
      "Iteration 25073 => Loss: 6.70522829027203215446\n",
      "Iteration 25074 => Loss: 6.70522734022871258475\n",
      "Iteration 25075 => Loss: 6.70522639031341771698\n",
      "Iteration 25076 => Loss: 6.70522544052611735310\n",
      "Iteration 25077 => Loss: 6.70522449086680083497\n",
      "Iteration 25078 => Loss: 6.70522354133546105714\n",
      "Iteration 25079 => Loss: 6.70522259193206071615\n",
      "Iteration 25080 => Loss: 6.70522164265659537108\n",
      "Iteration 25081 => Loss: 6.70522069350904370566\n",
      "Iteration 25082 => Loss: 6.70521974448939417357\n",
      "Iteration 25083 => Loss: 6.70521879559761746492\n",
      "Iteration 25084 => Loss: 6.70521784683371002700\n",
      "Iteration 25085 => Loss: 6.70521689819765054352\n",
      "Iteration 25086 => Loss: 6.70521594968941414550\n",
      "Iteration 25087 => Loss: 6.70521500130899195113\n",
      "Iteration 25088 => Loss: 6.70521405305635997962\n",
      "Iteration 25089 => Loss: 6.70521310493151023735\n",
      "Iteration 25090 => Loss: 6.70521215693442051986\n",
      "Iteration 25091 => Loss: 6.70521120906506862269\n",
      "Iteration 25092 => Loss: 6.70521026132344832860\n",
      "Iteration 25093 => Loss: 6.70520931370952943951\n",
      "Iteration 25094 => Loss: 6.70520836622330929089\n",
      "Iteration 25095 => Loss: 6.70520741886476034921\n",
      "Iteration 25096 => Loss: 6.70520647163386751544\n",
      "Iteration 25097 => Loss: 6.70520552453061302600\n",
      "Iteration 25098 => Loss: 6.70520457755498355823\n",
      "Iteration 25099 => Loss: 6.70520363070695513130\n",
      "Iteration 25100 => Loss: 6.70520268398651708708\n",
      "Iteration 25101 => Loss: 6.70520173739364899745\n",
      "Iteration 25102 => Loss: 6.70520079092833221068\n",
      "Iteration 25103 => Loss: 6.70519984459055784498\n",
      "Iteration 25104 => Loss: 6.70519889838029836682\n",
      "Iteration 25105 => Loss: 6.70519795229753778898\n",
      "Iteration 25106 => Loss: 6.70519700634226634151\n",
      "Iteration 25107 => Loss: 6.70519606051446093176\n",
      "Iteration 25108 => Loss: 6.70519511481410912523\n",
      "Iteration 25109 => Loss: 6.70519416924118250023\n",
      "Iteration 25110 => Loss: 6.70519322379567928039\n",
      "Iteration 25111 => Loss: 6.70519227847757726124\n",
      "Iteration 25112 => Loss: 6.70519133328685068562\n",
      "Iteration 25113 => Loss: 6.70519038822348711903\n",
      "Iteration 25114 => Loss: 6.70518944328747679151\n",
      "Iteration 25115 => Loss: 6.70518849847878684045\n",
      "Iteration 25116 => Loss: 6.70518755379741904221\n",
      "Iteration 25117 => Loss: 6.70518660924334408691\n",
      "Iteration 25118 => Loss: 6.70518566481655664546\n",
      "Iteration 25119 => Loss: 6.70518472051702119074\n",
      "Iteration 25120 => Loss: 6.70518377634472617643\n",
      "Iteration 25121 => Loss: 6.70518283229966716164\n",
      "Iteration 25122 => Loss: 6.70518188838181394829\n",
      "Iteration 25123 => Loss: 6.70518094459115765460\n",
      "Iteration 25124 => Loss: 6.70518000092767429976\n",
      "Iteration 25125 => Loss: 6.70517905739135056109\n",
      "Iteration 25126 => Loss: 6.70517811398216689867\n",
      "Iteration 25127 => Loss: 6.70517717070011265434\n",
      "Iteration 25128 => Loss: 6.70517622754516029460\n",
      "Iteration 25129 => Loss: 6.70517528451729738492\n",
      "Iteration 25130 => Loss: 6.70517434161650971447\n",
      "Iteration 25131 => Loss: 6.70517339884277863149\n",
      "Iteration 25132 => Loss: 6.70517245619608726059\n",
      "Iteration 25133 => Loss: 6.70517151367641783821\n",
      "Iteration 25134 => Loss: 6.70517057128374727171\n",
      "Iteration 25135 => Loss: 6.70516962901807289654\n",
      "Iteration 25136 => Loss: 6.70516868687936007376\n",
      "Iteration 25137 => Loss: 6.70516774486760613883\n",
      "Iteration 25138 => Loss: 6.70516680298278977546\n",
      "Iteration 25139 => Loss: 6.70516586122488522648\n",
      "Iteration 25140 => Loss: 6.70516491959389160371\n",
      "Iteration 25141 => Loss: 6.70516397808977959727\n",
      "Iteration 25142 => Loss: 6.70516303671253499630\n",
      "Iteration 25143 => Loss: 6.70516209546213737269\n",
      "Iteration 25144 => Loss: 6.70516115433857695649\n",
      "Iteration 25145 => Loss: 6.70516021334183243141\n",
      "Iteration 25146 => Loss: 6.70515927247188248117\n",
      "Iteration 25147 => Loss: 6.70515833172872000034\n",
      "Iteration 25148 => Loss: 6.70515739111232544900\n",
      "Iteration 25149 => Loss: 6.70515645062267306997\n",
      "Iteration 25150 => Loss: 6.70515551025975753419\n",
      "Iteration 25151 => Loss: 6.70515457002354953175\n",
      "Iteration 25152 => Loss: 6.70515362991404550996\n",
      "Iteration 25153 => Loss: 6.70515268993121615893\n",
      "Iteration 25154 => Loss: 6.70515175007504904414\n",
      "Iteration 25155 => Loss: 6.70515081034553350747\n",
      "Iteration 25156 => Loss: 6.70514987074264112721\n",
      "Iteration 25157 => Loss: 6.70514893126636124521\n",
      "Iteration 25158 => Loss: 6.70514799191667343337\n",
      "Iteration 25159 => Loss: 6.70514705269357058626\n",
      "Iteration 25160 => Loss: 6.70514611359702072946\n",
      "Iteration 25161 => Loss: 6.70514517462701498118\n",
      "Iteration 25162 => Loss: 6.70514423578354001876\n",
      "Iteration 25163 => Loss: 6.70514329706657274954\n",
      "Iteration 25164 => Loss: 6.70514235847609452179\n",
      "Iteration 25165 => Loss: 6.70514142001209112465\n",
      "Iteration 25166 => Loss: 6.70514048167454923544\n",
      "Iteration 25167 => Loss: 6.70513954346345109059\n",
      "Iteration 25168 => Loss: 6.70513860537877537382\n",
      "Iteration 25169 => Loss: 6.70513766742050432157\n",
      "Iteration 25170 => Loss: 6.70513672958861572937\n",
      "Iteration 25171 => Loss: 6.70513579188311403811\n",
      "Iteration 25172 => Loss: 6.70513485430396283249\n",
      "Iteration 25173 => Loss: 6.70513391685115056617\n",
      "Iteration 25174 => Loss: 6.70513297952465947560\n",
      "Iteration 25175 => Loss: 6.70513204232447002084\n",
      "Iteration 25176 => Loss: 6.70513110525057154376\n",
      "Iteration 25177 => Loss: 6.70513016830294183990\n",
      "Iteration 25178 => Loss: 6.70512923148156847475\n",
      "Iteration 25179 => Loss: 6.70512829478643723746\n",
      "Iteration 25180 => Loss: 6.70512735821751792997\n",
      "Iteration 25181 => Loss: 6.70512642177479811778\n",
      "Iteration 25182 => Loss: 6.70512548545826980728\n",
      "Iteration 25183 => Loss: 6.70512454926791079401\n",
      "Iteration 25184 => Loss: 6.70512361320370509077\n",
      "Iteration 25185 => Loss: 6.70512267726563138126\n",
      "Iteration 25186 => Loss: 6.70512174145367190192\n",
      "Iteration 25187 => Loss: 6.70512080576781865915\n",
      "Iteration 25188 => Loss: 6.70511987020804767212\n",
      "Iteration 25189 => Loss: 6.70511893477433940092\n",
      "Iteration 25190 => Loss: 6.70511799946668407557\n",
      "Iteration 25191 => Loss: 6.70511706428505949162\n",
      "Iteration 25192 => Loss: 6.70511612922945765547\n",
      "Iteration 25193 => Loss: 6.70511519429985014540\n",
      "Iteration 25194 => Loss: 6.70511425949622452691\n",
      "Iteration 25195 => Loss: 6.70511332481856658916\n",
      "Iteration 25196 => Loss: 6.70511239026684968678\n",
      "Iteration 25197 => Loss: 6.70511145584106937889\n",
      "Iteration 25198 => Loss: 6.70511052154119635560\n",
      "Iteration 25199 => Loss: 6.70510958736722884055\n",
      "Iteration 25200 => Loss: 6.70510865331913574749\n",
      "Iteration 25201 => Loss: 6.70510771939690464194\n",
      "Iteration 25202 => Loss: 6.70510678560052486574\n",
      "Iteration 25203 => Loss: 6.70510585192996977355\n",
      "Iteration 25204 => Loss: 6.70510491838523314811\n",
      "Iteration 25205 => Loss: 6.70510398496628656773\n",
      "Iteration 25206 => Loss: 6.70510305167311493335\n",
      "Iteration 25207 => Loss: 6.70510211850571113956\n",
      "Iteration 25208 => Loss: 6.70510118546404765283\n",
      "Iteration 25209 => Loss: 6.70510025254812003226\n",
      "Iteration 25210 => Loss: 6.70509931975789008618\n",
      "Iteration 25211 => Loss: 6.70509838709336314366\n",
      "Iteration 25212 => Loss: 6.70509745455450456575\n",
      "Iteration 25213 => Loss: 6.70509652214131435244\n",
      "Iteration 25214 => Loss: 6.70509558985376585838\n",
      "Iteration 25215 => Loss: 6.70509465769183687911\n",
      "Iteration 25216 => Loss: 6.70509372565552208556\n",
      "Iteration 25217 => Loss: 6.70509279374479660873\n",
      "Iteration 25218 => Loss: 6.70509186195965423138\n",
      "Iteration 25219 => Loss: 6.70509093030005587366\n",
      "Iteration 25220 => Loss: 6.70508999876600864098\n",
      "Iteration 25221 => Loss: 6.70508906735747967076\n",
      "Iteration 25222 => Loss: 6.70508813607446452210\n",
      "Iteration 25223 => Loss: 6.70508720491693743782\n",
      "Iteration 25224 => Loss: 6.70508627388487976617\n",
      "Iteration 25225 => Loss: 6.70508534297828440174\n",
      "Iteration 25226 => Loss: 6.70508441219712647552\n",
      "Iteration 25227 => Loss: 6.70508348154139177666\n",
      "Iteration 25228 => Loss: 6.70508255101105721252\n",
      "Iteration 25229 => Loss: 6.70508162060611923039\n",
      "Iteration 25230 => Loss: 6.70508069032655207309\n",
      "Iteration 25231 => Loss: 6.70507976017233442434\n",
      "Iteration 25232 => Loss: 6.70507883014346539596\n",
      "Iteration 25233 => Loss: 6.70507790023991034900\n",
      "Iteration 25234 => Loss: 6.70507697046166217802\n",
      "Iteration 25235 => Loss: 6.70507604080870134311\n",
      "Iteration 25236 => Loss: 6.70507511128101008069\n",
      "Iteration 25237 => Loss: 6.70507418187857684444\n",
      "Iteration 25238 => Loss: 6.70507325260137676537\n",
      "Iteration 25239 => Loss: 6.70507232344939740898\n",
      "Iteration 25240 => Loss: 6.70507139442262189988\n",
      "Iteration 25241 => Loss: 6.70507046552102892178\n",
      "Iteration 25242 => Loss: 6.70506953674461847470\n",
      "Iteration 25243 => Loss: 6.70506860809335059059\n",
      "Iteration 25244 => Loss: 6.70506767956721283497\n",
      "Iteration 25245 => Loss: 6.70506675116620254329\n",
      "Iteration 25246 => Loss: 6.70506582289029129385\n",
      "Iteration 25247 => Loss: 6.70506489473946576396\n",
      "Iteration 25248 => Loss: 6.70506396671371707185\n",
      "Iteration 25249 => Loss: 6.70506303881301146674\n",
      "Iteration 25250 => Loss: 6.70506211103733562595\n",
      "Iteration 25251 => Loss: 6.70506118338668510859\n",
      "Iteration 25252 => Loss: 6.70506025586103859837\n",
      "Iteration 25253 => Loss: 6.70505932846036500905\n",
      "Iteration 25254 => Loss: 6.70505840118466611699\n",
      "Iteration 25255 => Loss: 6.70505747403392060590\n",
      "Iteration 25256 => Loss: 6.70505654700810094226\n",
      "Iteration 25257 => Loss: 6.70505562010720712607\n",
      "Iteration 25258 => Loss: 6.70505469333120363018\n",
      "Iteration 25259 => Loss: 6.70505376668009400731\n",
      "Iteration 25260 => Loss: 6.70505284015384095397\n",
      "Iteration 25261 => Loss: 6.70505191375243914109\n",
      "Iteration 25262 => Loss: 6.70505098747587524599\n",
      "Iteration 25263 => Loss: 6.70505006132412351150\n",
      "Iteration 25264 => Loss: 6.70504913529716706222\n",
      "Iteration 25265 => Loss: 6.70504820939499879273\n",
      "Iteration 25266 => Loss: 6.70504728361759827493\n",
      "Iteration 25267 => Loss: 6.70504635796493353439\n",
      "Iteration 25268 => Loss: 6.70504543243701522925\n",
      "Iteration 25269 => Loss: 6.70504450703380783239\n",
      "Iteration 25270 => Loss: 6.70504358175529446839\n",
      "Iteration 25271 => Loss: 6.70504265660146536732\n",
      "Iteration 25272 => Loss: 6.70504173157229921287\n",
      "Iteration 25273 => Loss: 6.70504080666778357056\n",
      "Iteration 25274 => Loss: 6.70503988188789090685\n",
      "Iteration 25275 => Loss: 6.70503895723262477446\n",
      "Iteration 25276 => Loss: 6.70503803270194431718\n",
      "Iteration 25277 => Loss: 6.70503710829585308772\n",
      "Iteration 25278 => Loss: 6.70503618401431644713\n",
      "Iteration 25279 => Loss: 6.70503525985733705994\n",
      "Iteration 25280 => Loss: 6.70503433582488117537\n",
      "Iteration 25281 => Loss: 6.70503341191694257617\n",
      "Iteration 25282 => Loss: 6.70503248813349905788\n",
      "Iteration 25283 => Loss: 6.70503156447453729783\n",
      "Iteration 25284 => Loss: 6.70503064094003686790\n",
      "Iteration 25285 => Loss: 6.70502971752998444543\n",
      "Iteration 25286 => Loss: 6.70502879424435249689\n",
      "Iteration 25287 => Loss: 6.70502787108314279862\n",
      "Iteration 25288 => Loss: 6.70502694804632870529\n",
      "Iteration 25289 => Loss: 6.70502602513389334149\n",
      "Iteration 25290 => Loss: 6.70502510234581716730\n",
      "Iteration 25291 => Loss: 6.70502417968208774823\n",
      "Iteration 25292 => Loss: 6.70502325714269264978\n",
      "Iteration 25293 => Loss: 6.70502233472760611477\n",
      "Iteration 25294 => Loss: 6.70502141243681215599\n",
      "Iteration 25295 => Loss: 6.70502049027029922712\n",
      "Iteration 25296 => Loss: 6.70501956822804423552\n",
      "Iteration 25297 => Loss: 6.70501864631004362849\n",
      "Iteration 25298 => Loss: 6.70501772451626809612\n",
      "Iteration 25299 => Loss: 6.70501680284670076304\n",
      "Iteration 25300 => Loss: 6.70501588130133185928\n",
      "Iteration 25301 => Loss: 6.70501495988013918037\n",
      "Iteration 25302 => Loss: 6.70501403858311118000\n",
      "Iteration 25303 => Loss: 6.70501311741022298918\n",
      "Iteration 25304 => Loss: 6.70501219636146927883\n",
      "Iteration 25305 => Loss: 6.70501127543682162724\n",
      "Iteration 25306 => Loss: 6.70501035463627204081\n",
      "Iteration 25307 => Loss: 6.70500943395979920325\n",
      "Iteration 25308 => Loss: 6.70500851340738979189\n",
      "Iteration 25309 => Loss: 6.70500759297901804956\n",
      "Iteration 25310 => Loss: 6.70500667267467687083\n",
      "Iteration 25311 => Loss: 6.70500575249434671576\n",
      "Iteration 25312 => Loss: 6.70500483243801337352\n",
      "Iteration 25313 => Loss: 6.70500391250565908052\n",
      "Iteration 25314 => Loss: 6.70500299269726784956\n",
      "Iteration 25315 => Loss: 6.70500207301281303529\n",
      "Iteration 25316 => Loss: 6.70500115345229463770\n",
      "Iteration 25317 => Loss: 6.70500023401567890602\n",
      "Iteration 25318 => Loss: 6.70499931470295962299\n",
      "Iteration 25319 => Loss: 6.70499839551411813687\n",
      "Iteration 25320 => Loss: 6.70499747644913668410\n",
      "Iteration 25321 => Loss: 6.70499655750800283016\n",
      "Iteration 25322 => Loss: 6.70499563869068726518\n",
      "Iteration 25323 => Loss: 6.70499471999719354187\n",
      "Iteration 25324 => Loss: 6.70499380142749146216\n",
      "Iteration 25325 => Loss: 6.70499288298156059795\n",
      "Iteration 25326 => Loss: 6.70499196465939117928\n",
      "Iteration 25327 => Loss: 6.70499104646097254800\n",
      "Iteration 25328 => Loss: 6.70499012838628161148\n",
      "Iteration 25329 => Loss: 6.70498921043529527708\n",
      "Iteration 25330 => Loss: 6.70498829260800821572\n",
      "Iteration 25331 => Loss: 6.70498737490438934117\n",
      "Iteration 25332 => Loss: 6.70498645732443954159\n",
      "Iteration 25333 => Loss: 6.70498553986813572436\n",
      "Iteration 25334 => Loss: 6.70498462253544680323\n",
      "Iteration 25335 => Loss: 6.70498370532637899544\n",
      "Iteration 25336 => Loss: 6.70498278824089766204\n",
      "Iteration 25337 => Loss: 6.70498187127900102666\n",
      "Iteration 25338 => Loss: 6.70498095444066599669\n",
      "Iteration 25339 => Loss: 6.70498003772587125582\n",
      "Iteration 25340 => Loss: 6.70497912113460881045\n",
      "Iteration 25341 => Loss: 6.70497820466684402163\n",
      "Iteration 25342 => Loss: 6.70497728832258310661\n",
      "Iteration 25343 => Loss: 6.70497637210180208456\n",
      "Iteration 25344 => Loss: 6.70497545600448141556\n",
      "Iteration 25345 => Loss: 6.70497454003059267791\n",
      "Iteration 25346 => Loss: 6.70497362418014386520\n",
      "Iteration 25347 => Loss: 6.70497270845310477938\n",
      "Iteration 25348 => Loss: 6.70497179284945676869\n",
      "Iteration 25349 => Loss: 6.70497087736918384593\n",
      "Iteration 25350 => Loss: 6.70496996201228068202\n",
      "Iteration 25351 => Loss: 6.70496904677871441436\n",
      "Iteration 25352 => Loss: 6.70496813166847793752\n",
      "Iteration 25353 => Loss: 6.70496721668155348794\n",
      "Iteration 25354 => Loss: 6.70496630181792507841\n",
      "Iteration 25355 => Loss: 6.70496538707757228082\n",
      "Iteration 25356 => Loss: 6.70496447246047910795\n",
      "Iteration 25357 => Loss: 6.70496355796663401350\n",
      "Iteration 25358 => Loss: 6.70496264359601301663\n",
      "Iteration 25359 => Loss: 6.70496172934860634740\n",
      "Iteration 25360 => Loss: 6.70496081522439624223\n",
      "Iteration 25361 => Loss: 6.70495990122336227302\n",
      "Iteration 25362 => Loss: 6.70495898734548756437\n",
      "Iteration 25363 => Loss: 6.70495807359076323451\n",
      "Iteration 25364 => Loss: 6.70495715995916530261\n",
      "Iteration 25365 => Loss: 6.70495624645067778147\n",
      "Iteration 25366 => Loss: 6.70495533306528646023\n",
      "Iteration 25367 => Loss: 6.70495441980297446349\n",
      "Iteration 25368 => Loss: 6.70495350666373113313\n",
      "Iteration 25369 => Loss: 6.70495259364751561293\n",
      "Iteration 25370 => Loss: 6.70495168075434833099\n",
      "Iteration 25371 => Loss: 6.70495076798418399022\n",
      "Iteration 25372 => Loss: 6.70494985533701459701\n",
      "Iteration 25373 => Loss: 6.70494894281282771686\n",
      "Iteration 25374 => Loss: 6.70494803041160203350\n",
      "Iteration 25375 => Loss: 6.70494711813332422423\n",
      "Iteration 25376 => Loss: 6.70494620597797563732\n",
      "Iteration 25377 => Loss: 6.70494529394553673285\n",
      "Iteration 25378 => Loss: 6.70494438203599596449\n",
      "Iteration 25379 => Loss: 6.70494347024934356227\n",
      "Iteration 25380 => Loss: 6.70494255858554399907\n",
      "Iteration 25381 => Loss: 6.70494164704459283399\n",
      "Iteration 25382 => Loss: 6.70494073562647852071\n",
      "Iteration 25383 => Loss: 6.70493982433116642028\n",
      "Iteration 25384 => Loss: 6.70493891315865742087\n",
      "Iteration 25385 => Loss: 6.70493800210893287073\n",
      "Iteration 25386 => Loss: 6.70493709118196878904\n",
      "Iteration 25387 => Loss: 6.70493618037775096496\n",
      "Iteration 25388 => Loss: 6.70493526969626163492\n",
      "Iteration 25389 => Loss: 6.70493435913749102895\n",
      "Iteration 25390 => Loss: 6.70493344870141338987\n",
      "Iteration 25391 => Loss: 6.70493253838802161226\n",
      "Iteration 25392 => Loss: 6.70493162819729970892\n",
      "Iteration 25393 => Loss: 6.70493071812921392905\n",
      "Iteration 25394 => Loss: 6.70492980818376960173\n",
      "Iteration 25395 => Loss: 6.70492889836093297617\n",
      "Iteration 25396 => Loss: 6.70492798866070494057\n",
      "Iteration 25397 => Loss: 6.70492707908305529685\n",
      "Iteration 25398 => Loss: 6.70492616962796716962\n",
      "Iteration 25399 => Loss: 6.70492526029542812438\n",
      "Iteration 25400 => Loss: 6.70492435108542483846\n",
      "Iteration 25401 => Loss: 6.70492344199794043647\n",
      "Iteration 25402 => Loss: 6.70492253303294827305\n",
      "Iteration 25403 => Loss: 6.70492162419044746002\n",
      "Iteration 25404 => Loss: 6.70492071547040691115\n",
      "Iteration 25405 => Loss: 6.70491980687281863283\n",
      "Iteration 25406 => Loss: 6.70491889839766130876\n",
      "Iteration 25407 => Loss: 6.70491799004492428082\n",
      "Iteration 25408 => Loss: 6.70491708181458179183\n",
      "Iteration 25409 => Loss: 6.70491617370663650632\n",
      "Iteration 25410 => Loss: 6.70491526572104756809\n",
      "Iteration 25411 => Loss: 6.70491435785781852985\n",
      "Iteration 25412 => Loss: 6.70491345011691564082\n",
      "Iteration 25413 => Loss: 6.70491254249833090739\n",
      "Iteration 25414 => Loss: 6.70491163500205278325\n",
      "Iteration 25415 => Loss: 6.70491072762805639940\n",
      "Iteration 25416 => Loss: 6.70490982037633020951\n",
      "Iteration 25417 => Loss: 6.70490891324685822639\n",
      "Iteration 25418 => Loss: 6.70490800623962091009\n",
      "Iteration 25419 => Loss: 6.70490709935460227342\n",
      "Iteration 25420 => Loss: 6.70490619259178366462\n",
      "Iteration 25421 => Loss: 6.70490528595115620192\n",
      "Iteration 25422 => Loss: 6.70490437943269945720\n",
      "Iteration 25423 => Loss: 6.70490347303639211418\n",
      "Iteration 25424 => Loss: 6.70490256676221996202\n",
      "Iteration 25425 => Loss: 6.70490166061016790167\n",
      "Iteration 25426 => Loss: 6.70490075458023149224\n",
      "Iteration 25427 => Loss: 6.70489984867237165389\n",
      "Iteration 25428 => Loss: 6.70489894288658661026\n",
      "Iteration 25429 => Loss: 6.70489803722285593324\n",
      "Iteration 25430 => Loss: 6.70489713168116541198\n",
      "Iteration 25431 => Loss: 6.70489622626149106566\n",
      "Iteration 25432 => Loss: 6.70489532096382578885\n",
      "Iteration 25433 => Loss: 6.70489441578814648892\n",
      "Iteration 25434 => Loss: 6.70489351073444517226\n",
      "Iteration 25435 => Loss: 6.70489260580269785805\n",
      "Iteration 25436 => Loss: 6.70489170099288678273\n",
      "Iteration 25437 => Loss: 6.70489079630500395268\n",
      "Iteration 25438 => Loss: 6.70488989173902361074\n",
      "Iteration 25439 => Loss: 6.70488898729493332240\n",
      "Iteration 25440 => Loss: 6.70488808297272154135\n",
      "Iteration 25441 => Loss: 6.70488717877236251041\n",
      "Iteration 25442 => Loss: 6.70488627469385001234\n",
      "Iteration 25443 => Loss: 6.70488537073715473724\n",
      "Iteration 25444 => Loss: 6.70488446690227668512\n",
      "Iteration 25445 => Loss: 6.70488356318918388155\n",
      "Iteration 25446 => Loss: 6.70488265959786922110\n",
      "Iteration 25447 => Loss: 6.70488175612830605843\n",
      "Iteration 25448 => Loss: 6.70488085278049883442\n",
      "Iteration 25449 => Loss: 6.70487994955440758105\n",
      "Iteration 25450 => Loss: 6.70487904645003141013\n",
      "Iteration 25451 => Loss: 6.70487814346734456450\n",
      "Iteration 25452 => Loss: 6.70487724060633727419\n",
      "Iteration 25453 => Loss: 6.70487633786699355198\n",
      "Iteration 25454 => Loss: 6.70487543524928941707\n",
      "Iteration 25455 => Loss: 6.70487453275321776403\n",
      "Iteration 25456 => Loss: 6.70487363037874661842\n",
      "Iteration 25457 => Loss: 6.70487272812587775661\n",
      "Iteration 25458 => Loss: 6.70487182599459075050\n",
      "Iteration 25459 => Loss: 6.70487092398485895473\n",
      "Iteration 25460 => Loss: 6.70487002209668236929\n",
      "Iteration 25461 => Loss: 6.70486912033002990796\n",
      "Iteration 25462 => Loss: 6.70486821868489002441\n",
      "Iteration 25463 => Loss: 6.70486731716124406688\n",
      "Iteration 25464 => Loss: 6.70486641575908581814\n",
      "Iteration 25465 => Loss: 6.70486551447838241558\n",
      "Iteration 25466 => Loss: 6.70486461331913297101\n",
      "Iteration 25467 => Loss: 6.70486371228131794453\n",
      "Iteration 25468 => Loss: 6.70486281136491424348\n",
      "Iteration 25469 => Loss: 6.70486191056990588066\n",
      "Iteration 25470 => Loss: 6.70486100989628308611\n",
      "Iteration 25471 => Loss: 6.70486010934402543171\n",
      "Iteration 25472 => Loss: 6.70485920891311959480\n",
      "Iteration 25473 => Loss: 6.70485830860354159455\n",
      "Iteration 25474 => Loss: 6.70485740841528432554\n",
      "Iteration 25475 => Loss: 6.70485650834832824785\n",
      "Iteration 25476 => Loss: 6.70485560840265115701\n",
      "Iteration 25477 => Loss: 6.70485470857824417124\n",
      "Iteration 25478 => Loss: 6.70485380887508952696\n",
      "Iteration 25479 => Loss: 6.70485290929316857245\n",
      "Iteration 25480 => Loss: 6.70485200983246709683\n",
      "Iteration 25481 => Loss: 6.70485111049297177743\n",
      "Iteration 25482 => Loss: 6.70485021127465774526\n",
      "Iteration 25483 => Loss: 6.70484931217751167765\n",
      "Iteration 25484 => Loss: 6.70484841320152735733\n",
      "Iteration 25485 => Loss: 6.70484751434667636261\n",
      "Iteration 25486 => Loss: 6.70484661561294004173\n",
      "Iteration 25487 => Loss: 6.70484571700031040109\n",
      "Iteration 25488 => Loss: 6.70484481850877589437\n",
      "Iteration 25489 => Loss: 6.70484392013831254076\n",
      "Iteration 25490 => Loss: 6.70484302188890168850\n",
      "Iteration 25491 => Loss: 6.70484212376052912674\n",
      "Iteration 25492 => Loss: 6.70484122575317620374\n",
      "Iteration 25493 => Loss: 6.70484032786684203131\n",
      "Iteration 25494 => Loss: 6.70483943010148131236\n",
      "Iteration 25495 => Loss: 6.70483853245710115232\n",
      "Iteration 25496 => Loss: 6.70483763493368378761\n",
      "Iteration 25497 => Loss: 6.70483673753119902017\n",
      "Iteration 25498 => Loss: 6.70483584024964773818\n",
      "Iteration 25499 => Loss: 6.70483494308899619085\n",
      "Iteration 25500 => Loss: 6.70483404604924615455\n",
      "Iteration 25501 => Loss: 6.70483314913037453664\n",
      "Iteration 25502 => Loss: 6.70483225233235646812\n",
      "Iteration 25503 => Loss: 6.70483135565517507359\n",
      "Iteration 25504 => Loss: 6.70483045909883834668\n",
      "Iteration 25505 => Loss: 6.70482956266330276662\n",
      "Iteration 25506 => Loss: 6.70482866634855856347\n",
      "Iteration 25507 => Loss: 6.70482777015459330272\n",
      "Iteration 25508 => Loss: 6.70482687408139632623\n",
      "Iteration 25509 => Loss: 6.70482597812894010048\n",
      "Iteration 25510 => Loss: 6.70482508229721840820\n",
      "Iteration 25511 => Loss: 6.70482418658620549223\n",
      "Iteration 25512 => Loss: 6.70482329099589335897\n",
      "Iteration 25513 => Loss: 6.70482239552626158030\n",
      "Iteration 25514 => Loss: 6.70482150017728883995\n",
      "Iteration 25515 => Loss: 6.70482060494897247338\n",
      "Iteration 25516 => Loss: 6.70481970984128228253\n",
      "Iteration 25517 => Loss: 6.70481881485421205014\n",
      "Iteration 25518 => Loss: 6.70481791998773690722\n",
      "Iteration 25519 => Loss: 6.70481702524184530745\n",
      "Iteration 25520 => Loss: 6.70481613061652748087\n",
      "Iteration 25521 => Loss: 6.70481523611175678212\n",
      "Iteration 25522 => Loss: 6.70481434172751900036\n",
      "Iteration 25523 => Loss: 6.70481344746379992472\n",
      "Iteration 25524 => Loss: 6.70481255332058356800\n",
      "Iteration 25525 => Loss: 6.70481165929785216662\n",
      "Iteration 25526 => Loss: 6.70481076539559239791\n",
      "Iteration 25527 => Loss: 6.70480987161378649830\n",
      "Iteration 25528 => Loss: 6.70480897795241848058\n",
      "Iteration 25529 => Loss: 6.70480808441146969301\n",
      "Iteration 25530 => Loss: 6.70480719099092414837\n",
      "Iteration 25531 => Loss: 6.70480629769076497126\n",
      "Iteration 25532 => Loss: 6.70480540451098327992\n",
      "Iteration 25533 => Loss: 6.70480451145155509352\n",
      "Iteration 25534 => Loss: 6.70480361851247064209\n",
      "Iteration 25535 => Loss: 6.70480272569370239211\n",
      "Iteration 25536 => Loss: 6.70480183299524590268\n",
      "Iteration 25537 => Loss: 6.70480094041707808117\n",
      "Iteration 25538 => Loss: 6.70480004795918826943\n",
      "Iteration 25539 => Loss: 6.70479915562155159847\n",
      "Iteration 25540 => Loss: 6.70479826340416895647\n",
      "Iteration 25541 => Loss: 6.70479737130700392811\n",
      "Iteration 25542 => Loss: 6.70479647933005562521\n",
      "Iteration 25543 => Loss: 6.70479558747329473789\n",
      "Iteration 25544 => Loss: 6.70479469573671771343\n",
      "Iteration 25545 => Loss: 6.70479380412028991287\n",
      "Iteration 25546 => Loss: 6.70479291262402110618\n",
      "Iteration 25547 => Loss: 6.70479202124787487804\n",
      "Iteration 25548 => Loss: 6.70479112999184145849\n",
      "Iteration 25549 => Loss: 6.70479023885590574849\n",
      "Iteration 25550 => Loss: 6.70478934784005708991\n",
      "Iteration 25551 => Loss: 6.70478845694426350832\n",
      "Iteration 25552 => Loss: 6.70478756616852056283\n",
      "Iteration 25553 => Loss: 6.70478667551281048986\n",
      "Iteration 25554 => Loss: 6.70478578497711552586\n",
      "Iteration 25555 => Loss: 6.70478489456142323633\n",
      "Iteration 25556 => Loss: 6.70478400426571674586\n",
      "Iteration 25557 => Loss: 6.70478311408996940912\n",
      "Iteration 25558 => Loss: 6.70478222403417678521\n",
      "Iteration 25559 => Loss: 6.70478133409831755785\n",
      "Iteration 25560 => Loss: 6.70478044428237751617\n",
      "Iteration 25561 => Loss: 6.70477955458634955477\n",
      "Iteration 25562 => Loss: 6.70477866501019903467\n",
      "Iteration 25563 => Loss: 6.70477777555392062681\n",
      "Iteration 25564 => Loss: 6.70477688621750367304\n",
      "Iteration 25565 => Loss: 6.70477599700091175805\n",
      "Iteration 25566 => Loss: 6.70477510790414665820\n",
      "Iteration 25567 => Loss: 6.70477421892718705720\n",
      "Iteration 25568 => Loss: 6.70477333007002052057\n",
      "Iteration 25569 => Loss: 6.70477244133263017289\n",
      "Iteration 25570 => Loss: 6.70477155271499558609\n",
      "Iteration 25571 => Loss: 6.70477066421709988475\n",
      "Iteration 25572 => Loss: 6.70476977583891997625\n",
      "Iteration 25573 => Loss: 6.70476888758046651873\n",
      "Iteration 25574 => Loss: 6.70476799944168888601\n",
      "Iteration 25575 => Loss: 6.70476711142260572984\n",
      "Iteration 25576 => Loss: 6.70476622352317175313\n",
      "Iteration 25577 => Loss: 6.70476533574338695587\n",
      "Iteration 25578 => Loss: 6.70476444808322913360\n",
      "Iteration 25579 => Loss: 6.70476356054268407547\n",
      "Iteration 25580 => Loss: 6.70476267312173046520\n",
      "Iteration 25581 => Loss: 6.70476178582036297371\n",
      "Iteration 25582 => Loss: 6.70476089863855584383\n",
      "Iteration 25583 => Loss: 6.70476001157630374649\n",
      "Iteration 25584 => Loss: 6.70475912463357648363\n",
      "Iteration 25585 => Loss: 6.70475823781036162075\n",
      "Iteration 25586 => Loss: 6.70475735110664938787\n",
      "Iteration 25587 => Loss: 6.70475646452242468598\n",
      "Iteration 25588 => Loss: 6.70475557805766531061\n",
      "Iteration 25589 => Loss: 6.70475469171235349819\n",
      "Iteration 25590 => Loss: 6.70475380548648036694\n",
      "Iteration 25591 => Loss: 6.70475291938002460057\n",
      "Iteration 25592 => Loss: 6.70475203339297198823\n",
      "Iteration 25593 => Loss: 6.70475114752530831908\n",
      "Iteration 25594 => Loss: 6.70475026177701050045\n",
      "Iteration 25595 => Loss: 6.70474937614807320330\n",
      "Iteration 25596 => Loss: 6.70474849063846711772\n",
      "Iteration 25597 => Loss: 6.70474760524819313190\n",
      "Iteration 25598 => Loss: 6.70474671997721838324\n",
      "Iteration 25599 => Loss: 6.70474583482553487812\n",
      "Iteration 25600 => Loss: 6.70474494979312929388\n",
      "Iteration 25601 => Loss: 6.70474406487998297877\n",
      "Iteration 25602 => Loss: 6.70474318008606928743\n",
      "Iteration 25603 => Loss: 6.70474229541139177257\n",
      "Iteration 25604 => Loss: 6.70474141085591934797\n",
      "Iteration 25605 => Loss: 6.70474052641963691457\n",
      "Iteration 25606 => Loss: 6.70473964210254180784\n",
      "Iteration 25607 => Loss: 6.70473875790460382973\n",
      "Iteration 25608 => Loss: 6.70473787382580876937\n",
      "Iteration 25609 => Loss: 6.70473698986614508044\n",
      "Iteration 25610 => Loss: 6.70473610602559677574\n",
      "Iteration 25611 => Loss: 6.70473522230414697987\n",
      "Iteration 25612 => Loss: 6.70473433870177881744\n",
      "Iteration 25613 => Loss: 6.70473345521847452488\n",
      "Iteration 25614 => Loss: 6.70473257185421811499\n",
      "Iteration 25615 => Loss: 6.70473168860899892962\n",
      "Iteration 25616 => Loss: 6.70473080548279565249\n",
      "Iteration 25617 => Loss: 6.70472992247558874368\n",
      "Iteration 25618 => Loss: 6.70472903958737109775\n",
      "Iteration 25619 => Loss: 6.70472815681812495114\n",
      "Iteration 25620 => Loss: 6.70472727416782632304\n",
      "Iteration 25621 => Loss: 6.70472639163647254890\n",
      "Iteration 25622 => Loss: 6.70472550922403431883\n",
      "Iteration 25623 => Loss: 6.70472462693050097471\n",
      "Iteration 25624 => Loss: 6.70472374475585919384\n",
      "Iteration 25625 => Loss: 6.70472286270009565357\n",
      "Iteration 25626 => Loss: 6.70472198076317749127\n",
      "Iteration 25627 => Loss: 6.70472109894510914785\n",
      "Iteration 25628 => Loss: 6.70472021724586308977\n",
      "Iteration 25629 => Loss: 6.70471933566542599436\n",
      "Iteration 25630 => Loss: 6.70471845420378365077\n",
      "Iteration 25631 => Loss: 6.70471757286091474271\n",
      "Iteration 25632 => Loss: 6.70471669163680772385\n",
      "Iteration 25633 => Loss: 6.70471581053144483064\n",
      "Iteration 25634 => Loss: 6.70471492954481540494\n",
      "Iteration 25635 => Loss: 6.70471404867689724227\n",
      "Iteration 25636 => Loss: 6.70471316792767346726\n",
      "Iteration 25637 => Loss: 6.70471228729713342176\n",
      "Iteration 25638 => Loss: 6.70471140678525578949\n",
      "Iteration 25639 => Loss: 6.70471052639203346502\n",
      "Iteration 25640 => Loss: 6.70470964611743447392\n",
      "Iteration 25641 => Loss: 6.70470876596146236892\n",
      "Iteration 25642 => Loss: 6.70470788592408251105\n",
      "Iteration 25643 => Loss: 6.70470700600529223578\n",
      "Iteration 25644 => Loss: 6.70470612620507289137\n",
      "Iteration 25645 => Loss: 6.70470524652340316152\n",
      "Iteration 25646 => Loss: 6.70470436696027061174\n",
      "Iteration 25647 => Loss: 6.70470348751566280754\n",
      "Iteration 25648 => Loss: 6.70470260818956109716\n",
      "Iteration 25649 => Loss: 6.70470172898194238797\n",
      "Iteration 25650 => Loss: 6.70470084989280223908\n",
      "Iteration 25651 => Loss: 6.70469997092212111056\n",
      "Iteration 25652 => Loss: 6.70469909206987413341\n",
      "Iteration 25653 => Loss: 6.70469821333605775493\n",
      "Iteration 25654 => Loss: 6.70469733472064444157\n",
      "Iteration 25655 => Loss: 6.70469645622363152881\n",
      "Iteration 25656 => Loss: 6.70469557784499503583\n",
      "Iteration 25657 => Loss: 6.70469469958471897542\n",
      "Iteration 25658 => Loss: 6.70469382144278647218\n",
      "Iteration 25659 => Loss: 6.70469294341918775615\n",
      "Iteration 25660 => Loss: 6.70469206551389707016\n",
      "Iteration 25661 => Loss: 6.70469118772690730879\n",
      "Iteration 25662 => Loss: 6.70469031005820603752\n",
      "Iteration 25663 => Loss: 6.70468943250776572285\n",
      "Iteration 25664 => Loss: 6.70468855507556948936\n",
      "Iteration 25665 => Loss: 6.70468767776161378436\n",
      "Iteration 25666 => Loss: 6.70468680056587462701\n",
      "Iteration 25667 => Loss: 6.70468592348833958283\n",
      "Iteration 25668 => Loss: 6.70468504652898378282\n",
      "Iteration 25669 => Loss: 6.70468416968780545062\n",
      "Iteration 25670 => Loss: 6.70468329296479037538\n",
      "Iteration 25671 => Loss: 6.70468241635989770089\n",
      "Iteration 25672 => Loss: 6.70468153987313186803\n",
      "Iteration 25673 => Loss: 6.70468066350447600144\n",
      "Iteration 25674 => Loss: 6.70467978725390434391\n",
      "Iteration 25675 => Loss: 6.70467891112141067822\n",
      "Iteration 25676 => Loss: 6.70467803510698079350\n",
      "Iteration 25677 => Loss: 6.70467715921058449169\n",
      "Iteration 25678 => Loss: 6.70467628343222177278\n",
      "Iteration 25679 => Loss: 6.70467540777187309686\n",
      "Iteration 25680 => Loss: 6.70467453222951359493\n",
      "Iteration 25681 => Loss: 6.70467365680513527337\n",
      "Iteration 25682 => Loss: 6.70467278149871948045\n",
      "Iteration 25683 => Loss: 6.70467190631025466985\n",
      "Iteration 25684 => Loss: 6.70467103123972307799\n",
      "Iteration 25685 => Loss: 6.70467015628710427677\n",
      "Iteration 25686 => Loss: 6.70466928145238227899\n",
      "Iteration 25687 => Loss: 6.70466840673554909102\n",
      "Iteration 25688 => Loss: 6.70466753213658162025\n",
      "Iteration 25689 => Loss: 6.70466665765546832034\n",
      "Iteration 25690 => Loss: 6.70466578329218965138\n",
      "Iteration 25691 => Loss: 6.70466490904673317885\n",
      "Iteration 25692 => Loss: 6.70466403491908025103\n",
      "Iteration 25693 => Loss: 6.70466316090921576887\n",
      "Iteration 25694 => Loss: 6.70466228701712285698\n",
      "Iteration 25695 => Loss: 6.70466141324279085723\n",
      "Iteration 25696 => Loss: 6.70466053958619490061\n",
      "Iteration 25697 => Loss: 6.70465966604732965806\n",
      "Iteration 25698 => Loss: 6.70465879262616670786\n",
      "Iteration 25699 => Loss: 6.70465791932270516185\n",
      "Iteration 25700 => Loss: 6.70465704613691571012\n",
      "Iteration 25701 => Loss: 6.70465617306879568815\n",
      "Iteration 25702 => Loss: 6.70465530011831578605\n",
      "Iteration 25703 => Loss: 6.70465442728546534568\n",
      "Iteration 25704 => Loss: 6.70465355457023637342\n",
      "Iteration 25705 => Loss: 6.70465268197259867122\n",
      "Iteration 25706 => Loss: 6.70465180949253980458\n",
      "Iteration 25707 => Loss: 6.70465093713005089171\n",
      "Iteration 25708 => Loss: 6.70465006488511949811\n",
      "Iteration 25709 => Loss: 6.70464919275771187301\n",
      "Iteration 25710 => Loss: 6.70464832074783245730\n",
      "Iteration 25711 => Loss: 6.70464744885545638198\n",
      "Iteration 25712 => Loss: 6.70464657708056943619\n",
      "Iteration 25713 => Loss: 6.70464570542314497459\n",
      "Iteration 25714 => Loss: 6.70464483388318743806\n",
      "Iteration 25715 => Loss: 6.70464396246066129947\n",
      "Iteration 25716 => Loss: 6.70464309115555767704\n",
      "Iteration 25717 => Loss: 6.70464221996786946534\n",
      "Iteration 25718 => Loss: 6.70464134889757001901\n",
      "Iteration 25719 => Loss: 6.70464047794464956809\n",
      "Iteration 25720 => Loss: 6.70463960710909212537\n",
      "Iteration 25721 => Loss: 6.70463873639087992728\n",
      "Iteration 25722 => Loss: 6.70463786578998810484\n",
      "Iteration 25723 => Loss: 6.70463699530642021074\n",
      "Iteration 25724 => Loss: 6.70463612494014871146\n",
      "Iteration 25725 => Loss: 6.70463525469115673161\n",
      "Iteration 25726 => Loss: 6.70463438455942384309\n",
      "Iteration 25727 => Loss: 6.70463351454494826953\n",
      "Iteration 25728 => Loss: 6.70463264464770691831\n",
      "Iteration 25729 => Loss: 6.70463177486769001945\n",
      "Iteration 25730 => Loss: 6.70463090520486471036\n",
      "Iteration 25731 => Loss: 6.70463003565923720828\n",
      "Iteration 25732 => Loss: 6.70462916623077198608\n",
      "Iteration 25733 => Loss: 6.70462829691946637922\n",
      "Iteration 25734 => Loss: 6.70462742772530084778\n",
      "Iteration 25735 => Loss: 6.70462655864826295726\n",
      "Iteration 25736 => Loss: 6.70462568968832250960\n",
      "Iteration 25737 => Loss: 6.70462482084548483385\n",
      "Iteration 25738 => Loss: 6.70462395211971795561\n",
      "Iteration 25739 => Loss: 6.70462308351101299309\n",
      "Iteration 25740 => Loss: 6.70462221501935307089\n",
      "Iteration 25741 => Loss: 6.70462134664472664269\n",
      "Iteration 25742 => Loss: 6.70462047838711150405\n",
      "Iteration 25743 => Loss: 6.70461961024649522045\n",
      "Iteration 25744 => Loss: 6.70461874222286002833\n",
      "Iteration 25745 => Loss: 6.70461787431618727595\n",
      "Iteration 25746 => Loss: 6.70461700652646985787\n",
      "Iteration 25747 => Loss: 6.70461613885368290511\n",
      "Iteration 25748 => Loss: 6.70461527129781931222\n",
      "Iteration 25749 => Loss: 6.70461440385885332205\n",
      "Iteration 25750 => Loss: 6.70461353653677960551\n",
      "Iteration 25751 => Loss: 6.70461266933157684633\n",
      "Iteration 25752 => Loss: 6.70461180224322905730\n",
      "Iteration 25753 => Loss: 6.70461093527171847484\n",
      "Iteration 25754 => Loss: 6.70461006841704154624\n",
      "Iteration 25755 => Loss: 6.70460920167916629708\n",
      "Iteration 25756 => Loss: 6.70460833505809183919\n",
      "Iteration 25757 => Loss: 6.70460746855377909270\n",
      "Iteration 25758 => Loss: 6.70460660216623960395\n",
      "Iteration 25759 => Loss: 6.70460573589543873396\n",
      "Iteration 25760 => Loss: 6.70460486974137470639\n",
      "Iteration 25761 => Loss: 6.70460400370402531678\n",
      "Iteration 25762 => Loss: 6.70460313778336836066\n",
      "Iteration 25763 => Loss: 6.70460227197939850896\n",
      "Iteration 25764 => Loss: 6.70460140629209888630\n",
      "Iteration 25765 => Loss: 6.70460054072144817638\n",
      "Iteration 25766 => Loss: 6.70459967526743483290\n",
      "Iteration 25767 => Loss: 6.70459880993003665139\n",
      "Iteration 25768 => Loss: 6.70459794470924297372\n",
      "Iteration 25769 => Loss: 6.70459707960504136537\n",
      "Iteration 25770 => Loss: 6.70459621461741317461\n",
      "Iteration 25771 => Loss: 6.70459534974633708515\n",
      "Iteration 25772 => Loss: 6.70459448499180510339\n",
      "Iteration 25773 => Loss: 6.70459362035379413669\n",
      "Iteration 25774 => Loss: 6.70459275583230063233\n",
      "Iteration 25775 => Loss: 6.70459189142729616862\n",
      "Iteration 25776 => Loss: 6.70459102713877719282\n",
      "Iteration 25777 => Loss: 6.70459016296671617141\n",
      "Iteration 25778 => Loss: 6.70458929891110599897\n",
      "Iteration 25779 => Loss: 6.70458843497192003014\n",
      "Iteration 25780 => Loss: 6.70458757114916004127\n",
      "Iteration 25781 => Loss: 6.70458670744279139342\n",
      "Iteration 25782 => Loss: 6.70458584385280875750\n",
      "Iteration 25783 => Loss: 6.70458498037920147539\n",
      "Iteration 25784 => Loss: 6.70458411702194645443\n",
      "Iteration 25785 => Loss: 6.70458325378101793746\n",
      "Iteration 25786 => Loss: 6.70458239065642214172\n",
      "Iteration 25787 => Loss: 6.70458152764812798097\n",
      "Iteration 25788 => Loss: 6.70458066475612568524\n",
      "Iteration 25789 => Loss: 6.70457980198039571462\n",
      "Iteration 25790 => Loss: 6.70457893932092474643\n",
      "Iteration 25791 => Loss: 6.70457807677770389887\n",
      "Iteration 25792 => Loss: 6.70457721435069764482\n",
      "Iteration 25793 => Loss: 6.70457635203991664241\n",
      "Iteration 25794 => Loss: 6.70457548984532447633\n",
      "Iteration 25795 => Loss: 6.70457462776692025841\n",
      "Iteration 25796 => Loss: 6.70457376580467556693\n",
      "Iteration 25797 => Loss: 6.70457290395858063192\n",
      "Iteration 25798 => Loss: 6.70457204222862035436\n",
      "Iteration 25799 => Loss: 6.70457118061477608251\n",
      "Iteration 25800 => Loss: 6.70457031911703804639\n",
      "Iteration 25801 => Loss: 6.70456945773538581790\n",
      "Iteration 25802 => Loss: 6.70456859646981051526\n",
      "Iteration 25803 => Loss: 6.70456773532027927587\n",
      "Iteration 25804 => Loss: 6.70456687428679121155\n",
      "Iteration 25805 => Loss: 6.70456601336933211144\n",
      "Iteration 25806 => Loss: 6.70456515256787799473\n",
      "Iteration 25807 => Loss: 6.70456429188242175599\n",
      "Iteration 25808 => Loss: 6.70456343131293763804\n",
      "Iteration 25809 => Loss: 6.70456257085942120000\n",
      "Iteration 25810 => Loss: 6.70456171052184224379\n",
      "Iteration 25811 => Loss: 6.70456085030019721671\n",
      "Iteration 25812 => Loss: 6.70455999019447368426\n",
      "Iteration 25813 => Loss: 6.70455913020464411289\n",
      "Iteration 25814 => Loss: 6.70455827033069429177\n",
      "Iteration 25815 => Loss: 6.70455741057261711546\n",
      "Iteration 25816 => Loss: 6.70455655093039393222\n",
      "Iteration 25817 => Loss: 6.70455569140399987305\n",
      "Iteration 25818 => Loss: 6.70455483199343849066\n",
      "Iteration 25819 => Loss: 6.70455397269867781063\n",
      "Iteration 25820 => Loss: 6.70455311351970628664\n",
      "Iteration 25821 => Loss: 6.70455225445651059601\n",
      "Iteration 25822 => Loss: 6.70455139550907386337\n",
      "Iteration 25823 => Loss: 6.70455053667737743694\n",
      "Iteration 25824 => Loss: 6.70454967796140977043\n",
      "Iteration 25825 => Loss: 6.70454881936115310026\n",
      "Iteration 25826 => Loss: 6.70454796087659410375\n",
      "Iteration 25827 => Loss: 6.70454710250771768187\n",
      "Iteration 25828 => Loss: 6.70454624425450518288\n",
      "Iteration 25829 => Loss: 6.70454538611693884320\n",
      "Iteration 25830 => Loss: 6.70454452809500978105\n",
      "Iteration 25831 => Loss: 6.70454367018870645012\n",
      "Iteration 25832 => Loss: 6.70454281239799687597\n",
      "Iteration 25833 => Loss: 6.70454195472287928226\n",
      "Iteration 25834 => Loss: 6.70454109716332435909\n",
      "Iteration 25835 => Loss: 6.70454023971933477100\n",
      "Iteration 25836 => Loss: 6.70453938239088298445\n",
      "Iteration 25837 => Loss: 6.70453852517796189403\n",
      "Iteration 25838 => Loss: 6.70453766808054751891\n",
      "Iteration 25839 => Loss: 6.70453681109862120735\n",
      "Iteration 25840 => Loss: 6.70453595423217230120\n",
      "Iteration 25841 => Loss: 6.70453509748118925415\n",
      "Iteration 25842 => Loss: 6.70453424084565874352\n",
      "Iteration 25843 => Loss: 6.70453338432555501214\n",
      "Iteration 25844 => Loss: 6.70453252792087273093\n",
      "Iteration 25845 => Loss: 6.70453167163158259001\n",
      "Iteration 25846 => Loss: 6.70453081545767659577\n",
      "Iteration 25847 => Loss: 6.70452995939915030732\n",
      "Iteration 25848 => Loss: 6.70452910345597352659\n",
      "Iteration 25849 => Loss: 6.70452824762813559545\n",
      "Iteration 25850 => Loss: 6.70452739191561697396\n",
      "Iteration 25851 => Loss: 6.70452653631840966852\n",
      "Iteration 25852 => Loss: 6.70452568083649325104\n",
      "Iteration 25853 => Loss: 6.70452482546984818157\n",
      "Iteration 25854 => Loss: 6.70452397021847446013\n",
      "Iteration 25855 => Loss: 6.70452311508234011228\n",
      "Iteration 25856 => Loss: 6.70452226006142915082\n",
      "Iteration 25857 => Loss: 6.70452140515573535851\n",
      "Iteration 25858 => Loss: 6.70452055036524363629\n",
      "Iteration 25859 => Loss: 6.70451969568994243787\n",
      "Iteration 25860 => Loss: 6.70451884112979978880\n",
      "Iteration 25861 => Loss: 6.70451798668480769550\n",
      "Iteration 25862 => Loss: 6.70451713235495372345\n",
      "Iteration 25863 => Loss: 6.70451627814021922092\n",
      "Iteration 25864 => Loss: 6.70451542404059352975\n",
      "Iteration 25865 => Loss: 6.70451457005605622186\n",
      "Iteration 25866 => Loss: 6.70451371618659575091\n",
      "Iteration 25867 => Loss: 6.70451286243219435335\n",
      "Iteration 25868 => Loss: 6.70451200879283071288\n",
      "Iteration 25869 => Loss: 6.70451115526849950044\n",
      "Iteration 25870 => Loss: 6.70451030185917584703\n",
      "Iteration 25871 => Loss: 6.70450944856485975265\n",
      "Iteration 25872 => Loss: 6.70450859538551569017\n",
      "Iteration 25873 => Loss: 6.70450774232114365958\n",
      "Iteration 25874 => Loss: 6.70450688937171701554\n",
      "Iteration 25875 => Loss: 6.70450603653723042896\n",
      "Iteration 25876 => Loss: 6.70450518381766258358\n",
      "Iteration 25877 => Loss: 6.70450433121299749217\n",
      "Iteration 25878 => Loss: 6.70450347872322272025\n",
      "Iteration 25879 => Loss: 6.70450262634831428699\n",
      "Iteration 25880 => Loss: 6.70450177408826686332\n",
      "Iteration 25881 => Loss: 6.70450092194305735660\n",
      "Iteration 25882 => Loss: 6.70450006991268310230\n",
      "Iteration 25883 => Loss: 6.70449921799711479053\n",
      "Iteration 25884 => Loss: 6.70449836619634531587\n",
      "Iteration 25885 => Loss: 6.70449751451035425021\n",
      "Iteration 25886 => Loss: 6.70449666293912471815\n",
      "Iteration 25887 => Loss: 6.70449581148264872610\n",
      "Iteration 25888 => Loss: 6.70449496014090762230\n",
      "Iteration 25889 => Loss: 6.70449410891388186684\n",
      "Iteration 25890 => Loss: 6.70449325780156168975\n",
      "Iteration 25891 => Loss: 6.70449240680392488656\n",
      "Iteration 25892 => Loss: 6.70449155592096257550\n",
      "Iteration 25893 => Loss: 6.70449070515265610481\n",
      "Iteration 25894 => Loss: 6.70448985449899215183\n",
      "Iteration 25895 => Loss: 6.70448900395995206480\n",
      "Iteration 25896 => Loss: 6.70448815353552429741\n",
      "Iteration 25897 => Loss: 6.70448730322568930973\n",
      "Iteration 25898 => Loss: 6.70448645303043733179\n",
      "Iteration 25899 => Loss: 6.70448560294974882368\n",
      "Iteration 25900 => Loss: 6.70448475298360868635\n",
      "Iteration 25901 => Loss: 6.70448390313199560353\n",
      "Iteration 25902 => Loss: 6.70448305339490513433\n",
      "Iteration 25903 => Loss: 6.70448220377231329792\n",
      "Iteration 25904 => Loss: 6.70448135426421210070\n",
      "Iteration 25905 => Loss: 6.70448050487058022640\n",
      "Iteration 25906 => Loss: 6.70447965559140346414\n",
      "Iteration 25907 => Loss: 6.70447880642666937945\n",
      "Iteration 25908 => Loss: 6.70447795737635843238\n",
      "Iteration 25909 => Loss: 6.70447710844046085299\n",
      "Iteration 25910 => Loss: 6.70447625961894555502\n",
      "Iteration 25911 => Loss: 6.70447541091182497297\n",
      "Iteration 25912 => Loss: 6.70447456231905558610\n",
      "Iteration 25913 => Loss: 6.70447371384064005895\n",
      "Iteration 25914 => Loss: 6.70447286547655796340\n",
      "Iteration 25915 => Loss: 6.70447201722679508862\n",
      "Iteration 25916 => Loss: 6.70447116909132834195\n",
      "Iteration 25917 => Loss: 6.70447032107014617708\n",
      "Iteration 25918 => Loss: 6.70446947316324060040\n",
      "Iteration 25919 => Loss: 6.70446862537058674292\n",
      "Iteration 25920 => Loss: 6.70446777769217572285\n",
      "Iteration 25921 => Loss: 6.70446693012798977662\n",
      "Iteration 25922 => Loss: 6.70446608267800758796\n",
      "Iteration 25923 => Loss: 6.70446523534222293961\n",
      "Iteration 25924 => Loss: 6.70446438812061629164\n",
      "Iteration 25925 => Loss: 6.70446354101317520957\n",
      "Iteration 25926 => Loss: 6.70446269401988281800\n",
      "Iteration 25927 => Loss: 6.70446184714072135336\n",
      "Iteration 25928 => Loss: 6.70446100037567482843\n",
      "Iteration 25929 => Loss: 6.70446015372473258509\n",
      "Iteration 25930 => Loss: 6.70445930718787241887\n",
      "Iteration 25931 => Loss: 6.70445846076508544797\n",
      "Iteration 25932 => Loss: 6.70445761445635568521\n",
      "Iteration 25933 => Loss: 6.70445676826166447881\n",
      "Iteration 25934 => Loss: 6.70445592218099850612\n",
      "Iteration 25935 => Loss: 6.70445507621434089174\n",
      "Iteration 25936 => Loss: 6.70445423036168453024\n",
      "Iteration 25937 => Loss: 6.70445338462299478266\n",
      "Iteration 25938 => Loss: 6.70445253899827431354\n",
      "Iteration 25939 => Loss: 6.70445169348750535931\n",
      "Iteration 25940 => Loss: 6.70445084809066305098\n",
      "Iteration 25941 => Loss: 6.70445000280774117130\n",
      "Iteration 25942 => Loss: 6.70444915763872018033\n",
      "Iteration 25943 => Loss: 6.70444831258358853177\n",
      "Iteration 25944 => Loss: 6.70444746764232935021\n",
      "Iteration 25945 => Loss: 6.70444662281492131939\n",
      "Iteration 25946 => Loss: 6.70444577810135466933\n",
      "Iteration 25947 => Loss: 6.70444493350161607736\n",
      "Iteration 25948 => Loss: 6.70444408901568067449\n",
      "Iteration 25949 => Loss: 6.70444324464354668436\n",
      "Iteration 25950 => Loss: 6.70444240038518657343\n",
      "Iteration 25951 => Loss: 6.70444155624059323628\n",
      "Iteration 25952 => Loss: 6.70444071220974890934\n",
      "Iteration 25953 => Loss: 6.70443986829263316451\n",
      "Iteration 25954 => Loss: 6.70443902448924422544\n",
      "Iteration 25955 => Loss: 6.70443818079955011768\n",
      "Iteration 25956 => Loss: 6.70443733722354640037\n",
      "Iteration 25957 => Loss: 6.70443649376121264538\n",
      "Iteration 25958 => Loss: 6.70443565041253641823\n",
      "Iteration 25959 => Loss: 6.70443480717750528441\n",
      "Iteration 25960 => Loss: 6.70443396405609970401\n",
      "Iteration 25961 => Loss: 6.70443312104830457798\n",
      "Iteration 25962 => Loss: 6.70443227815409592552\n",
      "Iteration 25963 => Loss: 6.70443143537347996386\n",
      "Iteration 25964 => Loss: 6.70443059270642116587\n",
      "Iteration 25965 => Loss: 6.70442975015291064977\n",
      "Iteration 25966 => Loss: 6.70442890771293864560\n",
      "Iteration 25967 => Loss: 6.70442806538648294890\n",
      "Iteration 25968 => Loss: 6.70442722317353290151\n",
      "Iteration 25969 => Loss: 6.70442638107406985171\n",
      "Iteration 25970 => Loss: 6.70442553908808225316\n",
      "Iteration 25971 => Loss: 6.70442469721554434869\n",
      "Iteration 25972 => Loss: 6.70442385545645702649\n",
      "Iteration 25973 => Loss: 6.70442301381079097666\n",
      "Iteration 25974 => Loss: 6.70442217227854087014\n",
      "Iteration 25975 => Loss: 6.70442133085969249606\n",
      "Iteration 25976 => Loss: 6.70442048955421388001\n",
      "Iteration 25977 => Loss: 6.70441964836210768652\n",
      "Iteration 25978 => Loss: 6.70441880728334815842\n",
      "Iteration 25979 => Loss: 6.70441796631793351935\n",
      "Iteration 25980 => Loss: 6.70441712546583357124\n",
      "Iteration 25981 => Loss: 6.70441628472703587960\n",
      "Iteration 25982 => Loss: 6.70441544410152179267\n",
      "Iteration 25983 => Loss: 6.70441460358929042229\n",
      "Iteration 25984 => Loss: 6.70441376319031689945\n",
      "Iteration 25985 => Loss: 6.70441292290459056602\n",
      "Iteration 25986 => Loss: 6.70441208273209365842\n",
      "Iteration 25987 => Loss: 6.70441124267280486038\n",
      "Iteration 25988 => Loss: 6.70441040272671706646\n",
      "Iteration 25989 => Loss: 6.70440956289380984856\n",
      "Iteration 25990 => Loss: 6.70440872317406899583\n",
      "Iteration 25991 => Loss: 6.70440788356748473831\n",
      "Iteration 25992 => Loss: 6.70440704407403487153\n",
      "Iteration 25993 => Loss: 6.70440620469370873735\n",
      "Iteration 25994 => Loss: 6.70440536542648590768\n",
      "Iteration 25995 => Loss: 6.70440452627235394800\n",
      "Iteration 25996 => Loss: 6.70440368723130308837\n",
      "Iteration 25997 => Loss: 6.70440284830330490706\n",
      "Iteration 25998 => Loss: 6.70440200948836384498\n",
      "Iteration 25999 => Loss: 6.70440117078644348680\n",
      "Iteration 26000 => Loss: 6.70440033219753850346\n",
      "Iteration 26001 => Loss: 6.70439949372163823682\n",
      "Iteration 26002 => Loss: 6.70439865535872225877\n",
      "Iteration 26003 => Loss: 6.70439781710877280574\n",
      "Iteration 26004 => Loss: 6.70439697897178010777\n",
      "Iteration 26005 => Loss: 6.70439614094772817765\n",
      "Iteration 26006 => Loss: 6.70439530303659303456\n",
      "Iteration 26007 => Loss: 6.70439446523837467851\n",
      "Iteration 26008 => Loss: 6.70439362755304557595\n",
      "Iteration 26009 => Loss: 6.70439278998059151604\n",
      "Iteration 26010 => Loss: 6.70439195252100983424\n",
      "Iteration 26011 => Loss: 6.70439111517427033249\n",
      "Iteration 26012 => Loss: 6.70439027794035613539\n",
      "Iteration 26013 => Loss: 6.70438944081926990748\n",
      "Iteration 26014 => Loss: 6.70438860381098145069\n",
      "Iteration 26015 => Loss: 6.70438776691548365960\n",
      "Iteration 26016 => Loss: 6.70438693013275432975\n",
      "Iteration 26017 => Loss: 6.70438609346277747392\n",
      "Iteration 26018 => Loss: 6.70438525690554776304\n",
      "Iteration 26019 => Loss: 6.70438442046104210448\n",
      "Iteration 26020 => Loss: 6.70438358412924984009\n",
      "Iteration 26021 => Loss: 6.70438274791014965359\n",
      "Iteration 26022 => Loss: 6.70438191180373266320\n",
      "Iteration 26023 => Loss: 6.70438107580998821078\n",
      "Iteration 26024 => Loss: 6.70438023992888165736\n",
      "Iteration 26025 => Loss: 6.70437940416040678571\n",
      "Iteration 26026 => Loss: 6.70437856850456892488\n",
      "Iteration 26027 => Loss: 6.70437773296132544232\n",
      "Iteration 26028 => Loss: 6.70437689753066745624\n",
      "Iteration 26029 => Loss: 6.70437606221258874939\n",
      "Iteration 26030 => Loss: 6.70437522700707422274\n",
      "Iteration 26031 => Loss: 6.70437439191409279005\n",
      "Iteration 26032 => Loss: 6.70437355693364889220\n",
      "Iteration 26033 => Loss: 6.70437272206571766020\n",
      "Iteration 26034 => Loss: 6.70437188731027955413\n",
      "Iteration 26035 => Loss: 6.70437105266732924491\n",
      "Iteration 26036 => Loss: 6.70437021813684896898\n",
      "Iteration 26037 => Loss: 6.70436938371881474552\n",
      "Iteration 26038 => Loss: 6.70436854941322390999\n",
      "Iteration 26039 => Loss: 6.70436771522005869883\n",
      "Iteration 26040 => Loss: 6.70436688113929779576\n",
      "Iteration 26041 => Loss: 6.70436604717093054262\n",
      "Iteration 26042 => Loss: 6.70436521331493384679\n",
      "Iteration 26043 => Loss: 6.70436437957130326737\n",
      "Iteration 26044 => Loss: 6.70436354594002104079\n",
      "Iteration 26045 => Loss: 6.70436271242107473256\n",
      "Iteration 26046 => Loss: 6.70436187901443947368\n",
      "Iteration 26047 => Loss: 6.70436104572010282965\n",
      "Iteration 26048 => Loss: 6.70436021253805503051\n",
      "Iteration 26049 => Loss: 6.70435937946828452993\n",
      "Iteration 26050 => Loss: 6.70435854651076468258\n",
      "Iteration 26051 => Loss: 6.70435771366549104755\n",
      "Iteration 26052 => Loss: 6.70435688093243609131\n",
      "Iteration 26053 => Loss: 6.70435604831159803751\n",
      "Iteration 26054 => Loss: 6.70435521580295290534\n",
      "Iteration 26055 => Loss: 6.70435438340648559574\n",
      "Iteration 26056 => Loss: 6.70435355112219255602\n",
      "Iteration 26057 => Loss: 6.70435271895004269993\n",
      "Iteration 26058 => Loss: 6.70435188689002714568\n",
      "Iteration 26059 => Loss: 6.70435105494213257060\n",
      "Iteration 26060 => Loss: 6.70435022310634654019\n",
      "Iteration 26061 => Loss: 6.70434939138265217906\n",
      "Iteration 26062 => Loss: 6.70434855977102373004\n",
      "Iteration 26063 => Loss: 6.70434772827146119312\n",
      "Iteration 26064 => Loss: 6.70434689688394236384\n",
      "Iteration 26065 => Loss: 6.70434606560845747225\n",
      "Iteration 26066 => Loss: 6.70434523444498164935\n",
      "Iteration 26067 => Loss: 6.70434440339350423699\n",
      "Iteration 26068 => Loss: 6.70434357245401280068\n",
      "Iteration 26069 => Loss: 6.70434274162649490592\n",
      "Iteration 26070 => Loss: 6.70434191091092213100\n",
      "Iteration 26071 => Loss: 6.70434108030729181138\n",
      "Iteration 26072 => Loss: 6.70434024981559062439\n",
      "Iteration 26073 => Loss: 6.70433941943578926015\n",
      "Iteration 26074 => Loss: 6.70433858916789038318\n",
      "Iteration 26075 => Loss: 6.70433775901186912449\n",
      "Iteration 26076 => Loss: 6.70433692896770061509\n",
      "Iteration 26077 => Loss: 6.70433609903539728947\n",
      "Iteration 26078 => Loss: 6.70433526921491740325\n",
      "Iteration 26079 => Loss: 6.70433443950625385099\n",
      "Iteration 26080 => Loss: 6.70433360990939508639\n",
      "Iteration 26081 => Loss: 6.70433278042433489219\n",
      "Iteration 26082 => Loss: 6.70433195105103685307\n",
      "Iteration 26083 => Loss: 6.70433112178949652815\n",
      "Iteration 26084 => Loss: 6.70433029263970592382\n",
      "Iteration 26085 => Loss: 6.70432946360164194743\n",
      "Iteration 26086 => Loss: 6.70432863467528683543\n",
      "Iteration 26087 => Loss: 6.70432780586063081785\n",
      "Iteration 26088 => Loss: 6.70432697715766234836\n",
      "Iteration 26089 => Loss: 6.70432614856635478162\n",
      "Iteration 26090 => Loss: 6.70432532008669923584\n",
      "Iteration 26091 => Loss: 6.70432449171868860560\n",
      "Iteration 26092 => Loss: 6.70432366346229802190\n",
      "Iteration 26093 => Loss: 6.70432283531751060934\n",
      "Iteration 26094 => Loss: 6.70432200728431748615\n",
      "Iteration 26095 => Loss: 6.70432117936270888237\n",
      "Iteration 26096 => Loss: 6.70432035155265726445\n",
      "Iteration 26097 => Loss: 6.70431952385415552698\n",
      "Iteration 26098 => Loss: 6.70431869626717880095\n",
      "Iteration 26099 => Loss: 6.70431786879172975091\n",
      "Iteration 26100 => Loss: 6.70431704142777729061\n",
      "Iteration 26101 => Loss: 6.70431621417531520279\n",
      "Iteration 26102 => Loss: 6.70431538703431950665\n",
      "Iteration 26103 => Loss: 6.70431456000478664947\n",
      "Iteration 26104 => Loss: 6.70431373308669531497\n",
      "Iteration 26105 => Loss: 6.70431290628002773957\n",
      "Iteration 26106 => Loss: 6.70431207958477681785\n",
      "Iteration 26107 => Loss: 6.70431125300092034536\n",
      "Iteration 26108 => Loss: 6.70431042652845032848\n",
      "Iteration 26109 => Loss: 6.70430960016734278639\n",
      "Iteration 26110 => Loss: 6.70430877391758972550\n",
      "Iteration 26111 => Loss: 6.70430794777916805316\n",
      "Iteration 26112 => Loss: 6.70430712175207865755\n",
      "Iteration 26113 => Loss: 6.70430629583629134061\n",
      "Iteration 26114 => Loss: 6.70430547003179544419\n",
      "Iteration 26115 => Loss: 6.70430464433858119833\n",
      "Iteration 26116 => Loss: 6.70430381875662462221\n",
      "Iteration 26117 => Loss: 6.70430299328591505770\n",
      "Iteration 26118 => Loss: 6.70430216792643918211\n",
      "Iteration 26119 => Loss: 6.70430134267817745553\n",
      "Iteration 26120 => Loss: 6.70430051754112454887\n",
      "Iteration 26121 => Loss: 6.70429969251525381679\n",
      "Iteration 26122 => Loss: 6.70429886760055548933\n",
      "Iteration 26123 => Loss: 6.70429804279702068470\n",
      "Iteration 26124 => Loss: 6.70429721810462542209\n",
      "Iteration 26125 => Loss: 6.70429639352334838520\n",
      "Iteration 26126 => Loss: 6.70429556905319312676\n",
      "Iteration 26127 => Loss: 6.70429474469413477777\n",
      "Iteration 26128 => Loss: 6.70429392044615823920\n",
      "Iteration 26129 => Loss: 6.70429309630924663566\n",
      "Iteration 26130 => Loss: 6.70429227228338842082\n",
      "Iteration 26131 => Loss: 6.70429144836856671930\n",
      "Iteration 26132 => Loss: 6.70429062456477442566\n",
      "Iteration 26133 => Loss: 6.70428980087197956550\n",
      "Iteration 26134 => Loss: 6.70428897729018391516\n",
      "Iteration 26135 => Loss: 6.70428815381936793472\n",
      "Iteration 26136 => Loss: 6.70428733045951030789\n",
      "Iteration 26137 => Loss: 6.70428650721059593565\n",
      "Iteration 26138 => Loss: 6.70428568407262481799\n",
      "Iteration 26139 => Loss: 6.70428486104556498049\n",
      "Iteration 26140 => Loss: 6.70428403812941287043\n",
      "Iteration 26141 => Loss: 6.70428321532414717154\n",
      "Iteration 26142 => Loss: 6.70428239262975100843\n",
      "Iteration 26143 => Loss: 6.70428157004621461112\n",
      "Iteration 26144 => Loss: 6.70428074757351843971\n",
      "Iteration 26145 => Loss: 6.70427992521165183604\n",
      "Iteration 26146 => Loss: 6.70427910296060947104\n",
      "Iteration 26147 => Loss: 6.70427828082035137669\n",
      "Iteration 26148 => Loss: 6.70427745879088110570\n",
      "Iteration 26149 => Loss: 6.70427663687218178268\n",
      "Iteration 26150 => Loss: 6.70427581506423564406\n",
      "Iteration 26151 => Loss: 6.70427499336703203170\n",
      "Iteration 26152 => Loss: 6.70427417178054074753\n",
      "Iteration 26153 => Loss: 6.70427335030476445610\n",
      "Iteration 26154 => Loss: 6.70427252893967917657\n",
      "Iteration 26155 => Loss: 6.70427170768527869171\n",
      "Iteration 26156 => Loss: 6.70427088654153990888\n",
      "Iteration 26157 => Loss: 6.70427006550845927535\n",
      "Iteration 26158 => Loss: 6.70426924458599771128\n",
      "Iteration 26159 => Loss: 6.70426842377416676300\n",
      "Iteration 26160 => Loss: 6.70426760307293445607\n",
      "Iteration 26161 => Loss: 6.70426678248228746781\n",
      "Iteration 26162 => Loss: 6.70426596200222135735\n",
      "Iteration 26163 => Loss: 6.70426514163271214386\n",
      "Iteration 26164 => Loss: 6.70426432137374828102\n",
      "Iteration 26165 => Loss: 6.70426350122531555797\n",
      "Iteration 26166 => Loss: 6.70426268118740065205\n",
      "Iteration 26167 => Loss: 6.70426186125998224696\n",
      "Iteration 26168 => Loss: 6.70426104144304968457\n",
      "Iteration 26169 => Loss: 6.70426022173658875403\n",
      "Iteration 26170 => Loss: 6.70425940214057991540\n",
      "Iteration 26171 => Loss: 6.70425858265501872779\n",
      "Iteration 26172 => Loss: 6.70425776327987232861\n",
      "Iteration 26173 => Loss: 6.70425694401514782328\n",
      "Iteration 26174 => Loss: 6.70425612486081767827\n",
      "Iteration 26175 => Loss: 6.70425530581686235365\n",
      "Iteration 26176 => Loss: 6.70425448688327474400\n",
      "Iteration 26177 => Loss: 6.70425366806004419118\n",
      "Iteration 26178 => Loss: 6.70425284934714227347\n",
      "Iteration 26179 => Loss: 6.70425203074456987906\n",
      "Iteration 26180 => Loss: 6.70425121225230036259\n",
      "Iteration 26181 => Loss: 6.70425039387031684868\n",
      "Iteration 26182 => Loss: 6.70424957559861489642\n",
      "Iteration 26183 => Loss: 6.70424875743718740040\n",
      "Iteration 26184 => Loss: 6.70424793938598995169\n",
      "Iteration 26185 => Loss: 6.70424712144503232025\n",
      "Iteration 26186 => Loss: 6.70424630361429318981\n",
      "Iteration 26187 => Loss: 6.70424548589375213226\n",
      "Iteration 26188 => Loss: 6.70424466828340470670\n",
      "Iteration 26189 => Loss: 6.70424385078322693232\n",
      "Iteration 26190 => Loss: 6.70424303339321170370\n",
      "Iteration 26191 => Loss: 6.70424221611333237547\n",
      "Iteration 26192 => Loss: 6.70424139894358273040\n",
      "Iteration 26193 => Loss: 6.70424058188394944580\n",
      "Iteration 26194 => Loss: 6.70423976493441386992\n",
      "Iteration 26195 => Loss: 6.70423894809496356828\n",
      "Iteration 26196 => Loss: 6.70423813136558255366\n",
      "Iteration 26197 => Loss: 6.70423731474626016791\n",
      "Iteration 26198 => Loss: 6.70423649823696887751\n",
      "Iteration 26199 => Loss: 6.70423568183770157702\n",
      "Iteration 26200 => Loss: 6.70423486554845471375\n",
      "Iteration 26201 => Loss: 6.70423404936919187236\n",
      "Iteration 26202 => Loss: 6.70423323329991571740\n",
      "Iteration 26203 => Loss: 6.70423241734059960351\n",
      "Iteration 26204 => Loss: 6.70423160149124353069\n",
      "Iteration 26205 => Loss: 6.70423078575181818906\n",
      "Iteration 26206 => Loss: 6.70422997012231203229\n",
      "Iteration 26207 => Loss: 6.70422915460271529042\n",
      "Iteration 26208 => Loss: 6.70422833919300753536\n",
      "Iteration 26209 => Loss: 6.70422752389317455624\n",
      "Iteration 26210 => Loss: 6.70422670870320391856\n",
      "Iteration 26211 => Loss: 6.70422589362308318783\n",
      "Iteration 26212 => Loss: 6.70422507865279637684\n",
      "Iteration 26213 => Loss: 6.70422426379232039295\n",
      "Iteration 26214 => Loss: 6.70422344904164635437\n",
      "Iteration 26215 => Loss: 6.70422263440076360297\n",
      "Iteration 26216 => Loss: 6.70422181986965348699\n",
      "Iteration 26217 => Loss: 6.70422100544830712465\n",
      "Iteration 26218 => Loss: 6.70422019113669520607\n",
      "Iteration 26219 => Loss: 6.70421937693481950760\n",
      "Iteration 26220 => Loss: 6.70421856284264716663\n",
      "Iteration 26221 => Loss: 6.70421774886018351225\n",
      "Iteration 26222 => Loss: 6.70421693498740101091\n",
      "Iteration 26223 => Loss: 6.70421612122429078084\n",
      "Iteration 26224 => Loss: 6.70421530757083061758\n",
      "Iteration 26225 => Loss: 6.70421449402700986298\n",
      "Iteration 26226 => Loss: 6.70421368059281252982\n",
      "Iteration 26227 => Loss: 6.70421286726823328905\n",
      "Iteration 26228 => Loss: 6.70421205405324371895\n",
      "Iteration 26229 => Loss: 6.70421124094783138503\n",
      "Iteration 26230 => Loss: 6.70421042795199539910\n",
      "Iteration 26231 => Loss: 6.70420961506570645128\n",
      "Iteration 26232 => Loss: 6.70420880228894500164\n",
      "Iteration 26233 => Loss: 6.70420798962171904378\n",
      "Iteration 26234 => Loss: 6.70420717706399216240\n",
      "Iteration 26235 => Loss: 6.70420636461576258114\n",
      "Iteration 26236 => Loss: 6.70420555227700276646\n",
      "Iteration 26237 => Loss: 6.70420474004771271836\n",
      "Iteration 26238 => Loss: 6.70420392792787023239\n",
      "Iteration 26239 => Loss: 6.70420311591746020952\n",
      "Iteration 26240 => Loss: 6.70420230401646843887\n",
      "Iteration 26241 => Loss: 6.70420149222487449236\n",
      "Iteration 26242 => Loss: 6.70420068054268014635\n",
      "Iteration 26243 => Loss: 6.70419986896985253821\n",
      "Iteration 26244 => Loss: 6.70419905750638633890\n",
      "Iteration 26245 => Loss: 6.70419824615226467301\n",
      "Iteration 26246 => Loss: 6.70419743490747155334\n",
      "Iteration 26247 => Loss: 6.70419662377199809811\n",
      "Iteration 26248 => Loss: 6.70419581274582299102\n",
      "Iteration 26249 => Loss: 6.70419500182893290940\n",
      "Iteration 26250 => Loss: 6.70419419102131453059\n",
      "Iteration 26251 => Loss: 6.70419338032295275553\n",
      "Iteration 26252 => Loss: 6.70419256973382626796\n",
      "Iteration 26253 => Loss: 6.70419175925393417970\n",
      "Iteration 26254 => Loss: 6.70419094888325606263\n",
      "Iteration 26255 => Loss: 6.70419013862176971230\n",
      "Iteration 26256 => Loss: 6.70418932846947068782\n",
      "Iteration 26257 => Loss: 6.70418851842633500837\n",
      "Iteration 26258 => Loss: 6.70418770849235556852\n",
      "Iteration 26259 => Loss: 6.70418689866751460471\n",
      "Iteration 26260 => Loss: 6.70418608895179790608\n",
      "Iteration 26261 => Loss: 6.70418527934519303813\n",
      "Iteration 26262 => Loss: 6.70418446984767957275\n",
      "Iteration 26263 => Loss: 6.70418366045924241092\n",
      "Iteration 26264 => Loss: 6.70418285117987444721\n",
      "Iteration 26265 => Loss: 6.70418204200955702987\n",
      "Iteration 26266 => Loss: 6.70418123294827594805\n",
      "Iteration 26267 => Loss: 6.70418042399601521453\n",
      "Iteration 26268 => Loss: 6.70417961515275795392\n",
      "Iteration 26269 => Loss: 6.70417880641850150170\n",
      "Iteration 26270 => Loss: 6.70417799779321299525\n",
      "Iteration 26271 => Loss: 6.70417718927688799369\n",
      "Iteration 26272 => Loss: 6.70417638086951139798\n",
      "Iteration 26273 => Loss: 6.70417557257106899726\n",
      "Iteration 26274 => Loss: 6.70417476438154391616\n",
      "Iteration 26275 => Loss: 6.70417395630092194381\n",
      "Iteration 26276 => Loss: 6.70417314832919064571\n",
      "Iteration 26277 => Loss: 6.70417234046633403466\n",
      "Iteration 26278 => Loss: 6.70417153271233345890\n",
      "Iteration 26279 => Loss: 6.70417072506718270120\n",
      "Iteration 26280 => Loss: 6.70416991753085778072\n",
      "Iteration 26281 => Loss: 6.70416911010335336840\n",
      "Iteration 26282 => Loss: 6.70416830278464370707\n",
      "Iteration 26283 => Loss: 6.70416749557472169130\n",
      "Iteration 26284 => Loss: 6.70416668847357399841\n",
      "Iteration 26285 => Loss: 6.70416588148118464119\n",
      "Iteration 26286 => Loss: 6.70416507459753141518\n",
      "Iteration 26287 => Loss: 6.70416426782260987949\n",
      "Iteration 26288 => Loss: 6.70416346115640227055\n",
      "Iteration 26289 => Loss: 6.70416265459889260114\n",
      "Iteration 26290 => Loss: 6.70416184815006666042\n",
      "Iteration 26291 => Loss: 6.70416104180991023753\n",
      "Iteration 26292 => Loss: 6.70416023557840468072\n",
      "Iteration 26293 => Loss: 6.70415942945554199639\n",
      "Iteration 26294 => Loss: 6.70415862344130442096\n",
      "Iteration 26295 => Loss: 6.70415781753567507906\n",
      "Iteration 26296 => Loss: 6.70415701173864420070\n",
      "Iteration 26297 => Loss: 6.70415620605019402234\n",
      "Iteration 26298 => Loss: 6.70415540047030322768\n",
      "Iteration 26299 => Loss: 6.70415459499897625761\n",
      "Iteration 26300 => Loss: 6.70415378963617758501\n",
      "Iteration 26301 => Loss: 6.70415298438190809804\n",
      "Iteration 26302 => Loss: 6.70415217923613759865\n",
      "Iteration 26303 => Loss: 6.70415137419887141590\n",
      "Iteration 26304 => Loss: 6.70415056927007757537\n",
      "Iteration 26305 => Loss: 6.70414976444975074799\n",
      "Iteration 26306 => Loss: 6.70414895973786961747\n",
      "Iteration 26307 => Loss: 6.70414815513443063111\n",
      "Iteration 26308 => Loss: 6.70414735063940536719\n",
      "Iteration 26309 => Loss: 6.70414654625278494393\n",
      "Iteration 26310 => Loss: 6.70414574197456047955\n",
      "Iteration 26311 => Loss: 6.70414493780471065776\n",
      "Iteration 26312 => Loss: 6.70414413374322748496\n",
      "Iteration 26313 => Loss: 6.70414332979009142122\n",
      "Iteration 26314 => Loss: 6.70414252594527759754\n",
      "Iteration 26315 => Loss: 6.70414172220878779029\n",
      "Iteration 26316 => Loss: 6.70414091858060601226\n",
      "Iteration 26317 => Loss: 6.70414011506071183533\n",
      "Iteration 26318 => Loss: 6.70413931164908571958\n",
      "Iteration 26319 => Loss: 6.70413850834572055959\n",
      "Iteration 26320 => Loss: 6.70413770515060214450\n",
      "Iteration 26321 => Loss: 6.70413690206372070435\n",
      "Iteration 26322 => Loss: 6.70413609908504959378\n",
      "Iteration 26323 => Loss: 6.70413529621457993102\n",
      "Iteration 26324 => Loss: 6.70413449345229484067\n",
      "Iteration 26325 => Loss: 6.70413369079818455276\n",
      "Iteration 26326 => Loss: 6.70413288825223130374\n",
      "Iteration 26327 => Loss: 6.70413208581442177092\n",
      "Iteration 26328 => Loss: 6.70413128348474263163\n",
      "Iteration 26329 => Loss: 6.70413048126317523412\n",
      "Iteration 26330 => Loss: 6.70412967914970803207\n",
      "Iteration 26331 => Loss: 6.70412887714432770281\n",
      "Iteration 26332 => Loss: 6.70412807524701293005\n",
      "Iteration 26333 => Loss: 6.70412727345775660837\n",
      "Iteration 26334 => Loss: 6.70412647177654097419\n",
      "Iteration 26335 => Loss: 6.70412567020335270485\n",
      "Iteration 26336 => Loss: 6.70412486873817492494\n",
      "Iteration 26337 => Loss: 6.70412406738099253545\n",
      "Iteration 26338 => Loss: 6.70412326613179931911\n",
      "Iteration 26339 => Loss: 6.70412246499057307147\n",
      "Iteration 26340 => Loss: 6.70412166395729958168\n",
      "Iteration 26341 => Loss: 6.70412086303196375070\n",
      "Iteration 26342 => Loss: 6.70412006221455758492\n",
      "Iteration 26343 => Loss: 6.70411926150505443900\n",
      "Iteration 26344 => Loss: 6.70411846090344809568\n",
      "Iteration 26345 => Loss: 6.70411766040972700864\n",
      "Iteration 26346 => Loss: 6.70411686002386719707\n",
      "Iteration 26347 => Loss: 6.70411605974587043733\n",
      "Iteration 26348 => Loss: 6.70411525957570209044\n",
      "Iteration 26349 => Loss: 6.70411445951335327464\n",
      "Iteration 26350 => Loss: 6.70411365955881510814\n",
      "Iteration 26351 => Loss: 6.70411285971207959733\n",
      "Iteration 26352 => Loss: 6.70411205997311832050\n",
      "Iteration 26353 => Loss: 6.70411126034192061951\n",
      "Iteration 26354 => Loss: 6.70411046081847317168\n",
      "Iteration 26355 => Loss: 6.70410966140276176617\n",
      "Iteration 26356 => Loss: 6.70410886209477308029\n",
      "Iteration 26357 => Loss: 6.70410806289448757411\n",
      "Iteration 26358 => Loss: 6.70410726380189903040\n",
      "Iteration 26359 => Loss: 6.70410646481698346832\n",
      "Iteration 26360 => Loss: 6.70410566593973999971\n",
      "Iteration 26361 => Loss: 6.70410486717013576197\n",
      "Iteration 26362 => Loss: 6.70410406850816897872\n",
      "Iteration 26363 => Loss: 6.70410326995382899185\n",
      "Iteration 26364 => Loss: 6.70410247150708027419\n",
      "Iteration 26365 => Loss: 6.70410167316793437209\n",
      "Iteration 26366 => Loss: 6.70410087493635842293\n",
      "Iteration 26367 => Loss: 6.70410007681234176857\n",
      "Iteration 26368 => Loss: 6.70409927879587819177\n",
      "Iteration 26369 => Loss: 6.70409848088694637624\n",
      "Iteration 26370 => Loss: 6.70409768308553033478\n",
      "Iteration 26371 => Loss: 6.70409688539162207377\n",
      "Iteration 26372 => Loss: 6.70409608780520294147\n",
      "Iteration 26373 => Loss: 6.70409529032625250977\n",
      "Iteration 26374 => Loss: 6.70409449295476722597\n",
      "Iteration 26375 => Loss: 6.70409369569072843831\n",
      "Iteration 26376 => Loss: 6.70409289853411927140\n",
      "Iteration 26377 => Loss: 6.70409210148493084347\n",
      "Iteration 26378 => Loss: 6.70409130454314539094\n",
      "Iteration 26379 => Loss: 6.70409050770873982117\n",
      "Iteration 26380 => Loss: 6.70408971098170969327\n",
      "Iteration 26381 => Loss: 6.70408891436204523728\n",
      "Iteration 26382 => Loss: 6.70408811784971803149\n",
      "Iteration 26383 => Loss: 6.70408732144472097048\n",
      "Iteration 26384 => Loss: 6.70408652514705050152\n",
      "Iteration 26385 => Loss: 6.70408572895666576841\n",
      "Iteration 26386 => Loss: 6.70408493287357476476\n",
      "Iteration 26387 => Loss: 6.70408413689775706246\n",
      "Iteration 26388 => Loss: 6.70408334102920111519\n",
      "Iteration 26389 => Loss: 6.70408254526788116578\n",
      "Iteration 26390 => Loss: 6.70408174961379277335\n",
      "Iteration 26391 => Loss: 6.70408095406691906248\n",
      "Iteration 26392 => Loss: 6.70408015862724848688\n",
      "Iteration 26393 => Loss: 6.70407936329475617754\n",
      "Iteration 26394 => Loss: 6.70407856806943769357\n",
      "Iteration 26395 => Loss: 6.70407777295127882411\n",
      "Iteration 26396 => Loss: 6.70407697794026535831\n",
      "Iteration 26397 => Loss: 6.70407618303636887447\n",
      "Iteration 26398 => Loss: 6.70407538823959292529\n",
      "Iteration 26399 => Loss: 6.70407459354991708267\n",
      "Iteration 26400 => Loss: 6.70407379896732269486\n",
      "Iteration 26401 => Loss: 6.70407300449180354462\n",
      "Iteration 26402 => Loss: 6.70407221012333387478\n",
      "Iteration 26403 => Loss: 6.70407141586190125082\n",
      "Iteration 26404 => Loss: 6.70407062170750123187\n",
      "Iteration 26405 => Loss: 6.70406982766011871888\n",
      "Iteration 26406 => Loss: 6.70406903371972351380\n",
      "Iteration 26407 => Loss: 6.70406823988631916933\n",
      "Iteration 26408 => Loss: 6.70406744615988259284\n",
      "Iteration 26409 => Loss: 6.70406665254040312618\n",
      "Iteration 26410 => Loss: 6.70406585902786034126\n",
      "Iteration 26411 => Loss: 6.70406506562223913903\n",
      "Iteration 26412 => Loss: 6.70406427232353507861\n",
      "Iteration 26413 => Loss: 6.70406347913173217279\n",
      "Iteration 26414 => Loss: 6.70406268604680821710\n",
      "Iteration 26415 => Loss: 6.70406189306874811251\n",
      "Iteration 26416 => Loss: 6.70406110019754120088\n",
      "Iteration 26417 => Loss: 6.70406030743318304133\n",
      "Iteration 26418 => Loss: 6.70405951477564787666\n",
      "Iteration 26419 => Loss: 6.70405872222491172607\n",
      "Iteration 26420 => Loss: 6.70405792978098524770\n",
      "Iteration 26421 => Loss: 6.70405713744383824348\n",
      "Iteration 26422 => Loss: 6.70405634521345383803\n",
      "Iteration 26423 => Loss: 6.70405555308982670226\n",
      "Iteration 26424 => Loss: 6.70405476107293907262\n",
      "Iteration 26425 => Loss: 6.70405396916276963282\n",
      "Iteration 26426 => Loss: 6.70405317735931305378\n",
      "Iteration 26427 => Loss: 6.70405238566255157195\n",
      "Iteration 26428 => Loss: 6.70405159407247364101\n",
      "Iteration 26429 => Loss: 6.70405080258906238555\n",
      "Iteration 26430 => Loss: 6.70405001121230448291\n",
      "Iteration 26431 => Loss: 6.70404921994218128134\n",
      "Iteration 26432 => Loss: 6.70404842877868478723\n",
      "Iteration 26433 => Loss: 6.70404763772179101977\n",
      "Iteration 26434 => Loss: 6.70404684677149553806\n",
      "Iteration 26435 => Loss: 6.70404605592778768397\n",
      "Iteration 26436 => Loss: 6.70404526519063992396\n",
      "Iteration 26437 => Loss: 6.70404447456004692896\n",
      "Iteration 26438 => Loss: 6.70404368403598294179\n",
      "Iteration 26439 => Loss: 6.70404289361845417972\n",
      "Iteration 26440 => Loss: 6.70404210330742600377\n",
      "Iteration 26441 => Loss: 6.70404131310289486123\n",
      "Iteration 26442 => Loss: 6.70404052300484032401\n",
      "Iteration 26443 => Loss: 6.70403973301325972756\n",
      "Iteration 26444 => Loss: 6.70403894312812376199\n",
      "Iteration 26445 => Loss: 6.70403815334942798643\n",
      "Iteration 26446 => Loss: 6.70403736367715819000\n",
      "Iteration 26447 => Loss: 6.70403657411128861554\n",
      "Iteration 26448 => Loss: 6.70403578465181748669\n",
      "Iteration 26449 => Loss: 6.70403499529872970442\n",
      "Iteration 26450 => Loss: 6.70403420605200306426\n",
      "Iteration 26451 => Loss: 6.70403341691162513172\n",
      "Iteration 26452 => Loss: 6.70403262787758880137\n",
      "Iteration 26453 => Loss: 6.70403183894987098057\n",
      "Iteration 26454 => Loss: 6.70403105012846367572\n",
      "Iteration 26455 => Loss: 6.70403026141334823507\n",
      "Iteration 26456 => Loss: 6.70402947280451666501\n",
      "Iteration 26457 => Loss: 6.70402868430194320837\n",
      "Iteration 26458 => Loss: 6.70402789590562342426\n",
      "Iteration 26459 => Loss: 6.70402710761554310182\n",
      "Iteration 26460 => Loss: 6.70402631943168536566\n",
      "Iteration 26461 => Loss: 6.70402553135403334039\n",
      "Iteration 26462 => Loss: 6.70402474338257015063\n",
      "Iteration 26463 => Loss: 6.70402395551728869094\n",
      "Iteration 26464 => Loss: 6.70402316775817652683\n",
      "Iteration 26465 => Loss: 6.70402238010521944744\n",
      "Iteration 26466 => Loss: 6.70402159255838192564\n",
      "Iteration 26467 => Loss: 6.70402080511767728410\n",
      "Iteration 26468 => Loss: 6.70402001778307976565\n",
      "Iteration 26469 => Loss: 6.70401923055457782397\n",
      "Iteration 26470 => Loss: 6.70401844343215369548\n",
      "Iteration 26471 => Loss: 6.70401765641578872845\n",
      "Iteration 26472 => Loss: 6.70401686950547670563\n",
      "Iteration 26473 => Loss: 6.70401608270120430433\n",
      "Iteration 26474 => Loss: 6.70401529600294665556\n",
      "Iteration 26475 => Loss: 6.70401450941070997658\n",
      "Iteration 26476 => Loss: 6.70401372292445874024\n",
      "Iteration 26477 => Loss: 6.70401293654418228840\n",
      "Iteration 26478 => Loss: 6.70401215026987706835\n",
      "Iteration 26479 => Loss: 6.70401136410151821110\n",
      "Iteration 26480 => Loss: 6.70401057803909772304\n",
      "Iteration 26481 => Loss: 6.70400979208260316966\n",
      "Iteration 26482 => Loss: 6.70400900623201057016\n",
      "Iteration 26483 => Loss: 6.70400822048731104275\n",
      "Iteration 26484 => Loss: 6.70400743484850014653\n",
      "Iteration 26485 => Loss: 6.70400664931554146619\n",
      "Iteration 26486 => Loss: 6.70400586388844210717\n",
      "Iteration 26487 => Loss: 6.70400507856717187138\n",
      "Iteration 26488 => Loss: 6.70400429335173075884\n",
      "Iteration 26489 => Loss: 6.70400350824209212419\n",
      "Iteration 26490 => Loss: 6.70400272323824886200\n",
      "Iteration 26491 => Loss: 6.70400193834018676142\n",
      "Iteration 26492 => Loss: 6.70400115354788805888\n",
      "Iteration 26493 => Loss: 6.70400036886133854352\n",
      "Iteration 26494 => Loss: 6.70399958428052578085\n",
      "Iteration 26495 => Loss: 6.70399879980544177727\n",
      "Iteration 26496 => Loss: 6.70399801543605544651\n",
      "Iteration 26497 => Loss: 6.70399723117237034131\n",
      "Iteration 26498 => Loss: 6.70399644701436603356\n",
      "Iteration 26499 => Loss: 6.70399566296202298332\n",
      "Iteration 26500 => Loss: 6.70399487901533053247\n",
      "Iteration 26501 => Loss: 6.70399409517428068739\n",
      "Iteration 26502 => Loss: 6.70399331143884680273\n",
      "Iteration 26503 => Loss: 6.70399252780901999671\n",
      "Iteration 26504 => Loss: 6.70399174428479316390\n",
      "Iteration 26505 => Loss: 6.70399096086614676437\n",
      "Iteration 26506 => Loss: 6.70399017755306037003\n",
      "Iteration 26507 => Loss: 6.70398939434553220451\n",
      "Iteration 26508 => Loss: 6.70398861124353562246\n",
      "Iteration 26509 => Loss: 6.70398782824706529482\n",
      "Iteration 26510 => Loss: 6.70398704535610434618\n",
      "Iteration 26511 => Loss: 6.70398626257063767753\n",
      "Iteration 26512 => Loss: 6.70398547989065018982\n",
      "Iteration 26513 => Loss: 6.70398469731613033673\n",
      "Iteration 26514 => Loss: 6.70398391484706035470\n",
      "Iteration 26515 => Loss: 6.70398313248342958559\n",
      "Iteration 26516 => Loss: 6.70398235022522115401\n",
      "Iteration 26517 => Loss: 6.70398156807241996091\n",
      "Iteration 26518 => Loss: 6.70398078602501978907\n",
      "Iteration 26519 => Loss: 6.70398000408299576947\n",
      "Iteration 26520 => Loss: 6.70397922224633990851\n",
      "Iteration 26521 => Loss: 6.70397844051504065988\n",
      "Iteration 26522 => Loss: 6.70397765888907670728\n",
      "Iteration 26523 => Loss: 6.70397687736843295170\n",
      "Iteration 26524 => Loss: 6.70397609595310051134\n",
      "Iteration 26525 => Loss: 6.70397531464306783988\n",
      "Iteration 26526 => Loss: 6.70397453343831006833\n",
      "Iteration 26527 => Loss: 6.70397375233882897305\n",
      "Iteration 26528 => Loss: 6.70397297134459524415\n",
      "Iteration 26529 => Loss: 6.70397219045559733530\n",
      "Iteration 26530 => Loss: 6.70397140967183435833\n",
      "Iteration 26531 => Loss: 6.70397062899327522700\n",
      "Iteration 26532 => Loss: 6.70396984841991194770\n",
      "Iteration 26533 => Loss: 6.70396906795173475047\n",
      "Iteration 26534 => Loss: 6.70396828758872409537\n",
      "Iteration 26535 => Loss: 6.70396750733086754792\n",
      "Iteration 26536 => Loss: 6.70396672717814468001\n",
      "Iteration 26537 => Loss: 6.70396594713054749803\n",
      "Iteration 26538 => Loss: 6.70396516718808133106\n",
      "Iteration 26539 => Loss: 6.70396438735069111203\n",
      "Iteration 26540 => Loss: 6.70396360761839460451\n",
      "Iteration 26541 => Loss: 6.70396282799116516316\n",
      "Iteration 26542 => Loss: 6.70396204846898591256\n",
      "Iteration 26543 => Loss: 6.70396126905184885914\n",
      "Iteration 26544 => Loss: 6.70396048973974156837\n",
      "Iteration 26545 => Loss: 6.70395971053264538853\n",
      "Iteration 26546 => Loss: 6.70395893143054788510\n",
      "Iteration 26547 => Loss: 6.70395815243343218270\n",
      "Iteration 26548 => Loss: 6.70395737354129206409\n",
      "Iteration 26549 => Loss: 6.70395659475409999573\n",
      "Iteration 26550 => Loss: 6.70395581607185508943\n",
      "Iteration 26551 => Loss: 6.70395503749453691711\n",
      "Iteration 26552 => Loss: 6.70395425902213215608\n",
      "Iteration 26553 => Loss: 6.70395348065461860187\n",
      "Iteration 26554 => Loss: 6.70395270239200247175\n",
      "Iteration 26555 => Loss: 6.70395192423425179129\n",
      "Iteration 26556 => Loss: 6.70395114618135501416\n",
      "Iteration 26557 => Loss: 6.70395036823330503495\n",
      "Iteration 26558 => Loss: 6.70394959039008497825\n",
      "Iteration 26559 => Loss: 6.70394881265167708051\n",
      "Iteration 26560 => Loss: 6.70394803501806713086\n",
      "Iteration 26561 => Loss: 6.70394725748924447117\n",
      "Iteration 26562 => Loss: 6.70394648006519400241\n",
      "Iteration 26563 => Loss: 6.70394570274590773096\n",
      "Iteration 26564 => Loss: 6.70394492553135545876\n",
      "Iteration 26565 => Loss: 6.70394414842154251488\n",
      "Iteration 26566 => Loss: 6.70394337141643781308\n",
      "Iteration 26567 => Loss: 6.70394259451603602429\n",
      "Iteration 26568 => Loss: 6.70394181772032204947\n",
      "Iteration 26569 => Loss: 6.70394104102928078959\n",
      "Iteration 26570 => Loss: 6.70394026444290247468\n",
      "Iteration 26571 => Loss: 6.70393948796116312394\n",
      "Iteration 26572 => Loss: 6.70393871158405918464\n",
      "Iteration 26573 => Loss: 6.70393793531157733412\n",
      "Iteration 26574 => Loss: 6.70393715914368648612\n",
      "Iteration 26575 => Loss: 6.70393638308038930518\n",
      "Iteration 26576 => Loss: 6.70393560712167069227\n",
      "Iteration 26577 => Loss: 6.70393483126750400203\n",
      "Iteration 26578 => Loss: 6.70393405551789367536\n",
      "Iteration 26579 => Loss: 6.70393327987280507330\n",
      "Iteration 26580 => Loss: 6.70393250433224263674\n",
      "Iteration 26581 => Loss: 6.70393172889618238486\n",
      "Iteration 26582 => Loss: 6.70393095356461277134\n",
      "Iteration 26583 => Loss: 6.70393017833751692081\n",
      "Iteration 26584 => Loss: 6.70392940321488328692\n",
      "Iteration 26585 => Loss: 6.70392862819670032337\n",
      "Iteration 26586 => Loss: 6.70392785328295381930\n",
      "Iteration 26587 => Loss: 6.70392707847362068208\n",
      "Iteration 26588 => Loss: 6.70392630376868670083\n",
      "Iteration 26589 => Loss: 6.70392552916815986919\n",
      "Iteration 26590 => Loss: 6.70392475467200554817\n",
      "Iteration 26591 => Loss: 6.70392398028020508605\n",
      "Iteration 26592 => Loss: 6.70392320599276558823\n",
      "Iteration 26593 => Loss: 6.70392243180965508031\n",
      "Iteration 26594 => Loss: 6.70392165773087089775\n",
      "Iteration 26595 => Loss: 6.70392088375639172426\n",
      "Iteration 26596 => Loss: 6.70392010988620157264\n",
      "Iteration 26597 => Loss: 6.70391933612029244927\n",
      "Iteration 26598 => Loss: 6.70391856245865280783\n",
      "Iteration 26599 => Loss: 6.70391778890126222024\n",
      "Iteration 26600 => Loss: 6.70391701544810114655\n",
      "Iteration 26601 => Loss: 6.70391624209917669219\n",
      "Iteration 26602 => Loss: 6.70391546885444533643\n",
      "Iteration 26603 => Loss: 6.70391469571392040194\n",
      "Iteration 26604 => Loss: 6.70391392267757169066\n",
      "Iteration 26605 => Loss: 6.70391314974539387350\n",
      "Iteration 26606 => Loss: 6.70391237691736030513\n",
      "Iteration 26607 => Loss: 6.70391160419346920918\n",
      "Iteration 26608 => Loss: 6.70391083157370903933\n",
      "Iteration 26609 => Loss: 6.70391005905805403842\n",
      "Iteration 26610 => Loss: 6.70390928664649621282\n",
      "Iteration 26611 => Loss: 6.70390851433901868717\n",
      "Iteration 26612 => Loss: 6.70390774213561524419\n",
      "Iteration 26613 => Loss: 6.70390697003626190309\n",
      "Iteration 26614 => Loss: 6.70390619804094356482\n",
      "Iteration 26615 => Loss: 6.70390542614966467028\n",
      "Iteration 26616 => Loss: 6.70390465436239058050\n",
      "Iteration 26617 => Loss: 6.70390388267911863096\n",
      "Iteration 26618 => Loss: 6.70390311109982572901\n",
      "Iteration 26619 => Loss: 6.70390233962450210470\n",
      "Iteration 26620 => Loss: 6.70390156825313621169\n",
      "Iteration 26621 => Loss: 6.70390079698571561551\n",
      "Iteration 26622 => Loss: 6.70390002582222699345\n",
      "Iteration 26623 => Loss: 6.70389925476264991744\n",
      "Iteration 26624 => Loss: 6.70389848380697017660\n",
      "Iteration 26625 => Loss: 6.70389771295517622463\n",
      "Iteration 26626 => Loss: 6.70389694220725385065\n",
      "Iteration 26627 => Loss: 6.70389617156319594926\n",
      "Iteration 26628 => Loss: 6.70389540102297942781\n",
      "Iteration 26629 => Loss: 6.70389463058659007544\n",
      "Iteration 26630 => Loss: 6.70389386025401723401\n",
      "Iteration 26631 => Loss: 6.70389309002525468628\n",
      "Iteration 26632 => Loss: 6.70389231990027223418\n",
      "Iteration 26633 => Loss: 6.70389154987906454863\n",
      "Iteration 26634 => Loss: 6.70389077996162363604\n",
      "Iteration 26635 => Loss: 6.70389001014792551558\n",
      "Iteration 26636 => Loss: 6.70388924043795775276\n",
      "Iteration 26637 => Loss: 6.70388847083170968943\n",
      "Iteration 26638 => Loss: 6.70388770132917422018\n",
      "Iteration 26639 => Loss: 6.70388693193031759421\n",
      "Iteration 26640 => Loss: 6.70388616263514336424\n",
      "Iteration 26641 => Loss: 6.70388539344363021399\n",
      "Iteration 26642 => Loss: 6.70388462435575949172\n",
      "Iteration 26643 => Loss: 6.70388385537153386196\n",
      "Iteration 26644 => Loss: 6.70388308649092490299\n",
      "Iteration 26645 => Loss: 6.70388231771391929215\n",
      "Iteration 26646 => Loss: 6.70388154904051347671\n",
      "Iteration 26647 => Loss: 6.70388078047068258769\n",
      "Iteration 26648 => Loss: 6.70388001200441774330\n",
      "Iteration 26649 => Loss: 6.70387924364170029179\n",
      "Iteration 26650 => Loss: 6.70387847538252223956\n",
      "Iteration 26651 => Loss: 6.70387770722686493485\n",
      "Iteration 26652 => Loss: 6.70387693917471683136\n",
      "Iteration 26653 => Loss: 6.70387617122606993547\n",
      "Iteration 26654 => Loss: 6.70387540338089671366\n",
      "Iteration 26655 => Loss: 6.70387463563919538956\n",
      "Iteration 26656 => Loss: 6.70387386800094553507\n",
      "Iteration 26657 => Loss: 6.70387310046613382752\n",
      "Iteration 26658 => Loss: 6.70387233303475316148\n",
      "Iteration 26659 => Loss: 6.70387156570677600342\n",
      "Iteration 26660 => Loss: 6.70387079848220235334\n",
      "Iteration 26661 => Loss: 6.70387003136100734224\n",
      "Iteration 26662 => Loss: 6.70386926434318297652\n",
      "Iteration 26663 => Loss: 6.70386849742872303892\n",
      "Iteration 26664 => Loss: 6.70386773061759022596\n",
      "Iteration 26665 => Loss: 6.70386696390979430760\n",
      "Iteration 26666 => Loss: 6.70386619730531574390\n",
      "Iteration 26667 => Loss: 6.70386543080412966589\n",
      "Iteration 26668 => Loss: 6.70386466440623163265\n",
      "Iteration 26669 => Loss: 6.70386389811160920971\n",
      "Iteration 26670 => Loss: 6.70386313192023752805\n",
      "Iteration 26671 => Loss: 6.70386236583211836404\n",
      "Iteration 26672 => Loss: 6.70386159984722684868\n",
      "Iteration 26673 => Loss: 6.70386083396554965930\n",
      "Iteration 26674 => Loss: 6.70386006818707347321\n",
      "Iteration 26675 => Loss: 6.70385930251179118500\n",
      "Iteration 26676 => Loss: 6.70385853693968591926\n",
      "Iteration 26677 => Loss: 6.70385777147073280702\n",
      "Iteration 26678 => Loss: 6.70385700610493362461\n",
      "Iteration 26679 => Loss: 6.70385624084226527941\n",
      "Iteration 26680 => Loss: 6.70385547568271888963\n",
      "Iteration 26681 => Loss: 6.70385471062626603356\n",
      "Iteration 26682 => Loss: 6.70385394567291914569\n",
      "Iteration 26683 => Loss: 6.70385318082264269890\n",
      "Iteration 26684 => Loss: 6.70385241607542958775\n",
      "Iteration 26685 => Loss: 6.70385165143127270682\n",
      "Iteration 26686 => Loss: 6.70385088689014718710\n",
      "Iteration 26687 => Loss: 6.70385012245204059411\n",
      "Iteration 26688 => Loss: 6.70384935811694848695\n",
      "Iteration 26689 => Loss: 6.70384859388484688481\n",
      "Iteration 26690 => Loss: 6.70384782975572512953\n",
      "Iteration 26691 => Loss: 6.70384706572957078663\n",
      "Iteration 26692 => Loss: 6.70384630180636431618\n",
      "Iteration 26693 => Loss: 6.70384553798610127728\n",
      "Iteration 26694 => Loss: 6.70384477426876568273\n",
      "Iteration 26695 => Loss: 6.70384401065433799261\n",
      "Iteration 26696 => Loss: 6.70384324714280843693\n",
      "Iteration 26697 => Loss: 6.70384248373415925215\n",
      "Iteration 26698 => Loss: 6.70384172042838244465\n",
      "Iteration 26699 => Loss: 6.70384095722545669815\n",
      "Iteration 26700 => Loss: 6.70384019412537845994\n",
      "Iteration 26701 => Loss: 6.70383943112812286103\n",
      "Iteration 26702 => Loss: 6.70383866823368546051\n",
      "Iteration 26703 => Loss: 6.70383790544205204753\n",
      "Iteration 26704 => Loss: 6.70383714275319597675\n",
      "Iteration 26705 => Loss: 6.70383638016711369545\n",
      "Iteration 26706 => Loss: 6.70383561768379543366\n",
      "Iteration 26707 => Loss: 6.70383485530321188151\n",
      "Iteration 26708 => Loss: 6.70383409302537192076\n",
      "Iteration 26709 => Loss: 6.70383333085023647158\n",
      "Iteration 26710 => Loss: 6.70383256877781441574\n",
      "Iteration 26711 => Loss: 6.70383180680807910790\n",
      "Iteration 26712 => Loss: 6.70383104494101544901\n",
      "Iteration 26713 => Loss: 6.70383028317661722184\n",
      "Iteration 26714 => Loss: 6.70382952151486133374\n",
      "Iteration 26715 => Loss: 6.70382875995574600836\n",
      "Iteration 26716 => Loss: 6.70382799849924904123\n",
      "Iteration 26717 => Loss: 6.70382723714535888604\n",
      "Iteration 26718 => Loss: 6.70382647589405866739\n",
      "Iteration 26719 => Loss: 6.70382571474533328626\n",
      "Iteration 26720 => Loss: 6.70382495369918629535\n",
      "Iteration 26721 => Loss: 6.70382419275558216754\n",
      "Iteration 26722 => Loss: 6.70382343191450846831\n",
      "Iteration 26723 => Loss: 6.70382267117596608585\n",
      "Iteration 26724 => Loss: 6.70382191053993192753\n",
      "Iteration 26725 => Loss: 6.70382115000639267066\n",
      "Iteration 26726 => Loss: 6.70382038957534120982\n",
      "Iteration 26727 => Loss: 6.70381962924675267601\n",
      "Iteration 26728 => Loss: 6.70381886902061285838\n",
      "Iteration 26729 => Loss: 6.70381810889692442146\n",
      "Iteration 26730 => Loss: 6.70381734887565894354\n",
      "Iteration 26731 => Loss: 6.70381658895680310195\n",
      "Iteration 26732 => Loss: 6.70381582914034801490\n",
      "Iteration 26733 => Loss: 6.70381506942628035972\n",
      "Iteration 26734 => Loss: 6.70381430981458148466\n",
      "Iteration 26735 => Loss: 6.70381355030523717886\n",
      "Iteration 26736 => Loss: 6.70381279089824388961\n",
      "Iteration 26737 => Loss: 6.70381203159357763610\n",
      "Iteration 26738 => Loss: 6.70381127239122687200\n",
      "Iteration 26739 => Loss: 6.70381051329118271553\n",
      "Iteration 26740 => Loss: 6.70380975429342207406\n",
      "Iteration 26741 => Loss: 6.70380899539794405939\n",
      "Iteration 26742 => Loss: 6.70380823660471847347\n",
      "Iteration 26743 => Loss: 6.70380747791374709266\n",
      "Iteration 26744 => Loss: 6.70380671932499883070\n",
      "Iteration 26745 => Loss: 6.70380596083848434574\n",
      "Iteration 26746 => Loss: 6.70380520245416722247\n",
      "Iteration 26747 => Loss: 6.70380444417204390817\n",
      "Iteration 26748 => Loss: 6.70380368599209841562\n",
      "Iteration 26749 => Loss: 6.70380292791432363941\n",
      "Iteration 26750 => Loss: 6.70380216993869648689\n",
      "Iteration 26751 => Loss: 6.70380141206519830632\n",
      "Iteration 26752 => Loss: 6.70380065429383709130\n",
      "Iteration 26753 => Loss: 6.70379989662457997923\n",
      "Iteration 26754 => Loss: 6.70379913905741542379\n",
      "Iteration 26755 => Loss: 6.70379838159233543138\n",
      "Iteration 26756 => Loss: 6.70379762422932490296\n",
      "Iteration 26757 => Loss: 6.70379686696837229221\n",
      "Iteration 26758 => Loss: 6.70379610980945894738\n",
      "Iteration 26759 => Loss: 6.70379535275256976945\n",
      "Iteration 26760 => Loss: 6.70379459579769232391\n",
      "Iteration 26761 => Loss: 6.70379383894482128170\n",
      "Iteration 26762 => Loss: 6.70379308219392999746\n",
      "Iteration 26763 => Loss: 6.70379232554501491848\n",
      "Iteration 26764 => Loss: 6.70379156899805561665\n",
      "Iteration 26765 => Loss: 6.70379081255304765108\n",
      "Iteration 26766 => Loss: 6.70379005620996348824\n",
      "Iteration 26767 => Loss: 6.70378929996880046360\n",
      "Iteration 26768 => Loss: 6.70378854382953104363\n",
      "Iteration 26769 => Loss: 6.70378778779215789285\n",
      "Iteration 26770 => Loss: 6.70378703185666591224\n",
      "Iteration 26771 => Loss: 6.70378627602303645006\n",
      "Iteration 26772 => Loss: 6.70378552029125085454\n",
      "Iteration 26773 => Loss: 6.70378476466129935574\n",
      "Iteration 26774 => Loss: 6.70378400913317396004\n",
      "Iteration 26775 => Loss: 6.70378325370685157480\n",
      "Iteration 26776 => Loss: 6.70378249838232598279\n",
      "Iteration 26777 => Loss: 6.70378174315958297314\n",
      "Iteration 26778 => Loss: 6.70378098803859767685\n",
      "Iteration 26779 => Loss: 6.70378023301937275846\n",
      "Iteration 26780 => Loss: 6.70377947810187890809\n",
      "Iteration 26781 => Loss: 6.70377872328611612573\n",
      "Iteration 26782 => Loss: 6.70377796857206043057\n",
      "Iteration 26783 => Loss: 6.70377721395971093443\n",
      "Iteration 26784 => Loss: 6.70377645944904099196\n",
      "Iteration 26785 => Loss: 6.70377570504003994500\n",
      "Iteration 26786 => Loss: 6.70377495073269003001\n",
      "Iteration 26787 => Loss: 6.70377419652698858243\n",
      "Iteration 26788 => Loss: 6.70377344242292050325\n",
      "Iteration 26789 => Loss: 6.70377268842046625252\n",
      "Iteration 26790 => Loss: 6.70377193451961250759\n",
      "Iteration 26791 => Loss: 6.70377118072034239304\n",
      "Iteration 26792 => Loss: 6.70377042702265057983\n",
      "Iteration 26793 => Loss: 6.70376967342652108073\n",
      "Iteration 26794 => Loss: 6.70376891993194590214\n",
      "Iteration 26795 => Loss: 6.70376816653889218145\n",
      "Iteration 26796 => Loss: 6.70376741324736258321\n",
      "Iteration 26797 => Loss: 6.70376666005733667930\n",
      "Iteration 26798 => Loss: 6.70376590696881002884\n",
      "Iteration 26799 => Loss: 6.70376515398175332194\n",
      "Iteration 26800 => Loss: 6.70376440109616478225\n",
      "Iteration 26801 => Loss: 6.70376364831203375161\n",
      "Iteration 26802 => Loss: 6.70376289562933269650\n",
      "Iteration 26803 => Loss: 6.70376214304805717603\n",
      "Iteration 26804 => Loss: 6.70376139056819031481\n",
      "Iteration 26805 => Loss: 6.70376063818972411923\n",
      "Iteration 26806 => Loss: 6.70375988591264260208\n",
      "Iteration 26807 => Loss: 6.70375913373692178254\n",
      "Iteration 26808 => Loss: 6.70375838166256521333\n",
      "Iteration 26809 => Loss: 6.70375762968954980181\n",
      "Iteration 26810 => Loss: 6.70375687781786133712\n",
      "Iteration 26811 => Loss: 6.70375612604748649659\n",
      "Iteration 26812 => Loss: 6.70375537437841195754\n",
      "Iteration 26813 => Loss: 6.70375462281062528547\n",
      "Iteration 26814 => Loss: 6.70375387134411582224\n",
      "Iteration 26815 => Loss: 6.70375311997886402793\n",
      "Iteration 26816 => Loss: 6.70375236871485835621\n",
      "Iteration 26817 => Loss: 6.70375161755208814895\n",
      "Iteration 26818 => Loss: 6.70375086649053653076\n",
      "Iteration 26819 => Loss: 6.70375011553019195532\n",
      "Iteration 26820 => Loss: 6.70374936467103399451\n",
      "Iteration 26821 => Loss: 6.70374861391305554292\n",
      "Iteration 26822 => Loss: 6.70374786325624416605\n",
      "Iteration 26823 => Loss: 6.70374711270058476487\n",
      "Iteration 26824 => Loss: 6.70374636224605691126\n",
      "Iteration 26825 => Loss: 6.70374561189266149341\n",
      "Iteration 26826 => Loss: 6.70374486164036920144\n",
      "Iteration 26827 => Loss: 6.70374411148917648262\n",
      "Iteration 26828 => Loss: 6.70374336143906557339\n",
      "Iteration 26829 => Loss: 6.70374261149002581561\n",
      "Iteration 26830 => Loss: 6.70374186164204122207\n",
      "Iteration 26831 => Loss: 6.70374111189509758191\n",
      "Iteration 26832 => Loss: 6.70374036224918423699\n",
      "Iteration 26833 => Loss: 6.70373961270428608827\n",
      "Iteration 26834 => Loss: 6.70373886326038359584\n",
      "Iteration 26835 => Loss: 6.70373811391747231880\n",
      "Iteration 26836 => Loss: 6.70373736467553715812\n",
      "Iteration 26837 => Loss: 6.70373661553455857387\n",
      "Iteration 26838 => Loss: 6.70373586649452946062\n",
      "Iteration 26839 => Loss: 6.70373511755543116664\n",
      "Iteration 26840 => Loss: 6.70373436871725303376\n",
      "Iteration 26841 => Loss: 6.70373361997998351569\n",
      "Iteration 26842 => Loss: 6.70373287134360307249\n",
      "Iteration 26843 => Loss: 6.70373212280811081598\n",
      "Iteration 26844 => Loss: 6.70373137437346944267\n",
      "Iteration 26845 => Loss: 6.70373062603969582796\n",
      "Iteration 26846 => Loss: 6.70372987780675000380\n",
      "Iteration 26847 => Loss: 6.70372912967462486478\n",
      "Iteration 26848 => Loss: 6.70372838164331508182\n",
      "Iteration 26849 => Loss: 6.70372763371280466771\n",
      "Iteration 26850 => Loss: 6.70372688588307497071\n",
      "Iteration 26851 => Loss: 6.70372613815411622085\n",
      "Iteration 26852 => Loss: 6.70372539052591331910\n",
      "Iteration 26853 => Loss: 6.70372464299845649549\n",
      "Iteration 26854 => Loss: 6.70372389557172354557\n",
      "Iteration 26855 => Loss: 6.70372314824571446934\n",
      "Iteration 26856 => Loss: 6.70372240102040173326\n",
      "Iteration 26857 => Loss: 6.70372165389577467920\n",
      "Iteration 26858 => Loss: 6.70372090687182886626\n",
      "Iteration 26859 => Loss: 6.70372015994854297816\n",
      "Iteration 26860 => Loss: 6.70371941312590546858\n",
      "Iteration 26861 => Loss: 6.70371866640390301484\n",
      "Iteration 26862 => Loss: 6.70371791978251607702\n",
      "Iteration 26863 => Loss: 6.70371717326174554330\n",
      "Iteration 26864 => Loss: 6.70371642684156032743\n",
      "Iteration 26865 => Loss: 6.70371568052196042942\n",
      "Iteration 26866 => Loss: 6.70371493430292275661\n",
      "Iteration 26867 => Loss: 6.70371418818444553267\n",
      "Iteration 26868 => Loss: 6.70371344216650122405\n",
      "Iteration 26869 => Loss: 6.70371269624908094897\n",
      "Iteration 26870 => Loss: 6.70371195043217760201\n",
      "Iteration 26871 => Loss: 6.70371120471577253142\n",
      "Iteration 26872 => Loss: 6.70371045909985152633\n",
      "Iteration 26873 => Loss: 6.70370971358440659316\n",
      "Iteration 26874 => Loss: 6.70370896816941996832\n",
      "Iteration 26875 => Loss: 6.70370822285487211190\n",
      "Iteration 26876 => Loss: 6.70370747764075591846\n",
      "Iteration 26877 => Loss: 6.70370673252706339440\n",
      "Iteration 26878 => Loss: 6.70370598751376789437\n",
      "Iteration 26879 => Loss: 6.70370524260086764201\n",
      "Iteration 26880 => Loss: 6.70370449778834576193\n",
      "Iteration 26881 => Loss: 6.70370375307618360239\n",
      "Iteration 26882 => Loss: 6.70370300846437139342\n",
      "Iteration 26883 => Loss: 6.70370226395289225962\n",
      "Iteration 26884 => Loss: 6.70370151954174708919\n",
      "Iteration 26885 => Loss: 6.70370077523090301952\n",
      "Iteration 26886 => Loss: 6.70370003102035738607\n",
      "Iteration 26887 => Loss: 6.70369928691009420163\n",
      "Iteration 26888 => Loss: 6.70369854290009925535\n",
      "Iteration 26889 => Loss: 6.70369779899035833637\n",
      "Iteration 26890 => Loss: 6.70369705518085812201\n",
      "Iteration 26891 => Loss: 6.70369631147159239504\n",
      "Iteration 26892 => Loss: 6.70369556786253628644\n",
      "Iteration 26893 => Loss: 6.70369482435368446716\n",
      "Iteration 26894 => Loss: 6.70369408094502361450\n",
      "Iteration 26895 => Loss: 6.70369333763653152403\n",
      "Iteration 26896 => Loss: 6.70369259442820286665\n",
      "Iteration 26897 => Loss: 6.70369185132001721428\n",
      "Iteration 26898 => Loss: 6.70369110831197101419\n",
      "Iteration 26899 => Loss: 6.70369036540403939739\n",
      "Iteration 26900 => Loss: 6.70368962259622502842\n",
      "Iteration 26901 => Loss: 6.70368887988849326831\n",
      "Iteration 26902 => Loss: 6.70368813728084589343\n",
      "Iteration 26903 => Loss: 6.70368739477326958109\n",
      "Iteration 26904 => Loss: 6.70368665236573413324\n",
      "Iteration 26905 => Loss: 6.70368591005825020801\n",
      "Iteration 26906 => Loss: 6.70368516785078405462\n",
      "Iteration 26907 => Loss: 6.70368442574333389672\n",
      "Iteration 26908 => Loss: 6.70368368373588729980\n",
      "Iteration 26909 => Loss: 6.70368294182841939488\n",
      "Iteration 26910 => Loss: 6.70368220002092574106\n",
      "Iteration 26911 => Loss: 6.70368145831338857477\n",
      "Iteration 26912 => Loss: 6.70368071670580256693\n",
      "Iteration 26913 => Loss: 6.70367997519814551310\n",
      "Iteration 26914 => Loss: 6.70367923379040053788\n",
      "Iteration 26915 => Loss: 6.70367849248256497674\n",
      "Iteration 26916 => Loss: 6.70367775127462195428\n",
      "Iteration 26917 => Loss: 6.70367701016656081237\n",
      "Iteration 26918 => Loss: 6.70367626915835490564\n",
      "Iteration 26919 => Loss: 6.70367552825000245775\n",
      "Iteration 26920 => Loss: 6.70367478744149192238\n",
      "Iteration 26921 => Loss: 6.70367404673280287142\n",
      "Iteration 26922 => Loss: 6.70367330612391665312\n",
      "Iteration 26923 => Loss: 6.70367256561483859656\n",
      "Iteration 26924 => Loss: 6.70367182520554560909\n",
      "Iteration 26925 => Loss: 6.70367108489601459809\n",
      "Iteration 26926 => Loss: 6.70367034468624733989\n",
      "Iteration 26927 => Loss: 6.70366960457621630098\n",
      "Iteration 26928 => Loss: 6.70366886456592325771\n",
      "Iteration 26929 => Loss: 6.70366812465533978838\n",
      "Iteration 26930 => Loss: 6.70366738484445701118\n",
      "Iteration 26931 => Loss: 6.70366664513327137342\n",
      "Iteration 26932 => Loss: 6.70366590552175356521\n",
      "Iteration 26933 => Loss: 6.70366516600990625108\n",
      "Iteration 26934 => Loss: 6.70366442659771255563\n",
      "Iteration 26935 => Loss: 6.70366368728513872810\n",
      "Iteration 26936 => Loss: 6.70366294807220075569\n",
      "Iteration 26937 => Loss: 6.70366220895886222308\n",
      "Iteration 26938 => Loss: 6.70366146994512668300\n",
      "Iteration 26939 => Loss: 6.70366073103097370733\n",
      "Iteration 26940 => Loss: 6.70365999221638997341\n",
      "Iteration 26941 => Loss: 6.70365925350135150040\n",
      "Iteration 26942 => Loss: 6.70365851488586450557\n",
      "Iteration 26943 => Loss: 6.70365777636990323174\n",
      "Iteration 26944 => Loss: 6.70365703795345790894\n",
      "Iteration 26945 => Loss: 6.70365629963650899725\n",
      "Iteration 26946 => Loss: 6.70365556141905827303\n",
      "Iteration 26947 => Loss: 6.70365482330107376185\n",
      "Iteration 26948 => Loss: 6.70365408528255546372\n",
      "Iteration 26949 => Loss: 6.70365334736348206235\n",
      "Iteration 26950 => Loss: 6.70365260954384378778\n",
      "Iteration 26951 => Loss: 6.70365187182362998186\n",
      "Iteration 26952 => Loss: 6.70365113420282288104\n",
      "Iteration 26953 => Loss: 6.70365039668140294538\n",
      "Iteration 26954 => Loss: 6.70364965925937461577\n",
      "Iteration 26955 => Loss: 6.70364892193671213505\n",
      "Iteration 26956 => Loss: 6.70364818471339773964\n",
      "Iteration 26957 => Loss: 6.70364744758943320591\n",
      "Iteration 26958 => Loss: 6.70364671056478744759\n",
      "Iteration 26959 => Loss: 6.70364597363945602382\n",
      "Iteration 26960 => Loss: 6.70364523681343360550\n",
      "Iteration 26961 => Loss: 6.70364450008669443548\n",
      "Iteration 26962 => Loss: 6.70364376345922607925\n",
      "Iteration 26963 => Loss: 6.70364302693102320774\n",
      "Iteration 26964 => Loss: 6.70364229050206095195\n",
      "Iteration 26965 => Loss: 6.70364155417234108825\n",
      "Iteration 26966 => Loss: 6.70364081794183963581\n",
      "Iteration 26967 => Loss: 6.70364008181054149560\n",
      "Iteration 26968 => Loss: 6.70363934577843867402\n",
      "Iteration 26969 => Loss: 6.70363860984551696021\n",
      "Iteration 26970 => Loss: 6.70363787401175947878\n",
      "Iteration 26971 => Loss: 6.70363713827715823612\n",
      "Iteration 26972 => Loss: 6.70363640264169724503\n",
      "Iteration 26973 => Loss: 6.70363566710536140647\n",
      "Iteration 26974 => Loss: 6.70363493166813917412\n",
      "Iteration 26975 => Loss: 6.70363419633001633713\n",
      "Iteration 26976 => Loss: 6.70363346109098579007\n",
      "Iteration 26977 => Loss: 6.70363272595102266393\n",
      "Iteration 26978 => Loss: 6.70363199091012340602\n",
      "Iteration 26979 => Loss: 6.70363125596827202912\n",
      "Iteration 26980 => Loss: 6.70363052112544544059\n",
      "Iteration 26981 => Loss: 6.70362978638165429857\n",
      "Iteration 26982 => Loss: 6.70362905173685597049\n",
      "Iteration 26983 => Loss: 6.70362831719105933814\n",
      "Iteration 26984 => Loss: 6.70362758274423775617\n",
      "Iteration 26985 => Loss: 6.70362684839638411916\n",
      "Iteration 26986 => Loss: 6.70362611414748599259\n",
      "Iteration 26987 => Loss: 6.70362537999752827744\n",
      "Iteration 26988 => Loss: 6.70362464594649676286\n",
      "Iteration 26989 => Loss: 6.70362391199437812617\n",
      "Iteration 26990 => Loss: 6.70362317814115638015\n",
      "Iteration 26991 => Loss: 6.70362244438682530756\n",
      "Iteration 26992 => Loss: 6.70362171073136536847\n",
      "Iteration 26993 => Loss: 6.70362097717476679293\n",
      "Iteration 26994 => Loss: 6.70362024371701537007\n",
      "Iteration 26995 => Loss: 6.70361951035809777721\n",
      "Iteration 26996 => Loss: 6.70361877709800157987\n",
      "Iteration 26997 => Loss: 6.70361804393671256719\n",
      "Iteration 26998 => Loss: 6.70361731087421031106\n",
      "Iteration 26999 => Loss: 6.70361657791049481148\n",
      "Iteration 27000 => Loss: 6.70361584504554297581\n",
      "Iteration 27001 => Loss: 6.70361511227934769863\n",
      "Iteration 27002 => Loss: 6.70361437961189210455\n",
      "Iteration 27003 => Loss: 6.70361364704316198271\n",
      "Iteration 27004 => Loss: 6.70361291457314578679\n",
      "Iteration 27005 => Loss: 6.70361218220182397687\n",
      "Iteration 27006 => Loss: 6.70361144992920010566\n",
      "Iteration 27007 => Loss: 6.70361071775524752780\n",
      "Iteration 27008 => Loss: 6.70360998567995469699\n",
      "Iteration 27009 => Loss: 6.70360925370330473783\n",
      "Iteration 27010 => Loss: 6.70360852182529232124\n",
      "Iteration 27011 => Loss: 6.70360779004590323638\n",
      "Iteration 27012 => Loss: 6.70360705836511616695\n",
      "Iteration 27013 => Loss: 6.70360632678292311937\n",
      "Iteration 27014 => Loss: 6.70360559529930988276\n",
      "Iteration 27015 => Loss: 6.70360486391427112807\n",
      "Iteration 27016 => Loss: 6.70360413262778376264\n",
      "Iteration 27017 => Loss: 6.70360340143983624017\n",
      "Iteration 27018 => Loss: 6.70360267035041079708\n",
      "Iteration 27019 => Loss: 6.70360193935951009792\n",
      "Iteration 27020 => Loss: 6.70360120846710216824\n",
      "Iteration 27021 => Loss: 6.70360047767318878442\n",
      "Iteration 27022 => Loss: 6.70359974697774596564\n",
      "Iteration 27023 => Loss: 6.70359901638076571828\n",
      "Iteration 27024 => Loss: 6.70359828588222583790\n",
      "Iteration 27025 => Loss: 6.70359755548213254173\n",
      "Iteration 27026 => Loss: 6.70359682518045651989\n",
      "Iteration 27027 => Loss: 6.70359609497718622606\n",
      "Iteration 27028 => Loss: 6.70359536487231544299\n",
      "Iteration 27029 => Loss: 6.70359463486582463077\n",
      "Iteration 27030 => Loss: 6.70359390495769957852\n",
      "Iteration 27031 => Loss: 6.70359317514793140447\n",
      "Iteration 27032 => Loss: 6.70359244543650323322\n",
      "Iteration 27033 => Loss: 6.70359171582340529483\n",
      "Iteration 27034 => Loss: 6.70359098630862249024\n",
      "Iteration 27035 => Loss: 6.70359025689214238497\n",
      "Iteration 27036 => Loss: 6.70358952757395876176\n",
      "Iteration 27037 => Loss: 6.70358879835403609349\n",
      "Iteration 27038 => Loss: 6.70358806923237793285\n",
      "Iteration 27039 => Loss: 6.70358734020897895078\n",
      "Iteration 27040 => Loss: 6.70358661128381339012\n",
      "Iteration 27041 => Loss: 6.70358588245686881635\n",
      "Iteration 27042 => Loss: 6.70358515372813190680\n",
      "Iteration 27043 => Loss: 6.70358442509759111516\n",
      "Iteration 27044 => Loss: 6.70358369656523400693\n",
      "Iteration 27045 => Loss: 6.70358296813104903578\n",
      "Iteration 27046 => Loss: 6.70358223979502110268\n",
      "Iteration 27047 => Loss: 6.70358151155713066771\n",
      "Iteration 27048 => Loss: 6.70358078341737773087\n",
      "Iteration 27049 => Loss: 6.70358005537573031773\n",
      "Iteration 27050 => Loss: 6.70357932743219819827\n",
      "Iteration 27051 => Loss: 6.70357859958675827983\n",
      "Iteration 27052 => Loss: 6.70357787183938302888\n",
      "Iteration 27053 => Loss: 6.70357714419008043905\n",
      "Iteration 27054 => Loss: 6.70357641663882919403\n",
      "Iteration 27055 => Loss: 6.70357568918561064208\n",
      "Iteration 27056 => Loss: 6.70357496183042034232\n",
      "Iteration 27057 => Loss: 6.70357423457323786664\n",
      "Iteration 27058 => Loss: 6.70357350741406055050\n",
      "Iteration 27059 => Loss: 6.70357278035285641948\n",
      "Iteration 27060 => Loss: 6.70357205338963346719\n",
      "Iteration 27061 => Loss: 6.70357132652436593645\n",
      "Iteration 27062 => Loss: 6.70357059975704139276\n",
      "Iteration 27063 => Loss: 6.70356987308764917799\n",
      "Iteration 27064 => Loss: 6.70356914651617596945\n",
      "Iteration 27065 => Loss: 6.70356842004261288537\n",
      "Iteration 27066 => Loss: 6.70356769366693949763\n",
      "Iteration 27067 => Loss: 6.70356696738914425993\n",
      "Iteration 27068 => Loss: 6.70356624120921384957\n",
      "Iteration 27069 => Loss: 6.70356551512714027297\n",
      "Iteration 27070 => Loss: 6.70356478914290132565\n",
      "Iteration 27071 => Loss: 6.70356406325649523126\n",
      "Iteration 27072 => Loss: 6.70356333746789712080\n",
      "Iteration 27073 => Loss: 6.70356261177710255339\n",
      "Iteration 27074 => Loss: 6.70356188618408666002\n",
      "Iteration 27075 => Loss: 6.70356116068885299342\n",
      "Iteration 27076 => Loss: 6.70356043529138290182\n",
      "Iteration 27077 => Loss: 6.70355970999164529900\n",
      "Iteration 27078 => Loss: 6.70355898478965528398\n",
      "Iteration 27079 => Loss: 6.70355825968538265869\n",
      "Iteration 27080 => Loss: 6.70355753467881676499\n",
      "Iteration 27081 => Loss: 6.70355680976994605658\n",
      "Iteration 27082 => Loss: 6.70355608495875987529\n",
      "Iteration 27083 => Loss: 6.70355536024523424032\n",
      "Iteration 27084 => Loss: 6.70355463562937181621\n",
      "Iteration 27085 => Loss: 6.70355391111115128666\n",
      "Iteration 27086 => Loss: 6.70355318669055666447\n",
      "Iteration 27087 => Loss: 6.70355246236757462697\n",
      "Iteration 27088 => Loss: 6.70355173814219718054\n",
      "Iteration 27089 => Loss: 6.70355101401440833797\n",
      "Iteration 27090 => Loss: 6.70355028998420099384\n",
      "Iteration 27091 => Loss: 6.70354956605155294369\n",
      "Iteration 27092 => Loss: 6.70354884221646063480\n",
      "Iteration 27093 => Loss: 6.70354811847889831000\n",
      "Iteration 27094 => Loss: 6.70354739483886330476\n",
      "Iteration 27095 => Loss: 6.70354667129633430278\n",
      "Iteration 27096 => Loss: 6.70354594785130242229\n",
      "Iteration 27097 => Loss: 6.70354522450376055787\n",
      "Iteration 27098 => Loss: 6.70354450125368739322\n",
      "Iteration 27099 => Loss: 6.70354377810106871749\n",
      "Iteration 27100 => Loss: 6.70354305504589742526\n",
      "Iteration 27101 => Loss: 6.70354233208816285838\n",
      "Iteration 27102 => Loss: 6.70354160922784458876\n",
      "Iteration 27103 => Loss: 6.70354088646493018189\n",
      "Iteration 27104 => Loss: 6.70354016379940542691\n",
      "Iteration 27105 => Loss: 6.70353944123126233023\n",
      "Iteration 27106 => Loss: 6.70353871876048934553\n",
      "Iteration 27107 => Loss: 6.70353799638706071562\n",
      "Iteration 27108 => Loss: 6.70353727411097910505\n",
      "Iteration 27109 => Loss: 6.70353655193222230935\n",
      "Iteration 27110 => Loss: 6.70353582985077345313\n",
      "Iteration 27111 => Loss: 6.70353510786663964183\n",
      "Iteration 27112 => Loss: 6.70353438597977646651\n",
      "Iteration 27113 => Loss: 6.70353366419019103262\n",
      "Iteration 27114 => Loss: 6.70353294249787623471\n",
      "Iteration 27115 => Loss: 6.70353222090280365109\n",
      "Iteration 27116 => Loss: 6.70353149940496884085\n",
      "Iteration 27117 => Loss: 6.70353077800435404043\n",
      "Iteration 27118 => Loss: 6.70353005670094592716\n",
      "Iteration 27119 => Loss: 6.70352933549473384289\n",
      "Iteration 27120 => Loss: 6.70352861438570357677\n",
      "Iteration 27121 => Loss: 6.70352789337384802337\n",
      "Iteration 27122 => Loss: 6.70352717245914675459\n",
      "Iteration 27123 => Loss: 6.70352645164158555957\n",
      "Iteration 27124 => Loss: 6.70352573092115555653\n",
      "Iteration 27125 => Loss: 6.70352501029785230457\n",
      "Iteration 27126 => Loss: 6.70352428977163761203\n",
      "Iteration 27127 => Loss: 6.70352356934252302523\n",
      "Iteration 27128 => Loss: 6.70352284901048367516\n",
      "Iteration 27129 => Loss: 6.70352212877550890369\n",
      "Iteration 27130 => Loss: 6.70352140863758716449\n",
      "Iteration 27131 => Loss: 6.70352068859670158218\n",
      "Iteration 27132 => Loss: 6.70351996865284238680\n",
      "Iteration 27133 => Loss: 6.70351924880599980838\n",
      "Iteration 27134 => Loss: 6.70351852905615253064\n",
      "Iteration 27135 => Loss: 6.70351780940329433633\n",
      "Iteration 27136 => Loss: 6.70351708984740835007\n",
      "Iteration 27137 => Loss: 6.70351637038848124917\n",
      "Iteration 27138 => Loss: 6.70351565102649793459\n",
      "Iteration 27139 => Loss: 6.70351493176146018271\n",
      "Iteration 27140 => Loss: 6.70351421259333424274\n",
      "Iteration 27141 => Loss: 6.70351349352212100285\n",
      "Iteration 27142 => Loss: 6.70351277454779470588\n",
      "Iteration 27143 => Loss: 6.70351205567035890454\n",
      "Iteration 27144 => Loss: 6.70351133688979139436\n",
      "Iteration 27145 => Loss: 6.70351061820607174724\n",
      "Iteration 27146 => Loss: 6.70350989961919729865\n",
      "Iteration 27147 => Loss: 6.70350918112915916680\n",
      "Iteration 27148 => Loss: 6.70350846273593070634\n",
      "Iteration 27149 => Loss: 6.70350774443951014092\n",
      "Iteration 27150 => Loss: 6.70350702623987881879\n",
      "Iteration 27151 => Loss: 6.70350630813702252908\n",
      "Iteration 27152 => Loss: 6.70350559013093239003\n",
      "Iteration 27153 => Loss: 6.70350487222159152623\n",
      "Iteration 27154 => Loss: 6.70350415440899372044\n",
      "Iteration 27155 => Loss: 6.70350343669312387362\n",
      "Iteration 27156 => Loss: 6.70350271907395356408\n",
      "Iteration 27157 => Loss: 6.70350200155149078540\n",
      "Iteration 27158 => Loss: 6.70350128412571955039\n",
      "Iteration 27159 => Loss: 6.70350056679661054915\n",
      "Iteration 27160 => Loss: 6.70349984956416733439\n",
      "Iteration 27161 => Loss: 6.70349913242837125438\n",
      "Iteration 27162 => Loss: 6.70349841538920898643\n",
      "Iteration 27163 => Loss: 6.70349769844666898422\n",
      "Iteration 27164 => Loss: 6.70349698160073526054\n",
      "Iteration 27165 => Loss: 6.70349626485139538090\n",
      "Iteration 27166 => Loss: 6.70349554819864046351\n",
      "Iteration 27167 => Loss: 6.70349483164245718569\n",
      "Iteration 27168 => Loss: 6.70349411518281979028\n",
      "Iteration 27169 => Loss: 6.70349339881973094180\n",
      "Iteration 27170 => Loss: 6.70349268255317465304\n",
      "Iteration 27171 => Loss: 6.70349196638313138408\n",
      "Iteration 27172 => Loss: 6.70349125030959402949\n",
      "Iteration 27173 => Loss: 6.70349053433255015477\n",
      "Iteration 27174 => Loss: 6.70348981845197844365\n",
      "Iteration 27175 => Loss: 6.70348910266787534340\n",
      "Iteration 27176 => Loss: 6.70348838698022664317\n",
      "Iteration 27177 => Loss: 6.70348767138901280305\n",
      "Iteration 27178 => Loss: 6.70348695589422849395\n",
      "Iteration 27179 => Loss: 6.70348624049584884688\n",
      "Iteration 27180 => Loss: 6.70348552519387741455\n",
      "Iteration 27181 => Loss: 6.70348480998829376887\n",
      "Iteration 27182 => Loss: 6.70348409487907481719\n",
      "Iteration 27183 => Loss: 6.70348337986622677676\n",
      "Iteration 27184 => Loss: 6.70348266494972211405\n",
      "Iteration 27185 => Loss: 6.70348195012955638816\n",
      "Iteration 27186 => Loss: 6.70348123540570828283\n",
      "Iteration 27187 => Loss: 6.70348052077816891625\n",
      "Iteration 27188 => Loss: 6.70347980624692763030\n",
      "Iteration 27189 => Loss: 6.70347909181196932593\n",
      "Iteration 27190 => Loss: 6.70347837747328334501\n",
      "Iteration 27191 => Loss: 6.70347766323085281215\n",
      "Iteration 27192 => Loss: 6.70347694908466884556\n",
      "Iteration 27193 => Loss: 6.70347623503471101714\n",
      "Iteration 27194 => Loss: 6.70347552108097310963\n",
      "Iteration 27195 => Loss: 6.70347480722343735948\n",
      "Iteration 27196 => Loss: 6.70347409346210110215\n",
      "Iteration 27197 => Loss: 6.70347337979694124499\n",
      "Iteration 27198 => Loss: 6.70347266622795334712\n",
      "Iteration 27199 => Loss: 6.70347195275510987500\n",
      "Iteration 27200 => Loss: 6.70347123937841349317\n",
      "Iteration 27201 => Loss: 6.70347052609783666810\n",
      "Iteration 27202 => Loss: 6.70346981291338295250\n",
      "Iteration 27203 => Loss: 6.70346909982502392467\n",
      "Iteration 27204 => Loss: 6.70346838683275869641\n",
      "Iteration 27205 => Loss: 6.70346767393656417511\n",
      "Iteration 27206 => Loss: 6.70346696113644124893\n",
      "Iteration 27207 => Loss: 6.70346624843235971980\n",
      "Iteration 27208 => Loss: 6.70346553582431248230\n",
      "Iteration 27209 => Loss: 6.70346482331229776008\n",
      "Iteration 27210 => Loss: 6.70346411089629334867\n",
      "Iteration 27211 => Loss: 6.70346339857628770176\n",
      "Iteration 27212 => Loss: 6.70346268635225950305\n",
      "Iteration 27213 => Loss: 6.70346197422420964074\n",
      "Iteration 27214 => Loss: 6.70346126219212301578\n",
      "Iteration 27215 => Loss: 6.70346055025597475918\n",
      "Iteration 27216 => Loss: 6.70345983841576131823\n",
      "Iteration 27217 => Loss: 6.70345912667146670572\n",
      "Iteration 27218 => Loss: 6.70345841502308736892\n",
      "Iteration 27219 => Loss: 6.70345770347059666250\n",
      "Iteration 27220 => Loss: 6.70345699201399369827\n",
      "Iteration 27221 => Loss: 6.70345628065325183087\n",
      "Iteration 27222 => Loss: 6.70345556938837106031\n",
      "Iteration 27223 => Loss: 6.70345485821933007031\n",
      "Iteration 27224 => Loss: 6.70345414714612086726\n",
      "Iteration 27225 => Loss: 6.70345343616872835213\n",
      "Iteration 27226 => Loss: 6.70345272528713831406\n",
      "Iteration 27227 => Loss: 6.70345201450134187127\n",
      "Iteration 27228 => Loss: 6.70345130381132392472\n",
      "Iteration 27229 => Loss: 6.70345059321707026356\n",
      "Iteration 27230 => Loss: 6.70344988271857111783\n",
      "Iteration 27231 => Loss: 6.70344917231581316486\n",
      "Iteration 27232 => Loss: 6.70344846200878041742\n",
      "Iteration 27233 => Loss: 6.70344775179745777649\n",
      "Iteration 27234 => Loss: 6.70344704168184168935\n",
      "Iteration 27235 => Loss: 6.70344633166190018159\n",
      "Iteration 27236 => Loss: 6.70344562173765545765\n",
      "Iteration 27237 => Loss: 6.70344491190905955591\n",
      "Iteration 27238 => Loss: 6.70344420217611869361\n",
      "Iteration 27239 => Loss: 6.70344349253881066630\n",
      "Iteration 27240 => Loss: 6.70344278299713103308\n",
      "Iteration 27241 => Loss: 6.70344207355105492496\n",
      "Iteration 27242 => Loss: 6.70344136420058234194\n",
      "Iteration 27243 => Loss: 6.70344065494569107955\n",
      "Iteration 27244 => Loss: 6.70343994578636959147\n",
      "Iteration 27245 => Loss: 6.70343923672261698954\n",
      "Iteration 27246 => Loss: 6.70343852775440662839\n",
      "Iteration 27247 => Loss: 6.70343781888172873806\n",
      "Iteration 27248 => Loss: 6.70343711010457088406\n",
      "Iteration 27249 => Loss: 6.70343640142291885553\n",
      "Iteration 27250 => Loss: 6.70343569283676288251\n",
      "Iteration 27251 => Loss: 6.70343498434609053049\n",
      "Iteration 27252 => Loss: 6.70343427595088758864\n",
      "Iteration 27253 => Loss: 6.70343356765114339879\n",
      "Iteration 27254 => Loss: 6.70343285944684108557\n",
      "Iteration 27255 => Loss: 6.70343215133796643812\n",
      "Iteration 27256 => Loss: 6.70343144332451057466\n",
      "Iteration 27257 => Loss: 6.70343073540645306707\n",
      "Iteration 27258 => Loss: 6.70343002758380013262\n",
      "Iteration 27259 => Loss: 6.70342931985651979687\n",
      "Iteration 27260 => Loss: 6.70342861222460761894\n",
      "Iteration 27261 => Loss: 6.70342790468804494708\n",
      "Iteration 27262 => Loss: 6.70342719724682645221\n",
      "Iteration 27263 => Loss: 6.70342648990093969985\n",
      "Iteration 27264 => Loss: 6.70342578265036692642\n",
      "Iteration 27265 => Loss: 6.70342507549508770381\n",
      "Iteration 27266 => Loss: 6.70342436843510025568\n",
      "Iteration 27267 => Loss: 6.70342366147039303570\n",
      "Iteration 27268 => Loss: 6.70342295460094739212\n",
      "Iteration 27269 => Loss: 6.70342224782675266681\n",
      "Iteration 27270 => Loss: 6.70342154114779642526\n",
      "Iteration 27271 => Loss: 6.70342083456406623299\n",
      "Iteration 27272 => Loss: 6.70342012807554521459\n",
      "Iteration 27273 => Loss: 6.70341942168222537646\n",
      "Iteration 27274 => Loss: 6.70341871538409161957\n",
      "Iteration 27275 => Loss: 6.70341800918112973307\n",
      "Iteration 27276 => Loss: 6.70341730307333349970\n",
      "Iteration 27277 => Loss: 6.70341659706067893865\n",
      "Iteration 27278 => Loss: 6.70341589114316960263\n",
      "Iteration 27279 => Loss: 6.70341518532077529358\n",
      "Iteration 27280 => Loss: 6.70341447959348979424\n",
      "Iteration 27281 => Loss: 6.70341377396130155830\n",
      "Iteration 27282 => Loss: 6.70341306842419903944\n",
      "Iteration 27283 => Loss: 6.70341236298216980316\n",
      "Iteration 27284 => Loss: 6.70341165763519963861\n",
      "Iteration 27285 => Loss: 6.70341095238327167039\n",
      "Iteration 27286 => Loss: 6.70341024722637524036\n",
      "Iteration 27287 => Loss: 6.70340954216449702585\n",
      "Iteration 27288 => Loss: 6.70340883719763613868\n",
      "Iteration 27289 => Loss: 6.70340813232575616354\n",
      "Iteration 27290 => Loss: 6.70340742754887219945\n",
      "Iteration 27291 => Loss: 6.70340672286694339022\n",
      "Iteration 27292 => Loss: 6.70340601827998217033\n",
      "Iteration 27293 => Loss: 6.70340531378795567718\n",
      "Iteration 27294 => Loss: 6.70340460939086391079\n",
      "Iteration 27295 => Loss: 6.70340390508868644304\n",
      "Iteration 27296 => Loss: 6.70340320088141261579\n",
      "Iteration 27297 => Loss: 6.70340249676903088272\n",
      "Iteration 27298 => Loss: 6.70340179275153857930\n",
      "Iteration 27299 => Loss: 6.70340108882890550746\n",
      "Iteration 27300 => Loss: 6.70340038500112278541\n",
      "Iteration 27301 => Loss: 6.70339968126818241956\n",
      "Iteration 27302 => Loss: 6.70339897763007286358\n",
      "Iteration 27303 => Loss: 6.70339827408677724208\n",
      "Iteration 27304 => Loss: 6.70339757063828312056\n",
      "Iteration 27305 => Loss: 6.70339686728457895271\n",
      "Iteration 27306 => Loss: 6.70339616402565230402\n",
      "Iteration 27307 => Loss: 6.70339546086148718729\n",
      "Iteration 27308 => Loss: 6.70339475779207916162\n",
      "Iteration 27309 => Loss: 6.70339405481740602255\n",
      "Iteration 27310 => Loss: 6.70339335193746332919\n",
      "Iteration 27311 => Loss: 6.70339264915222976526\n",
      "Iteration 27312 => Loss: 6.70339194646169023173\n",
      "Iteration 27313 => Loss: 6.70339124386585538673\n",
      "Iteration 27314 => Loss: 6.70339054136468437406\n",
      "Iteration 27315 => Loss: 6.70338983895817364100\n",
      "Iteration 27316 => Loss: 6.70338913664630986489\n",
      "Iteration 27317 => Loss: 6.70338843442908771664\n",
      "Iteration 27318 => Loss: 6.70338773230649298540\n",
      "Iteration 27319 => Loss: 6.70338703027850790761\n",
      "Iteration 27320 => Loss: 6.70338632834511205516\n",
      "Iteration 27321 => Loss: 6.70338562650630898077\n",
      "Iteration 27322 => Loss: 6.70338492476208269721\n",
      "Iteration 27323 => Loss: 6.70338422311240744733\n",
      "Iteration 27324 => Loss: 6.70338352155728856019\n",
      "Iteration 27325 => Loss: 6.70338282009669672590\n",
      "Iteration 27326 => Loss: 6.70338211873062927992\n",
      "Iteration 27327 => Loss: 6.70338141745907289959\n",
      "Iteration 27328 => Loss: 6.70338071628201248586\n",
      "Iteration 27329 => Loss: 6.70338001519943382789\n",
      "Iteration 27330 => Loss: 6.70337931421132893206\n",
      "Iteration 27331 => Loss: 6.70337861331767381756\n",
      "Iteration 27332 => Loss: 6.70337791251847203711\n",
      "Iteration 27333 => Loss: 6.70337721181371026802\n",
      "Iteration 27334 => Loss: 6.70337651120335475952\n",
      "Iteration 27335 => Loss: 6.70337581068741350521\n",
      "Iteration 27336 => Loss: 6.70337511026586607699\n",
      "Iteration 27337 => Loss: 6.70337440993869559946\n",
      "Iteration 27338 => Loss: 6.70337370970589940811\n",
      "Iteration 27339 => Loss: 6.70337300956745707481\n",
      "Iteration 27340 => Loss: 6.70337230952336415868\n",
      "Iteration 27341 => Loss: 6.70337160957359579072\n",
      "Iteration 27342 => Loss: 6.70337090971814486551\n",
      "Iteration 27343 => Loss: 6.70337020995700960668\n",
      "Iteration 27344 => Loss: 6.70336951029015537529\n",
      "Iteration 27345 => Loss: 6.70336881071758128314\n",
      "Iteration 27346 => Loss: 6.70336811123928466571\n",
      "Iteration 27347 => Loss: 6.70336741185523621311\n",
      "Iteration 27348 => Loss: 6.70336671256543237263\n",
      "Iteration 27349 => Loss: 6.70336601336985626887\n",
      "Iteration 27350 => Loss: 6.70336531426849724369\n",
      "Iteration 27351 => Loss: 6.70336461526134375077\n",
      "Iteration 27352 => Loss: 6.70336391634837536202\n",
      "Iteration 27353 => Loss: 6.70336321752959385378\n",
      "Iteration 27354 => Loss: 6.70336251880497169253\n",
      "Iteration 27355 => Loss: 6.70336182017450443738\n",
      "Iteration 27356 => Loss: 6.70336112163818587106\n",
      "Iteration 27357 => Loss: 6.70336042319598401917\n",
      "Iteration 27358 => Loss: 6.70335972484790154624\n",
      "Iteration 27359 => Loss: 6.70335902659392068870\n",
      "Iteration 27360 => Loss: 6.70335832843402990022\n",
      "Iteration 27361 => Loss: 6.70335763036821763450\n",
      "Iteration 27362 => Loss: 6.70335693239647145703\n",
      "Iteration 27363 => Loss: 6.70335623451876649881\n",
      "Iteration 27364 => Loss: 6.70335553673511075345\n",
      "Iteration 27365 => Loss: 6.70335483904547668743\n",
      "Iteration 27366 => Loss: 6.70335414144986163620\n",
      "Iteration 27367 => Loss: 6.70335344394824694803\n",
      "Iteration 27368 => Loss: 6.70335274654061574751\n",
      "Iteration 27369 => Loss: 6.70335204922696448193\n",
      "Iteration 27370 => Loss: 6.70335135200727716409\n",
      "Iteration 27371 => Loss: 6.70335065488153691859\n",
      "Iteration 27372 => Loss: 6.70334995784973575184\n",
      "Iteration 27373 => Loss: 6.70334926091185412389\n",
      "Iteration 27374 => Loss: 6.70334856406789381111\n",
      "Iteration 27375 => Loss: 6.70334786731783172087\n",
      "Iteration 27376 => Loss: 6.70334717066165364230\n",
      "Iteration 27377 => Loss: 6.70334647409935069362\n",
      "Iteration 27378 => Loss: 6.70334577763091044034\n",
      "Iteration 27379 => Loss: 6.70334508125631423070\n",
      "Iteration 27380 => Loss: 6.70334438497556117653\n",
      "Iteration 27381 => Loss: 6.70334368878862996155\n",
      "Iteration 27382 => Loss: 6.70334299269550726308\n",
      "Iteration 27383 => Loss: 6.70334229669618952840\n",
      "Iteration 27384 => Loss: 6.70334160079065455307\n",
      "Iteration 27385 => Loss: 6.70334090497889256710\n",
      "Iteration 27386 => Loss: 6.70334020926088669512\n",
      "Iteration 27387 => Loss: 6.70333951363663338441\n",
      "Iteration 27388 => Loss: 6.70333881810611931229\n",
      "Iteration 27389 => Loss: 6.70333812266932316248\n",
      "Iteration 27390 => Loss: 6.70333742732623605320\n",
      "Iteration 27391 => Loss: 6.70333673207685176720\n",
      "Iteration 27392 => Loss: 6.70333603692115165273\n",
      "Iteration 27393 => Loss: 6.70333534185911705805\n",
      "Iteration 27394 => Loss: 6.70333464689075153586\n",
      "Iteration 27395 => Loss: 6.70333395201602133540\n",
      "Iteration 27396 => Loss: 6.70333325723493533843\n",
      "Iteration 27397 => Loss: 6.70333256254746956415\n",
      "Iteration 27398 => Loss: 6.70333186795360891352\n",
      "Iteration 27399 => Loss: 6.70333117345334716930\n",
      "Iteration 27400 => Loss: 6.70333047904666745609\n",
      "Iteration 27401 => Loss: 6.70332978473356355664\n",
      "Iteration 27402 => Loss: 6.70332909051401415468\n",
      "Iteration 27403 => Loss: 6.70332839638801480930\n",
      "Iteration 27404 => Loss: 6.70332770235555308602\n",
      "Iteration 27405 => Loss: 6.70332700841660411584\n",
      "Iteration 27406 => Loss: 6.70332631457115812879\n",
      "Iteration 27407 => Loss: 6.70332562081921956576\n",
      "Iteration 27408 => Loss: 6.70332492716075467598\n",
      "Iteration 27409 => Loss: 6.70332423359576701216\n",
      "Iteration 27410 => Loss: 6.70332354012424058709\n",
      "Iteration 27411 => Loss: 6.70332284674615230813\n",
      "Iteration 27412 => Loss: 6.70332215346149773438\n",
      "Iteration 27413 => Loss: 6.70332146027026620771\n",
      "Iteration 27414 => Loss: 6.70332076717243996455\n",
      "Iteration 27415 => Loss: 6.70332007416801278765\n",
      "Iteration 27416 => Loss: 6.70331938125696424891\n",
      "Iteration 27417 => Loss: 6.70331868843928369017\n",
      "Iteration 27418 => Loss: 6.70331799571496489420\n",
      "Iteration 27419 => Loss: 6.70331730308399009743\n",
      "Iteration 27420 => Loss: 6.70331661054634420083\n",
      "Iteration 27421 => Loss: 6.70331591810202009896\n",
      "Iteration 27422 => Loss: 6.70331522575099914008\n",
      "Iteration 27423 => Loss: 6.70331453349327333058\n",
      "Iteration 27424 => Loss: 6.70331384132883290050\n",
      "Iteration 27425 => Loss: 6.70331314925766186263\n",
      "Iteration 27426 => Loss: 6.70331245727974422977\n",
      "Iteration 27427 => Loss: 6.70331176539507556100\n",
      "Iteration 27428 => Loss: 6.70331107360363365189\n",
      "Iteration 27429 => Loss: 6.70331038190541494970\n",
      "Iteration 27430 => Loss: 6.70330969030039636181\n",
      "Iteration 27431 => Loss: 6.70330899878857433549\n",
      "Iteration 27432 => Loss: 6.70330830736994087715\n",
      "Iteration 27433 => Loss: 6.70330761604446667690\n",
      "Iteration 27434 => Loss: 6.70330692481214995837\n",
      "Iteration 27435 => Loss: 6.70330623367297917525\n",
      "Iteration 27436 => Loss: 6.70330554262693745216\n",
      "Iteration 27437 => Loss: 6.70330485167401590729\n",
      "Iteration 27438 => Loss: 6.70330416081420032981\n",
      "Iteration 27439 => Loss: 6.70330347004747917339\n",
      "Iteration 27440 => Loss: 6.70330277937384089171\n",
      "Iteration 27441 => Loss: 6.70330208879326416849\n",
      "Iteration 27442 => Loss: 6.70330139830574811555\n",
      "Iteration 27443 => Loss: 6.70330070791127319296\n",
      "Iteration 27444 => Loss: 6.70330001760982874259\n",
      "Iteration 27445 => Loss: 6.70329932740140410630\n",
      "Iteration 27446 => Loss: 6.70329863728598418504\n",
      "Iteration 27447 => Loss: 6.70329794726355832069\n",
      "Iteration 27448 => Loss: 6.70329725733411230237\n",
      "Iteration 27449 => Loss: 6.70329656749763724832\n",
      "Iteration 27450 => Loss: 6.70329587775411273043\n",
      "Iteration 27451 => Loss: 6.70329518810353430780\n",
      "Iteration 27452 => Loss: 6.70329449854588155233\n",
      "Iteration 27453 => Loss: 6.70329380908115357585\n",
      "Iteration 27454 => Loss: 6.70329311970932639753\n",
      "Iteration 27455 => Loss: 6.70329243043039291194\n",
      "Iteration 27456 => Loss: 6.70329174124434157278\n",
      "Iteration 27457 => Loss: 6.70329105215115639282\n",
      "Iteration 27458 => Loss: 6.70329036315082937847\n",
      "Iteration 27459 => Loss: 6.70328967424334098979\n",
      "Iteration 27460 => Loss: 6.70328898542868678589\n",
      "Iteration 27461 => Loss: 6.70328829670684722686\n",
      "Iteration 27462 => Loss: 6.70328760807781431907\n",
      "Iteration 27463 => Loss: 6.70328691954157651622\n",
      "Iteration 27464 => Loss: 6.70328623109811871927\n",
      "Iteration 27465 => Loss: 6.70328554274742316466\n",
      "Iteration 27466 => Loss: 6.70328485448948718783\n",
      "Iteration 27467 => Loss: 6.70328416632429480160\n",
      "Iteration 27468 => Loss: 6.70328347825183090691\n",
      "Iteration 27469 => Loss: 6.70328279027208484564\n",
      "Iteration 27470 => Loss: 6.70328210238504063057\n",
      "Iteration 27471 => Loss: 6.70328141459069737351\n",
      "Iteration 27472 => Loss: 6.70328072688903198184\n",
      "Iteration 27473 => Loss: 6.70328003928003468559\n",
      "Iteration 27474 => Loss: 6.70327935176368683301\n",
      "Iteration 27475 => Loss: 6.70327866433998575957\n",
      "Iteration 27476 => Loss: 6.70327797700891370170\n",
      "Iteration 27477 => Loss: 6.70327728977046088943\n",
      "Iteration 27478 => Loss: 6.70327660262461577645\n",
      "Iteration 27479 => Loss: 6.70327591557136592826\n",
      "Iteration 27480 => Loss: 6.70327522861068914040\n",
      "Iteration 27481 => Loss: 6.70327454174258630104\n",
      "Iteration 27482 => Loss: 6.70327385496703342938\n",
      "Iteration 27483 => Loss: 6.70327316828402963722\n",
      "Iteration 27484 => Loss: 6.70327248169354827922\n",
      "Iteration 27485 => Loss: 6.70327179519559201992\n",
      "Iteration 27486 => Loss: 6.70327110879013510214\n",
      "Iteration 27487 => Loss: 6.70327042247718019041\n",
      "Iteration 27488 => Loss: 6.70326973625670152757\n",
      "Iteration 27489 => Loss: 6.70326905012869023182\n",
      "Iteration 27490 => Loss: 6.70326836409313919773\n",
      "Iteration 27491 => Loss: 6.70326767815002177997\n",
      "Iteration 27492 => Loss: 6.70326699229934241941\n",
      "Iteration 27493 => Loss: 6.70326630654108424068\n",
      "Iteration 27494 => Loss: 6.70326562087522592748\n",
      "Iteration 27495 => Loss: 6.70326493530176303892\n",
      "Iteration 27496 => Loss: 6.70326424982068047598\n",
      "Iteration 27497 => Loss: 6.70326356443196580415\n",
      "Iteration 27498 => Loss: 6.70326287913561014165\n",
      "Iteration 27499 => Loss: 6.70326219393159394855\n",
      "Iteration 27500 => Loss: 6.70326150881991722486\n",
      "Iteration 27501 => Loss: 6.70326082380055598975\n",
      "Iteration 27502 => Loss: 6.70326013887349869691\n",
      "Iteration 27503 => Loss: 6.70325945403873113548\n",
      "Iteration 27504 => Loss: 6.70325876929625064093\n",
      "Iteration 27505 => Loss: 6.70325808464604389059\n",
      "Iteration 27506 => Loss: 6.70325740008808690362\n",
      "Iteration 27507 => Loss: 6.70325671562237790368\n",
      "Iteration 27508 => Loss: 6.70325603124889735085\n",
      "Iteration 27509 => Loss: 6.70325534696763813969\n",
      "Iteration 27510 => Loss: 6.70325466277858961206\n",
      "Iteration 27511 => Loss: 6.70325397868173045168\n",
      "Iteration 27512 => Loss: 6.70325329467705177677\n",
      "Iteration 27513 => Loss: 6.70325261076454470555\n",
      "Iteration 27514 => Loss: 6.70325192694419413897\n",
      "Iteration 27515 => Loss: 6.70325124321599297161\n",
      "Iteration 27516 => Loss: 6.70325055957991544631\n",
      "Iteration 27517 => Loss: 6.70324987603596778030\n",
      "Iteration 27518 => Loss: 6.70324919258412332823\n",
      "Iteration 27519 => Loss: 6.70324850922437054379\n",
      "Iteration 27520 => Loss: 6.70324782595670587426\n",
      "Iteration 27521 => Loss: 6.70324714278110889154\n",
      "Iteration 27522 => Loss: 6.70324645969757249020\n",
      "Iteration 27523 => Loss: 6.70324577670608157121\n",
      "Iteration 27524 => Loss: 6.70324509380662281188\n",
      "Iteration 27525 => Loss: 6.70324441099918555409\n",
      "Iteration 27526 => Loss: 6.70324372828374936972\n",
      "Iteration 27527 => Loss: 6.70324304566031692332\n",
      "Iteration 27528 => Loss: 6.70324236312886601041\n",
      "Iteration 27529 => Loss: 6.70324168068938863740\n",
      "Iteration 27530 => Loss: 6.70324099834186704072\n",
      "Iteration 27531 => Loss: 6.70324031608629855583\n",
      "Iteration 27532 => Loss: 6.70323963392265120831\n",
      "Iteration 27533 => Loss: 6.70323895185093299176\n",
      "Iteration 27534 => Loss: 6.70323826987112170173\n",
      "Iteration 27535 => Loss: 6.70323758798321112096\n",
      "Iteration 27536 => Loss: 6.70323690618718170953\n",
      "Iteration 27537 => Loss: 6.70323622448302725019\n",
      "Iteration 27538 => Loss: 6.70323554287073086755\n",
      "Iteration 27539 => Loss: 6.70323486135027746258\n",
      "Iteration 27540 => Loss: 6.70323417992166792345\n",
      "Iteration 27541 => Loss: 6.70323349858487382846\n",
      "Iteration 27542 => Loss: 6.70323281733989873032\n",
      "Iteration 27543 => Loss: 6.70323213618670887826\n",
      "Iteration 27544 => Loss: 6.70323145512531315404\n",
      "Iteration 27545 => Loss: 6.70323077415569024140\n",
      "Iteration 27546 => Loss: 6.70323009327782681765\n",
      "Iteration 27547 => Loss: 6.70322941249170689559\n",
      "Iteration 27548 => Loss: 6.70322873179732159343\n",
      "Iteration 27549 => Loss: 6.70322805119467357571\n",
      "Iteration 27550 => Loss: 6.70322737068372553892\n",
      "Iteration 27551 => Loss: 6.70322669026447837126\n",
      "Iteration 27552 => Loss: 6.70322600993692141458\n",
      "Iteration 27553 => Loss: 6.70322532970102979988\n",
      "Iteration 27554 => Loss: 6.70322464955680530352\n",
      "Iteration 27555 => Loss: 6.70322396950423904372\n",
      "Iteration 27556 => Loss: 6.70322328954329993422\n",
      "Iteration 27557 => Loss: 6.70322260967399152776\n",
      "Iteration 27558 => Loss: 6.70322192989628984350\n",
      "Iteration 27559 => Loss: 6.70322125021018688784\n",
      "Iteration 27560 => Loss: 6.70322057061567289082\n",
      "Iteration 27561 => Loss: 6.70321989111273719431\n",
      "Iteration 27562 => Loss: 6.70321921170136025836\n",
      "Iteration 27563 => Loss: 6.70321853238154208299\n",
      "Iteration 27564 => Loss: 6.70321785315325691101\n",
      "Iteration 27565 => Loss: 6.70321717401650118973\n",
      "Iteration 27566 => Loss: 6.70321649497125271466\n",
      "Iteration 27567 => Loss: 6.70321581601750882129\n",
      "Iteration 27568 => Loss: 6.70321513715525885146\n",
      "Iteration 27569 => Loss: 6.70321445838447882437\n",
      "Iteration 27570 => Loss: 6.70321377970516518729\n",
      "Iteration 27571 => Loss: 6.70321310111730461756\n",
      "Iteration 27572 => Loss: 6.70321242262088290431\n",
      "Iteration 27573 => Loss: 6.70321174421588938941\n",
      "Iteration 27574 => Loss: 6.70321106590231252653\n",
      "Iteration 27575 => Loss: 6.70321038768013544029\n",
      "Iteration 27576 => Loss: 6.70320970954935013708\n",
      "Iteration 27577 => Loss: 6.70320903150994418240\n",
      "Iteration 27578 => Loss: 6.70320835356190425358\n",
      "Iteration 27579 => Loss: 6.70320767570521702794\n",
      "Iteration 27580 => Loss: 6.70320699793986918280\n",
      "Iteration 27581 => Loss: 6.70320632026585006003\n",
      "Iteration 27582 => Loss: 6.70320564268314900147\n",
      "Iteration 27583 => Loss: 6.70320496519175534900\n",
      "Iteration 27584 => Loss: 6.70320428779164689814\n",
      "Iteration 27585 => Loss: 6.70320361048282187255\n",
      "Iteration 27586 => Loss: 6.70320293326526961408\n",
      "Iteration 27587 => Loss: 6.70320225613896436556\n",
      "Iteration 27588 => Loss: 6.70320157910390879152\n",
      "Iteration 27589 => Loss: 6.70320090216007624662\n",
      "Iteration 27590 => Loss: 6.70320022530746761902\n",
      "Iteration 27591 => Loss: 6.70319954854605803973\n",
      "Iteration 27592 => Loss: 6.70319887187585194965\n",
      "Iteration 27593 => Loss: 6.70319819529682181525\n",
      "Iteration 27594 => Loss: 6.70319751880895875473\n",
      "Iteration 27595 => Loss: 6.70319684241225655086\n",
      "Iteration 27596 => Loss: 6.70319616610669477552\n",
      "Iteration 27597 => Loss: 6.70319548989226898783\n",
      "Iteration 27598 => Loss: 6.70319481376896053604\n",
      "Iteration 27599 => Loss: 6.70319413773676497925\n",
      "Iteration 27600 => Loss: 6.70319346179566011301\n",
      "Iteration 27601 => Loss: 6.70319278594563794371\n",
      "Iteration 27602 => Loss: 6.70319211018668958957\n",
      "Iteration 27603 => Loss: 6.70319143451879551066\n",
      "Iteration 27604 => Loss: 6.70319075894195393062\n",
      "Iteration 27605 => Loss: 6.70319008345614175681\n",
      "Iteration 27606 => Loss: 6.70318940806135010746\n",
      "Iteration 27607 => Loss: 6.70318873275756743624\n",
      "Iteration 27608 => Loss: 6.70318805754478663772\n",
      "Iteration 27609 => Loss: 6.70318738242299261287\n",
      "Iteration 27610 => Loss: 6.70318670739216138088\n",
      "Iteration 27611 => Loss: 6.70318603245229738263\n",
      "Iteration 27612 => Loss: 6.70318535760338551910\n",
      "Iteration 27613 => Loss: 6.70318468284540358582\n",
      "Iteration 27614 => Loss: 6.70318400817834891825\n",
      "Iteration 27615 => Loss: 6.70318333360220197648\n",
      "Iteration 27616 => Loss: 6.70318265911695299053\n",
      "Iteration 27617 => Loss: 6.70318198472259219045\n",
      "Iteration 27618 => Loss: 6.70318131041911158263\n",
      "Iteration 27619 => Loss: 6.70318063620648896261\n",
      "Iteration 27620 => Loss: 6.70317996208471900133\n",
      "Iteration 27621 => Loss: 6.70317928805378393520\n",
      "Iteration 27622 => Loss: 6.70317861411367399427\n",
      "Iteration 27623 => Loss: 6.70317794026438207311\n",
      "Iteration 27624 => Loss: 6.70317726650588863180\n",
      "Iteration 27625 => Loss: 6.70317659283818567673\n",
      "Iteration 27626 => Loss: 6.70317591926125544433\n",
      "Iteration 27627 => Loss: 6.70317524577509171735\n",
      "Iteration 27628 => Loss: 6.70317457237968294947\n",
      "Iteration 27629 => Loss: 6.70317389907500871260\n",
      "Iteration 27630 => Loss: 6.70317322586106723037\n",
      "Iteration 27631 => Loss: 6.70317255273783807468\n",
      "Iteration 27632 => Loss: 6.70317187970531858099\n",
      "Iteration 27633 => Loss: 6.70317120676348476849\n",
      "Iteration 27634 => Loss: 6.70317053391232953174\n",
      "Iteration 27635 => Loss: 6.70316986115184132444\n",
      "Iteration 27636 => Loss: 6.70316918848201304115\n",
      "Iteration 27637 => Loss: 6.70316851590282070106\n",
      "Iteration 27638 => Loss: 6.70316784341425897509\n",
      "Iteration 27639 => Loss: 6.70316717101631720510\n",
      "Iteration 27640 => Loss: 6.70316649870897851571\n",
      "Iteration 27641 => Loss: 6.70316582649223935420\n",
      "Iteration 27642 => Loss: 6.70316515436607662792\n",
      "Iteration 27643 => Loss: 6.70316448233048323146\n",
      "Iteration 27644 => Loss: 6.70316381038544850668\n",
      "Iteration 27645 => Loss: 6.70316313853095824271\n",
      "Iteration 27646 => Loss: 6.70316246676700266960\n",
      "Iteration 27647 => Loss: 6.70316179509356313559\n",
      "Iteration 27648 => Loss: 6.70316112351062987074\n",
      "Iteration 27649 => Loss: 6.70316045201819310506\n",
      "Iteration 27650 => Loss: 6.70315978061624573314\n",
      "Iteration 27651 => Loss: 6.70315910930476466234\n",
      "Iteration 27652 => Loss: 6.70315843808374278723\n",
      "Iteration 27653 => Loss: 6.70315776695317300238\n",
      "Iteration 27654 => Loss: 6.70315709591303043879\n",
      "Iteration 27655 => Loss: 6.70315642496331953737\n",
      "Iteration 27656 => Loss: 6.70315575410401276457\n",
      "Iteration 27657 => Loss: 6.70315508333510834404\n",
      "Iteration 27658 => Loss: 6.70315441265658407133\n",
      "Iteration 27659 => Loss: 6.70315374206843461735\n",
      "Iteration 27660 => Loss: 6.70315307157065376487\n",
      "Iteration 27661 => Loss: 6.70315240116321753305\n",
      "Iteration 27662 => Loss: 6.70315173084611881649\n",
      "Iteration 27663 => Loss: 6.70315106061934784520\n",
      "Iteration 27664 => Loss: 6.70315039048289218471\n",
      "Iteration 27665 => Loss: 6.70314972043673318325\n",
      "Iteration 27666 => Loss: 6.70314905048086107087\n",
      "Iteration 27667 => Loss: 6.70314838061526874213\n",
      "Iteration 27668 => Loss: 6.70314771083994465073\n",
      "Iteration 27669 => Loss: 6.70314704115486392766\n",
      "Iteration 27670 => Loss: 6.70314637156002923746\n",
      "Iteration 27671 => Loss: 6.70314570205541837566\n",
      "Iteration 27672 => Loss: 6.70314503264103045410\n",
      "Iteration 27673 => Loss: 6.70314436331683971559\n",
      "Iteration 27674 => Loss: 6.70314369408284616014\n",
      "Iteration 27675 => Loss: 6.70314302493902491875\n",
      "Iteration 27676 => Loss: 6.70314235588537332688\n",
      "Iteration 27677 => Loss: 6.70314168692187983822\n",
      "Iteration 27678 => Loss: 6.70314101804853024191\n",
      "Iteration 27679 => Loss: 6.70314034926530943892\n",
      "Iteration 27680 => Loss: 6.70313968057219788932\n",
      "Iteration 27681 => Loss: 6.70313901196921158032\n",
      "Iteration 27682 => Loss: 6.70313834345630166212\n",
      "Iteration 27683 => Loss: 6.70313767503347790466\n",
      "Iteration 27684 => Loss: 6.70313700670072876164\n",
      "Iteration 27685 => Loss: 6.70313633845804002220\n",
      "Iteration 27686 => Loss: 6.70313567030539747549\n",
      "Iteration 27687 => Loss: 6.70313500224277802886\n",
      "Iteration 27688 => Loss: 6.70313433427018434685\n",
      "Iteration 27689 => Loss: 6.70313366638760488314\n",
      "Iteration 27690 => Loss: 6.70313299859501388056\n",
      "Iteration 27691 => Loss: 6.70313233089241578000\n",
      "Iteration 27692 => Loss: 6.70313166327979192971\n",
      "Iteration 27693 => Loss: 6.70313099575712634248\n",
      "Iteration 27694 => Loss: 6.70313032832440480746\n",
      "Iteration 27695 => Loss: 6.70312966098162910100\n",
      "Iteration 27696 => Loss: 6.70312899372876902504\n",
      "Iteration 27697 => Loss: 6.70312832656582813229\n",
      "Iteration 27698 => Loss: 6.70312765949278333011\n",
      "Iteration 27699 => Loss: 6.70312699250962840125\n",
      "Iteration 27700 => Loss: 6.70312632561634913486\n",
      "Iteration 27701 => Loss: 6.70312565881293043191\n",
      "Iteration 27702 => Loss: 6.70312499209936962785\n",
      "Iteration 27703 => Loss: 6.70312432547564540641\n",
      "Iteration 27704 => Loss: 6.70312365894175066217\n",
      "Iteration 27705 => Loss: 6.70312299249767118425\n",
      "Iteration 27706 => Loss: 6.70312232614339009729\n",
      "Iteration 27707 => Loss: 6.70312165987890740126\n",
      "Iteration 27708 => Loss: 6.70312099370419733901\n",
      "Iteration 27709 => Loss: 6.70312032761925991053\n",
      "Iteration 27710 => Loss: 6.70311966162406758230\n",
      "Iteration 27711 => Loss: 6.70311899571862834790\n",
      "Iteration 27712 => Loss: 6.70311832990292000289\n",
      "Iteration 27713 => Loss: 6.70311766417692656006\n",
      "Iteration 27714 => Loss: 6.70311699854064091397\n",
      "Iteration 27715 => Loss: 6.70311633299404796560\n",
      "Iteration 27716 => Loss: 6.70311566753714416222\n",
      "Iteration 27717 => Loss: 6.70311500216989752943\n",
      "Iteration 27718 => Loss: 6.70311433689231872535\n",
      "Iteration 27719 => Loss: 6.70311367170439265095\n",
      "Iteration 27720 => Loss: 6.70311300660608644364\n",
      "Iteration 27721 => Loss: 6.70311234159740720884\n",
      "Iteration 27722 => Loss: 6.70311167667834428840\n",
      "Iteration 27723 => Loss: 6.70311101184887103699\n",
      "Iteration 27724 => Loss: 6.70311034710898479005\n",
      "Iteration 27725 => Loss: 6.70310968245867666582\n",
      "Iteration 27726 => Loss: 6.70310901789792978889\n",
      "Iteration 27727 => Loss: 6.70310835342672906023\n",
      "Iteration 27728 => Loss: 6.70310768904506293353\n",
      "Iteration 27729 => Loss: 6.70310702475292519154\n",
      "Iteration 27730 => Loss: 6.70310636055030872882\n",
      "Iteration 27731 => Loss: 6.70310569643718778821\n",
      "Iteration 27732 => Loss: 6.70310503241355082338\n",
      "Iteration 27733 => Loss: 6.70310436847940049887\n",
      "Iteration 27734 => Loss: 6.70310370463471016933\n",
      "Iteration 27735 => Loss: 6.70310304087947095297\n",
      "Iteration 27736 => Loss: 6.70310237721367929709\n",
      "Iteration 27737 => Loss: 6.70310171363731388539\n",
      "Iteration 27738 => Loss: 6.70310105015036317155\n",
      "Iteration 27739 => Loss: 6.70310038675281649745\n",
      "Iteration 27740 => Loss: 6.70309972344466320493\n",
      "Iteration 27741 => Loss: 6.70309906022589796493\n",
      "Iteration 27742 => Loss: 6.70309839709649235573\n",
      "Iteration 27743 => Loss: 6.70309773405644993005\n",
      "Iteration 27744 => Loss: 6.70309707110574404254\n",
      "Iteration 27745 => Loss: 6.70309640824437824591\n",
      "Iteration 27746 => Loss: 6.70309574547232767117\n",
      "Iteration 27747 => Loss: 6.70309508278959320648\n",
      "Iteration 27748 => Loss: 6.70309442019614998287\n",
      "Iteration 27749 => Loss: 6.70309375769199355943\n",
      "Iteration 27750 => Loss: 6.70309309527710794896\n",
      "Iteration 27751 => Loss: 6.70309243295148249331\n",
      "Iteration 27752 => Loss: 6.70309177071510386980\n",
      "Iteration 27753 => Loss: 6.70309110856796852573\n",
      "Iteration 27754 => Loss: 6.70309044651005159210\n",
      "Iteration 27755 => Loss: 6.70308978454134596348\n",
      "Iteration 27756 => Loss: 6.70308912266184186990\n",
      "Iteration 27757 => Loss: 6.70308846087152776505\n",
      "Iteration 27758 => Loss: 6.70308779917038854990\n",
      "Iteration 27759 => Loss: 6.70308713755841800719\n",
      "Iteration 27760 => Loss: 6.70308647603559748518\n",
      "Iteration 27761 => Loss: 6.70308581460191721391\n",
      "Iteration 27762 => Loss: 6.70308515325735676527\n",
      "Iteration 27763 => Loss: 6.70308449200192058015\n",
      "Iteration 27764 => Loss: 6.70308383083559267135\n",
      "Iteration 27765 => Loss: 6.70308316975834728169\n",
      "Iteration 27766 => Loss: 6.70308250877018885205\n",
      "Iteration 27767 => Loss: 6.70308184787109340164\n",
      "Iteration 27768 => Loss: 6.70308118706106270679\n",
      "Iteration 27769 => Loss: 6.70308052634006923398\n",
      "Iteration 27770 => Loss: 6.70307986570811031868\n",
      "Iteration 27771 => Loss: 6.70307920516516908549\n",
      "Iteration 27772 => Loss: 6.70307854471124286988\n",
      "Iteration 27773 => Loss: 6.70307788434630325014\n",
      "Iteration 27774 => Loss: 6.70307722407035555534\n",
      "Iteration 27775 => Loss: 6.70307656388337758102\n",
      "Iteration 27776 => Loss: 6.70307590378535955722\n",
      "Iteration 27777 => Loss: 6.70307524377628638490\n",
      "Iteration 27778 => Loss: 6.70307458385615628771\n",
      "Iteration 27779 => Loss: 6.70307392402494084394\n",
      "Iteration 27780 => Loss: 6.70307326428265159990\n",
      "Iteration 27781 => Loss: 6.70307260462924592304\n",
      "Iteration 27782 => Loss: 6.70307194506474424145\n",
      "Iteration 27783 => Loss: 6.70307128558911191618\n",
      "Iteration 27784 => Loss: 6.70307062620234717087\n",
      "Iteration 27785 => Loss: 6.70306996690442780107\n",
      "Iteration 27786 => Loss: 6.70306930769535647130\n",
      "Iteration 27787 => Loss: 6.70306864857511008893\n",
      "Iteration 27788 => Loss: 6.70306798954367710763\n",
      "Iteration 27789 => Loss: 6.70306733060105397470\n",
      "Iteration 27790 => Loss: 6.70306667174722026203\n",
      "Iteration 27791 => Loss: 6.70306601298216797602\n",
      "Iteration 27792 => Loss: 6.70306535430588024127\n",
      "Iteration 27793 => Loss: 6.70306469571835439325\n",
      "Iteration 27794 => Loss: 6.70306403721957355657\n",
      "Iteration 27795 => Loss: 6.70306337880952529673\n",
      "Iteration 27796 => Loss: 6.70306272048819451470\n",
      "Iteration 27797 => Loss: 6.70306206225558121048\n",
      "Iteration 27798 => Loss: 6.70306140411165429782\n",
      "Iteration 27799 => Loss: 6.70306074605641466491\n",
      "Iteration 27800 => Loss: 6.70306008808985254177\n",
      "Iteration 27801 => Loss: 6.70305943021194305942\n",
      "Iteration 27802 => Loss: 6.70305877242269421146\n",
      "Iteration 27803 => Loss: 6.70305811472207402346\n",
      "Iteration 27804 => Loss: 6.70305745711008515997\n",
      "Iteration 27805 => Loss: 6.70305679958670452834\n",
      "Iteration 27806 => Loss: 6.70305614215192502314\n",
      "Iteration 27807 => Loss: 6.70305548480574131531\n",
      "Iteration 27808 => Loss: 6.70305482754812675950\n",
      "Iteration 27809 => Loss: 6.70305417037908135569\n",
      "Iteration 27810 => Loss: 6.70305351329859089304\n",
      "Iteration 27811 => Loss: 6.70305285630663760799\n",
      "Iteration 27812 => Loss: 6.70305219940321972416\n",
      "Iteration 27813 => Loss: 6.70305154258831858982\n",
      "Iteration 27814 => Loss: 6.70305088586192265865\n",
      "Iteration 27815 => Loss: 6.70305022922401860797\n",
      "Iteration 27816 => Loss: 6.70304957267460110870\n",
      "Iteration 27817 => Loss: 6.70304891621364351550\n",
      "Iteration 27818 => Loss: 6.70304825984115204562\n",
      "Iteration 27819 => Loss: 6.70304760355710715913\n",
      "Iteration 27820 => Loss: 6.70304694736149553336\n",
      "Iteration 27821 => Loss: 6.70304629125430651015\n",
      "Iteration 27822 => Loss: 6.70304563523553031956\n",
      "Iteration 27823 => Loss: 6.70304497930514742166\n",
      "Iteration 27824 => Loss: 6.70304432346315248736\n",
      "Iteration 27825 => Loss: 6.70304366770953397037\n",
      "Iteration 27826 => Loss: 6.70304301204427677163\n",
      "Iteration 27827 => Loss: 6.70304235646737200938\n",
      "Iteration 27828 => Loss: 6.70304170097880369639\n",
      "Iteration 27829 => Loss: 6.70304104557856739177\n",
      "Iteration 27830 => Loss: 6.70304039026664533196\n",
      "Iteration 27831 => Loss: 6.70303973504301886521\n",
      "Iteration 27832 => Loss: 6.70303907990769509695\n",
      "Iteration 27833 => Loss: 6.70303842486064471728\n",
      "Iteration 27834 => Loss: 6.70303776990186417351\n",
      "Iteration 27835 => Loss: 6.70303711503133747840\n",
      "Iteration 27836 => Loss: 6.70303646024905397383\n",
      "Iteration 27837 => Loss: 6.70303580555500655436\n",
      "Iteration 27838 => Loss: 6.70303515094917301553\n",
      "Iteration 27839 => Loss: 6.70303449643155246918\n",
      "Iteration 27840 => Loss: 6.70303384200212537536\n",
      "Iteration 27841 => Loss: 6.70303318766088196412\n",
      "Iteration 27842 => Loss: 6.70303253340781690639\n",
      "Iteration 27843 => Loss: 6.70303187924290888589\n",
      "Iteration 27844 => Loss: 6.70303122516614902082\n",
      "Iteration 27845 => Loss: 6.70303057117752665306\n",
      "Iteration 27846 => Loss: 6.70302991727702757174\n",
      "Iteration 27847 => Loss: 6.70302926346464378327\n",
      "Iteration 27848 => Loss: 6.70302860974035930042\n",
      "Iteration 27849 => Loss: 6.70302795610416790595\n",
      "Iteration 27850 => Loss: 6.70302730255605450083\n",
      "Iteration 27851 => Loss: 6.70302664909600132148\n",
      "Iteration 27852 => Loss: 6.70302599572400481520\n",
      "Iteration 27853 => Loss: 6.70302534244004899477\n",
      "Iteration 27854 => Loss: 6.70302468924412586659\n",
      "Iteration 27855 => Loss: 6.70302403613622210798\n",
      "Iteration 27856 => Loss: 6.70302338311631551448\n",
      "Iteration 27857 => Loss: 6.70302273018440697427\n",
      "Iteration 27858 => Loss: 6.70302207734048760557\n",
      "Iteration 27859 => Loss: 6.70302142458453520391\n",
      "Iteration 27860 => Loss: 6.70302077191654355204\n",
      "Iteration 27861 => Loss: 6.70302011933649755093\n",
      "Iteration 27862 => Loss: 6.70301946684438476609\n",
      "Iteration 27863 => Loss: 6.70301881444019898026\n",
      "Iteration 27864 => Loss: 6.70301816212391976535\n",
      "Iteration 27865 => Loss: 6.70301750989554623317\n",
      "Iteration 27866 => Loss: 6.70301685775505795561\n",
      "Iteration 27867 => Loss: 6.70301620570244427455\n",
      "Iteration 27868 => Loss: 6.70301555373769009094\n",
      "Iteration 27869 => Loss: 6.70301490186079806932\n",
      "Iteration 27870 => Loss: 6.70301425007174511705\n",
      "Iteration 27871 => Loss: 6.70301359837052146418\n",
      "Iteration 27872 => Loss: 6.70301294675710668258\n",
      "Iteration 27873 => Loss: 6.70301229523150166045\n",
      "Iteration 27874 => Loss: 6.70301164379368508150\n",
      "Iteration 27875 => Loss: 6.70301099244365605756\n",
      "Iteration 27876 => Loss: 6.70301034118139771323\n",
      "Iteration 27877 => Loss: 6.70300969000689406130\n",
      "Iteration 27878 => Loss: 6.70300903892013177909\n",
      "Iteration 27879 => Loss: 6.70300838792111086661\n",
      "Iteration 27880 => Loss: 6.70300773700980734304\n",
      "Iteration 27881 => Loss: 6.70300708618621410295\n",
      "Iteration 27882 => Loss: 6.70300643545032492909\n",
      "Iteration 27883 => Loss: 6.70300578480211584065\n",
      "Iteration 27884 => Loss: 6.70300513424158772580\n",
      "Iteration 27885 => Loss: 6.70300448376871305101\n",
      "Iteration 27886 => Loss: 6.70300383338349714535\n",
      "Iteration 27887 => Loss: 6.70300318308592224525\n",
      "Iteration 27888 => Loss: 6.70300253287596881080\n",
      "Iteration 27889 => Loss: 6.70300188275363506563\n",
      "Iteration 27890 => Loss: 6.70300123271890946342\n",
      "Iteration 27891 => Loss: 6.70300058277176802335\n",
      "Iteration 27892 => Loss: 6.70299993291221163361\n",
      "Iteration 27893 => Loss: 6.70299928314022164244\n",
      "Iteration 27894 => Loss: 6.70299863345579005625\n",
      "Iteration 27895 => Loss: 6.70299798385890266417\n",
      "Iteration 27896 => Loss: 6.70299733434954791989\n",
      "Iteration 27897 => Loss: 6.70299668492771694162\n",
      "Iteration 27898 => Loss: 6.70299603559339551850\n",
      "Iteration 27899 => Loss: 6.70299538634657388059\n",
      "Iteration 27900 => Loss: 6.70299473718722893523\n",
      "Iteration 27901 => Loss: 6.70299408811536601149\n",
      "Iteration 27902 => Loss: 6.70299343913096556946\n",
      "Iteration 27903 => Loss: 6.70299279023401428645\n",
      "Iteration 27904 => Loss: 6.70299214142450061615\n",
      "Iteration 27905 => Loss: 6.70299149270241390042\n",
      "Iteration 27906 => Loss: 6.70299084406774703382\n",
      "Iteration 27907 => Loss: 6.70299019552047603554\n",
      "Iteration 27908 => Loss: 6.70298954706060534647\n",
      "Iteration 27909 => Loss: 6.70298889868810832127\n",
      "Iteration 27910 => Loss: 6.70298825040298407174\n",
      "Iteration 27911 => Loss: 6.70298760220521128161\n",
      "Iteration 27912 => Loss: 6.70298695409478728635\n",
      "Iteration 27913 => Loss: 6.70298630607169787510\n",
      "Iteration 27914 => Loss: 6.70298565813592528428\n",
      "Iteration 27915 => Loss: 6.70298501028746240848\n",
      "Iteration 27916 => Loss: 6.70298436252629947774\n",
      "Iteration 27917 => Loss: 6.70298371485242050483\n",
      "Iteration 27918 => Loss: 6.70298306726581571979\n",
      "Iteration 27919 => Loss: 6.70298241976648068174\n",
      "Iteration 27920 => Loss: 6.70298177235438341626\n",
      "Iteration 27921 => Loss: 6.70298112502953369329\n",
      "Iteration 27922 => Loss: 6.70298047779190664386\n",
      "Iteration 27923 => Loss: 6.70297983064149605070\n",
      "Iteration 27924 => Loss: 6.70297918357829214386\n",
      "Iteration 27925 => Loss: 6.70297853660227893613\n",
      "Iteration 27926 => Loss: 6.70297788971344132847\n",
      "Iteration 27927 => Loss: 6.70297724291177932088\n",
      "Iteration 27928 => Loss: 6.70297659619726626801\n",
      "Iteration 27929 => Loss: 6.70297594956990483439\n",
      "Iteration 27930 => Loss: 6.70297530302967103921\n",
      "Iteration 27931 => Loss: 6.70297465657656310611\n",
      "Iteration 27932 => Loss: 6.70297401021056149517\n",
      "Iteration 27933 => Loss: 6.70297336393165732460\n",
      "Iteration 27934 => Loss: 6.70297271773983549537\n",
      "Iteration 27935 => Loss: 6.70297207163509778383\n",
      "Iteration 27936 => Loss: 6.70297142561741932099\n",
      "Iteration 27937 => Loss: 6.70297077968678589599\n",
      "Iteration 27938 => Loss: 6.70297013384320017337\n",
      "Iteration 27939 => Loss: 6.70296948808663817232\n",
      "Iteration 27940 => Loss: 6.70296884241709101104\n",
      "Iteration 27941 => Loss: 6.70296819683455513683\n",
      "Iteration 27942 => Loss: 6.70296755133900123980\n",
      "Iteration 27943 => Loss: 6.70296690593042843176\n",
      "Iteration 27944 => Loss: 6.70296626060883404818\n",
      "Iteration 27945 => Loss: 6.70296561537418877919\n",
      "Iteration 27946 => Loss: 6.70296497022649173658\n",
      "Iteration 27947 => Loss: 6.70296432516572782134\n",
      "Iteration 27948 => Loss: 6.70296368019188548715\n",
      "Iteration 27949 => Loss: 6.70296303530494519407\n",
      "Iteration 27950 => Loss: 6.70296239050491404754\n",
      "Iteration 27951 => Loss: 6.70296174579177073127\n",
      "Iteration 27952 => Loss: 6.70296110116550103442\n",
      "Iteration 27953 => Loss: 6.70296045662608808158\n",
      "Iteration 27954 => Loss: 6.70295981217353542547\n",
      "Iteration 27955 => Loss: 6.70295916780782086164\n",
      "Iteration 27956 => Loss: 6.70295852352893017922\n",
      "Iteration 27957 => Loss: 6.70295787933686337823\n",
      "Iteration 27958 => Loss: 6.70295723523160003054\n",
      "Iteration 27959 => Loss: 6.70295659121312414896\n",
      "Iteration 27960 => Loss: 6.70295594728142773988\n",
      "Iteration 27961 => Loss: 6.70295530343650991512\n",
      "Iteration 27962 => Loss: 6.70295465967834402932\n",
      "Iteration 27963 => Loss: 6.70295401600692652977\n",
      "Iteration 27964 => Loss: 6.70295337242223965291\n",
      "Iteration 27965 => Loss: 6.70295272892428251055\n",
      "Iteration 27966 => Loss: 6.70295208551304178002\n",
      "Iteration 27967 => Loss: 6.70295144218849259232\n",
      "Iteration 27968 => Loss: 6.70295079895062606568\n",
      "Iteration 27969 => Loss: 6.70295015579944752915\n",
      "Iteration 27970 => Loss: 6.70294951273492056742\n",
      "Iteration 27971 => Loss: 6.70294886975705583865\n",
      "Iteration 27972 => Loss: 6.70294822686583646743\n",
      "Iteration 27973 => Loss: 6.70294758406123580841\n",
      "Iteration 27974 => Loss: 6.70294694134326451973\n",
      "Iteration 27975 => Loss: 6.70294629871189506787\n",
      "Iteration 27976 => Loss: 6.70294565616711590650\n",
      "Iteration 27977 => Loss: 6.70294501370892259473\n",
      "Iteration 27978 => Loss: 6.70294437133729914535\n",
      "Iteration 27979 => Loss: 6.70294372905223667658\n",
      "Iteration 27980 => Loss: 6.70294308685372719481\n",
      "Iteration 27981 => Loss: 6.70294244474174316650\n",
      "Iteration 27982 => Loss: 6.70294180271629169710\n",
      "Iteration 27983 => Loss: 6.70294116077735591119\n",
      "Iteration 27984 => Loss: 6.70294051892491538069\n",
      "Iteration 27985 => Loss: 6.70293987715896566471\n",
      "Iteration 27986 => Loss: 6.70293923547949077602\n",
      "Iteration 27987 => Loss: 6.70293859388648716191\n",
      "Iteration 27988 => Loss: 6.70293795237994149971\n",
      "Iteration 27989 => Loss: 6.70293731095982980861\n",
      "Iteration 27990 => Loss: 6.70293666962615297678\n",
      "Iteration 27991 => Loss: 6.70293602837889856971\n",
      "Iteration 27992 => Loss: 6.70293538721805060021\n",
      "Iteration 27993 => Loss: 6.70293474614359663377\n",
      "Iteration 27994 => Loss: 6.70293410515553222950\n",
      "Iteration 27995 => Loss: 6.70293346425383962384\n",
      "Iteration 27996 => Loss: 6.70293282343850815863\n",
      "Iteration 27997 => Loss: 6.70293218270952273485\n",
      "Iteration 27998 => Loss: 6.70293154206688690522\n",
      "Iteration 27999 => Loss: 6.70293090151056869530\n",
      "Iteration 28000 => Loss: 6.70293026104055922332\n",
      "Iteration 28001 => Loss: 6.70292962065686204198\n",
      "Iteration 28002 => Loss: 6.70292898035945761137\n",
      "Iteration 28003 => Loss: 6.70292834014833083245\n",
      "Iteration 28004 => Loss: 6.70292770002347193525\n",
      "Iteration 28005 => Loss: 6.70292705998487114982\n",
      "Iteration 28006 => Loss: 6.70292642003250982441\n",
      "Iteration 28007 => Loss: 6.70292578016638795901\n",
      "Iteration 28008 => Loss: 6.70292514038649667185\n",
      "Iteration 28009 => Loss: 6.70292450069280043579\n",
      "Iteration 28010 => Loss: 6.70292386108530724442\n",
      "Iteration 28011 => Loss: 6.70292322156400377509\n",
      "Iteration 28012 => Loss: 6.70292258212887581692\n",
      "Iteration 28013 => Loss: 6.70292194277991182361\n",
      "Iteration 28014 => Loss: 6.70292130351709669611\n",
      "Iteration 28015 => Loss: 6.70292066434042954626\n",
      "Iteration 28016 => Loss: 6.70292002524988372869\n",
      "Iteration 28017 => Loss: 6.70291938624546546066\n",
      "Iteration 28018 => Loss: 6.70291874732714010321\n",
      "Iteration 28019 => Loss: 6.70291810849491209723\n",
      "Iteration 28020 => Loss: 6.70291746974877344911\n",
      "Iteration 28021 => Loss: 6.70291683108870106622\n",
      "Iteration 28022 => Loss: 6.70291619251469139584\n",
      "Iteration 28023 => Loss: 6.70291555402673022712\n",
      "Iteration 28024 => Loss: 6.70291491562479624378\n",
      "Iteration 28025 => Loss: 6.70291427730889832759\n",
      "Iteration 28026 => Loss: 6.70291363907900272778\n",
      "Iteration 28027 => Loss: 6.70291300093511832614\n",
      "Iteration 28028 => Loss: 6.70291236287722291820\n",
      "Iteration 28029 => Loss: 6.70291172490530140493\n",
      "Iteration 28030 => Loss: 6.70291108701934579273\n",
      "Iteration 28031 => Loss: 6.70291044921935696976\n",
      "Iteration 28032 => Loss: 6.70290981150529319166\n",
      "Iteration 28033 => Loss: 6.70290917387717488651\n",
      "Iteration 28034 => Loss: 6.70290853633497096808\n",
      "Iteration 28035 => Loss: 6.70290789887867966002\n",
      "Iteration 28036 => Loss: 6.70290726150828408691\n",
      "Iteration 28037 => Loss: 6.70290662422377625518\n",
      "Iteration 28038 => Loss: 6.70290598702513396034\n",
      "Iteration 28039 => Loss: 6.70290534991236430784\n",
      "Iteration 28040 => Loss: 6.70290471288544331685\n",
      "Iteration 28041 => Loss: 6.70290407594435944105\n",
      "Iteration 28042 => Loss: 6.70290343908910379866\n",
      "Iteration 28043 => Loss: 6.70290280231966661972\n",
      "Iteration 28044 => Loss: 6.70290216563603102884\n",
      "Iteration 28045 => Loss: 6.70290152903818725605\n",
      "Iteration 28046 => Loss: 6.70290089252613618953\n",
      "Iteration 28047 => Loss: 6.70290025609984230215\n",
      "Iteration 28048 => Loss: 6.70289961975931358751\n",
      "Iteration 28049 => Loss: 6.70289898350452695297\n",
      "Iteration 28050 => Loss: 6.70289834733547884582\n",
      "Iteration 28051 => Loss: 6.70289771125215416703\n",
      "Iteration 28052 => Loss: 6.70289707525454137027\n",
      "Iteration 28053 => Loss: 6.70289643934263512648\n",
      "Iteration 28054 => Loss: 6.70289580351640346123\n",
      "Iteration 28055 => Loss: 6.70289516777586236174\n",
      "Iteration 28056 => Loss: 6.70289453212098340629\n",
      "Iteration 28057 => Loss: 6.70289389655176037763\n",
      "Iteration 28058 => Loss: 6.70289326106817906492\n",
      "Iteration 28059 => Loss: 6.70289262567023058637\n",
      "Iteration 28060 => Loss: 6.70289199035790250747\n",
      "Iteration 28061 => Loss: 6.70289135513117795284\n",
      "Iteration 28062 => Loss: 6.70289071999005781066\n",
      "Iteration 28063 => Loss: 6.70289008493451987647\n",
      "Iteration 28064 => Loss: 6.70288944996455349212\n",
      "Iteration 28065 => Loss: 6.70288881508015155219\n",
      "Iteration 28066 => Loss: 6.70288818028129718130\n",
      "Iteration 28067 => Loss: 6.70288754556798949125\n",
      "Iteration 28068 => Loss: 6.70288691094020006034\n",
      "Iteration 28069 => Loss: 6.70288627639792267132\n",
      "Iteration 28070 => Loss: 6.70288564194116265327\n",
      "Iteration 28071 => Loss: 6.70288500756989424900\n",
      "Iteration 28072 => Loss: 6.70288437328410235949\n",
      "Iteration 28073 => Loss: 6.70288373908378432020\n",
      "Iteration 28074 => Loss: 6.70288310496891970303\n",
      "Iteration 28075 => Loss: 6.70288247093950317890\n",
      "Iteration 28076 => Loss: 6.70288183699552764239\n",
      "Iteration 28077 => Loss: 6.70288120313697266539\n",
      "Iteration 28078 => Loss: 6.70288056936382936613\n",
      "Iteration 28079 => Loss: 6.70287993567609063916\n",
      "Iteration 28080 => Loss: 6.70287930207373694458\n",
      "Iteration 28081 => Loss: 6.70287866855676206512\n",
      "Iteration 28082 => Loss: 6.70287803512515356630\n",
      "Iteration 28083 => Loss: 6.70287740177890434268\n",
      "Iteration 28084 => Loss: 6.70287676851799218980\n",
      "Iteration 28085 => Loss: 6.70287613534241799584\n",
      "Iteration 28086 => Loss: 6.70287550225216044453\n",
      "Iteration 28087 => Loss: 6.70287486924721331860\n",
      "Iteration 28088 => Loss: 6.70287423632756418357\n",
      "Iteration 28089 => Loss: 6.70287360349319794040\n",
      "Iteration 28090 => Loss: 6.70287297074411014819\n",
      "Iteration 28091 => Loss: 6.70287233808028393156\n",
      "Iteration 28092 => Loss: 6.70287170550170952055\n",
      "Iteration 28093 => Loss: 6.70287107300836826340\n",
      "Iteration 28094 => Loss: 6.70287044060026460102\n",
      "Iteration 28095 => Loss: 6.70286980827737899347\n",
      "Iteration 28096 => Loss: 6.70286917603968657176\n",
      "Iteration 28097 => Loss: 6.70286854388720421127\n",
      "Iteration 28098 => Loss: 6.70286791181989194399\n",
      "Iteration 28099 => Loss: 6.70286727983775953987\n",
      "Iteration 28100 => Loss: 6.70286664794078035357\n",
      "Iteration 28101 => Loss: 6.70286601612895527325\n",
      "Iteration 28102 => Loss: 6.70286538440225410085\n",
      "Iteration 28103 => Loss: 6.70286475276069459994\n",
      "Iteration 28104 => Loss: 6.70286412120424124339\n",
      "Iteration 28105 => Loss: 6.70286348973289491937\n",
      "Iteration 28106 => Loss: 6.70286285834662987071\n",
      "Iteration 28107 => Loss: 6.70286222704545320283\n",
      "Iteration 28108 => Loss: 6.70286159582934093493\n",
      "Iteration 28109 => Loss: 6.70286096469828329703\n",
      "Iteration 28110 => Loss: 6.70286033365226963099\n",
      "Iteration 28111 => Loss: 6.70285970269129460775\n",
      "Iteration 28112 => Loss: 6.70285907181533691102\n",
      "Iteration 28113 => Loss: 6.70285844102439032355\n",
      "Iteration 28114 => Loss: 6.70285781031844507538\n",
      "Iteration 28115 => Loss: 6.70285717969748517930\n",
      "Iteration 28116 => Loss: 6.70285654916150797078\n",
      "Iteration 28117 => Loss: 6.70285591871048502810\n",
      "Iteration 28118 => Loss: 6.70285528834442523305\n",
      "Iteration 28119 => Loss: 6.70285465806330105210\n",
      "Iteration 28120 => Loss: 6.70285402786711514977\n",
      "Iteration 28121 => Loss: 6.70285339775583732802\n",
      "Iteration 28122 => Loss: 6.70285276772947025137\n",
      "Iteration 28123 => Loss: 6.70285213778800326168\n",
      "Iteration 28124 => Loss: 6.70285150793141770720\n",
      "Iteration 28125 => Loss: 6.70285087815970914704\n",
      "Iteration 28126 => Loss: 6.70285024847285804128\n",
      "Iteration 28127 => Loss: 6.70284961887085994903\n",
      "Iteration 28128 => Loss: 6.70284898935369533035\n",
      "Iteration 28129 => Loss: 6.70284835992137217886\n",
      "Iteration 28130 => Loss: 6.70284773057385052653\n",
      "Iteration 28131 => Loss: 6.70284710131114191967\n",
      "Iteration 28132 => Loss: 6.70284647213321971293\n",
      "Iteration 28133 => Loss: 6.70284584304008301814\n",
      "Iteration 28134 => Loss: 6.70284521403171762444\n",
      "Iteration 28135 => Loss: 6.70284458510811020915\n",
      "Iteration 28136 => Loss: 6.70284395626924833778\n",
      "Iteration 28137 => Loss: 6.70284332751512490489\n",
      "Iteration 28138 => Loss: 6.70284269884572569964\n",
      "Iteration 28139 => Loss: 6.70284207026104184024\n",
      "Iteration 28140 => Loss: 6.70284144176105201041\n",
      "Iteration 28141 => Loss: 6.70284081334576153921\n",
      "Iteration 28142 => Loss: 6.70284018501514911037\n",
      "Iteration 28143 => Loss: 6.70283955676920051303\n",
      "Iteration 28144 => Loss: 6.70283892860790775359\n",
      "Iteration 28145 => Loss: 6.70283830053126017390\n",
      "Iteration 28146 => Loss: 6.70283767253924533946\n",
      "Iteration 28147 => Loss: 6.70283704463186591482\n",
      "Iteration 28148 => Loss: 6.70283641680908015559\n",
      "Iteration 28149 => Loss: 6.70283578907089871990\n",
      "Iteration 28150 => Loss: 6.70283516141730206783\n",
      "Iteration 28151 => Loss: 6.70283453384828664667\n",
      "Iteration 28152 => Loss: 6.70283390636383558103\n",
      "Iteration 28153 => Loss: 6.70283327896393554823\n",
      "Iteration 28154 => Loss: 6.70283265164857766649\n",
      "Iteration 28155 => Loss: 6.70283202441775660674\n",
      "Iteration 28156 => Loss: 6.70283139727144927633\n",
      "Iteration 28157 => Loss: 6.70283077020964768167\n",
      "Iteration 28158 => Loss: 6.70283014323234915821\n",
      "Iteration 28159 => Loss: 6.70282951633953683057\n",
      "Iteration 28160 => Loss: 6.70282888953119115882\n",
      "Iteration 28161 => Loss: 6.70282826280731125479\n",
      "Iteration 28162 => Loss: 6.70282763616788201944\n",
      "Iteration 28163 => Loss: 6.70282700961289190644\n",
      "Iteration 28164 => Loss: 6.70282638314232581678\n",
      "Iteration 28165 => Loss: 6.70282575675619174405\n",
      "Iteration 28166 => Loss: 6.70282513045444794386\n",
      "Iteration 28167 => Loss: 6.70282450423710862708\n",
      "Iteration 28168 => Loss: 6.70282387810414626017\n",
      "Iteration 28169 => Loss: 6.70282325205555551406\n",
      "Iteration 28170 => Loss: 6.70282262609132395426\n",
      "Iteration 28171 => Loss: 6.70282200021144003443\n",
      "Iteration 28172 => Loss: 6.70282137441590553095\n",
      "Iteration 28173 => Loss: 6.70282074870468669303\n",
      "Iteration 28174 => Loss: 6.70282012307777641524\n",
      "Iteration 28175 => Loss: 6.70281949753517736212\n",
      "Iteration 28176 => Loss: 6.70281887207686999375\n",
      "Iteration 28177 => Loss: 6.70281824670284720469\n",
      "Iteration 28178 => Loss: 6.70281762141308501413\n",
      "Iteration 28179 => Loss: 6.70281699620758697478\n",
      "Iteration 28180 => Loss: 6.70281637108633621125\n",
      "Iteration 28181 => Loss: 6.70281574604931140726\n",
      "Iteration 28182 => Loss: 6.70281512109651966824\n",
      "Iteration 28183 => Loss: 6.70281449622793878973\n",
      "Iteration 28184 => Loss: 6.70281387144355900176\n",
      "Iteration 28185 => Loss: 6.70281324674336609348\n",
      "Iteration 28186 => Loss: 6.70281262212735029493\n",
      "Iteration 28187 => Loss: 6.70281199759550982975\n",
      "Iteration 28188 => Loss: 6.70281137314781449987\n",
      "Iteration 28189 => Loss: 6.70281074878426696984\n",
      "Iteration 28190 => Loss: 6.70281012450485480514\n",
      "Iteration 28191 => Loss: 6.70280950030957090036\n",
      "Iteration 28192 => Loss: 6.70280887619838861013\n",
      "Iteration 28193 => Loss: 6.70280825217130615812\n",
      "Iteration 28194 => Loss: 6.70280762822831377434\n",
      "Iteration 28195 => Loss: 6.70280700436939014253\n",
      "Iteration 28196 => Loss: 6.70280638059453703903\n",
      "Iteration 28197 => Loss: 6.70280575690373936482\n",
      "Iteration 28198 => Loss: 6.70280513329698557357\n",
      "Iteration 28199 => Loss: 6.70280450977425612535\n",
      "Iteration 28200 => Loss: 6.70280388633555368472\n",
      "Iteration 28201 => Loss: 6.70280326298085871173\n",
      "Iteration 28202 => Loss: 6.70280263971015255464\n",
      "Iteration 28203 => Loss: 6.70280201652343965435\n",
      "Iteration 28204 => Loss: 6.70280139342069958275\n",
      "Iteration 28205 => Loss: 6.70280077040192256987\n",
      "Iteration 28206 => Loss: 6.70280014746709706941\n",
      "Iteration 28207 => Loss: 6.70279952461621597593\n",
      "Iteration 28208 => Loss: 6.70279890184925886132\n",
      "Iteration 28209 => Loss: 6.70279827916622217288\n",
      "Iteration 28210 => Loss: 6.70279765656708903521\n",
      "Iteration 28211 => Loss: 6.70279703405184879017\n",
      "Iteration 28212 => Loss: 6.70279641162050232595\n",
      "Iteration 28213 => Loss: 6.70279578927302743807\n",
      "Iteration 28214 => Loss: 6.70279516700940369844\n",
      "Iteration 28215 => Loss: 6.70279454482963377160\n",
      "Iteration 28216 => Loss: 6.70279392273370788757\n",
      "Iteration 28217 => Loss: 6.70279330072161094733\n",
      "Iteration 28218 => Loss: 6.70279267879332429914\n",
      "Iteration 28219 => Loss: 6.70279205694884616662\n",
      "Iteration 28220 => Loss: 6.70279143518815700986\n",
      "Iteration 28221 => Loss: 6.70279081351125771704\n",
      "Iteration 28222 => Loss: 6.70279019191812341916\n",
      "Iteration 28223 => Loss: 6.70278957040874789897\n",
      "Iteration 28224 => Loss: 6.70278894898312049833\n",
      "Iteration 28225 => Loss: 6.70278832764123588817\n",
      "Iteration 28226 => Loss: 6.70278770638307097585\n",
      "Iteration 28227 => Loss: 6.70278708520862487319\n",
      "Iteration 28228 => Loss: 6.70278646411787892845\n",
      "Iteration 28229 => Loss: 6.70278584311082958891\n",
      "Iteration 28230 => Loss: 6.70278522218745909100\n",
      "Iteration 28231 => Loss: 6.70278460134775588841\n",
      "Iteration 28232 => Loss: 6.70278398059171198753\n",
      "Iteration 28233 => Loss: 6.70278335991930962479\n",
      "Iteration 28234 => Loss: 6.70278273933055324107\n",
      "Iteration 28235 => Loss: 6.70278211882541619104\n",
      "Iteration 28236 => Loss: 6.70278149840389314562\n",
      "Iteration 28237 => Loss: 6.70278087806597344667\n",
      "Iteration 28238 => Loss: 6.70278025781163933061\n",
      "Iteration 28239 => Loss: 6.70277963764088635656\n",
      "Iteration 28240 => Loss: 6.70277901755370120185\n",
      "Iteration 28241 => Loss: 6.70277839755007764921\n",
      "Iteration 28242 => Loss: 6.70277777762999793509\n",
      "Iteration 28243 => Loss: 6.70277715779345228952\n",
      "Iteration 28244 => Loss: 6.70277653804042028440\n",
      "Iteration 28245 => Loss: 6.70277591837090991334\n",
      "Iteration 28246 => Loss: 6.70277529878489897186\n",
      "Iteration 28247 => Loss: 6.70277467928237413730\n",
      "Iteration 28248 => Loss: 6.70277405986333452148\n",
      "Iteration 28249 => Loss: 6.70277344052775791994\n",
      "Iteration 28250 => Loss: 6.70277282127563456271\n",
      "Iteration 28251 => Loss: 6.70277220210695112712\n",
      "Iteration 28252 => Loss: 6.70277158302171116588\n",
      "Iteration 28253 => Loss: 6.70277096401988714547\n",
      "Iteration 28254 => Loss: 6.70277034510147107227\n",
      "Iteration 28255 => Loss: 6.70276972626645672904\n",
      "Iteration 28256 => Loss: 6.70276910751483168127\n",
      "Iteration 28257 => Loss: 6.70276848884658615901\n",
      "Iteration 28258 => Loss: 6.70276787026170151051\n",
      "Iteration 28259 => Loss: 6.70276725176016974217\n",
      "Iteration 28260 => Loss: 6.70276663334199529487\n",
      "Iteration 28261 => Loss: 6.70276601500713375970\n",
      "Iteration 28262 => Loss: 6.70276539675560112386\n",
      "Iteration 28263 => Loss: 6.70276477858737873561\n",
      "Iteration 28264 => Loss: 6.70276416050245238409\n",
      "Iteration 28265 => Loss: 6.70276354250081407571\n",
      "Iteration 28266 => Loss: 6.70276292458245492867\n",
      "Iteration 28267 => Loss: 6.70276230674735362669\n",
      "Iteration 28268 => Loss: 6.70276168899551016978\n",
      "Iteration 28269 => Loss: 6.70276107132690679435\n",
      "Iteration 28270 => Loss: 6.70276045374153817136\n",
      "Iteration 28271 => Loss: 6.70275983623938387268\n",
      "Iteration 28272 => Loss: 6.70275921882044389832\n",
      "Iteration 28273 => Loss: 6.70275860148468893840\n",
      "Iteration 28274 => Loss: 6.70275798423213675648\n",
      "Iteration 28275 => Loss: 6.70275736706275093724\n",
      "Iteration 28276 => Loss: 6.70275674997652881615\n",
      "Iteration 28277 => Loss: 6.70275613297345618236\n",
      "Iteration 28278 => Loss: 6.70275551605353303586\n",
      "Iteration 28279 => Loss: 6.70275489921673273130\n",
      "Iteration 28280 => Loss: 6.70275428246305793323\n",
      "Iteration 28281 => Loss: 6.70275366579247755539\n",
      "Iteration 28282 => Loss: 6.70275304920500669681\n",
      "Iteration 28283 => Loss: 6.70275243270061427125\n",
      "Iteration 28284 => Loss: 6.70275181627930027872\n",
      "Iteration 28285 => Loss: 6.70275119994104873200\n",
      "Iteration 28286 => Loss: 6.70275058368584542023\n",
      "Iteration 28287 => Loss: 6.70274996751368412617\n",
      "Iteration 28288 => Loss: 6.70274935142455152715\n",
      "Iteration 28289 => Loss: 6.70274873541843518865\n",
      "Iteration 28290 => Loss: 6.70274811949532978161\n",
      "Iteration 28291 => Loss: 6.70274750365521931883\n",
      "Iteration 28292 => Loss: 6.70274688789809669487\n",
      "Iteration 28293 => Loss: 6.70274627222394236981\n",
      "Iteration 28294 => Loss: 6.70274565663274568550\n",
      "Iteration 28295 => Loss: 6.70274504112451108284\n",
      "Iteration 28296 => Loss: 6.70274442569920658741\n",
      "Iteration 28297 => Loss: 6.70274381035683308738\n",
      "Iteration 28298 => Loss: 6.70274319509738347733\n",
      "Iteration 28299 => Loss: 6.70274257992083377644\n",
      "Iteration 28300 => Loss: 6.70274196482717865564\n",
      "Iteration 28301 => Loss: 6.70274134981641012132\n",
      "Iteration 28302 => Loss: 6.70274073488851573899\n",
      "Iteration 28303 => Loss: 6.70274012004347863325\n",
      "Iteration 28304 => Loss: 6.70273950528129525139\n",
      "Iteration 28305 => Loss: 6.70273889060195227074\n",
      "Iteration 28306 => Loss: 6.70273827600543281591\n",
      "Iteration 28307 => Loss: 6.70273766149173422235\n",
      "Iteration 28308 => Loss: 6.70273704706083783833\n",
      "Iteration 28309 => Loss: 6.70273643271273655841\n",
      "Iteration 28310 => Loss: 6.70273581844741883629\n",
      "Iteration 28311 => Loss: 6.70273520426488023105\n",
      "Iteration 28312 => Loss: 6.70273459016509409736\n",
      "Iteration 28313 => Loss: 6.70273397614805954703\n",
      "Iteration 28314 => Loss: 6.70273336221376769828\n",
      "Iteration 28315 => Loss: 6.70273274836220078754\n",
      "Iteration 28316 => Loss: 6.70273213459334904485\n",
      "Iteration 28317 => Loss: 6.70273152090721158203\n",
      "Iteration 28318 => Loss: 6.70273090730375642465\n",
      "Iteration 28319 => Loss: 6.70273029378299156633\n",
      "Iteration 28320 => Loss: 6.70272968034489391442\n",
      "Iteration 28321 => Loss: 6.70272906698946080439\n",
      "Iteration 28322 => Loss: 6.70272845371667802539\n",
      "Iteration 28323 => Loss: 6.70272784052652692566\n",
      "Iteration 28324 => Loss: 6.70272722741901549881\n",
      "Iteration 28325 => Loss: 6.70272661439410821771\n",
      "Iteration 28326 => Loss: 6.70272600145181574050\n",
      "Iteration 28327 => Loss: 6.70272538859211053364\n",
      "Iteration 28328 => Loss: 6.70272477581498904442\n",
      "Iteration 28329 => Loss: 6.70272416312044150288\n",
      "Iteration 28330 => Loss: 6.70272355050845458635\n",
      "Iteration 28331 => Loss: 6.70272293797901408396\n",
      "Iteration 28332 => Loss: 6.70272232553211377848\n",
      "Iteration 28333 => Loss: 6.70272171316773412997\n",
      "Iteration 28334 => Loss: 6.70272110088588490839\n",
      "Iteration 28335 => Loss: 6.70272048868653236298\n",
      "Iteration 28336 => Loss: 6.70271987656967382918\n",
      "Iteration 28337 => Loss: 6.70271926453529776069\n",
      "Iteration 28338 => Loss: 6.70271865258339527571\n",
      "Iteration 28339 => Loss: 6.70271804071395838065\n",
      "Iteration 28340 => Loss: 6.70271742892695954197\n",
      "Iteration 28341 => Loss: 6.70271681722240764145\n",
      "Iteration 28342 => Loss: 6.70271620560027781011\n",
      "Iteration 28343 => Loss: 6.70271559406056827157\n",
      "Iteration 28344 => Loss: 6.70271498260326037411\n",
      "Iteration 28345 => Loss: 6.70271437122834701228\n",
      "Iteration 28346 => Loss: 6.70271375993582196884\n",
      "Iteration 28347 => Loss: 6.70271314872565948662\n",
      "Iteration 28348 => Loss: 6.70271253759786489468\n",
      "Iteration 28349 => Loss: 6.70271192655241243585\n",
      "Iteration 28350 => Loss: 6.70271131558930655103\n",
      "Iteration 28351 => Loss: 6.70271070470852237122\n",
      "Iteration 28352 => Loss: 6.70271009391006078459\n",
      "Iteration 28353 => Loss: 6.70270948319389425762\n",
      "Iteration 28354 => Loss: 6.70270887256003167209\n",
      "Iteration 28355 => Loss: 6.70270826200844549447\n",
      "Iteration 28356 => Loss: 6.70270765153913661294\n",
      "Iteration 28357 => Loss: 6.70270704115208104668\n",
      "Iteration 28358 => Loss: 6.70270643084728146022\n",
      "Iteration 28359 => Loss: 6.70270582062472009000\n",
      "Iteration 28360 => Loss: 6.70270521048438538969\n",
      "Iteration 28361 => Loss: 6.70270460042626492481\n",
      "Iteration 28362 => Loss: 6.70270399045035603081\n",
      "Iteration 28363 => Loss: 6.70270338055663206234\n",
      "Iteration 28364 => Loss: 6.70270277074509923665\n",
      "Iteration 28365 => Loss: 6.70270216101573090839\n",
      "Iteration 28366 => Loss: 6.70270155136852796574\n",
      "Iteration 28367 => Loss: 6.70270094180348241508\n",
      "Iteration 28368 => Loss: 6.70270033232056583472\n",
      "Iteration 28369 => Loss: 6.70269972291978088919\n",
      "Iteration 28370 => Loss: 6.70269911360111159127\n",
      "Iteration 28371 => Loss: 6.70269850436454639464\n",
      "Iteration 28372 => Loss: 6.70269789521008085842\n",
      "Iteration 28373 => Loss: 6.70269728613769366632\n",
      "Iteration 28374 => Loss: 6.70269667714738037745\n",
      "Iteration 28375 => Loss: 6.70269606823912944549\n",
      "Iteration 28376 => Loss: 6.70269545941293021230\n",
      "Iteration 28377 => Loss: 6.70269485066876846702\n",
      "Iteration 28378 => Loss: 6.70269424200663976876\n",
      "Iteration 28379 => Loss: 6.70269363342652546578\n",
      "Iteration 28380 => Loss: 6.70269302492841223540\n",
      "Iteration 28381 => Loss: 6.70269241651229652490\n",
      "Iteration 28382 => Loss: 6.70269180817817034068\n",
      "Iteration 28383 => Loss: 6.70269119992601414282\n",
      "Iteration 28384 => Loss: 6.70269059175581283228\n",
      "Iteration 28385 => Loss: 6.70268998366757884355\n",
      "Iteration 28386 => Loss: 6.70268937566126954408\n",
      "Iteration 28387 => Loss: 6.70268876773689115112\n",
      "Iteration 28388 => Loss: 6.70268815989444100012\n",
      "Iteration 28389 => Loss: 6.70268755213388445213\n",
      "Iteration 28390 => Loss: 6.70268694445523305347\n",
      "Iteration 28391 => Loss: 6.70268633685846548786\n",
      "Iteration 28392 => Loss: 6.70268572934356487991\n",
      "Iteration 28393 => Loss: 6.70268512191052945326\n",
      "Iteration 28394 => Loss: 6.70268451455934766159\n",
      "Iteration 28395 => Loss: 6.70268390729000618222\n",
      "Iteration 28396 => Loss: 6.70268330010249346884\n",
      "Iteration 28397 => Loss: 6.70268269299680508055\n",
      "Iteration 28398 => Loss: 6.70268208597291881290\n",
      "Iteration 28399 => Loss: 6.70268147903083111316\n",
      "Iteration 28400 => Loss: 6.70268087217052599414\n",
      "Iteration 28401 => Loss: 6.70268026539199812674\n",
      "Iteration 28402 => Loss: 6.70267965869523330014\n",
      "Iteration 28403 => Loss: 6.70267905208021819163\n",
      "Iteration 28404 => Loss: 6.70267844554694924852\n",
      "Iteration 28405 => Loss: 6.70267783909540337817\n",
      "Iteration 28406 => Loss: 6.70267723272558058056\n",
      "Iteration 28407 => Loss: 6.70267662643747197393\n",
      "Iteration 28408 => Loss: 6.70267602023105268927\n",
      "Iteration 28409 => Loss: 6.70267541410632894383\n",
      "Iteration 28410 => Loss: 6.70267480806327409226\n",
      "Iteration 28411 => Loss: 6.70267420210187747642\n",
      "Iteration 28412 => Loss: 6.70267359622214353720\n",
      "Iteration 28413 => Loss: 6.70267299042404562925\n",
      "Iteration 28414 => Loss: 6.70267238470758552893\n",
      "Iteration 28415 => Loss: 6.70267177907274103177\n",
      "Iteration 28416 => Loss: 6.70267117351950858506\n",
      "Iteration 28417 => Loss: 6.70267056804787575430\n",
      "Iteration 28418 => Loss: 6.70266996265782388775\n",
      "Iteration 28419 => Loss: 6.70266935734935564994\n",
      "Iteration 28420 => Loss: 6.70266875212245327731\n",
      "Iteration 28421 => Loss: 6.70266814697709989446\n",
      "Iteration 28422 => Loss: 6.70266754191328750778\n",
      "Iteration 28423 => Loss: 6.70266693693101434093\n",
      "Iteration 28424 => Loss: 6.70266633203025730126\n",
      "Iteration 28425 => Loss: 6.70266572721101194787\n",
      "Iteration 28426 => Loss: 6.70266512247326851082\n",
      "Iteration 28427 => Loss: 6.70266451781701633195\n",
      "Iteration 28428 => Loss: 6.70266391324223675952\n",
      "Iteration 28429 => Loss: 6.70266330874892268810\n",
      "Iteration 28430 => Loss: 6.70266270433707234133\n",
      "Iteration 28431 => Loss: 6.70266210000665640933\n",
      "Iteration 28432 => Loss: 6.70266149575767578028\n",
      "Iteration 28433 => Loss: 6.70266089159012423693\n",
      "Iteration 28434 => Loss: 6.70266028750398312752\n",
      "Iteration 28435 => Loss: 6.70265968349923735303\n",
      "Iteration 28436 => Loss: 6.70265907957587803168\n",
      "Iteration 28437 => Loss: 6.70265847573390782799\n",
      "Iteration 28438 => Loss: 6.70265787197330276115\n",
      "Iteration 28439 => Loss: 6.70265726829404950848\n",
      "Iteration 28440 => Loss: 6.70265666469613918821\n",
      "Iteration 28441 => Loss: 6.70265606117956913579\n",
      "Iteration 28442 => Loss: 6.70265545774432780490\n",
      "Iteration 28443 => Loss: 6.70265485439039210291\n",
      "Iteration 28444 => Loss: 6.70265425111776202982\n",
      "Iteration 28445 => Loss: 6.70265364792642248659\n",
      "Iteration 28446 => Loss: 6.70265304481636370326\n",
      "Iteration 28447 => Loss: 6.70265244178757235716\n",
      "Iteration 28448 => Loss: 6.70265183884003956649\n",
      "Iteration 28449 => Loss: 6.70265123597375289677\n",
      "Iteration 28450 => Loss: 6.70265063318870524256\n",
      "Iteration 28451 => Loss: 6.70265003048488505755\n",
      "Iteration 28452 => Loss: 6.70264942786227191363\n",
      "Iteration 28453 => Loss: 6.70264882532086492262\n",
      "Iteration 28454 => Loss: 6.70264822286065520274\n",
      "Iteration 28455 => Loss: 6.70264762048161699681\n",
      "Iteration 28456 => Loss: 6.70264701818375741027\n",
      "Iteration 28457 => Loss: 6.70264641596705690318\n",
      "Iteration 28458 => Loss: 6.70264581383149948834\n",
      "Iteration 28459 => Loss: 6.70264521177708783028\n",
      "Iteration 28460 => Loss: 6.70264460980379794819\n",
      "Iteration 28461 => Loss: 6.70264400791162806570\n",
      "Iteration 28462 => Loss: 6.70264340610056397196\n",
      "Iteration 28463 => Loss: 6.70264280437058879158\n",
      "Iteration 28464 => Loss: 6.70264220272169986004\n",
      "Iteration 28465 => Loss: 6.70264160115387674921\n",
      "Iteration 28466 => Loss: 6.70264099966712034728\n",
      "Iteration 28467 => Loss: 6.70264039826141644340\n",
      "Iteration 28468 => Loss: 6.70263979693675349125\n",
      "Iteration 28469 => Loss: 6.70263919569311195090\n",
      "Iteration 28470 => Loss: 6.70263859453049537507\n",
      "Iteration 28471 => Loss: 6.70263799344888067111\n",
      "Iteration 28472 => Loss: 6.70263739244825806907\n",
      "Iteration 28473 => Loss: 6.70263679152862934529\n",
      "Iteration 28474 => Loss: 6.70263619068996430173\n",
      "Iteration 28475 => Loss: 6.70263558993227537286\n",
      "Iteration 28476 => Loss: 6.70263498925552791974\n",
      "Iteration 28477 => Loss: 6.70263438865972904779\n",
      "Iteration 28478 => Loss: 6.70263378814485033530\n",
      "Iteration 28479 => Loss: 6.70263318771089711134\n",
      "Iteration 28480 => Loss: 6.70263258735785782960\n",
      "Iteration 28481 => Loss: 6.70263198708571206197\n",
      "Iteration 28482 => Loss: 6.70263138689445003848\n",
      "Iteration 28483 => Loss: 6.70263078678406820643\n",
      "Iteration 28484 => Loss: 6.70263018675454791406\n",
      "Iteration 28485 => Loss: 6.70262958680587939142\n",
      "Iteration 28486 => Loss: 6.70262898693805819761\n",
      "Iteration 28487 => Loss: 6.70262838715106745724\n",
      "Iteration 28488 => Loss: 6.70262778744489917671\n",
      "Iteration 28489 => Loss: 6.70262718781954269787\n",
      "Iteration 28490 => Loss: 6.70262658827498558622\n",
      "Iteration 28491 => Loss: 6.70262598881121718364\n",
      "Iteration 28492 => Loss: 6.70262538942822327925\n",
      "Iteration 28493 => Loss: 6.70262479012599854400\n",
      "Iteration 28494 => Loss: 6.70262419090452521431\n",
      "Iteration 28495 => Loss: 6.70262359176380506653\n",
      "Iteration 28496 => Loss: 6.70262299270381234351\n",
      "Iteration 28497 => Loss: 6.70262239372454438069\n",
      "Iteration 28498 => Loss: 6.70262179482599496083\n",
      "Iteration 28499 => Loss: 6.70262119600814010312\n",
      "Iteration 28500 => Loss: 6.70262059727098513662\n",
      "Iteration 28501 => Loss: 6.70261999861450075144\n",
      "Iteration 28502 => Loss: 6.70261940003868783577\n",
      "Iteration 28503 => Loss: 6.70261880154353217875\n",
      "Iteration 28504 => Loss: 6.70261820312902401042\n",
      "Iteration 28505 => Loss: 6.70261760479515622535\n",
      "Iteration 28506 => Loss: 6.70261700654190750726\n",
      "Iteration 28507 => Loss: 6.70261640836927696796\n",
      "Iteration 28508 => Loss: 6.70261581027725039661\n",
      "Iteration 28509 => Loss: 6.70261521226581535871\n",
      "Iteration 28510 => Loss: 6.70261461433496386064\n",
      "Iteration 28511 => Loss: 6.70261401648468613246\n",
      "Iteration 28512 => Loss: 6.70261341871496441058\n",
      "Iteration 28513 => Loss: 6.70261282102579514230\n",
      "Iteration 28514 => Loss: 6.70261222341716322859\n",
      "Iteration 28515 => Loss: 6.70261162588905889947\n",
      "Iteration 28516 => Loss: 6.70261102844147327318\n",
      "Iteration 28517 => Loss: 6.70261043107438858613\n",
      "Iteration 28518 => Loss: 6.70260983378780217379\n",
      "Iteration 28519 => Loss: 6.70260923658170337802\n",
      "Iteration 28520 => Loss: 6.70260863945607532344\n",
      "Iteration 28521 => Loss: 6.70260804241091179279\n",
      "Iteration 28522 => Loss: 6.70260744544620301610\n",
      "Iteration 28523 => Loss: 6.70260684856192590075\n",
      "Iteration 28524 => Loss: 6.70260625175808399945\n",
      "Iteration 28525 => Loss: 6.70260565503466043680\n",
      "Iteration 28526 => Loss: 6.70260505839164988373\n",
      "Iteration 28527 => Loss: 6.70260446182903013579\n",
      "Iteration 28528 => Loss: 6.70260386534680208115\n",
      "Iteration 28529 => Loss: 6.70260326894494351535\n",
      "Iteration 28530 => Loss: 6.70260267262346243200\n",
      "Iteration 28531 => Loss: 6.70260207638232419214\n",
      "Iteration 28532 => Loss: 6.70260148022153501302\n",
      "Iteration 28533 => Loss: 6.70260088414107357835\n",
      "Iteration 28534 => Loss: 6.70260028814093899996\n",
      "Iteration 28535 => Loss: 6.70259969222111262610\n",
      "Iteration 28536 => Loss: 6.70259909638159445677\n",
      "Iteration 28537 => Loss: 6.70259850062235162937\n",
      "Iteration 28538 => Loss: 6.70259790494340279565\n",
      "Iteration 28539 => Loss: 6.70259730934471154029\n",
      "Iteration 28540 => Loss: 6.70259671382627786329\n",
      "Iteration 28541 => Loss: 6.70259611838809554740\n",
      "Iteration 28542 => Loss: 6.70259552303014594088\n",
      "Iteration 28543 => Loss: 6.70259492775242016194\n",
      "Iteration 28544 => Loss: 6.70259433255491021697\n",
      "Iteration 28545 => Loss: 6.70259373743760100695\n",
      "Iteration 28546 => Loss: 6.70259314240047832101\n",
      "Iteration 28547 => Loss: 6.70259254744354748823\n",
      "Iteration 28548 => Loss: 6.70259195256678008690\n",
      "Iteration 28549 => Loss: 6.70259135777017434066\n",
      "Iteration 28550 => Loss: 6.70259076305372047955\n",
      "Iteration 28551 => Loss: 6.70259016841740074000\n",
      "Iteration 28552 => Loss: 6.70258957386120801658\n",
      "Iteration 28553 => Loss: 6.70258897938513253933\n",
      "Iteration 28554 => Loss: 6.70258838498916453830\n",
      "Iteration 28555 => Loss: 6.70258779067329690804\n",
      "Iteration 28556 => Loss: 6.70258719643749767414\n",
      "Iteration 28557 => Loss: 6.70258660228178637652\n",
      "Iteration 28558 => Loss: 6.70258600820613104077\n",
      "Iteration 28559 => Loss: 6.70258541421052811415\n",
      "Iteration 28560 => Loss: 6.70258482029496782673\n",
      "Iteration 28561 => Loss: 6.70258422645943330309\n",
      "Iteration 28562 => Loss: 6.70258363270392365507\n",
      "Iteration 28563 => Loss: 6.70258303902841490185\n",
      "Iteration 28564 => Loss: 6.70258244543291148432\n",
      "Iteration 28565 => Loss: 6.70258185191739475073\n",
      "Iteration 28566 => Loss: 6.70258125848185137841\n",
      "Iteration 28567 => Loss: 6.70258066512627515010\n",
      "Iteration 28568 => Loss: 6.70258007185065363132\n",
      "Iteration 28569 => Loss: 6.70257947865497616391\n",
      "Iteration 28570 => Loss: 6.70257888553923297792\n",
      "Iteration 28571 => Loss: 6.70257829250341252703\n",
      "Iteration 28572 => Loss: 6.70257769954749971220\n",
      "Iteration 28573 => Loss: 6.70257710667148920436\n",
      "Iteration 28574 => Loss: 6.70257651387536768084\n",
      "Iteration 28575 => Loss: 6.70257592115912892439\n",
      "Iteration 28576 => Loss: 6.70257532852275694779\n",
      "Iteration 28577 => Loss: 6.70257473596624109291\n",
      "Iteration 28578 => Loss: 6.70257414348957425432\n",
      "Iteration 28579 => Loss: 6.70257355109274310934\n",
      "Iteration 28580 => Loss: 6.70257295877574232890\n",
      "Iteration 28581 => Loss: 6.70257236653855237307\n",
      "Iteration 28582 => Loss: 6.70257177438115725465\n",
      "Iteration 28583 => Loss: 6.70257118230356674360\n",
      "Iteration 28584 => Loss: 6.70257059030575597092\n",
      "Iteration 28585 => Loss: 6.70256999838771783118\n",
      "Iteration 28586 => Loss: 6.70256940654944166624\n",
      "Iteration 28587 => Loss: 6.70256881479091326526\n",
      "Iteration 28588 => Loss: 6.70256822311212374643\n",
      "Iteration 28589 => Loss: 6.70256763151306156345\n",
      "Iteration 28590 => Loss: 6.70256703999372316360\n",
      "Iteration 28591 => Loss: 6.70256644855408900696\n",
      "Iteration 28592 => Loss: 6.70256585719415376445\n",
      "Iteration 28593 => Loss: 6.70256526591389878433\n",
      "Iteration 28594 => Loss: 6.70256467471332406660\n",
      "Iteration 28595 => Loss: 6.70256408359241184769\n",
      "Iteration 28596 => Loss: 6.70256349255115146946\n",
      "Iteration 28597 => Loss: 6.70256290158953227376\n",
      "Iteration 28598 => Loss: 6.70256231070755159607\n",
      "Iteration 28599 => Loss: 6.70256171990518723192\n",
      "Iteration 28600 => Loss: 6.70256112918243740495\n",
      "Iteration 28601 => Loss: 6.70256053853928790431\n",
      "Iteration 28602 => Loss: 6.70255994797571830190\n",
      "Iteration 28603 => Loss: 6.70255935749173659133\n",
      "Iteration 28604 => Loss: 6.70255876708731612723\n",
      "Iteration 28605 => Loss: 6.70255817676246135051\n",
      "Iteration 28606 => Loss: 6.70255758651714650398\n",
      "Iteration 28607 => Loss: 6.70255699635136803494\n",
      "Iteration 28608 => Loss: 6.70255640626512061431\n",
      "Iteration 28609 => Loss: 6.70255581625837848492\n",
      "Iteration 28610 => Loss: 6.70255522633114431130\n",
      "Iteration 28611 => Loss: 6.70255463648340388261\n",
      "Iteration 28612 => Loss: 6.70255404671513765891\n",
      "Iteration 28613 => Loss: 6.70255345702635096927\n",
      "Iteration 28614 => Loss: 6.70255286741702516196\n",
      "Iteration 28615 => Loss: 6.70255227788714780246\n",
      "Iteration 28616 => Loss: 6.70255168843670379175\n",
      "Iteration 28617 => Loss: 6.70255109906569312983\n",
      "Iteration 28618 => Loss: 6.70255050977410071766\n",
      "Iteration 28619 => Loss: 6.70254992056191856165\n",
      "Iteration 28620 => Loss: 6.70254933142912801003\n",
      "Iteration 28621 => Loss: 6.70254874237572018103\n",
      "Iteration 28622 => Loss: 6.70254815340169418647\n",
      "Iteration 28623 => Loss: 6.70254756450702782189\n",
      "Iteration 28624 => Loss: 6.70254697569171664639\n",
      "Iteration 28625 => Loss: 6.70254638695574733731\n",
      "Iteration 28626 => Loss: 6.70254579829911545374\n",
      "Iteration 28627 => Loss: 6.70254520972180056759\n",
      "Iteration 28628 => Loss: 6.70254462122379646161\n",
      "Iteration 28629 => Loss: 6.70254403280508892493\n",
      "Iteration 28630 => Loss: 6.70254344446567706939\n",
      "Iteration 28631 => Loss: 6.70254285620554579594\n",
      "Iteration 28632 => Loss: 6.70254226802467645285\n",
      "Iteration 28633 => Loss: 6.70254167992306992829\n",
      "Iteration 28634 => Loss: 6.70254109190070845870\n",
      "Iteration 28635 => Loss: 6.70254050395758405045\n",
      "Iteration 28636 => Loss: 6.70253991609367805182\n",
      "Iteration 28637 => Loss: 6.70253932830898868644\n",
      "Iteration 28638 => Loss: 6.70253874060350529618\n",
      "Iteration 28639 => Loss: 6.70253815297721722288\n",
      "Iteration 28640 => Loss: 6.70253756543011114388\n",
      "Iteration 28641 => Loss: 6.70253697796217373650\n",
      "Iteration 28642 => Loss: 6.70253639057340144802\n",
      "Iteration 28643 => Loss: 6.70253580326378184395\n",
      "Iteration 28644 => Loss: 6.70253521603330248979\n",
      "Iteration 28645 => Loss: 6.70253462888194739833\n",
      "Iteration 28646 => Loss: 6.70253404180971035231\n",
      "Iteration 28647 => Loss: 6.70253345481658513449\n",
      "Iteration 28648 => Loss: 6.70253286790255398131\n",
      "Iteration 28649 => Loss: 6.70253228106761689276\n",
      "Iteration 28650 => Loss: 6.70253169431174899984\n",
      "Iteration 28651 => Loss: 6.70253110763494408531\n",
      "Iteration 28652 => Loss: 6.70253052103720037280\n",
      "Iteration 28653 => Loss: 6.70252993451849565787\n",
      "Iteration 28654 => Loss: 6.70252934807882905233\n",
      "Iteration 28655 => Loss: 6.70252876171818545714\n",
      "Iteration 28656 => Loss: 6.70252817543655599053\n",
      "Iteration 28657 => Loss: 6.70252758923391844803\n",
      "Iteration 28658 => Loss: 6.70252700311027904689\n",
      "Iteration 28659 => Loss: 6.70252641706561380630\n",
      "Iteration 28660 => Loss: 6.70252583109992183807\n",
      "Iteration 28661 => Loss: 6.70252524521319070772\n",
      "Iteration 28662 => Loss: 6.70252465940540975708\n",
      "Iteration 28663 => Loss: 6.70252407367656033443\n",
      "Iteration 28664 => Loss: 6.70252348802664155158\n",
      "Iteration 28665 => Loss: 6.70252290245563919768\n",
      "Iteration 28666 => Loss: 6.70252231696354083823\n",
      "Iteration 28667 => Loss: 6.70252173155034025598\n",
      "Iteration 28668 => Loss: 6.70252114621602057554\n",
      "Iteration 28669 => Loss: 6.70252056096056936241\n",
      "Iteration 28670 => Loss: 6.70251997578399727473\n",
      "Iteration 28671 => Loss: 6.70251939068626700902\n",
      "Iteration 28672 => Loss: 6.70251880566738034162\n",
      "Iteration 28673 => Loss: 6.70251822072732394986\n",
      "Iteration 28674 => Loss: 6.70251763586608984014\n",
      "Iteration 28675 => Loss: 6.70251705108366291341\n",
      "Iteration 28676 => Loss: 6.70251646638004050516\n",
      "Iteration 28677 => Loss: 6.70251588175520573998\n",
      "Iteration 28678 => Loss: 6.70251529720914618338\n",
      "Iteration 28679 => Loss: 6.70251471274185828264\n",
      "Iteration 28680 => Loss: 6.70251412835332427420\n",
      "Iteration 28681 => Loss: 6.70251354404354149352\n",
      "Iteration 28682 => Loss: 6.70251295981248595979\n",
      "Iteration 28683 => Loss: 6.70251237566015944935\n",
      "Iteration 28684 => Loss: 6.70251179158655485679\n",
      "Iteration 28685 => Loss: 6.70251120759164198404\n",
      "Iteration 28686 => Loss: 6.70251062367542616016\n",
      "Iteration 28687 => Loss: 6.70251003983789850338\n",
      "Iteration 28688 => Loss: 6.70250945607903947376\n",
      "Iteration 28689 => Loss: 6.70250887239884374225\n",
      "Iteration 28690 => Loss: 6.70250828879729620979\n",
      "Iteration 28691 => Loss: 6.70250770527439065916\n",
      "Iteration 28692 => Loss: 6.70250712183011465584\n",
      "Iteration 28693 => Loss: 6.70250653846445931805\n",
      "Iteration 28694 => Loss: 6.70250595517741132312\n",
      "Iteration 28695 => Loss: 6.70250537196895912473\n",
      "Iteration 28696 => Loss: 6.70250478883909472927\n",
      "Iteration 28697 => Loss: 6.70250420578780659042\n",
      "Iteration 28698 => Loss: 6.70250362281509293183\n",
      "Iteration 28699 => Loss: 6.70250303992092621996\n",
      "Iteration 28700 => Loss: 6.70250245710530556664\n",
      "Iteration 28701 => Loss: 6.70250187436822209008\n",
      "Iteration 28702 => Loss: 6.70250129170965802672\n",
      "Iteration 28703 => Loss: 6.70250070912960804748\n",
      "Iteration 28704 => Loss: 6.70250012662806504693\n",
      "Iteration 28705 => Loss: 6.70249954420501303787\n",
      "Iteration 28706 => Loss: 6.70249896186043869761\n",
      "Iteration 28707 => Loss: 6.70249837959434024981\n",
      "Iteration 28708 => Loss: 6.70249779740669549000\n",
      "Iteration 28709 => Loss: 6.70249721529751063542\n",
      "Iteration 28710 => Loss: 6.70249663326675726438\n",
      "Iteration 28711 => Loss: 6.70249605131443981776\n",
      "Iteration 28712 => Loss: 6.70249546944053342656\n",
      "Iteration 28713 => Loss: 6.70249488764503542626\n",
      "Iteration 28714 => Loss: 6.70249430592793427053\n",
      "Iteration 28715 => Loss: 6.70249372428922374212\n",
      "Iteration 28716 => Loss: 6.70249314272888341293\n",
      "Iteration 28717 => Loss: 6.70249256124691505931\n",
      "Iteration 28718 => Loss: 6.70249197984329914135\n",
      "Iteration 28719 => Loss: 6.70249139851802411272\n",
      "Iteration 28720 => Loss: 6.70249081727108197981\n",
      "Iteration 28721 => Loss: 6.70249023610246741356\n",
      "Iteration 28722 => Loss: 6.70248965501216353857\n",
      "Iteration 28723 => Loss: 6.70248907400015880853\n",
      "Iteration 28724 => Loss: 6.70248849306645055890\n",
      "Iteration 28725 => Loss: 6.70248791221101658522\n",
      "Iteration 28726 => Loss: 6.70248733143386044020\n",
      "Iteration 28727 => Loss: 6.70248675073496080756\n",
      "Iteration 28728 => Loss: 6.70248617011431147006\n",
      "Iteration 28729 => Loss: 6.70248558957190265772\n",
      "Iteration 28730 => Loss: 6.70248500910771216610\n",
      "Iteration 28731 => Loss: 6.70248442872174887697\n",
      "Iteration 28732 => Loss: 6.70248384841398969769\n",
      "Iteration 28733 => Loss: 6.70248326818442663466\n",
      "Iteration 28734 => Loss: 6.70248268803305524699\n",
      "Iteration 28735 => Loss: 6.70248210795985066568\n",
      "Iteration 28736 => Loss: 6.70248152796481910798\n",
      "Iteration 28737 => Loss: 6.70248094804793836943\n",
      "Iteration 28738 => Loss: 6.70248036820919601553\n",
      "Iteration 28739 => Loss: 6.70247978844858671721\n",
      "Iteration 28740 => Loss: 6.70247920876610869811\n",
      "Iteration 28741 => Loss: 6.70247862916174241832\n",
      "Iteration 28742 => Loss: 6.70247804963546567336\n",
      "Iteration 28743 => Loss: 6.70247747018729445045\n",
      "Iteration 28744 => Loss: 6.70247689081719943971\n",
      "Iteration 28745 => Loss: 6.70247631152517442388\n",
      "Iteration 28746 => Loss: 6.70247573231120608028\n",
      "Iteration 28747 => Loss: 6.70247515317529352075\n",
      "Iteration 28748 => Loss: 6.70247457411740921174\n",
      "Iteration 28749 => Loss: 6.70247399513755759415\n",
      "Iteration 28750 => Loss: 6.70247341623572889802\n",
      "Iteration 28751 => Loss: 6.70247283741189825435\n",
      "Iteration 28752 => Loss: 6.70247225866607543310\n",
      "Iteration 28753 => Loss: 6.70247167999823112439\n",
      "Iteration 28754 => Loss: 6.70247110140836355185\n",
      "Iteration 28755 => Loss: 6.70247052289646383372\n",
      "Iteration 28756 => Loss: 6.70246994446251154187\n",
      "Iteration 28757 => Loss: 6.70246936610651022903\n",
      "Iteration 28758 => Loss: 6.70246878782843946709\n",
      "Iteration 28759 => Loss: 6.70246820962829392698\n",
      "Iteration 28760 => Loss: 6.70246763150605939785\n",
      "Iteration 28761 => Loss: 6.70246705346172522155\n",
      "Iteration 28762 => Loss: 6.70246647549528518084\n",
      "Iteration 28763 => Loss: 6.70246589760671973579\n",
      "Iteration 28764 => Loss: 6.70246531979603243911\n",
      "Iteration 28765 => Loss: 6.70246474206320108635\n",
      "Iteration 28766 => Loss: 6.70246416440822567751\n",
      "Iteration 28767 => Loss: 6.70246358683108045540\n",
      "Iteration 28768 => Loss: 6.70246300933177074910\n",
      "Iteration 28769 => Loss: 6.70246243191027346597\n",
      "Iteration 28770 => Loss: 6.70246185456658505331\n",
      "Iteration 28771 => Loss: 6.70246127730069662931\n",
      "Iteration 28772 => Loss: 6.70246070011259575949\n",
      "Iteration 28773 => Loss: 6.70246012300227178571\n",
      "Iteration 28774 => Loss: 6.70245954596970783257\n",
      "Iteration 28775 => Loss: 6.70245896901490034736\n",
      "Iteration 28776 => Loss: 6.70245839213784577737\n",
      "Iteration 28777 => Loss: 6.70245781533851125999\n",
      "Iteration 28778 => Loss: 6.70245723861691011791\n",
      "Iteration 28779 => Loss: 6.70245666197301925848\n",
      "Iteration 28780 => Loss: 6.70245608540683068810\n",
      "Iteration 28781 => Loss: 6.70245550891833641316\n",
      "Iteration 28782 => Loss: 6.70245493250752399916\n",
      "Iteration 28783 => Loss: 6.70245435617438101161\n",
      "Iteration 28784 => Loss: 6.70245377991890034508\n",
      "Iteration 28785 => Loss: 6.70245320374107311778\n",
      "Iteration 28786 => Loss: 6.70245262764088156615\n",
      "Iteration 28787 => Loss: 6.70245205161831680840\n",
      "Iteration 28788 => Loss: 6.70245147567337884453\n",
      "Iteration 28789 => Loss: 6.70245089980604902280\n",
      "Iteration 28790 => Loss: 6.70245032401630869145\n",
      "Iteration 28791 => Loss: 6.70244974830415962685\n",
      "Iteration 28792 => Loss: 6.70244917266958939450\n",
      "Iteration 28793 => Loss: 6.70244859711259000079\n",
      "Iteration 28794 => Loss: 6.70244802163314279397\n",
      "Iteration 28795 => Loss: 6.70244744623124066862\n",
      "Iteration 28796 => Loss: 6.70244687090687030206\n",
      "Iteration 28797 => Loss: 6.70244629566003702337\n",
      "Iteration 28798 => Loss: 6.70244572049071152264\n",
      "Iteration 28799 => Loss: 6.70244514539888847082\n",
      "Iteration 28800 => Loss: 6.70244457038455987430\n",
      "Iteration 28801 => Loss: 6.70244399544771329857\n",
      "Iteration 28802 => Loss: 6.70244342058833986187\n",
      "Iteration 28803 => Loss: 6.70244284580643512328\n",
      "Iteration 28804 => Loss: 6.70244227110196977293\n",
      "Iteration 28805 => Loss: 6.70244169647495979802\n",
      "Iteration 28806 => Loss: 6.70244112192536967143\n",
      "Iteration 28807 => Loss: 6.70244054745320561040\n",
      "Iteration 28808 => Loss: 6.70243997305845162771\n",
      "Iteration 28809 => Loss: 6.70243939874109617705\n",
      "Iteration 28810 => Loss: 6.70243882450113570570\n",
      "Iteration 28811 => Loss: 6.70243825033854800921\n",
      "Iteration 28812 => Loss: 6.70243767625332598215\n",
      "Iteration 28813 => Loss: 6.70243710224546784815\n",
      "Iteration 28814 => Loss: 6.70243652831495406730\n",
      "Iteration 28815 => Loss: 6.70243595446178108688\n",
      "Iteration 28816 => Loss: 6.70243538068592847878\n",
      "Iteration 28817 => Loss: 6.70243480698739890755\n",
      "Iteration 28818 => Loss: 6.70243423336616839237\n",
      "Iteration 28819 => Loss: 6.70243365982224581501\n",
      "Iteration 28820 => Loss: 6.70243308635559653652\n",
      "Iteration 28821 => Loss: 6.70243251296622499780\n",
      "Iteration 28822 => Loss: 6.70243193965412054069\n",
      "Iteration 28823 => Loss: 6.70243136641926628982\n",
      "Iteration 28824 => Loss: 6.70243079326165691612\n",
      "Iteration 28825 => Loss: 6.70243022018127643236\n",
      "Iteration 28826 => Loss: 6.70242964717812395037\n",
      "Iteration 28827 => Loss: 6.70242907425217904205\n",
      "Iteration 28828 => Loss: 6.70242850140344526011\n",
      "Iteration 28829 => Loss: 6.70242792863189595920\n",
      "Iteration 28830 => Loss: 6.70242735593753202750\n",
      "Iteration 28831 => Loss: 6.70242678332033392508\n",
      "Iteration 28832 => Loss: 6.70242621078029454651\n",
      "Iteration 28833 => Loss: 6.70242563831740856273\n",
      "Iteration 28834 => Loss: 6.70242506593165821016\n",
      "Iteration 28835 => Loss: 6.70242449362304348881\n",
      "Iteration 28836 => Loss: 6.70242392139153864150\n",
      "Iteration 28837 => Loss: 6.70242334923715166184\n",
      "Iteration 28838 => Loss: 6.70242277715986123354\n",
      "Iteration 28839 => Loss: 6.70242220515965492211\n",
      "Iteration 28840 => Loss: 6.70242163323651940487\n",
      "Iteration 28841 => Loss: 6.70242106139045912272\n",
      "Iteration 28842 => Loss: 6.70242048962145098301\n",
      "Iteration 28843 => Loss: 6.70241991792948965667\n",
      "Iteration 28844 => Loss: 6.70241934631456715010\n",
      "Iteration 28845 => Loss: 6.70241877477666925245\n",
      "Iteration 28846 => Loss: 6.70241820331578264103\n",
      "Iteration 28847 => Loss: 6.70241763193190465131\n",
      "Iteration 28848 => Loss: 6.70241706062501396701\n",
      "Iteration 28849 => Loss: 6.70241648939511414085\n",
      "Iteration 28850 => Loss: 6.70241591824218652107\n",
      "Iteration 28851 => Loss: 6.70241534716622133772\n",
      "Iteration 28852 => Loss: 6.70241477616720704447\n",
      "Iteration 28853 => Loss: 6.70241420524513209500\n",
      "Iteration 28854 => Loss: 6.70241363439999116025\n",
      "Iteration 28855 => Loss: 6.70241306363177802297\n",
      "Iteration 28856 => Loss: 6.70241249294046514962\n",
      "Iteration 28857 => Loss: 6.70241192232606231016\n",
      "Iteration 28858 => Loss: 6.70241135178854374743\n",
      "Iteration 28859 => Loss: 6.70241078132790679689\n",
      "Iteration 28860 => Loss: 6.70241021094414080039\n",
      "Iteration 28861 => Loss: 6.70240964063723154709\n",
      "Iteration 28862 => Loss: 6.70240907040717281973\n",
      "Iteration 28863 => Loss: 6.70240850025396017742\n",
      "Iteration 28864 => Loss: 6.70240793017756164573\n",
      "Iteration 28865 => Loss: 6.70240736017798699464\n",
      "Iteration 28866 => Loss: 6.70240679025522112511\n",
      "Iteration 28867 => Loss: 6.70240622040924982628\n",
      "Iteration 28868 => Loss: 6.70240565064007132179\n",
      "Iteration 28869 => Loss: 6.70240508094766074265\n",
      "Iteration 28870 => Loss: 6.70240451133202519429\n",
      "Iteration 28871 => Loss: 6.70240394179313980771\n",
      "Iteration 28872 => Loss: 6.70240337233099747749\n",
      "Iteration 28873 => Loss: 6.70240280294559731544\n",
      "Iteration 28874 => Loss: 6.70240223363691534075\n",
      "Iteration 28875 => Loss: 6.70240166440495421796\n",
      "Iteration 28876 => Loss: 6.70240109524968996624\n",
      "Iteration 28877 => Loss: 6.70240052617112347377\n",
      "Iteration 28878 => Loss: 6.70239995716924497060\n",
      "Iteration 28879 => Loss: 6.70239938824403047590\n",
      "Iteration 28880 => Loss: 6.70239881939548176604\n",
      "Iteration 28881 => Loss: 6.70239825062358907104\n",
      "Iteration 28882 => Loss: 6.70239768192833285099\n",
      "Iteration 28883 => Loss: 6.70239711330970866499\n",
      "Iteration 28884 => Loss: 6.70239654476771384850\n",
      "Iteration 28885 => Loss: 6.70239597630231909164\n",
      "Iteration 28886 => Loss: 6.70239540791353238802\n",
      "Iteration 28887 => Loss: 6.70239483960133597407\n",
      "Iteration 28888 => Loss: 6.70239427136571475074\n",
      "Iteration 28889 => Loss: 6.70239370320666871805\n",
      "Iteration 28890 => Loss: 6.70239313512417922425\n",
      "Iteration 28891 => Loss: 6.70239256711824182844\n",
      "Iteration 28892 => Loss: 6.70239199918884231977\n",
      "Iteration 28893 => Loss: 6.70239143133596737556\n",
      "Iteration 28894 => Loss: 6.70239086355961521946\n",
      "Iteration 28895 => Loss: 6.70239029585976187064\n",
      "Iteration 28896 => Loss: 6.70238972823642242815\n",
      "Iteration 28897 => Loss: 6.70238916068955514760\n",
      "Iteration 28898 => Loss: 6.70238859321917335166\n",
      "Iteration 28899 => Loss: 6.70238802582525217133\n",
      "Iteration 28900 => Loss: 6.70238745850778983026\n",
      "Iteration 28901 => Loss: 6.70238689126677922303\n",
      "Iteration 28902 => Loss: 6.70238632410220169788\n",
      "Iteration 28903 => Loss: 6.70238575701404393214\n",
      "Iteration 28904 => Loss: 6.70238519000230770217\n",
      "Iteration 28905 => Loss: 6.70238462306697346804\n",
      "Iteration 28906 => Loss: 6.70238405620803412432\n",
      "Iteration 28907 => Loss: 6.70238348942547546017\n",
      "Iteration 28908 => Loss: 6.70238292271929569921\n",
      "Iteration 28909 => Loss: 6.70238235608948418331\n",
      "Iteration 28910 => Loss: 6.70238178953601693166\n",
      "Iteration 28911 => Loss: 6.70238122305889927333\n",
      "Iteration 28912 => Loss: 6.70238065665810811566\n",
      "Iteration 28913 => Loss: 6.70238009033364789957\n",
      "Iteration 28914 => Loss: 6.70237952408549020333\n",
      "Iteration 28915 => Loss: 6.70237895791364124420\n",
      "Iteration 28916 => Loss: 6.70237839181807615319\n",
      "Iteration 28917 => Loss: 6.70237782579880381206\n",
      "Iteration 28918 => Loss: 6.70237725985579491095\n",
      "Iteration 28919 => Loss: 6.70237669398905122620\n",
      "Iteration 28920 => Loss: 6.70237612819855588242\n",
      "Iteration 28921 => Loss: 6.70237556248429999783\n",
      "Iteration 28922 => Loss: 6.70237499684627646701\n",
      "Iteration 28923 => Loss: 6.70237443128446575002\n",
      "Iteration 28924 => Loss: 6.70237386579887850502\n",
      "Iteration 28925 => Loss: 6.70237330038948009303\n",
      "Iteration 28926 => Loss: 6.70237273505627140224\n",
      "Iteration 28927 => Loss: 6.70237216979923733362\n",
      "Iteration 28928 => Loss: 6.70237160461837699899\n",
      "Iteration 28929 => Loss: 6.70237103951367796384\n",
      "Iteration 28930 => Loss: 6.70237047448512601733\n",
      "Iteration 28931 => Loss: 6.70236990953270961313\n",
      "Iteration 28932 => Loss: 6.70236934465641986947\n",
      "Iteration 28933 => Loss: 6.70236877985624435183\n",
      "Iteration 28934 => Loss: 6.70236821513218572477\n",
      "Iteration 28935 => Loss: 6.70236765048421467839\n",
      "Iteration 28936 => Loss: 6.70236708591232765997\n",
      "Iteration 28937 => Loss: 6.70236652141652289316\n",
      "Iteration 28938 => Loss: 6.70236595699678616711\n",
      "Iteration 28939 => Loss: 6.70236539265310149460\n",
      "Iteration 28940 => Loss: 6.70236482838546177021\n",
      "Iteration 28941 => Loss: 6.70236426419385189490\n",
      "Iteration 28942 => Loss: 6.70236370007827364503\n",
      "Iteration 28943 => Loss: 6.70236313603870836886\n",
      "Iteration 28944 => Loss: 6.70236257207514984913\n",
      "Iteration 28945 => Loss: 6.70236200818758209863\n",
      "Iteration 28946 => Loss: 6.70236144437600511736\n",
      "Iteration 28947 => Loss: 6.70236088064039225998\n",
      "Iteration 28948 => Loss: 6.70236031698074619101\n",
      "Iteration 28949 => Loss: 6.70235975339705802867\n",
      "Iteration 28950 => Loss: 6.70235918988930734486\n",
      "Iteration 28951 => Loss: 6.70235862645748792232\n",
      "Iteration 28952 => Loss: 6.70235806310159620836\n",
      "Iteration 28953 => Loss: 6.70235749982161355121\n",
      "Iteration 28954 => Loss: 6.70235693661753195727\n",
      "Iteration 28955 => Loss: 6.70235637348934165658\n",
      "Iteration 28956 => Loss: 6.70235581043703643189\n",
      "Iteration 28957 => Loss: 6.70235524746060562506\n",
      "Iteration 28958 => Loss: 6.70235468456002969617\n",
      "Iteration 28959 => Loss: 6.70235412173530686886\n",
      "Iteration 28960 => Loss: 6.70235355898642826133\n",
      "Iteration 28961 => Loss: 6.70235299631337522186\n",
      "Iteration 28962 => Loss: 6.70235243371614153318\n",
      "Iteration 28963 => Loss: 6.70235187119471920170\n",
      "Iteration 28964 => Loss: 6.70235130874909668108\n",
      "Iteration 28965 => Loss: 6.70235074637926597774\n",
      "Iteration 28966 => Loss: 6.70235018408521376898\n",
      "Iteration 28967 => Loss: 6.70234962186692939667\n",
      "Iteration 28968 => Loss: 6.70234905972440664357\n",
      "Iteration 28969 => Loss: 6.70234849765763041063\n",
      "Iteration 28970 => Loss: 6.70234793566659536879\n",
      "Iteration 28971 => Loss: 6.70234737375127753722\n",
      "Iteration 28972 => Loss: 6.70234681191169201497\n",
      "Iteration 28973 => Loss: 6.70234625014780593943\n",
      "Iteration 28974 => Loss: 6.70234568845962641603\n",
      "Iteration 28975 => Loss: 6.70234512684712591124\n",
      "Iteration 28976 => Loss: 6.70234456531030886595\n",
      "Iteration 28977 => Loss: 6.70234400384915218751\n",
      "Iteration 28978 => Loss: 6.70234344246365409958\n",
      "Iteration 28979 => Loss: 6.70234288115380216766\n",
      "Iteration 28980 => Loss: 6.70234231991959550356\n",
      "Iteration 28981 => Loss: 6.70234175876100835012\n",
      "Iteration 28982 => Loss: 6.70234119767803715462\n",
      "Iteration 28983 => Loss: 6.70234063667066770620\n",
      "Iteration 28984 => Loss: 6.70234007573890533394\n",
      "Iteration 28985 => Loss: 6.70233951488271451069\n",
      "Iteration 28986 => Loss: 6.70233895410211388821\n",
      "Iteration 28987 => Loss: 6.70233839339707149207\n",
      "Iteration 28988 => Loss: 6.70233783276757666414\n",
      "Iteration 28989 => Loss: 6.70233727221363828619\n",
      "Iteration 28990 => Loss: 6.70233671173522882469\n",
      "Iteration 28991 => Loss: 6.70233615133234827965\n",
      "Iteration 28992 => Loss: 6.70233559100497533478\n",
      "Iteration 28993 => Loss: 6.70233503075311443098\n",
      "Iteration 28994 => Loss: 6.70233447057674336378\n",
      "Iteration 28995 => Loss: 6.70233391047585325140\n",
      "Iteration 28996 => Loss: 6.70233335045043876477\n",
      "Iteration 28997 => Loss: 6.70233279050049102210\n",
      "Iteration 28998 => Loss: 6.70233223062599581255\n",
      "Iteration 28999 => Loss: 6.70233167082694247796\n",
      "Iteration 29000 => Loss: 6.70233111110331591931\n",
      "Iteration 29001 => Loss: 6.70233055145512857109\n",
      "Iteration 29002 => Loss: 6.70232999188233780075\n",
      "Iteration 29003 => Loss: 6.70232943238495249005\n",
      "Iteration 29004 => Loss: 6.70232887296295931634\n",
      "Iteration 29005 => Loss: 6.70232831361635650325\n",
      "Iteration 29006 => Loss: 6.70232775434511918178\n",
      "Iteration 29007 => Loss: 6.70232719514924291104\n",
      "Iteration 29008 => Loss: 6.70232663602871969744\n",
      "Iteration 29009 => Loss: 6.70232607698354065917\n",
      "Iteration 29010 => Loss: 6.70232551801369069722\n",
      "Iteration 29011 => Loss: 6.70232495911916359432\n",
      "Iteration 29012 => Loss: 6.70232440029994425146\n",
      "Iteration 29013 => Loss: 6.70232384155603178044\n",
      "Iteration 29014 => Loss: 6.70232328288740397682\n",
      "Iteration 29015 => Loss: 6.70232272429405551151\n",
      "Iteration 29016 => Loss: 6.70232216577598460816\n",
      "Iteration 29017 => Loss: 6.70232160733316639778\n",
      "Iteration 29018 => Loss: 6.70232104896560443308\n",
      "Iteration 29019 => Loss: 6.70232049067327650960\n",
      "Iteration 29020 => Loss: 6.70231993245618173916\n",
      "Iteration 29021 => Loss: 6.70231937431431123997\n",
      "Iteration 29022 => Loss: 6.70231881624764902483\n",
      "Iteration 29023 => Loss: 6.70231825825617910652\n",
      "Iteration 29024 => Loss: 6.70231770033990237323\n",
      "Iteration 29025 => Loss: 6.70231714249880994316\n",
      "Iteration 29026 => Loss: 6.70231658473288405276\n",
      "Iteration 29027 => Loss: 6.70231602704211493204\n",
      "Iteration 29028 => Loss: 6.70231546942649547560\n",
      "Iteration 29029 => Loss: 6.70231491188601591347\n",
      "Iteration 29030 => Loss: 6.70231435442066114661\n",
      "Iteration 29031 => Loss: 6.70231379703042851048\n",
      "Iteration 29032 => Loss: 6.70231323971529935335\n",
      "Iteration 29033 => Loss: 6.70231268247527545157\n",
      "Iteration 29034 => Loss: 6.70231212531033371249\n",
      "Iteration 29035 => Loss: 6.70231156822047680066\n",
      "Iteration 29036 => Loss: 6.70231101120568339979\n",
      "Iteration 29037 => Loss: 6.70231045426594462810\n",
      "Iteration 29038 => Loss: 6.70230989740126137377\n",
      "Iteration 29039 => Loss: 6.70230934061161143234\n",
      "Iteration 29040 => Loss: 6.70230878389698769837\n",
      "Iteration 29041 => Loss: 6.70230822725738040191\n",
      "Iteration 29042 => Loss: 6.70230767069278687842\n",
      "Iteration 29043 => Loss: 6.70230711420318314708\n",
      "Iteration 29044 => Loss: 6.70230655778857364879\n",
      "Iteration 29045 => Loss: 6.70230600144892996184\n",
      "Iteration 29046 => Loss: 6.70230544518426096801\n",
      "Iteration 29047 => Loss: 6.70230488899455156826\n",
      "Iteration 29048 => Loss: 6.70230433287977955814\n",
      "Iteration 29049 => Loss: 6.70230377683995293125\n",
      "Iteration 29050 => Loss: 6.70230322087504593043\n",
      "Iteration 29051 => Loss: 6.70230266498506033201\n",
      "Iteration 29052 => Loss: 6.70230210916997837245\n",
      "Iteration 29053 => Loss: 6.70230155342979294630\n",
      "Iteration 29054 => Loss: 6.70230099776449783633\n",
      "Iteration 29055 => Loss: 6.70230044217407616713\n",
      "Iteration 29056 => Loss: 6.70229988665851461604\n",
      "Iteration 29057 => Loss: 6.70229933121781584759\n",
      "Iteration 29058 => Loss: 6.70229877585195943368\n",
      "Iteration 29059 => Loss: 6.70229822056093649252\n",
      "Iteration 29060 => Loss: 6.70229766534474524775\n",
      "Iteration 29061 => Loss: 6.70229711020336438310\n",
      "Iteration 29062 => Loss: 6.70229655513679212220\n",
      "Iteration 29063 => Loss: 6.70229600014501158967\n",
      "Iteration 29064 => Loss: 6.70229544522801745643\n",
      "Iteration 29065 => Loss: 6.70229489038579728799\n",
      "Iteration 29066 => Loss: 6.70229433561834575528\n",
      "Iteration 29067 => Loss: 6.70229378092564598290\n",
      "Iteration 29068 => Loss: 6.70229322630769797087\n",
      "Iteration 29069 => Loss: 6.70229267176447862653\n",
      "Iteration 29070 => Loss: 6.70229211729598617353\n",
      "Iteration 29071 => Loss: 6.70229156290220018377\n",
      "Iteration 29072 => Loss: 6.70229100858312865086\n",
      "Iteration 29073 => Loss: 6.70229045433875203486\n",
      "Iteration 29074 => Loss: 6.70228990016904724314\n",
      "Iteration 29075 => Loss: 6.70228934607403026291\n",
      "Iteration 29076 => Loss: 6.70228879205367356064\n",
      "Iteration 29077 => Loss: 6.70228823810797269545\n",
      "Iteration 29078 => Loss: 6.70228768423691345646\n",
      "Iteration 29079 => Loss: 6.70228713044049051462\n",
      "Iteration 29080 => Loss: 6.70228657671869143542\n",
      "Iteration 29081 => Loss: 6.70228602307150023165\n",
      "Iteration 29082 => Loss: 6.70228546949892400875\n",
      "Iteration 29083 => Loss: 6.70228491600093168046\n",
      "Iteration 29084 => Loss: 6.70228436257752946403\n",
      "Iteration 29085 => Loss: 6.70228380922869959591\n",
      "Iteration 29086 => Loss: 6.70228325595443319429\n",
      "Iteration 29087 => Loss: 6.70228270275471604833\n",
      "Iteration 29088 => Loss: 6.70228214962955082257\n",
      "Iteration 29089 => Loss: 6.70228159657891797707\n",
      "Iteration 29090 => Loss: 6.70228104360280152463\n",
      "Iteration 29091 => Loss: 6.70228049070120146524\n",
      "Iteration 29092 => Loss: 6.70227993787410891713\n",
      "Iteration 29093 => Loss: 6.70227938512150878125\n",
      "Iteration 29094 => Loss: 6.70227883244338862312\n",
      "Iteration 29095 => Loss: 6.70227827983974755455\n",
      "Iteration 29096 => Loss: 6.70227772731056425926\n",
      "Iteration 29097 => Loss: 6.70227717485583252000\n",
      "Iteration 29098 => Loss: 6.70227662247554789587\n",
      "Iteration 29099 => Loss: 6.70227607016969706422\n",
      "Iteration 29100 => Loss: 6.70227551793827380777\n",
      "Iteration 29101 => Loss: 6.70227496578125858662\n",
      "Iteration 29102 => Loss: 6.70227441369864873622\n",
      "Iteration 29103 => Loss: 6.70227386169042738118\n",
      "Iteration 29104 => Loss: 6.70227330975659363332\n",
      "Iteration 29105 => Loss: 6.70227275789713061727\n",
      "Iteration 29106 => Loss: 6.70227220611203300393\n",
      "Iteration 29107 => Loss: 6.70227165440129013518\n",
      "Iteration 29108 => Loss: 6.70227110276488158291\n",
      "Iteration 29109 => Loss: 6.70227055120281711709\n",
      "Iteration 29110 => Loss: 6.70226999971506920417\n",
      "Iteration 29111 => Loss: 6.70226944830163517963\n",
      "Iteration 29112 => Loss: 6.70226889696250793804\n",
      "Iteration 29113 => Loss: 6.70226834569766616312\n",
      "Iteration 29114 => Loss: 6.70226779450711340758\n",
      "Iteration 29115 => Loss: 6.70226724339082746695\n",
      "Iteration 29116 => Loss: 6.70226669234880834125\n",
      "Iteration 29117 => Loss: 6.70226614138104626051\n",
      "Iteration 29118 => Loss: 6.70226559048752523751\n",
      "Iteration 29119 => Loss: 6.70226503966823372593\n",
      "Iteration 29120 => Loss: 6.70226448892316284400\n",
      "Iteration 29121 => Loss: 6.70226393825231525625\n",
      "Iteration 29122 => Loss: 6.70226338765566609368\n",
      "Iteration 29123 => Loss: 6.70226283713320469815\n",
      "Iteration 29124 => Loss: 6.70226228668493106966\n",
      "Iteration 29125 => Loss: 6.70226173631083277371\n",
      "Iteration 29126 => Loss: 6.70226118601088494131\n",
      "Iteration 29127 => Loss: 6.70226063578510000696\n",
      "Iteration 29128 => Loss: 6.70226008563346287161\n",
      "Iteration 29129 => Loss: 6.70225953555594688993\n",
      "Iteration 29130 => Loss: 6.70225898555256183187\n",
      "Iteration 29131 => Loss: 6.70225843562328726932\n",
      "Iteration 29132 => Loss: 6.70225788576811165598\n",
      "Iteration 29133 => Loss: 6.70225733598703676819\n",
      "Iteration 29134 => Loss: 6.70225678628004040149\n",
      "Iteration 29135 => Loss: 6.70225623664711989136\n",
      "Iteration 29136 => Loss: 6.70225568708825836239\n",
      "Iteration 29137 => Loss: 6.70225513760345847913\n",
      "Iteration 29138 => Loss: 6.70225458819269093169\n",
      "Iteration 29139 => Loss: 6.70225403885596371367\n",
      "Iteration 29140 => Loss: 6.70225348959325817333\n",
      "Iteration 29141 => Loss: 6.70225294040456098799\n",
      "Iteration 29142 => Loss: 6.70225239128987304582\n",
      "Iteration 29143 => Loss: 6.70225184224917303055\n",
      "Iteration 29144 => Loss: 6.70225129328246183036\n",
      "Iteration 29145 => Loss: 6.70225074438971990531\n",
      "Iteration 29146 => Loss: 6.70225019557094370271\n",
      "Iteration 29147 => Loss: 6.70224964682612167621\n",
      "Iteration 29148 => Loss: 6.70224909815523872680\n",
      "Iteration 29149 => Loss: 6.70224854955829485448\n",
      "Iteration 29150 => Loss: 6.70224800103527140749\n",
      "Iteration 29151 => Loss: 6.70224745258616483312\n",
      "Iteration 29152 => Loss: 6.70224690421095736781\n",
      "Iteration 29153 => Loss: 6.70224635590964190612\n",
      "Iteration 29154 => Loss: 6.70224580768221489535\n",
      "Iteration 29155 => Loss: 6.70224525952865768375\n",
      "Iteration 29156 => Loss: 6.70224471144896671859\n",
      "Iteration 29157 => Loss: 6.70224416344312690086\n",
      "Iteration 29158 => Loss: 6.70224361551113734237\n",
      "Iteration 29159 => Loss: 6.70224306765297672683\n",
      "Iteration 29160 => Loss: 6.70224251986863706065\n",
      "Iteration 29161 => Loss: 6.70224197215811834383\n",
      "Iteration 29162 => Loss: 6.70224142452139570736\n",
      "Iteration 29163 => Loss: 6.70224087695847181578\n",
      "Iteration 29164 => Loss: 6.70224032946933867549\n",
      "Iteration 29165 => Loss: 6.70223978205397319385\n",
      "Iteration 29166 => Loss: 6.70223923471237092997\n",
      "Iteration 29167 => Loss: 6.70223868744452211388\n",
      "Iteration 29168 => Loss: 6.70223814025041786380\n",
      "Iteration 29169 => Loss: 6.70223759313005107430\n",
      "Iteration 29170 => Loss: 6.70223704608340842270\n",
      "Iteration 29171 => Loss: 6.70223649911047836270\n",
      "Iteration 29172 => Loss: 6.70223595221125201249\n",
      "Iteration 29173 => Loss: 6.70223540538572493119\n",
      "Iteration 29174 => Loss: 6.70223485863387846706\n",
      "Iteration 29175 => Loss: 6.70223431195570906738\n",
      "Iteration 29176 => Loss: 6.70223376535120163311\n",
      "Iteration 29177 => Loss: 6.70223321882034994701\n",
      "Iteration 29178 => Loss: 6.70223267236315134454\n",
      "Iteration 29179 => Loss: 6.70223212597957651582\n",
      "Iteration 29180 => Loss: 6.70223157966963700716\n",
      "Iteration 29181 => Loss: 6.70223103343330617321\n",
      "Iteration 29182 => Loss: 6.70223048727057868490\n",
      "Iteration 29183 => Loss: 6.70222994118144921316\n",
      "Iteration 29184 => Loss: 6.70222939516590621167\n",
      "Iteration 29185 => Loss: 6.70222884922393902229\n",
      "Iteration 29186 => Loss: 6.70222830335553432235\n",
      "Iteration 29187 => Loss: 6.70222775756069566455\n",
      "Iteration 29188 => Loss: 6.70222721183939107448\n",
      "Iteration 29189 => Loss: 6.70222666619163032209\n",
      "Iteration 29190 => Loss: 6.70222612061739564382\n",
      "Iteration 29191 => Loss: 6.70222557511666572339\n",
      "Iteration 29192 => Loss: 6.70222502968945299529\n",
      "Iteration 29193 => Loss: 6.70222448433573347870\n",
      "Iteration 29194 => Loss: 6.70222393905550095639\n",
      "Iteration 29195 => Loss: 6.70222339384875009927\n",
      "Iteration 29196 => Loss: 6.70222284871545426199\n",
      "Iteration 29197 => Loss: 6.70222230365562143817\n",
      "Iteration 29198 => Loss: 6.70222175866923564058\n",
      "Iteration 29199 => Loss: 6.70222121375629154016\n",
      "Iteration 29200 => Loss: 6.70222066891676959699\n",
      "Iteration 29201 => Loss: 6.70222012415066448199\n",
      "Iteration 29202 => Loss: 6.70221957945796376066\n",
      "Iteration 29203 => Loss: 6.70221903483866743301\n",
      "Iteration 29204 => Loss: 6.70221849029275862364\n",
      "Iteration 29205 => Loss: 6.70221794582021335174\n",
      "Iteration 29206 => Loss: 6.70221740142105204541\n",
      "Iteration 29207 => Loss: 6.70221685709524184205\n",
      "Iteration 29208 => Loss: 6.70221631284278096530\n",
      "Iteration 29209 => Loss: 6.70221576866366053338\n",
      "Iteration 29210 => Loss: 6.70221522455785923000\n",
      "Iteration 29211 => Loss: 6.70221468052538416060\n",
      "Iteration 29212 => Loss: 6.70221413656621312072\n",
      "Iteration 29213 => Loss: 6.70221359268034877488\n",
      "Iteration 29214 => Loss: 6.70221304886776358956\n",
      "Iteration 29215 => Loss: 6.70221250512846022929\n",
      "Iteration 29216 => Loss: 6.70221196146243247682\n",
      "Iteration 29217 => Loss: 6.70221141786965191045\n",
      "Iteration 29218 => Loss: 6.70221087435013007649\n",
      "Iteration 29219 => Loss: 6.70221033090384210595\n",
      "Iteration 29220 => Loss: 6.70220978753078444612\n",
      "Iteration 29221 => Loss: 6.70220924423095265610\n",
      "Iteration 29222 => Loss: 6.70220870100432097871\n",
      "Iteration 29223 => Loss: 6.70220815785089651939\n",
      "Iteration 29224 => Loss: 6.70220761477065885003\n",
      "Iteration 29225 => Loss: 6.70220707176359997703\n",
      "Iteration 29226 => Loss: 6.70220652882971190678\n",
      "Iteration 29227 => Loss: 6.70220598596898575749\n",
      "Iteration 29228 => Loss: 6.70220544318141442375\n",
      "Iteration 29229 => Loss: 6.70220490046697747744\n",
      "Iteration 29230 => Loss: 6.70220435782567047767\n",
      "Iteration 29231 => Loss: 6.70220381525748720719\n",
      "Iteration 29232 => Loss: 6.70220327276241434333\n",
      "Iteration 29233 => Loss: 6.70220273034044300431\n",
      "Iteration 29234 => Loss: 6.70220218799156075562\n",
      "Iteration 29235 => Loss: 6.70220164571576138002\n",
      "Iteration 29236 => Loss: 6.70220110351304132479\n",
      "Iteration 29237 => Loss: 6.70220056138337572094\n",
      "Iteration 29238 => Loss: 6.70220001932675923939\n",
      "Iteration 29239 => Loss: 6.70219947734319099197\n",
      "Iteration 29240 => Loss: 6.70219893543264877422\n",
      "Iteration 29241 => Loss: 6.70219839359513436250\n",
      "Iteration 29242 => Loss: 6.70219785183063176959\n",
      "Iteration 29243 => Loss: 6.70219731013912856099\n",
      "Iteration 29244 => Loss: 6.70219676852062473671\n",
      "Iteration 29245 => Loss: 6.70219622697509898046\n",
      "Iteration 29246 => Loss: 6.70219568550254596317\n",
      "Iteration 29247 => Loss: 6.70219514410296124396\n",
      "Iteration 29248 => Loss: 6.70219460277632528289\n",
      "Iteration 29249 => Loss: 6.70219406152263630361\n",
      "Iteration 29250 => Loss: 6.70219352034188187162\n",
      "Iteration 29251 => Loss: 6.70219297923404688788\n",
      "Iteration 29252 => Loss: 6.70219243819912868787\n",
      "Iteration 29253 => Loss: 6.70219189723711661344\n",
      "Iteration 29254 => Loss: 6.70219135634799556556\n",
      "Iteration 29255 => Loss: 6.70219081553176376786\n",
      "Iteration 29256 => Loss: 6.70219027478840523315\n",
      "Iteration 29257 => Loss: 6.70218973411791285599\n",
      "Iteration 29258 => Loss: 6.70218919352027420189\n",
      "Iteration 29259 => Loss: 6.70218865299548038905\n",
      "Iteration 29260 => Loss: 6.70218811254352608842\n",
      "Iteration 29261 => Loss: 6.70218757216439087188\n",
      "Iteration 29262 => Loss: 6.70218703185807740397\n",
      "Iteration 29263 => Loss: 6.70218649162456969748\n",
      "Iteration 29264 => Loss: 6.70218595146385620609\n",
      "Iteration 29265 => Loss: 6.70218541137593160073\n",
      "Iteration 29266 => Loss: 6.70218487136078433508\n",
      "Iteration 29267 => Loss: 6.70218433141840019829\n",
      "Iteration 29268 => Loss: 6.70218379154878007853\n",
      "Iteration 29269 => Loss: 6.70218325175190443588\n",
      "Iteration 29270 => Loss: 6.70218271202776083584\n",
      "Iteration 29271 => Loss: 6.70218217237635283112\n",
      "Iteration 29272 => Loss: 6.70218163279765821727\n",
      "Iteration 29273 => Loss: 6.70218109329167877064\n",
      "Iteration 29274 => Loss: 6.70218055385838962223\n",
      "Iteration 29275 => Loss: 6.70218001449779698930\n",
      "Iteration 29276 => Loss: 6.70217947520987067378\n",
      "Iteration 29277 => Loss: 6.70217893599462755105\n",
      "Iteration 29278 => Loss: 6.70217839685203653488\n",
      "Iteration 29279 => Loss: 6.70217785778209851344\n",
      "Iteration 29280 => Loss: 6.70217731878479749952\n",
      "Iteration 29281 => Loss: 6.70217677986012283498\n",
      "Iteration 29282 => Loss: 6.70217624100807007892\n",
      "Iteration 29283 => Loss: 6.70217570222862857321\n",
      "Iteration 29284 => Loss: 6.70217516352179210060\n",
      "Iteration 29285 => Loss: 6.70217462488754467387\n",
      "Iteration 29286 => Loss: 6.70217408632587208217\n",
      "Iteration 29287 => Loss: 6.70217354783678231911\n",
      "Iteration 29288 => Loss: 6.70217300942024429844\n",
      "Iteration 29289 => Loss: 6.70217247107625624380\n",
      "Iteration 29290 => Loss: 6.70217193280481637885\n",
      "Iteration 29291 => Loss: 6.70217139460590605182\n",
      "Iteration 29292 => Loss: 6.70217085647951993366\n",
      "Iteration 29293 => Loss: 6.70217031842564647803\n",
      "Iteration 29294 => Loss: 6.70216978044427591499\n",
      "Iteration 29295 => Loss: 6.70216924253539048095\n",
      "Iteration 29296 => Loss: 6.70216870469899550500\n",
      "Iteration 29297 => Loss: 6.70216816693507233538\n",
      "Iteration 29298 => Loss: 6.70216762924361830756\n",
      "Iteration 29299 => Loss: 6.70216709162461210525\n",
      "Iteration 29300 => Loss: 6.70216655407805284028\n",
      "Iteration 29301 => Loss: 6.70216601660392807815\n",
      "Iteration 29302 => Loss: 6.70216547920222360801\n",
      "Iteration 29303 => Loss: 6.70216494187293942986\n",
      "Iteration 29304 => Loss: 6.70216440461605511558\n",
      "Iteration 29305 => Loss: 6.70216386743157155337\n",
      "Iteration 29306 => Loss: 6.70216333031947542054\n",
      "Iteration 29307 => Loss: 6.70216279327974451263\n",
      "Iteration 29308 => Loss: 6.70216225631238859961\n",
      "Iteration 29309 => Loss: 6.70216171941738636519\n",
      "Iteration 29310 => Loss: 6.70216118259473425667\n",
      "Iteration 29311 => Loss: 6.70216064584441539864\n",
      "Iteration 29312 => Loss: 6.70216010916642002115\n",
      "Iteration 29313 => Loss: 6.70215957256074901238\n",
      "Iteration 29314 => Loss: 6.70215903602738816147\n",
      "Iteration 29315 => Loss: 6.70215849956631526396\n",
      "Iteration 29316 => Loss: 6.70215796317753742528\n",
      "Iteration 29317 => Loss: 6.70215742686104043457\n",
      "Iteration 29318 => Loss: 6.70215689061680208738\n",
      "Iteration 29319 => Loss: 6.70215635444482860095\n",
      "Iteration 29320 => Loss: 6.70215581834510487624\n",
      "Iteration 29321 => Loss: 6.70215528231761847877\n",
      "Iteration 29322 => Loss: 6.70215474636235875039\n",
      "Iteration 29323 => Loss: 6.70215421047933190835\n",
      "Iteration 29324 => Loss: 6.70215367466850509004\n",
      "Iteration 29325 => Loss: 6.70215313892988273636\n",
      "Iteration 29326 => Loss: 6.70215260326344353103\n",
      "Iteration 29327 => Loss: 6.70215206766919102677\n",
      "Iteration 29328 => Loss: 6.70215153214710923635\n",
      "Iteration 29329 => Loss: 6.70215099669719016617\n",
      "Iteration 29330 => Loss: 6.70215046131942138175\n",
      "Iteration 29331 => Loss: 6.70214992601380110671\n",
      "Iteration 29332 => Loss: 6.70214939078030624842\n",
      "Iteration 29333 => Loss: 6.70214885561893414234\n",
      "Iteration 29334 => Loss: 6.70214832052968212395\n",
      "Iteration 29335 => Loss: 6.70214778551252265970\n",
      "Iteration 29336 => Loss: 6.70214725056746640774\n",
      "Iteration 29337 => Loss: 6.70214671569448849908\n",
      "Iteration 29338 => Loss: 6.70214618089358449282\n",
      "Iteration 29339 => Loss: 6.70214564616474373082\n",
      "Iteration 29340 => Loss: 6.70214511150796621308\n",
      "Iteration 29341 => Loss: 6.70214457692322973514\n",
      "Iteration 29342 => Loss: 6.70214404241052719158\n",
      "Iteration 29343 => Loss: 6.70214350796984792424\n",
      "Iteration 29344 => Loss: 6.70214297360118838043\n",
      "Iteration 29345 => Loss: 6.70214243930453612563\n",
      "Iteration 29346 => Loss: 6.70214190507987250811\n",
      "Iteration 29347 => Loss: 6.70214137092719841604\n",
      "Iteration 29348 => Loss: 6.70214083684651296124\n",
      "Iteration 29349 => Loss: 6.70214030283778150476\n",
      "Iteration 29350 => Loss: 6.70213976890101204020\n",
      "Iteration 29351 => Loss: 6.70213923503618946853\n",
      "Iteration 29352 => Loss: 6.70213870124330313160\n",
      "Iteration 29353 => Loss: 6.70213816752235302943\n",
      "Iteration 29354 => Loss: 6.70213763387332406296\n",
      "Iteration 29355 => Loss: 6.70213710029619491593\n",
      "Iteration 29356 => Loss: 6.70213656679096914104\n",
      "Iteration 29357 => Loss: 6.70213603335763163926\n",
      "Iteration 29358 => Loss: 6.70213549999617530517\n",
      "Iteration 29359 => Loss: 6.70213496670658681609\n",
      "Iteration 29360 => Loss: 6.70213443348886173112\n",
      "Iteration 29361 => Loss: 6.70213390034298495124\n",
      "Iteration 29362 => Loss: 6.70213336726895470008\n",
      "Iteration 29363 => Loss: 6.70213283426675143772\n",
      "Iteration 29364 => Loss: 6.70213230133637871688\n",
      "Iteration 29365 => Loss: 6.70213176847781255674\n",
      "Iteration 29366 => Loss: 6.70213123569105206911\n",
      "Iteration 29367 => Loss: 6.70213070297607771408\n",
      "Iteration 29368 => Loss: 6.70213017033288860347\n",
      "Iteration 29369 => Loss: 6.70212963776147763184\n",
      "Iteration 29370 => Loss: 6.70212910526182881199\n",
      "Iteration 29371 => Loss: 6.70212857283392793306\n",
      "Iteration 29372 => Loss: 6.70212804047778032412\n",
      "Iteration 29373 => Loss: 6.70212750819336466890\n",
      "Iteration 29374 => Loss: 6.70212697598067297378\n",
      "Iteration 29375 => Loss: 6.70212644383970168604\n",
      "Iteration 29376 => Loss: 6.70212591177043304214\n",
      "Iteration 29377 => Loss: 6.70212537977286082480\n",
      "Iteration 29378 => Loss: 6.70212484784697704043\n",
      "Iteration 29379 => Loss: 6.70212431599277191907\n",
      "Iteration 29380 => Loss: 6.70212378421022858532\n",
      "Iteration 29381 => Loss: 6.70212325249934526283\n",
      "Iteration 29382 => Loss: 6.70212272086010774075\n",
      "Iteration 29383 => Loss: 6.70212218929251424271\n",
      "Iteration 29384 => Loss: 6.70212165779654522879\n",
      "Iteration 29385 => Loss: 6.70212112637219892264\n",
      "Iteration 29386 => Loss: 6.70212059501946200157\n",
      "Iteration 29387 => Loss: 6.70212006373831847839\n",
      "Iteration 29388 => Loss: 6.70211953252877457032\n",
      "Iteration 29389 => Loss: 6.70211900139080185568\n",
      "Iteration 29390 => Loss: 6.70211847032440299898\n",
      "Iteration 29391 => Loss: 6.70211793932957000663\n",
      "Iteration 29392 => Loss: 6.70211740840628156235\n",
      "Iteration 29393 => Loss: 6.70211687755454299520\n",
      "Iteration 29394 => Loss: 6.70211634677433387708\n",
      "Iteration 29395 => Loss: 6.70211581606564088531\n",
      "Iteration 29396 => Loss: 6.70211528542846757261\n",
      "Iteration 29397 => Loss: 6.70211475486279351088\n",
      "Iteration 29398 => Loss: 6.70211422436861870011\n",
      "Iteration 29399 => Loss: 6.70211369394592360038\n",
      "Iteration 29400 => Loss: 6.70211316359470909987\n",
      "Iteration 29401 => Loss: 6.70211263331495565865\n",
      "Iteration 29402 => Loss: 6.70211210310665528311\n",
      "Iteration 29403 => Loss: 6.70211157296979909148\n",
      "Iteration 29404 => Loss: 6.70211104290438619557\n",
      "Iteration 29405 => Loss: 6.70211051291039439093\n",
      "Iteration 29406 => Loss: 6.70210998298781746030\n",
      "Iteration 29407 => Loss: 6.70210945313665096279\n",
      "Iteration 29408 => Loss: 6.70210892335688246391\n",
      "Iteration 29409 => Loss: 6.70210839364850485822\n",
      "Iteration 29410 => Loss: 6.70210786401149682945\n",
      "Iteration 29411 => Loss: 6.70210733444586015395\n",
      "Iteration 29412 => Loss: 6.70210680495159216719\n",
      "Iteration 29413 => Loss: 6.70210627552866000656\n",
      "Iteration 29414 => Loss: 6.70210574617707344203\n",
      "Iteration 29415 => Loss: 6.70210521689681471003\n",
      "Iteration 29416 => Loss: 6.70210468768787936966\n",
      "Iteration 29417 => Loss: 6.70210415855025942733\n",
      "Iteration 29418 => Loss: 6.70210362948392912585\n",
      "Iteration 29419 => Loss: 6.70210310048890178791\n",
      "Iteration 29420 => Loss: 6.70210257156515076815\n",
      "Iteration 29421 => Loss: 6.70210204271267340204\n",
      "Iteration 29422 => Loss: 6.70210151393146258414\n",
      "Iteration 29423 => Loss: 6.70210098522149611000\n",
      "Iteration 29424 => Loss: 6.70210045658278197322\n",
      "Iteration 29425 => Loss: 6.70209992801529796935\n",
      "Iteration 29426 => Loss: 6.70209939951903965749\n",
      "Iteration 29427 => Loss: 6.70209887109399282679\n",
      "Iteration 29428 => Loss: 6.70209834274015747724\n",
      "Iteration 29429 => Loss: 6.70209781445751406892\n",
      "Iteration 29430 => Loss: 6.70209728624605638458\n",
      "Iteration 29431 => Loss: 6.70209675810577554245\n",
      "Iteration 29432 => Loss: 6.70209623003666266072\n",
      "Iteration 29433 => Loss: 6.70209570203870796945\n",
      "Iteration 29434 => Loss: 6.70209517411189548142\n",
      "Iteration 29435 => Loss: 6.70209464625622963752\n",
      "Iteration 29436 => Loss: 6.70209411847168823329\n",
      "Iteration 29437 => Loss: 6.70209359075826416330\n",
      "Iteration 29438 => Loss: 6.70209306311594765759\n",
      "Iteration 29439 => Loss: 6.70209253554474404524\n",
      "Iteration 29440 => Loss: 6.70209200804462046364\n",
      "Iteration 29441 => Loss: 6.70209148061557780096\n",
      "Iteration 29442 => Loss: 6.70209095325760184636\n",
      "Iteration 29443 => Loss: 6.70209042597069082348\n",
      "Iteration 29444 => Loss: 6.70208989875483851506\n",
      "Iteration 29445 => Loss: 6.70208937161001916394\n",
      "Iteration 29446 => Loss: 6.70208884453623987554\n",
      "Iteration 29447 => Loss: 6.70208831753348022175\n",
      "Iteration 29448 => Loss: 6.70208779060173664988\n",
      "Iteration 29449 => Loss: 6.70208726374099761358\n",
      "Iteration 29450 => Loss: 6.70208673695124979020\n",
      "Iteration 29451 => Loss: 6.70208621023248163340\n",
      "Iteration 29452 => Loss: 6.70208568358469669590\n",
      "Iteration 29453 => Loss: 6.70208515700787810232\n",
      "Iteration 29454 => Loss: 6.70208463050201785904\n",
      "Iteration 29455 => Loss: 6.70208410406709731433\n",
      "Iteration 29456 => Loss: 6.70208357770311646817\n",
      "Iteration 29457 => Loss: 6.70208305141006288608\n",
      "Iteration 29458 => Loss: 6.70208252518792679808\n",
      "Iteration 29459 => Loss: 6.70208199903670287512\n",
      "Iteration 29460 => Loss: 6.70208147295637512997\n",
      "Iteration 29461 => Loss: 6.70208094694694089810\n",
      "Iteration 29462 => Loss: 6.70208042100837886323\n",
      "Iteration 29463 => Loss: 6.70207989514069257808\n",
      "Iteration 29464 => Loss: 6.70207936934386339090\n",
      "Iteration 29465 => Loss: 6.70207884361789218985\n",
      "Iteration 29466 => Loss: 6.70207831796275232961\n",
      "Iteration 29467 => Loss: 6.70207779237844913922\n",
      "Iteration 29468 => Loss: 6.70207726686497018420\n",
      "Iteration 29469 => Loss: 6.70207674142229947734\n",
      "Iteration 29470 => Loss: 6.70207621605043968316\n",
      "Iteration 29471 => Loss: 6.70207569074937037357\n",
      "Iteration 29472 => Loss: 6.70207516551908444313\n",
      "Iteration 29473 => Loss: 6.70207464035957212189\n",
      "Iteration 29474 => Loss: 6.70207411527083429803\n",
      "Iteration 29475 => Loss: 6.70207359025283722076\n",
      "Iteration 29476 => Loss: 6.70207306530559687729\n",
      "Iteration 29477 => Loss: 6.70207254042909195135\n",
      "Iteration 29478 => Loss: 6.70207201562330912026\n",
      "Iteration 29479 => Loss: 6.70207149088825193672\n",
      "Iteration 29480 => Loss: 6.70207096622389553175\n",
      "Iteration 29481 => Loss: 6.70207044163023635264\n",
      "Iteration 29482 => Loss: 6.70206991710726995848\n",
      "Iteration 29483 => Loss: 6.70206939265498924385\n",
      "Iteration 29484 => Loss: 6.70206886827337022794\n",
      "Iteration 29485 => Loss: 6.70206834396241379892\n",
      "Iteration 29486 => Loss: 6.70206781972211196319\n",
      "Iteration 29487 => Loss: 6.70206729555244695717\n",
      "Iteration 29488 => Loss: 6.70206677145341433999\n",
      "Iteration 29489 => Loss: 6.70206624742500434166\n",
      "Iteration 29490 => Loss: 6.70206572346721429767\n",
      "Iteration 29491 => Loss: 6.70206519958002022719\n",
      "Iteration 29492 => Loss: 6.70206467576342301840\n",
      "Iteration 29493 => Loss: 6.70206415201739780230\n",
      "Iteration 29494 => Loss: 6.70206362834196500700\n",
      "Iteration 29495 => Loss: 6.70206310473708821718\n",
      "Iteration 29496 => Loss: 6.70206258120276388013\n",
      "Iteration 29497 => Loss: 6.70206205773899554856\n",
      "Iteration 29498 => Loss: 6.70206153434575746530\n",
      "Iteration 29499 => Loss: 6.70206101102304785400\n",
      "Iteration 29500 => Loss: 6.70206048777085250379\n",
      "Iteration 29501 => Loss: 6.70205996458917407921\n",
      "Iteration 29502 => Loss: 6.70205944147798859944\n",
      "Iteration 29503 => Loss: 6.70205891843729162360\n",
      "Iteration 29504 => Loss: 6.70205839546707871079\n",
      "Iteration 29505 => Loss: 6.70205787256733742652\n",
      "Iteration 29506 => Loss: 6.70205734973805178356\n",
      "Iteration 29507 => Loss: 6.70205682697921734103\n",
      "Iteration 29508 => Loss: 6.70205630429083054622\n",
      "Iteration 29509 => Loss: 6.70205578167287185920\n",
      "Iteration 29510 => Loss: 6.70205525912533417454\n",
      "Iteration 29511 => Loss: 6.70205473664821571589\n",
      "Iteration 29512 => Loss: 6.70205421424149871967\n",
      "Iteration 29513 => Loss: 6.70205369190516986322\n",
      "Iteration 29514 => Loss: 6.70205316963923181106\n",
      "Iteration 29515 => Loss: 6.70205264744366857599\n",
      "Iteration 29516 => Loss: 6.70205212531847127622\n",
      "Iteration 29517 => Loss: 6.70205160326362925360\n",
      "Iteration 29518 => Loss: 6.70205108127913362637\n",
      "Iteration 29519 => Loss: 6.70205055936497551272\n",
      "Iteration 29520 => Loss: 6.70205003752114603088\n",
      "Iteration 29521 => Loss: 6.70204951574763718725\n",
      "Iteration 29522 => Loss: 6.70204899404443565913\n",
      "Iteration 29523 => Loss: 6.70204847241153345294\n",
      "Iteration 29524 => Loss: 6.70204795084891902235\n",
      "Iteration 29525 => Loss: 6.70204742935658703828\n",
      "Iteration 29526 => Loss: 6.70204690793452861897\n",
      "Iteration 29527 => Loss: 6.70204638658273221807\n",
      "Iteration 29528 => Loss: 6.70204586530118273657\n",
      "Iteration 29529 => Loss: 6.70204534408988283900\n",
      "Iteration 29530 => Loss: 6.70204482294881120907\n",
      "Iteration 29531 => Loss: 6.70204430187796251772\n",
      "Iteration 29532 => Loss: 6.70204378087732699498\n",
      "Iteration 29533 => Loss: 6.70204325994690286450\n",
      "Iteration 29534 => Loss: 6.70204273908666969817\n",
      "Iteration 29535 => Loss: 6.70204221829662483145\n",
      "Iteration 29536 => Loss: 6.70204169757675227714\n",
      "Iteration 29537 => Loss: 6.70204117692705469977\n",
      "Iteration 29538 => Loss: 6.70204065634750634217\n",
      "Iteration 29539 => Loss: 6.70204013583810986887\n",
      "Iteration 29540 => Loss: 6.70203961539885195720\n",
      "Iteration 29541 => Loss: 6.70203909502972283718\n",
      "Iteration 29542 => Loss: 6.70203857473071096251\n",
      "Iteration 29543 => Loss: 6.70203805450181633319\n",
      "Iteration 29544 => Loss: 6.70203753434301852110\n",
      "Iteration 29545 => Loss: 6.70203701425431042082\n",
      "Iteration 29546 => Loss: 6.70203649423568670329\n",
      "Iteration 29547 => Loss: 6.70203597428713138129\n",
      "Iteration 29548 => Loss: 6.70203545440864623117\n",
      "Iteration 29549 => Loss: 6.70203493460021171302\n",
      "Iteration 29550 => Loss: 6.70203441486182338593\n",
      "Iteration 29551 => Loss: 6.70203389519346082182\n",
      "Iteration 29552 => Loss: 6.70203337559512934973\n",
      "Iteration 29553 => Loss: 6.70203285606681209430\n",
      "Iteration 29554 => Loss: 6.70203233660850994369\n",
      "Iteration 29555 => Loss: 6.70203181722019802891\n",
      "Iteration 29556 => Loss: 6.70203129790187190906\n",
      "Iteration 29557 => Loss: 6.70203077865352625508\n",
      "Iteration 29558 => Loss: 6.70203025947515218519\n",
      "Iteration 29559 => Loss: 6.70202974036673460034\n",
      "Iteration 29560 => Loss: 6.70202922132826639512\n",
      "Iteration 29561 => Loss: 6.70202870235973779955\n",
      "Iteration 29562 => Loss: 6.70202818346114348458\n",
      "Iteration 29563 => Loss: 6.70202766463246746298\n",
      "Iteration 29564 => Loss: 6.70202714587370884658\n",
      "Iteration 29565 => Loss: 6.70202662718484454274\n",
      "Iteration 29566 => Loss: 6.70202610856588076871\n",
      "Iteration 29567 => Loss: 6.70202559001679887274\n",
      "Iteration 29568 => Loss: 6.70202507153758908487\n",
      "Iteration 29569 => Loss: 6.70202455312824518785\n",
      "Iteration 29570 => Loss: 6.70202403478876007625\n",
      "Iteration 29571 => Loss: 6.70202351651911865105\n",
      "Iteration 29572 => Loss: 6.70202299831931380680\n",
      "Iteration 29573 => Loss: 6.70202248018934021445\n",
      "Iteration 29574 => Loss: 6.70202196212918543949\n",
      "Iteration 29575 => Loss: 6.70202144413883438290\n",
      "Iteration 29576 => Loss: 6.70202092621828171559\n",
      "Iteration 29577 => Loss: 6.70202040836752299668\n",
      "Iteration 29578 => Loss: 6.70201989058653957443\n",
      "Iteration 29579 => Loss: 6.70201937287533322518\n",
      "Iteration 29580 => Loss: 6.70201885523388529720\n",
      "Iteration 29581 => Loss: 6.70201833766219046140\n",
      "Iteration 29582 => Loss: 6.70201782016023717148\n",
      "Iteration 29583 => Loss: 6.70201730272801743382\n",
      "Iteration 29584 => Loss: 6.70201678536552503118\n",
      "Iteration 29585 => Loss: 6.70201626807274219999\n",
      "Iteration 29586 => Loss: 6.70201575084966894025\n",
      "Iteration 29587 => Loss: 6.70201523369629015292\n",
      "Iteration 29588 => Loss: 6.70201471661259517987\n",
      "Iteration 29589 => Loss: 6.70201419959858224473\n",
      "Iteration 29590 => Loss: 6.70201368265423447212\n",
      "Iteration 29591 => Loss: 6.70201316577954564480\n",
      "Iteration 29592 => Loss: 6.70201264897450155189\n",
      "Iteration 29593 => Loss: 6.70201213223909952887\n",
      "Iteration 29594 => Loss: 6.70201161557332980578\n",
      "Iteration 29595 => Loss: 6.70201109897718261266\n",
      "Iteration 29596 => Loss: 6.70201058245064018593\n",
      "Iteration 29597 => Loss: 6.70201006599370607830\n",
      "Iteration 29598 => Loss: 6.70200954960636252622\n",
      "Iteration 29599 => Loss: 6.70200903328860153607\n",
      "Iteration 29600 => Loss: 6.70200851704041244972\n",
      "Iteration 29601 => Loss: 6.70200800086179082626\n",
      "Iteration 29602 => Loss: 6.70200748475272511939\n",
      "Iteration 29603 => Loss: 6.70200696871320200643\n",
      "Iteration 29604 => Loss: 6.70200645274321171740\n",
      "Iteration 29605 => Loss: 6.70200593684275425232\n",
      "Iteration 29606 => Loss: 6.70200542101181806487\n",
      "Iteration 29607 => Loss: 6.70200490525038627965\n",
      "Iteration 29608 => Loss: 6.70200438955844735034\n",
      "Iteration 29609 => Loss: 6.70200387393600394148\n",
      "Iteration 29610 => Loss: 6.70200335838304273040\n",
      "Iteration 29611 => Loss: 6.70200284289954684169\n",
      "Iteration 29612 => Loss: 6.70200232748551805173\n",
      "Iteration 29613 => Loss: 6.70200181214093326787\n",
      "Iteration 29614 => Loss: 6.70200129686579870736\n",
      "Iteration 29615 => Loss: 6.70200078166009394209\n",
      "Iteration 29616 => Loss: 6.70200026652381275483\n",
      "Iteration 29617 => Loss: 6.70199975145695692191\n",
      "Iteration 29618 => Loss: 6.70199923645949091622\n",
      "Iteration 29619 => Loss: 6.70199872153142539588\n",
      "Iteration 29620 => Loss: 6.70199820667275147912\n",
      "Iteration 29621 => Loss: 6.70199769188344784965\n",
      "Iteration 29622 => Loss: 6.70199717716351983654\n",
      "Iteration 29623 => Loss: 6.70199666251293990626\n",
      "Iteration 29624 => Loss: 6.70199614793172138150\n",
      "Iteration 29625 => Loss: 6.70199563341983761688\n",
      "Iteration 29626 => Loss: 6.70199511897728239518\n",
      "Iteration 29627 => Loss: 6.70199460460405038731\n",
      "Iteration 29628 => Loss: 6.70199409030013271149\n",
      "Iteration 29629 => Loss: 6.70199357606550982780\n",
      "Iteration 29630 => Loss: 6.70199306190018706531\n",
      "Iteration 29631 => Loss: 6.70199254780414310773\n",
      "Iteration 29632 => Loss: 6.70199203377737795506\n",
      "Iteration 29633 => Loss: 6.70199151981987562010\n",
      "Iteration 29634 => Loss: 6.70199100593163255013\n",
      "Iteration 29635 => Loss: 6.70199049211262742887\n",
      "Iteration 29636 => Loss: 6.70198997836286913810\n",
      "Iteration 29637 => Loss: 6.70198946468233547336\n",
      "Iteration 29638 => Loss: 6.70198895107101755286\n",
      "Iteration 29639 => Loss: 6.70198843752890827119\n",
      "Iteration 29640 => Loss: 6.70198792405600762834\n",
      "Iteration 29641 => Loss: 6.70198741065228720259\n",
      "Iteration 29642 => Loss: 6.70198689731775232303\n",
      "Iteration 29643 => Loss: 6.70198638405238966698\n",
      "Iteration 29644 => Loss: 6.70198587085618946446\n",
      "Iteration 29645 => Loss: 6.70198535772914372188\n",
      "Iteration 29646 => Loss: 6.70198484467123556385\n",
      "Iteration 29647 => Loss: 6.70198433168246765490\n",
      "Iteration 29648 => Loss: 6.70198381876282578418\n",
      "Iteration 29649 => Loss: 6.70198330591229662900\n",
      "Iteration 29650 => Loss: 6.70198279313087308395\n",
      "Iteration 29651 => Loss: 6.70198228041854893178\n",
      "Iteration 29652 => Loss: 6.70198176777531262616\n",
      "Iteration 29653 => Loss: 6.70198125520115350895\n",
      "Iteration 29654 => Loss: 6.70198074269607158016\n",
      "Iteration 29655 => Loss: 6.70198023026004641167\n",
      "Iteration 29656 => Loss: 6.70197971789306556900\n",
      "Iteration 29657 => Loss: 6.70197920559512816396\n",
      "Iteration 29658 => Loss: 6.70197869336622709113\n",
      "Iteration 29659 => Loss: 6.70197818120634636330\n",
      "Iteration 29660 => Loss: 6.70197766911547798685\n",
      "Iteration 29661 => Loss: 6.70197715709362018544\n",
      "Iteration 29662 => Loss: 6.70197664514074809006\n",
      "Iteration 29663 => Loss: 6.70197613325686969432\n",
      "Iteration 29664 => Loss: 6.70197562144196190559\n",
      "Iteration 29665 => Loss: 6.70197510969602205932\n",
      "Iteration 29666 => Loss: 6.70197459801904127374\n",
      "Iteration 29667 => Loss: 6.70197408641101421978\n",
      "Iteration 29668 => Loss: 6.70197357487192313386\n",
      "Iteration 29669 => Loss: 6.70197306340175646966\n",
      "Iteration 29670 => Loss: 6.70197255200051333901\n",
      "Iteration 29671 => Loss: 6.70197204066818397195\n",
      "Iteration 29672 => Loss: 6.70197152940475238125\n",
      "Iteration 29673 => Loss: 6.70197101821021679058\n",
      "Iteration 29674 => Loss: 6.70197050708456654178\n",
      "Iteration 29675 => Loss: 6.70196999602778831218\n",
      "Iteration 29676 => Loss: 6.70196948503987321999\n",
      "Iteration 29677 => Loss: 6.70196897412081504797\n",
      "Iteration 29678 => Loss: 6.70196846327060580251\n",
      "Iteration 29679 => Loss: 6.70196795248923216093\n",
      "Iteration 29680 => Loss: 6.70196744177668524145\n",
      "Iteration 29681 => Loss: 6.70196693113295616229\n",
      "Iteration 29682 => Loss: 6.70196642055803781801\n",
      "Iteration 29683 => Loss: 6.70196591005192132684\n",
      "Iteration 29684 => Loss: 6.70196539961459336610\n",
      "Iteration 29685 => Loss: 6.70196488924605038306\n",
      "Iteration 29686 => Loss: 6.70196437894627194964\n",
      "Iteration 29687 => Loss: 6.70196386871526428308\n",
      "Iteration 29688 => Loss: 6.70196335855300162621\n",
      "Iteration 29689 => Loss: 6.70196284845949197262\n",
      "Iteration 29690 => Loss: 6.70196233843471222968\n",
      "Iteration 29691 => Loss: 6.70196182847865795651\n",
      "Iteration 29692 => Loss: 6.70196131859132826492\n",
      "Iteration 29693 => Loss: 6.70196080877269650955\n",
      "Iteration 29694 => Loss: 6.70196029902277068402\n",
      "Iteration 29695 => Loss: 6.70195978934153124840\n",
      "Iteration 29696 => Loss: 6.70195927972896488001\n",
      "Iteration 29697 => Loss: 6.70195877018507601974\n",
      "Iteration 29698 => Loss: 6.70195826070984512768\n",
      "Iteration 29699 => Loss: 6.70195775130326598656\n",
      "Iteration 29700 => Loss: 6.70195724196532971462\n",
      "Iteration 29701 => Loss: 6.70195673269602831823\n",
      "Iteration 29702 => Loss: 6.70195622349534581019\n",
      "Iteration 29703 => Loss: 6.70195571436328396686\n",
      "Iteration 29704 => Loss: 6.70195520529982857738\n",
      "Iteration 29705 => Loss: 6.70195469630497075997\n",
      "Iteration 29706 => Loss: 6.70195418737869186288\n",
      "Iteration 29707 => Loss: 6.70195367852099277428\n",
      "Iteration 29708 => Loss: 6.70195316973186905329\n",
      "Iteration 29709 => Loss: 6.70195266101129671910\n",
      "Iteration 29710 => Loss: 6.70195215235927932440\n",
      "Iteration 29711 => Loss: 6.70195164377580532289\n",
      "Iteration 29712 => Loss: 6.70195113526085783917\n",
      "Iteration 29713 => Loss: 6.70195062681443687325\n",
      "Iteration 29714 => Loss: 6.70195011843652554973\n",
      "Iteration 29715 => Loss: 6.70194961012711409865\n",
      "Iteration 29716 => Loss: 6.70194910188620607272\n",
      "Iteration 29717 => Loss: 6.70194859371378104385\n",
      "Iteration 29718 => Loss: 6.70194808560982924206\n",
      "Iteration 29719 => Loss: 6.70194757757434889101\n",
      "Iteration 29720 => Loss: 6.70194706960731778622\n",
      "Iteration 29721 => Loss: 6.70194656170874036860\n",
      "Iteration 29722 => Loss: 6.70194605387860420365\n",
      "Iteration 29723 => Loss: 6.70194554611690040957\n",
      "Iteration 29724 => Loss: 6.70194503842361655188\n",
      "Iteration 29725 => Loss: 6.70194453079874374879\n",
      "Iteration 29726 => Loss: 6.70194402324227223033\n",
      "Iteration 29727 => Loss: 6.70194351575419311473\n",
      "Iteration 29728 => Loss: 6.70194300833449574384\n",
      "Iteration 29729 => Loss: 6.70194250098317922948\n",
      "Iteration 29730 => Loss: 6.70194199370022491991\n",
      "Iteration 29731 => Loss: 6.70194148648562748605\n",
      "Iteration 29732 => Loss: 6.70194097933937538158\n",
      "Iteration 29733 => Loss: 6.70194047226146327745\n",
      "Iteration 29734 => Loss: 6.70193996525187696278\n",
      "Iteration 29735 => Loss: 6.70193945831061466123\n",
      "Iteration 29736 => Loss: 6.70193895143766749101\n",
      "Iteration 29737 => Loss: 6.70193844463300969494\n",
      "Iteration 29738 => Loss: 6.70193793789664926663\n",
      "Iteration 29739 => Loss: 6.70193743122857199523\n",
      "Iteration 29740 => Loss: 6.70193692462876811078\n",
      "Iteration 29741 => Loss: 6.70193641809722873148\n",
      "Iteration 29742 => Loss: 6.70193591163394319921\n",
      "Iteration 29743 => Loss: 6.70193540523890796123\n",
      "Iteration 29744 => Loss: 6.70193489891210170128\n",
      "Iteration 29745 => Loss: 6.70193439265353330114\n",
      "Iteration 29746 => Loss: 6.70193388646317256274\n",
      "Iteration 29747 => Loss: 6.70193338034102925604\n",
      "Iteration 29748 => Loss: 6.70193287428708117659\n",
      "Iteration 29749 => Loss: 6.70193236830132654802\n",
      "Iteration 29750 => Loss: 6.70193186238375115948\n",
      "Iteration 29751 => Loss: 6.70193135653434612919\n",
      "Iteration 29752 => Loss: 6.70193085075310968080\n",
      "Iteration 29753 => Loss: 6.70193034504002493890\n",
      "Iteration 29754 => Loss: 6.70192983939508923896\n",
      "Iteration 29755 => Loss: 6.70192933381828392925\n",
      "Iteration 29756 => Loss: 6.70192882830960634521\n",
      "Iteration 29757 => Loss: 6.70192832286904671690\n",
      "Iteration 29758 => Loss: 6.70192781749659260981\n",
      "Iteration 29759 => Loss: 6.70192731219224135941\n",
      "Iteration 29760 => Loss: 6.70192680695597786666\n",
      "Iteration 29761 => Loss: 6.70192630178779058525\n",
      "Iteration 29762 => Loss: 6.70192579668768484424\n",
      "Iteration 29763 => Loss: 6.70192529165563755100\n",
      "Iteration 29764 => Loss: 6.70192478669164337646\n",
      "Iteration 29765 => Loss: 6.70192428179568633340\n",
      "Iteration 29766 => Loss: 6.70192377696777086271\n",
      "Iteration 29767 => Loss: 6.70192327220788097719\n",
      "Iteration 29768 => Loss: 6.70192276751600424234\n",
      "Iteration 29769 => Loss: 6.70192226289214421087\n",
      "Iteration 29770 => Loss: 6.70192175833626446746\n",
      "Iteration 29771 => Loss: 6.70192125384838188751\n",
      "Iteration 29772 => Loss: 6.70192074942848670105\n",
      "Iteration 29773 => Loss: 6.70192024507656025634\n",
      "Iteration 29774 => Loss: 6.70191974079258923069\n",
      "Iteration 29775 => Loss: 6.70191923657657540048\n",
      "Iteration 29776 => Loss: 6.70191873242850011394\n",
      "Iteration 29777 => Loss: 6.70191822834835537748\n",
      "Iteration 29778 => Loss: 6.70191772433614385562\n",
      "Iteration 29779 => Loss: 6.70191722039184867299\n",
      "Iteration 29780 => Loss: 6.70191671651545473054\n",
      "Iteration 29781 => Loss: 6.70191621270696114010\n",
      "Iteration 29782 => Loss: 6.70191570896635546717\n",
      "Iteration 29783 => Loss: 6.70191520529362705361\n",
      "Iteration 29784 => Loss: 6.70191470168876612945\n",
      "Iteration 29785 => Loss: 6.70191419815177003017\n",
      "Iteration 29786 => Loss: 6.70191369468262543307\n",
      "Iteration 29787 => Loss: 6.70191319128132612093\n",
      "Iteration 29788 => Loss: 6.70191268794785255380\n",
      "Iteration 29789 => Loss: 6.70191218468221006077\n",
      "Iteration 29790 => Loss: 6.70191168148438087826\n",
      "Iteration 29791 => Loss: 6.70191117835435878902\n",
      "Iteration 29792 => Loss: 6.70191067529212780585\n",
      "Iteration 29793 => Loss: 6.70191017229768259966\n",
      "Iteration 29794 => Loss: 6.70190966937102761136\n",
      "Iteration 29795 => Loss: 6.70190916651213619559\n",
      "Iteration 29796 => Loss: 6.70190866372100302328\n",
      "Iteration 29797 => Loss: 6.70190816099762365354\n",
      "Iteration 29798 => Loss: 6.70190765834198476369\n",
      "Iteration 29799 => Loss: 6.70190715575407658378\n",
      "Iteration 29800 => Loss: 6.70190665323389556107\n",
      "Iteration 29801 => Loss: 6.70190615078143636651\n",
      "Iteration 29802 => Loss: 6.70190564839667324293\n",
      "Iteration 29803 => Loss: 6.70190514607960174942\n",
      "Iteration 29804 => Loss: 6.70190464383023343231\n",
      "Iteration 29805 => Loss: 6.70190414164852921175\n",
      "Iteration 29806 => Loss: 6.70190363953449796952\n",
      "Iteration 29807 => Loss: 6.70190313748812549477\n",
      "Iteration 29808 => Loss: 6.70190263550940468207\n",
      "Iteration 29809 => Loss: 6.70190213359832576145\n",
      "Iteration 29810 => Loss: 6.70190163175488073932\n",
      "Iteration 29811 => Loss: 6.70190112997906162207\n",
      "Iteration 29812 => Loss: 6.70190062827085508701\n",
      "Iteration 29813 => Loss: 6.70190012663025136419\n",
      "Iteration 29814 => Loss: 6.70189962505724246000\n",
      "Iteration 29815 => Loss: 6.70189912355182570991\n",
      "Iteration 29816 => Loss: 6.70189862211398246217\n",
      "Iteration 29817 => Loss: 6.70189812074370738770\n",
      "Iteration 29818 => Loss: 6.70189761944099782198\n",
      "Iteration 29819 => Loss: 6.70189711820583511326\n",
      "Iteration 29820 => Loss: 6.70189661703820949157\n",
      "Iteration 29821 => Loss: 6.70189611593812717416\n",
      "Iteration 29822 => Loss: 6.70189561490556062751\n",
      "Iteration 29823 => Loss: 6.70189511394050541071\n",
      "Iteration 29824 => Loss: 6.70189461304296685284\n",
      "Iteration 29825 => Loss: 6.70189411221291209131\n",
      "Iteration 29826 => Loss: 6.70189361145034911971\n",
      "Iteration 29827 => Loss: 6.70189311075526461536\n",
      "Iteration 29828 => Loss: 6.70189261012764703196\n",
      "Iteration 29829 => Loss: 6.70189210956749104042\n",
      "Iteration 29830 => Loss: 6.70189160907478420626\n",
      "Iteration 29831 => Loss: 6.70189110864951675950\n",
      "Iteration 29832 => Loss: 6.70189060829168337108\n",
      "Iteration 29833 => Loss: 6.70189010800126983014\n",
      "Iteration 29834 => Loss: 6.70188960777828057758\n",
      "Iteration 29835 => Loss: 6.70188910762269074439\n",
      "Iteration 29836 => Loss: 6.70188860753449677787\n",
      "Iteration 29837 => Loss: 6.70188810751369246077\n",
      "Iteration 29838 => Loss: 6.70188760756025736498\n",
      "Iteration 29839 => Loss: 6.70188710767420214864\n",
      "Iteration 29840 => Loss: 6.70188660785549750187\n",
      "Iteration 29841 => Loss: 6.70188610810414253649\n",
      "Iteration 29842 => Loss: 6.70188560842013547614\n",
      "Iteration 29843 => Loss: 6.70188510880345500453\n",
      "Iteration 29844 => Loss: 6.70188460925410645075\n",
      "Iteration 29845 => Loss: 6.70188410977206849850\n",
      "Iteration 29846 => Loss: 6.70188361035733226601\n",
      "Iteration 29847 => Loss: 6.70188311100989508873\n",
      "Iteration 29848 => Loss: 6.70188261172974630853\n",
      "Iteration 29849 => Loss: 6.70188211251687082637\n",
      "Iteration 29850 => Loss: 6.70188161337126953043\n",
      "Iteration 29851 => Loss: 6.70188111429292998622\n",
      "Iteration 29852 => Loss: 6.70188061528183620652\n",
      "Iteration 29853 => Loss: 6.70188011633798019773\n",
      "Iteration 29854 => Loss: 6.70187961746136462438\n",
      "Iteration 29855 => Loss: 6.70187911865197616379\n",
      "Iteration 29856 => Loss: 6.70187861990979083515\n",
      "Iteration 29857 => Loss: 6.70187812123481663207\n",
      "Iteration 29858 => Loss: 6.70187762262704112004\n",
      "Iteration 29859 => Loss: 6.70187712408644653550\n",
      "Iteration 29860 => Loss: 6.70187662561303998388\n",
      "Iteration 29861 => Loss: 6.70187612720679393163\n",
      "Iteration 29862 => Loss: 6.70187562886771193149\n",
      "Iteration 29863 => Loss: 6.70187513059577977259\n",
      "Iteration 29864 => Loss: 6.70187463239099567858\n",
      "Iteration 29865 => Loss: 6.70187413425333300410\n",
      "Iteration 29866 => Loss: 6.70187363618280151911\n",
      "Iteration 29867 => Loss: 6.70187313817938790095\n",
      "Iteration 29868 => Loss: 6.70187264024307260968\n",
      "Iteration 29869 => Loss: 6.70187214237386719162\n",
      "Iteration 29870 => Loss: 6.70187164457173789600\n",
      "Iteration 29871 => Loss: 6.70187114683669182824\n",
      "Iteration 29872 => Loss: 6.70187064916871744202\n",
      "Iteration 29873 => Loss: 6.70187015156779430924\n",
      "Iteration 29874 => Loss: 6.70186965403393131169\n",
      "Iteration 29875 => Loss: 6.70186915656711335032\n",
      "Iteration 29876 => Loss: 6.70186865916732532611\n",
      "Iteration 29877 => Loss: 6.70186816183455835727\n",
      "Iteration 29878 => Loss: 6.70186766456880889109\n",
      "Iteration 29879 => Loss: 6.70186716737007426303\n",
      "Iteration 29880 => Loss: 6.70186667023832782775\n",
      "Iteration 29881 => Loss: 6.70186617317357136159\n",
      "Iteration 29882 => Loss: 6.70186567617579509459\n",
      "Iteration 29883 => Loss: 6.70186517924499369769\n",
      "Iteration 29884 => Loss: 6.70186468238115118368\n",
      "Iteration 29885 => Loss: 6.70186418558425955894\n",
      "Iteration 29886 => Loss: 6.70186368885430905351\n",
      "Iteration 29887 => Loss: 6.70186319219129167379\n",
      "Iteration 29888 => Loss: 6.70186269559521097250\n",
      "Iteration 29889 => Loss: 6.70186219906603675156\n",
      "Iteration 29890 => Loss: 6.70186170260377345187\n",
      "Iteration 29891 => Loss: 6.70186120620840242168\n",
      "Iteration 29892 => Loss: 6.70186070987992987824\n",
      "Iteration 29893 => Loss: 6.70186021361833272891\n",
      "Iteration 29894 => Loss: 6.70185971742361008552\n",
      "Iteration 29895 => Loss: 6.70185922129574862538\n",
      "Iteration 29896 => Loss: 6.70185872523473769036\n",
      "Iteration 29897 => Loss: 6.70185822924056751049\n",
      "Iteration 29898 => Loss: 6.70185773331324075031\n",
      "Iteration 29899 => Loss: 6.70185723745273431717\n",
      "Iteration 29900 => Loss: 6.70185674165905265198\n",
      "Iteration 29901 => Loss: 6.70185624593217443845\n",
      "Iteration 29902 => Loss: 6.70185575027209345933\n",
      "Iteration 29903 => Loss: 6.70185525467880260919\n",
      "Iteration 29904 => Loss: 6.70185475915229833532\n",
      "Iteration 29905 => Loss: 6.70185426369256287416\n",
      "Iteration 29906 => Loss: 6.70185376829959178480\n",
      "Iteration 29907 => Loss: 6.70185327297336819186\n",
      "Iteration 29908 => Loss: 6.70185277771390097712\n",
      "Iteration 29909 => Loss: 6.70185228252116349523\n",
      "Iteration 29910 => Loss: 6.70185178739515752255\n",
      "Iteration 29911 => Loss: 6.70185129233586263098\n",
      "Iteration 29912 => Loss: 6.70185079734328237322\n",
      "Iteration 29913 => Loss: 6.70185030241740253842\n",
      "Iteration 29914 => Loss: 6.70184980755821069209\n",
      "Iteration 29915 => Loss: 6.70184931276570594605\n",
      "Iteration 29916 => Loss: 6.70184881803986964854\n",
      "Iteration 29917 => Loss: 6.70184832338070268776\n",
      "Iteration 29918 => Loss: 6.70184782878818552376\n",
      "Iteration 29919 => Loss: 6.70184733426231282749\n",
      "Iteration 29920 => Loss: 6.70184683980308548712\n",
      "Iteration 29921 => Loss: 6.70184634541048129819\n",
      "Iteration 29922 => Loss: 6.70184585108449404345\n",
      "Iteration 29923 => Loss: 6.70184535682512905197\n",
      "Iteration 29924 => Loss: 6.70184486263235523751\n",
      "Iteration 29925 => Loss: 6.70184436850618059367\n",
      "Iteration 29926 => Loss: 6.70184387444658469235\n",
      "Iteration 29927 => Loss: 6.70184338045356398084\n",
      "Iteration 29928 => Loss: 6.70184288652710602463\n",
      "Iteration 29929 => Loss: 6.70184239266721171191\n",
      "Iteration 29930 => Loss: 6.70184189887386239093\n",
      "Iteration 29931 => Loss: 6.70184140514704740355\n",
      "Iteration 29932 => Loss: 6.70184091148676674976\n",
      "Iteration 29933 => Loss: 6.70184041789300355418\n",
      "Iteration 29934 => Loss: 6.70183992436575159957\n",
      "Iteration 29935 => Loss: 6.70183943090500910955\n",
      "Iteration 29936 => Loss: 6.70183893751075476786\n",
      "Iteration 29937 => Loss: 6.70183844418299035084\n",
      "Iteration 29938 => Loss: 6.70183795092169987129\n",
      "Iteration 29939 => Loss: 6.70183745772687178288\n",
      "Iteration 29940 => Loss: 6.70183696459850519744\n",
      "Iteration 29941 => Loss: 6.70183647153658590412\n",
      "Iteration 29942 => Loss: 6.70183597854110857384\n",
      "Iteration 29943 => Loss: 6.70183548561206077210\n",
      "Iteration 29944 => Loss: 6.70183499274943983437\n",
      "Iteration 29945 => Loss: 6.70183449995322444437\n",
      "Iteration 29946 => Loss: 6.70183400722341726663\n",
      "Iteration 29947 => Loss: 6.70183351456000675483\n",
      "Iteration 29948 => Loss: 6.70183302196298491538\n",
      "Iteration 29949 => Loss: 6.70183252943234197829\n",
      "Iteration 29950 => Loss: 6.70183203696805929184\n",
      "Iteration 29951 => Loss: 6.70183154457014129690\n",
      "Iteration 29952 => Loss: 6.70183105223857289445\n",
      "Iteration 29953 => Loss: 6.70183055997334520271\n",
      "Iteration 29954 => Loss: 6.70183006777444845170\n",
      "Iteration 29955 => Loss: 6.70182957564188086508\n",
      "Iteration 29956 => Loss: 6.70182908357562201473\n",
      "Iteration 29957 => Loss: 6.70182859157567722974\n",
      "Iteration 29958 => Loss: 6.70182809964202430564\n",
      "Iteration 29959 => Loss: 6.70182760777465880153\n",
      "Iteration 29960 => Loss: 6.70182711597357272382\n",
      "Iteration 29961 => Loss: 6.70182662423875807889\n",
      "Iteration 29962 => Loss: 6.70182613257020598496\n",
      "Iteration 29963 => Loss: 6.70182564096790756025\n",
      "Iteration 29964 => Loss: 6.70182514943184415301\n",
      "Iteration 29965 => Loss: 6.70182465796202198050\n",
      "Iteration 29966 => Loss: 6.70182416655842505548\n",
      "Iteration 29967 => Loss: 6.70182367522104360802\n",
      "Iteration 29968 => Loss: 6.70182318394987142085\n",
      "Iteration 29969 => Loss: 6.70182269274489339494\n",
      "Iteration 29970 => Loss: 6.70182220160611130666\n",
      "Iteration 29971 => Loss: 6.70182171053351183332\n",
      "Iteration 29972 => Loss: 6.70182121952708254042\n",
      "Iteration 29973 => Loss: 6.70182072858681010530\n",
      "Iteration 29974 => Loss: 6.70182023771269452794\n",
      "Iteration 29975 => Loss: 6.70181974690472870293\n",
      "Iteration 29976 => Loss: 6.70181925616289486669\n",
      "Iteration 29977 => Loss: 6.70181876548719301923\n",
      "Iteration 29978 => Loss: 6.70181827487760095607\n",
      "Iteration 29979 => Loss: 6.70181778433412933538\n",
      "Iteration 29980 => Loss: 6.70181729385675239996\n",
      "Iteration 29981 => Loss: 6.70181680344546570893\n",
      "Iteration 29982 => Loss: 6.70181631310026393322\n",
      "Iteration 29983 => Loss: 6.70181582282114085558\n",
      "Iteration 29984 => Loss: 6.70181533260807871244\n",
      "Iteration 29985 => Loss: 6.70181484246107395109\n",
      "Iteration 29986 => Loss: 6.70181435238011413702\n",
      "Iteration 29987 => Loss: 6.70181386236519660571\n",
      "Iteration 29988 => Loss: 6.70181337241630625812\n",
      "Iteration 29989 => Loss: 6.70181288253343598882\n",
      "Iteration 29990 => Loss: 6.70181239271657691603\n",
      "Iteration 29991 => Loss: 6.70181190296572548704\n",
      "Iteration 29992 => Loss: 6.70181141328086393827\n",
      "Iteration 29993 => Loss: 6.70181092366198694066\n",
      "Iteration 29994 => Loss: 6.70181043410908916513\n",
      "Iteration 29995 => Loss: 6.70180994462215640084\n",
      "Iteration 29996 => Loss: 6.70180945520118154235\n",
      "Iteration 29997 => Loss: 6.70180896584615481970\n",
      "Iteration 29998 => Loss: 6.70180847655707445654\n",
      "Iteration 29999 => Loss: 6.70180798733392268929\n",
      "Iteration 30000 => Loss: 6.70180749817669241253\n",
      "Iteration 30001 => Loss: 6.70180700908537474447\n",
      "Iteration 30002 => Loss: 6.70180652005996702059\n",
      "Iteration 30003 => Loss: 6.70180603110044881277\n",
      "Iteration 30004 => Loss: 6.70180554220682900279\n",
      "Iteration 30005 => Loss: 6.70180505337907650443\n",
      "Iteration 30006 => Loss: 6.70180456461720197581\n",
      "Iteration 30007 => Loss: 6.70180407592118054794\n",
      "Iteration 30008 => Loss: 6.70180358729101843807\n",
      "Iteration 30009 => Loss: 6.70180309872669432991\n",
      "Iteration 30010 => Loss: 6.70180261022820111805\n",
      "Iteration 30011 => Loss: 6.70180212179554057883\n",
      "Iteration 30012 => Loss: 6.70180163342869583687\n",
      "Iteration 30013 => Loss: 6.70180114512765712220\n",
      "Iteration 30014 => Loss: 6.70180065689241022397\n",
      "Iteration 30015 => Loss: 6.70180016872296224761\n",
      "Iteration 30016 => Loss: 6.70179968061928743595\n",
      "Iteration 30017 => Loss: 6.70179919258139111804\n",
      "Iteration 30018 => Loss: 6.70179870460925641851\n",
      "Iteration 30019 => Loss: 6.70179821670287445556\n",
      "Iteration 30020 => Loss: 6.70179772886223723560\n",
      "Iteration 30021 => Loss: 6.70179724108733321231\n",
      "Iteration 30022 => Loss: 6.70179675337816416203\n",
      "Iteration 30023 => Loss: 6.70179626573471676210\n",
      "Iteration 30024 => Loss: 6.70179577815697236076\n",
      "Iteration 30025 => Loss: 6.70179529064492829349\n",
      "Iteration 30026 => Loss: 6.70179480319857745485\n",
      "Iteration 30027 => Loss: 6.70179431581791806849\n",
      "Iteration 30028 => Loss: 6.70179382850292881812\n",
      "Iteration 30029 => Loss: 6.70179334125359904561\n",
      "Iteration 30030 => Loss: 6.70179285406992697460\n",
      "Iteration 30031 => Loss: 6.70179236695190372330\n",
      "Iteration 30032 => Loss: 6.70179187989953017990\n",
      "Iteration 30033 => Loss: 6.70179139291277881085\n",
      "Iteration 30034 => Loss: 6.70179090599164783981\n",
      "Iteration 30035 => Loss: 6.70179041913613104953\n",
      "Iteration 30036 => Loss: 6.70178993234621511732\n",
      "Iteration 30037 => Loss: 6.70178944562190004319\n",
      "Iteration 30038 => Loss: 6.70178895896316539904\n",
      "Iteration 30039 => Loss: 6.70178847237001384940\n",
      "Iteration 30040 => Loss: 6.70178798584242318981\n",
      "Iteration 30041 => Loss: 6.70178749938039963752\n",
      "Iteration 30042 => Loss: 6.70178701298392276442\n",
      "Iteration 30043 => Loss: 6.70178652665299345870\n",
      "Iteration 30044 => Loss: 6.70178604038758951589\n",
      "Iteration 30045 => Loss: 6.70178555418771182417\n",
      "Iteration 30046 => Loss: 6.70178506805335416630\n",
      "Iteration 30047 => Loss: 6.70178458198449611416\n",
      "Iteration 30048 => Loss: 6.70178409598114033230\n",
      "Iteration 30049 => Loss: 6.70178361004327438621\n",
      "Iteration 30050 => Loss: 6.70178312417088850594\n",
      "Iteration 30051 => Loss: 6.70178263836396936881\n",
      "Iteration 30052 => Loss: 6.70178215262251519846\n",
      "Iteration 30053 => Loss: 6.70178166694652155400\n",
      "Iteration 30054 => Loss: 6.70178118133596267825\n",
      "Iteration 30055 => Loss: 6.70178069579084123575\n",
      "Iteration 30056 => Loss: 6.70178021031115278561\n",
      "Iteration 30057 => Loss: 6.70177972489688045243\n",
      "Iteration 30058 => Loss: 6.70177923954801180173\n",
      "Iteration 30059 => Loss: 6.70177875426455393892\n",
      "Iteration 30060 => Loss: 6.70177826904648199502\n",
      "Iteration 30061 => Loss: 6.70177778389379330548\n",
      "Iteration 30062 => Loss: 6.70177729880647721217\n",
      "Iteration 30063 => Loss: 6.70177681378453016237\n",
      "Iteration 30064 => Loss: 6.70177632882793883340\n",
      "Iteration 30065 => Loss: 6.70177584393670056073\n",
      "Iteration 30066 => Loss: 6.70177535911079935715\n",
      "Iteration 30067 => Loss: 6.70177487435022456452\n",
      "Iteration 30068 => Loss: 6.70177438965497263013\n",
      "Iteration 30069 => Loss: 6.70177390502503023129\n",
      "Iteration 30070 => Loss: 6.70177342046039381529\n",
      "Iteration 30071 => Loss: 6.70177293596105627671\n",
      "Iteration 30072 => Loss: 6.70177245152700074016\n",
      "Iteration 30073 => Loss: 6.70177196715822631745\n",
      "Iteration 30074 => Loss: 6.70177148285471346867\n",
      "Iteration 30075 => Loss: 6.70177099861647107559\n",
      "Iteration 30076 => Loss: 6.70177051444347160469\n",
      "Iteration 30077 => Loss: 6.70177003033571860868\n",
      "Iteration 30078 => Loss: 6.70176954629319610035\n",
      "Iteration 30079 => Loss: 6.70176906231590052698\n",
      "Iteration 30080 => Loss: 6.70176857840382567133\n",
      "Iteration 30081 => Loss: 6.70176809455694932893\n",
      "Iteration 30082 => Loss: 6.70176761077527771704\n",
      "Iteration 30083 => Loss: 6.70176712705878951937\n",
      "Iteration 30084 => Loss: 6.70176664340748917681\n",
      "Iteration 30085 => Loss: 6.70176615982135803762\n",
      "Iteration 30086 => Loss: 6.70176567630038544365\n",
      "Iteration 30087 => Loss: 6.70176519284457317127\n",
      "Iteration 30088 => Loss: 6.70176470945390523326\n",
      "Iteration 30089 => Loss: 6.70176422612837541237\n",
      "Iteration 30090 => Loss: 6.70176374286797127411\n",
      "Iteration 30091 => Loss: 6.70176325967268571304\n",
      "Iteration 30092 => Loss: 6.70176277654251162375\n",
      "Iteration 30093 => Loss: 6.70176229347744545350\n",
      "Iteration 30094 => Loss: 6.70176181047746055697\n",
      "Iteration 30095 => Loss: 6.70176132754256670410\n",
      "Iteration 30096 => Loss: 6.70176084467274968404\n",
      "Iteration 30097 => Loss: 6.70176036186799972683\n",
      "Iteration 30098 => Loss: 6.70175987912830706250\n",
      "Iteration 30099 => Loss: 6.70175939645366192110\n",
      "Iteration 30100 => Loss: 6.70175891384405897355\n",
      "Iteration 30101 => Loss: 6.70175843129948312082\n",
      "Iteration 30102 => Loss: 6.70175794881993525109\n",
      "Iteration 30103 => Loss: 6.70175746640539493626\n",
      "Iteration 30104 => Loss: 6.70175698405586750539\n",
      "Iteration 30105 => Loss: 6.70175650177133697127\n",
      "Iteration 30106 => Loss: 6.70175601955179001123\n",
      "Iteration 30107 => Loss: 6.70175553739722040802\n",
      "Iteration 30108 => Loss: 6.70175505530762904982\n",
      "Iteration 30109 => Loss: 6.70175457328299728488\n",
      "Iteration 30110 => Loss: 6.70175409132331090234\n",
      "Iteration 30111 => Loss: 6.70175360942857345492\n",
      "Iteration 30112 => Loss: 6.70175312759877339630\n",
      "Iteration 30113 => Loss: 6.70175264583389651563\n",
      "Iteration 30114 => Loss: 6.70175216413393659565\n",
      "Iteration 30115 => Loss: 6.70175168249889363636\n",
      "Iteration 30116 => Loss: 6.70175120092874276878\n",
      "Iteration 30117 => Loss: 6.70175071942348399290\n",
      "Iteration 30118 => Loss: 6.70175023798311020329\n",
      "Iteration 30119 => Loss: 6.70174975660761340635\n",
      "Iteration 30120 => Loss: 6.70174927529697850304\n",
      "Iteration 30121 => Loss: 6.70174879405120282883\n",
      "Iteration 30122 => Loss: 6.70174831287027394922\n",
      "Iteration 30123 => Loss: 6.70174783175417942971\n",
      "Iteration 30124 => Loss: 6.70174735070291838213\n",
      "Iteration 30125 => Loss: 6.70174686971648192468\n",
      "Iteration 30126 => Loss: 6.70174638879485495835\n",
      "Iteration 30127 => Loss: 6.70174590793803570676\n",
      "Iteration 30128 => Loss: 6.70174542714600551818\n",
      "Iteration 30129 => Loss: 6.70174494641877149803\n",
      "Iteration 30130 => Loss: 6.70174446575630700096\n",
      "Iteration 30131 => Loss: 6.70174398515861380332\n",
      "Iteration 30132 => Loss: 6.70174350462568657605\n",
      "Iteration 30133 => Loss: 6.70174302415750844375\n",
      "Iteration 30134 => Loss: 6.70174254375406786011\n",
      "Iteration 30135 => Loss: 6.70174206341537015419\n",
      "Iteration 30136 => Loss: 6.70174158314138956882\n",
      "Iteration 30137 => Loss: 6.70174110293213320944\n",
      "Iteration 30138 => Loss: 6.70174062278758064792\n",
      "Iteration 30139 => Loss: 6.70174014270773099611\n",
      "Iteration 30140 => Loss: 6.70173966269257004313\n",
      "Iteration 30141 => Loss: 6.70173918274209245993\n",
      "Iteration 30142 => Loss: 6.70173870285628758836\n",
      "Iteration 30143 => Loss: 6.70173822303514743481\n",
      "Iteration 30144 => Loss: 6.70173774327866400569\n",
      "Iteration 30145 => Loss: 6.70173726358682397830\n",
      "Iteration 30146 => Loss: 6.70173678395962646448\n",
      "Iteration 30147 => Loss: 6.70173630439705547701\n",
      "Iteration 30148 => Loss: 6.70173582489910923954\n",
      "Iteration 30149 => Loss: 6.70173534546577531756\n",
      "Iteration 30150 => Loss: 6.70173486609704482930\n",
      "Iteration 30151 => Loss: 6.70173438679290622844\n",
      "Iteration 30152 => Loss: 6.70173390755335596225\n",
      "Iteration 30153 => Loss: 6.70173342837838603714\n",
      "Iteration 30154 => Loss: 6.70173294926798046589\n",
      "Iteration 30155 => Loss: 6.70173247022213125490\n",
      "Iteration 30156 => Loss: 6.70173199124084373324\n",
      "Iteration 30157 => Loss: 6.70173151232409303191\n",
      "Iteration 30158 => Loss: 6.70173103347187559820\n",
      "Iteration 30159 => Loss: 6.70173055468418876757\n",
      "Iteration 30160 => Loss: 6.70173007596101477645\n",
      "Iteration 30161 => Loss: 6.70172959730235096032\n",
      "Iteration 30162 => Loss: 6.70172911870817777924\n",
      "Iteration 30163 => Loss: 6.70172864017850589136\n",
      "Iteration 30164 => Loss: 6.70172816171331131585\n",
      "Iteration 30165 => Loss: 6.70172768331259494090\n",
      "Iteration 30166 => Loss: 6.70172720497633722658\n",
      "Iteration 30167 => Loss: 6.70172672670453462018\n",
      "Iteration 30168 => Loss: 6.70172624849718268081\n",
      "Iteration 30169 => Loss: 6.70172577035426630943\n",
      "Iteration 30170 => Loss: 6.70172529227578639421\n",
      "Iteration 30171 => Loss: 6.70172481426172161889\n",
      "Iteration 30172 => Loss: 6.70172433631207109528\n",
      "Iteration 30173 => Loss: 6.70172385842681972434\n",
      "Iteration 30174 => Loss: 6.70172338060596928244\n",
      "Iteration 30175 => Loss: 6.70172290284950555872\n",
      "Iteration 30176 => Loss: 6.70172242515741611868\n",
      "Iteration 30177 => Loss: 6.70172194752969740961\n",
      "Iteration 30178 => Loss: 6.70172146996633788518\n",
      "Iteration 30179 => Loss: 6.70172099246733132816\n",
      "Iteration 30180 => Loss: 6.70172051503266974493\n",
      "Iteration 30181 => Loss: 6.70172003766233714828\n",
      "Iteration 30182 => Loss: 6.70171956035633620274\n",
      "Iteration 30183 => Loss: 6.70171908311464825658\n",
      "Iteration 30184 => Loss: 6.70171860593726886890\n",
      "Iteration 30185 => Loss: 6.70171812882418826973\n",
      "Iteration 30186 => Loss: 6.70171765177539846547\n",
      "Iteration 30187 => Loss: 6.70171717479089412706\n",
      "Iteration 30188 => Loss: 6.70171669787066548452\n",
      "Iteration 30189 => Loss: 6.70171622101469832700\n",
      "Iteration 30190 => Loss: 6.70171574422298910179\n",
      "Iteration 30191 => Loss: 6.70171526749552182167\n",
      "Iteration 30192 => Loss: 6.70171479083229737483\n",
      "Iteration 30193 => Loss: 6.70171431423330954402\n",
      "Iteration 30194 => Loss: 6.70171383769853790113\n",
      "Iteration 30195 => Loss: 6.70171336122797356438\n",
      "Iteration 30196 => Loss: 6.70171288482162363920\n",
      "Iteration 30197 => Loss: 6.70171240847946503294\n",
      "Iteration 30198 => Loss: 6.70171193220149685743\n",
      "Iteration 30199 => Loss: 6.70171145598769779639\n",
      "Iteration 30200 => Loss: 6.70171097983807761977\n",
      "Iteration 30201 => Loss: 6.70171050375261945220\n",
      "Iteration 30202 => Loss: 6.70171002773130020103\n",
      "Iteration 30203 => Loss: 6.70170955177413940618\n",
      "Iteration 30204 => Loss: 6.70170907588111219866\n",
      "Iteration 30205 => Loss: 6.70170860005220880851\n",
      "Iteration 30206 => Loss: 6.70170812428742479483\n",
      "Iteration 30207 => Loss: 6.70170764858674417042\n",
      "Iteration 30208 => Loss: 6.70170717295017492887\n",
      "Iteration 30209 => Loss: 6.70170669737769664209\n",
      "Iteration 30210 => Loss: 6.70170622186928888198\n",
      "Iteration 30211 => Loss: 6.70170574642496852391\n",
      "Iteration 30212 => Loss: 6.70170527104470536983\n",
      "Iteration 30213 => Loss: 6.70170479572850741334\n",
      "Iteration 30214 => Loss: 6.70170432047635422634\n",
      "Iteration 30215 => Loss: 6.70170384528824047976\n",
      "Iteration 30216 => Loss: 6.70170337016415817999\n",
      "Iteration 30217 => Loss: 6.70170289510410643885\n",
      "Iteration 30218 => Loss: 6.70170242010806127553\n",
      "Iteration 30219 => Loss: 6.70170194517602357820\n",
      "Iteration 30220 => Loss: 6.70170147030798357690\n",
      "Iteration 30221 => Loss: 6.70170099550393150167\n",
      "Iteration 30222 => Loss: 6.70170052076386113526\n",
      "Iteration 30223 => Loss: 6.70170004608776004318\n",
      "Iteration 30224 => Loss: 6.70169957147562023181\n",
      "Iteration 30225 => Loss: 6.70169909692743726026\n",
      "Iteration 30226 => Loss: 6.70169862244320224676\n",
      "Iteration 30227 => Loss: 6.70169814802289831590\n",
      "Iteration 30228 => Loss: 6.70169767366652635587\n",
      "Iteration 30229 => Loss: 6.70169719937407215582\n",
      "Iteration 30230 => Loss: 6.70169672514552683396\n",
      "Iteration 30231 => Loss: 6.70169625098088506121\n",
      "Iteration 30232 => Loss: 6.70169577688014239669\n",
      "Iteration 30233 => Loss: 6.70169530284328018865\n",
      "Iteration 30234 => Loss: 6.70169482887029133167\n",
      "Iteration 30235 => Loss: 6.70169435496117671391\n",
      "Iteration 30236 => Loss: 6.70169388111591946000\n",
      "Iteration 30237 => Loss: 6.70169340733451424086\n",
      "Iteration 30238 => Loss: 6.70169293361694595745\n",
      "Iteration 30239 => Loss: 6.70169245996321549796\n",
      "Iteration 30240 => Loss: 6.70169198637330953972\n",
      "Iteration 30241 => Loss: 6.70169151284721564821\n",
      "Iteration 30242 => Loss: 6.70169103938493471162\n",
      "Iteration 30243 => Loss: 6.70169056598644896638\n",
      "Iteration 30244 => Loss: 6.70169009265175574797\n",
      "Iteration 30245 => Loss: 6.70168961938084883911\n",
      "Iteration 30246 => Loss: 6.70168914617370603537\n",
      "Iteration 30247 => Loss: 6.70168867303033444216\n",
      "Iteration 30248 => Loss: 6.70168819995071363138\n",
      "Iteration 30249 => Loss: 6.70168772693484449121\n",
      "Iteration 30250 => Loss: 6.70168725398271458715\n",
      "Iteration 30251 => Loss: 6.70168678109431414924\n",
      "Iteration 30252 => Loss: 6.70168630826962985481\n",
      "Iteration 30253 => Loss: 6.70168583550867325016\n",
      "Iteration 30254 => Loss: 6.70168536281141413724\n",
      "Iteration 30255 => Loss: 6.70168489017784452244\n",
      "Iteration 30256 => Loss: 6.70168441760796707030\n",
      "Iteration 30257 => Loss: 6.70168394510177201084\n",
      "Iteration 30258 => Loss: 6.70168347265923536327\n",
      "Iteration 30259 => Loss: 6.70168300028036512117\n",
      "Iteration 30260 => Loss: 6.70168252796515950820\n",
      "Iteration 30261 => Loss: 6.70168205571358743811\n",
      "Iteration 30262 => Loss: 6.70168158352565335178\n",
      "Iteration 30263 => Loss: 6.70168111140135014381\n",
      "Iteration 30264 => Loss: 6.70168063934066005061\n",
      "Iteration 30265 => Loss: 6.70168016734358573672\n",
      "Iteration 30266 => Loss: 6.70167969541011121493\n",
      "Iteration 30267 => Loss: 6.70167922354022405074\n",
      "Iteration 30268 => Loss: 6.70167875173393046140\n",
      "Iteration 30269 => Loss: 6.70167827999120824245\n",
      "Iteration 30270 => Loss: 6.70167780831205295300\n",
      "Iteration 30271 => Loss: 6.70167733669645748762\n",
      "Iteration 30272 => Loss: 6.70167686514441118817\n",
      "Iteration 30273 => Loss: 6.70167639365590961376\n",
      "Iteration 30274 => Loss: 6.70167592223093677717\n",
      "Iteration 30275 => Loss: 6.70167545086948468480\n",
      "Iteration 30276 => Loss: 6.70167497957155866573\n",
      "Iteration 30277 => Loss: 6.70167450833713562730\n",
      "Iteration 30278 => Loss: 6.70167403716621290499\n",
      "Iteration 30279 => Loss: 6.70167356605878516973\n",
      "Iteration 30280 => Loss: 6.70167309501482844070\n",
      "Iteration 30281 => Loss: 6.70167262403435071150\n",
      "Iteration 30282 => Loss: 6.70167215311733777128\n",
      "Iteration 30283 => Loss: 6.70167168226377807372\n",
      "Iteration 30284 => Loss: 6.70167121147366717793\n",
      "Iteration 30285 => Loss: 6.70167074074699531394\n",
      "Iteration 30286 => Loss: 6.70167027008375271180\n",
      "Iteration 30287 => Loss: 6.70166979948393315425\n",
      "Iteration 30288 => Loss: 6.70166932894752953587\n",
      "Iteration 30289 => Loss: 6.70166885847453475122\n",
      "Iteration 30290 => Loss: 6.70166838806492215497\n",
      "Iteration 30291 => Loss: 6.70166791771870506977\n",
      "Iteration 30292 => Loss: 6.70166744743587106115\n",
      "Iteration 30293 => Loss: 6.70166697721640325369\n",
      "Iteration 30294 => Loss: 6.70166650706030431195\n",
      "Iteration 30295 => Loss: 6.70166603696754936692\n",
      "Iteration 30296 => Loss: 6.70166556693813841861\n",
      "Iteration 30297 => Loss: 6.70166509697206880247\n",
      "Iteration 30298 => Loss: 6.70166462706933341309\n",
      "Iteration 30299 => Loss: 6.70166415722991182236\n",
      "Iteration 30300 => Loss: 6.70166368745379337213\n",
      "Iteration 30301 => Loss: 6.70166321774098872055\n",
      "Iteration 30302 => Loss: 6.70166274809147122227\n",
      "Iteration 30303 => Loss: 6.70166227850524354182\n",
      "Iteration 30304 => Loss: 6.70166180898228880380\n",
      "Iteration 30305 => Loss: 6.70166133952260434370\n",
      "Iteration 30306 => Loss: 6.70166087012617861518\n",
      "Iteration 30307 => Loss: 6.70166040079300895371\n",
      "Iteration 30308 => Loss: 6.70165993152307670755\n",
      "Iteration 30309 => Loss: 6.70165946231637388308\n",
      "Iteration 30310 => Loss: 6.70165899317290758574\n",
      "Iteration 30311 => Loss: 6.70165852409264939382\n",
      "Iteration 30312 => Loss: 6.70165805507560552456\n",
      "Iteration 30313 => Loss: 6.70165758612175910258\n",
      "Iteration 30314 => Loss: 6.70165711723110835152\n",
      "Iteration 30315 => Loss: 6.70165664840363550780\n",
      "Iteration 30316 => Loss: 6.70165617963934057144\n",
      "Iteration 30317 => Loss: 6.70165571093820933157\n",
      "Iteration 30318 => Loss: 6.70165524230023734731\n",
      "Iteration 30319 => Loss: 6.70165477372541129597\n",
      "Iteration 30320 => Loss: 6.70165430521372673667\n",
      "Iteration 30321 => Loss: 6.70165383676517745215\n",
      "Iteration 30322 => Loss: 6.70165336837975189610\n",
      "Iteration 30323 => Loss: 6.70165290005743674584\n",
      "Iteration 30324 => Loss: 6.70165243179822844866\n",
      "Iteration 30325 => Loss: 6.70165196360212256366\n",
      "Iteration 30326 => Loss: 6.70165149546910487999\n",
      "Iteration 30327 => Loss: 6.70165102739916296315\n",
      "Iteration 30328 => Loss: 6.70165055939229414861\n",
      "Iteration 30329 => Loss: 6.70165009144849932454\n",
      "Iteration 30330 => Loss: 6.70164962356775451013\n",
      "Iteration 30331 => Loss: 6.70164915575005259996\n",
      "Iteration 30332 => Loss: 6.70164868799538560040\n",
      "Iteration 30333 => Loss: 6.70164822030376061690\n",
      "Iteration 30334 => Loss: 6.70164775267515366863\n",
      "Iteration 30335 => Loss: 6.70164728510955054475\n",
      "Iteration 30336 => Loss: 6.70164681760696634427\n",
      "Iteration 30337 => Loss: 6.70164635016736465190\n",
      "Iteration 30338 => Loss: 6.70164588279075967847\n",
      "Iteration 30339 => Loss: 6.70164541547712921954\n",
      "Iteration 30340 => Loss: 6.70164494822646705785\n",
      "Iteration 30341 => Loss: 6.70164448103876786433\n",
      "Iteration 30342 => Loss: 6.70164401391402897445\n",
      "Iteration 30343 => Loss: 6.70164354685222907193\n",
      "Iteration 30344 => Loss: 6.70164307985336815676\n",
      "Iteration 30345 => Loss: 6.70164261291743290627\n",
      "Iteration 30346 => Loss: 6.70164214604442243228\n",
      "Iteration 30347 => Loss: 6.70164167923432341212\n",
      "Iteration 30348 => Loss: 6.70164121248712341128\n",
      "Iteration 30349 => Loss: 6.70164074580281621252\n",
      "Iteration 30350 => Loss: 6.70164027918139826312\n",
      "Iteration 30351 => Loss: 6.70163981262285357587\n",
      "Iteration 30352 => Loss: 6.70163934612718126260\n",
      "Iteration 30353 => Loss: 6.70163887969436800063\n",
      "Iteration 30354 => Loss: 6.70163841332440313181\n",
      "Iteration 30355 => Loss: 6.70163794701728932068\n",
      "Iteration 30356 => Loss: 6.70163748077300258643\n",
      "Iteration 30357 => Loss: 6.70163701459154470541\n",
      "Iteration 30358 => Loss: 6.70163654847290590766\n",
      "Iteration 30359 => Loss: 6.70163608241708352864\n",
      "Iteration 30360 => Loss: 6.70163561642405181118\n",
      "Iteration 30361 => Loss: 6.70163515049381608435\n",
      "Iteration 30362 => Loss: 6.70163468462636391365\n",
      "Iteration 30363 => Loss: 6.70163421882168908184\n",
      "Iteration 30364 => Loss: 6.70163375307978181894\n",
      "Iteration 30365 => Loss: 6.70163328740062969047\n",
      "Iteration 30366 => Loss: 6.70163282178423180824\n",
      "Iteration 30367 => Loss: 6.70163235623057218504\n",
      "Iteration 30368 => Loss: 6.70163189073964726816\n",
      "Iteration 30369 => Loss: 6.70163142531145172853\n",
      "Iteration 30370 => Loss: 6.70163095994596691440\n",
      "Iteration 30371 => Loss: 6.70163049464319104942\n",
      "Iteration 30372 => Loss: 6.70163002940311525180\n",
      "Iteration 30373 => Loss: 6.70162956422572797521\n",
      "Iteration 30374 => Loss: 6.70162909911102833149\n",
      "Iteration 30375 => Loss: 6.70162863405900743885\n",
      "Iteration 30376 => Loss: 6.70162816906964575736\n",
      "Iteration 30377 => Loss: 6.70162770414293706978\n",
      "Iteration 30378 => Loss: 6.70162723927887959974\n",
      "Iteration 30379 => Loss: 6.70162677447746979453\n",
      "Iteration 30380 => Loss: 6.70162630973868189699\n",
      "Iteration 30381 => Loss: 6.70162584506253189431\n",
      "Iteration 30382 => Loss: 6.70162538044898159484\n",
      "Iteration 30383 => Loss: 6.70162491589804254488\n",
      "Iteration 30384 => Loss: 6.70162445140970763902\n",
      "Iteration 30385 => Loss: 6.70162398698396000185\n",
      "Iteration 30386 => Loss: 6.70162352262079519249\n",
      "Iteration 30387 => Loss: 6.70162305832019455920\n",
      "Iteration 30388 => Loss: 6.70162259408216609557\n",
      "Iteration 30389 => Loss: 6.70162212990669114987\n",
      "Iteration 30390 => Loss: 6.70162166579376705755\n",
      "Iteration 30391 => Loss: 6.70162120174338227230\n",
      "Iteration 30392 => Loss: 6.70162073775552524779\n",
      "Iteration 30393 => Loss: 6.70162027383018799043\n",
      "Iteration 30394 => Loss: 6.70161980996737050020\n",
      "Iteration 30395 => Loss: 6.70161934616705146084\n",
      "Iteration 30396 => Loss: 6.70161888242923797776\n",
      "Iteration 30397 => Loss: 6.70161841875391051104\n",
      "Iteration 30398 => Loss: 6.70161795514106373162\n",
      "Iteration 30399 => Loss: 6.70161749159068254045\n",
      "Iteration 30400 => Loss: 6.70161702810277315479\n",
      "Iteration 30401 => Loss: 6.70161656467730892928\n",
      "Iteration 30402 => Loss: 6.70161610131430052206\n",
      "Iteration 30403 => Loss: 6.70161563801372839322\n",
      "Iteration 30404 => Loss: 6.70161517477558277278\n",
      "Iteration 30405 => Loss: 6.70161471159986987800\n",
      "Iteration 30406 => Loss: 6.70161424848655951081\n",
      "Iteration 30407 => Loss: 6.70161378543565522392\n",
      "Iteration 30408 => Loss: 6.70161332244714280648\n",
      "Iteration 30409 => Loss: 6.70161285952102225849\n",
      "Iteration 30410 => Loss: 6.70161239665727759274\n",
      "Iteration 30411 => Loss: 6.70161193385591502647\n",
      "Iteration 30412 => Loss: 6.70161147111690169709\n",
      "Iteration 30413 => Loss: 6.70161100844025003909\n",
      "Iteration 30414 => Loss: 6.70161054582594406526\n",
      "Iteration 30415 => Loss: 6.70161008327397134110\n",
      "Iteration 30416 => Loss: 6.70160962078433097844\n",
      "Iteration 30417 => Loss: 6.70160915835700699006\n",
      "Iteration 30418 => Loss: 6.70160869599200204050\n",
      "Iteration 30419 => Loss: 6.70160823368929570165\n",
      "Iteration 30420 => Loss: 6.70160777144888797352\n",
      "Iteration 30421 => Loss: 6.70160730927076553343\n",
      "Iteration 30422 => Loss: 6.70160684715491594687\n",
      "Iteration 30423 => Loss: 6.70160638510134276657\n",
      "Iteration 30424 => Loss: 6.70160592311003533439\n",
      "Iteration 30425 => Loss: 6.70160546118097322221\n",
      "Iteration 30426 => Loss: 6.70160499931415643005\n",
      "Iteration 30427 => Loss: 6.70160453750957874064\n",
      "Iteration 30428 => Loss: 6.70160407576723127221\n",
      "Iteration 30429 => Loss: 6.70160361408710070208\n",
      "Iteration 30430 => Loss: 6.70160315246918436571\n",
      "Iteration 30431 => Loss: 6.70160269091346716408\n",
      "Iteration 30432 => Loss: 6.70160222941994376811\n",
      "Iteration 30433 => Loss: 6.70160176798861417780\n",
      "Iteration 30434 => Loss: 6.70160130661945707686\n",
      "Iteration 30435 => Loss: 6.70160084531247157713\n",
      "Iteration 30436 => Loss: 6.70160038406764790864\n",
      "Iteration 30437 => Loss: 6.70159992288497186053\n",
      "Iteration 30438 => Loss: 6.70159946176444520916\n",
      "Iteration 30439 => Loss: 6.70159900070605019096\n",
      "Iteration 30440 => Loss: 6.70159853970978680593\n",
      "Iteration 30441 => Loss: 6.70159807877564261958\n",
      "Iteration 30442 => Loss: 6.70159761790360697375\n",
      "Iteration 30443 => Loss: 6.70159715709367542757\n",
      "Iteration 30444 => Loss: 6.70159669634583909925\n",
      "Iteration 30445 => Loss: 6.70159623566008644246\n",
      "Iteration 30446 => Loss: 6.70159577503640857543\n",
      "Iteration 30447 => Loss: 6.70159531447480460997\n",
      "Iteration 30448 => Loss: 6.70159485397525944705\n",
      "Iteration 30449 => Loss: 6.70159439353776686943\n",
      "Iteration 30450 => Loss: 6.70159393316231621895\n",
      "Iteration 30451 => Loss: 6.70159347284890838381\n",
      "Iteration 30452 => Loss: 6.70159301259752471225\n",
      "Iteration 30453 => Loss: 6.70159255240815365795\n",
      "Iteration 30454 => Loss: 6.70159209228080054999\n",
      "Iteration 30455 => Loss: 6.70159163221544051936\n",
      "Iteration 30456 => Loss: 6.70159117221208155968\n",
      "Iteration 30457 => Loss: 6.70159071227071034826\n",
      "Iteration 30458 => Loss: 6.70159025239131089791\n",
      "Iteration 30459 => Loss: 6.70158979257388587314\n",
      "Iteration 30460 => Loss: 6.70158933281841218133\n",
      "Iteration 30461 => Loss: 6.70158887312489603971\n",
      "Iteration 30462 => Loss: 6.70158841349332234927\n",
      "Iteration 30463 => Loss: 6.70158795392368755728\n",
      "Iteration 30464 => Loss: 6.70158749441597922925\n",
      "Iteration 30465 => Loss: 6.70158703497019203610\n",
      "Iteration 30466 => Loss: 6.70158657558630999063\n",
      "Iteration 30467 => Loss: 6.70158611626433131647\n",
      "Iteration 30468 => Loss: 6.70158565700424446732\n",
      "Iteration 30469 => Loss: 6.70158519780604500227\n",
      "Iteration 30470 => Loss: 6.70158473866971782229\n",
      "Iteration 30471 => Loss: 6.70158427959526736828\n",
      "Iteration 30472 => Loss: 6.70158382058267143577\n",
      "Iteration 30473 => Loss: 6.70158336163193357748\n",
      "Iteration 30474 => Loss: 6.70158290274303158895\n",
      "Iteration 30475 => Loss: 6.70158244391596991107\n",
      "Iteration 30476 => Loss: 6.70158198515073610935\n",
      "Iteration 30477 => Loss: 6.70158152644731686109\n",
      "Iteration 30478 => Loss: 6.70158106780570950178\n",
      "Iteration 30479 => Loss: 6.70158060922590159691\n",
      "Iteration 30480 => Loss: 6.70158015070789137013\n",
      "Iteration 30481 => Loss: 6.70157969225166905147\n",
      "Iteration 30482 => Loss: 6.70157923385722131826\n",
      "Iteration 30483 => Loss: 6.70157877552454017689\n",
      "Iteration 30484 => Loss: 6.70157831725361674557\n",
      "Iteration 30485 => Loss: 6.70157785904444924796\n",
      "Iteration 30486 => Loss: 6.70157740089702436137\n",
      "Iteration 30487 => Loss: 6.70157694281134030945\n",
      "Iteration 30488 => Loss: 6.70157648478737932862\n",
      "Iteration 30489 => Loss: 6.70157602682513608983\n",
      "Iteration 30490 => Loss: 6.70157556892460082310\n",
      "Iteration 30491 => Loss: 6.70157511108577352843\n",
      "Iteration 30492 => Loss: 6.70157465330863111319\n",
      "Iteration 30493 => Loss: 6.70157419559318157098\n",
      "Iteration 30494 => Loss: 6.70157373793940891460\n",
      "Iteration 30495 => Loss: 6.70157328034730781496\n",
      "Iteration 30496 => Loss: 6.70157282281686139669\n",
      "Iteration 30497 => Loss: 6.70157236534806699524\n",
      "Iteration 30498 => Loss: 6.70157190794092105790\n",
      "Iteration 30499 => Loss: 6.70157145059541115018\n",
      "Iteration 30500 => Loss: 6.70157099331152039667\n",
      "Iteration 30501 => Loss: 6.70157053608925856736\n",
      "Iteration 30502 => Loss: 6.70157007892859990505\n",
      "Iteration 30503 => Loss: 6.70156962182954618612\n",
      "Iteration 30504 => Loss: 6.70156916479208941695\n",
      "Iteration 30505 => Loss: 6.70156870781621893940\n",
      "Iteration 30506 => Loss: 6.70156825090191876626\n",
      "Iteration 30507 => Loss: 6.70156779404919156207\n",
      "Iteration 30508 => Loss: 6.70156733725802400414\n",
      "Iteration 30509 => Loss: 6.70156688052841520431\n",
      "Iteration 30510 => Loss: 6.70156642386034651082\n",
      "Iteration 30511 => Loss: 6.70156596725381614732\n",
      "Iteration 30512 => Loss: 6.70156551070881434384\n",
      "Iteration 30513 => Loss: 6.70156505422532600136\n",
      "Iteration 30514 => Loss: 6.70156459780335556076\n",
      "Iteration 30515 => Loss: 6.70156414144287904122\n",
      "Iteration 30516 => Loss: 6.70156368514390532454\n",
      "Iteration 30517 => Loss: 6.70156322890641398260\n",
      "Iteration 30518 => Loss: 6.70156277273039968634\n",
      "Iteration 30519 => Loss: 6.70156231661586687665\n",
      "Iteration 30520 => Loss: 6.70156186056277913821\n",
      "Iteration 30521 => Loss: 6.70156140457114979370\n",
      "Iteration 30522 => Loss: 6.70156094864096996133\n",
      "Iteration 30523 => Loss: 6.70156049277222454208\n",
      "Iteration 30524 => Loss: 6.70156003696490643051\n",
      "Iteration 30525 => Loss: 6.70155958121900763302\n",
      "Iteration 30526 => Loss: 6.70155912553452637326\n",
      "Iteration 30527 => Loss: 6.70155866991143867040\n",
      "Iteration 30528 => Loss: 6.70155821434975074169\n",
      "Iteration 30529 => Loss: 6.70155775884945725807\n",
      "Iteration 30530 => Loss: 6.70155730341053068599\n",
      "Iteration 30531 => Loss: 6.70155684803298168362\n",
      "Iteration 30532 => Loss: 6.70155639271679515190\n",
      "Iteration 30533 => Loss: 6.70155593746195776816\n",
      "Iteration 30534 => Loss: 6.70155548226846953241\n",
      "Iteration 30535 => Loss: 6.70155502713631712197\n",
      "Iteration 30536 => Loss: 6.70155457206549343141\n",
      "Iteration 30537 => Loss: 6.70155411705599313166\n",
      "Iteration 30538 => Loss: 6.70155366210780645275\n",
      "Iteration 30539 => Loss: 6.70155320722091740748\n",
      "Iteration 30540 => Loss: 6.70155275239533132492\n",
      "Iteration 30541 => Loss: 6.70155229763102866514\n",
      "Iteration 30542 => Loss: 6.70155184292800587542\n",
      "Iteration 30543 => Loss: 6.70155138828625585035\n",
      "Iteration 30544 => Loss: 6.70155093370576970813\n",
      "Iteration 30545 => Loss: 6.70155047918653856698\n",
      "Iteration 30546 => Loss: 6.70155002472855265694\n",
      "Iteration 30547 => Loss: 6.70154957033180220805\n",
      "Iteration 30548 => Loss: 6.70154911599628544394\n",
      "Iteration 30549 => Loss: 6.70154866172198726559\n",
      "Iteration 30550 => Loss: 6.70154820750890589665\n",
      "Iteration 30551 => Loss: 6.70154775335702979078\n",
      "Iteration 30552 => Loss: 6.70154729926634384896\n",
      "Iteration 30553 => Loss: 6.70154684523685340025\n",
      "Iteration 30554 => Loss: 6.70154639126854512199\n",
      "Iteration 30555 => Loss: 6.70154593736140657967\n",
      "Iteration 30556 => Loss: 6.70154548351542977969\n",
      "Iteration 30557 => Loss: 6.70154502973061116933\n",
      "Iteration 30558 => Loss: 6.70154457600694186681\n",
      "Iteration 30559 => Loss: 6.70154412234440766127\n",
      "Iteration 30560 => Loss: 6.70154366874300944090\n",
      "Iteration 30561 => Loss: 6.70154321520272677759\n",
      "Iteration 30562 => Loss: 6.70154276172356233587\n",
      "Iteration 30563 => Loss: 6.70154230830550368125\n",
      "Iteration 30564 => Loss: 6.70154185494854637284\n",
      "Iteration 30565 => Loss: 6.70154140165268152884\n",
      "Iteration 30566 => Loss: 6.70154094841789227388\n",
      "Iteration 30567 => Loss: 6.70154049524417683159\n",
      "Iteration 30568 => Loss: 6.70154004213152365566\n",
      "Iteration 30569 => Loss: 6.70153958907993452243\n",
      "Iteration 30570 => Loss: 6.70153913608938900381\n",
      "Iteration 30571 => Loss: 6.70153868315989154070\n",
      "Iteration 30572 => Loss: 6.70153823029141726408\n",
      "Iteration 30573 => Loss: 6.70153777748397327940\n",
      "Iteration 30574 => Loss: 6.70153732473753915855\n",
      "Iteration 30575 => Loss: 6.70153687205211490152\n",
      "Iteration 30576 => Loss: 6.70153641942769429107\n",
      "Iteration 30577 => Loss: 6.70153596686425956364\n",
      "Iteration 30578 => Loss: 6.70153551436181071921\n",
      "Iteration 30579 => Loss: 6.70153506192033443511\n",
      "Iteration 30580 => Loss: 6.70153460953983159953\n",
      "Iteration 30581 => Loss: 6.70153415722027467893\n",
      "Iteration 30582 => Loss: 6.70153370496167699599\n",
      "Iteration 30583 => Loss: 6.70153325276402167532\n",
      "Iteration 30584 => Loss: 6.70153280062729272970\n",
      "Iteration 30585 => Loss: 6.70153234855149726457\n",
      "Iteration 30586 => Loss: 6.70153189653661218728\n",
      "Iteration 30587 => Loss: 6.70153144458264282690\n",
      "Iteration 30588 => Loss: 6.70153099268956875534\n",
      "Iteration 30589 => Loss: 6.70153054085739441348\n",
      "Iteration 30590 => Loss: 6.70153008908609759686\n",
      "Iteration 30591 => Loss: 6.70152963737568097002\n",
      "Iteration 30592 => Loss: 6.70152918572612765757\n",
      "Iteration 30593 => Loss: 6.70152873413743588316\n",
      "Iteration 30594 => Loss: 6.70152828260960120588\n",
      "Iteration 30595 => Loss: 6.70152783114260319763\n",
      "Iteration 30596 => Loss: 6.70152737973644363478\n",
      "Iteration 30597 => Loss: 6.70152692839110741829\n",
      "Iteration 30598 => Loss: 6.70152647710659543634\n",
      "Iteration 30599 => Loss: 6.70152602588288814900\n",
      "Iteration 30600 => Loss: 6.70152557471998910898\n",
      "Iteration 30601 => Loss: 6.70152512361788232909\n",
      "Iteration 30602 => Loss: 6.70152467257656248023\n",
      "Iteration 30603 => Loss: 6.70152422159601357521\n",
      "Iteration 30604 => Loss: 6.70152377067624271945\n",
      "Iteration 30605 => Loss: 6.70152331981723570209\n",
      "Iteration 30606 => Loss: 6.70152286901897298321\n",
      "Iteration 30607 => Loss: 6.70152241828145456282\n",
      "Iteration 30608 => Loss: 6.70152196760468488179\n",
      "Iteration 30609 => Loss: 6.70152151698863107754\n",
      "Iteration 30610 => Loss: 6.70152106643330203184\n",
      "Iteration 30611 => Loss: 6.70152061593869063927\n",
      "Iteration 30612 => Loss: 6.70152016550477469536\n",
      "Iteration 30613 => Loss: 6.70151971513155952920\n",
      "Iteration 30614 => Loss: 6.70151926481903181809\n",
      "Iteration 30615 => Loss: 6.70151881456718356844\n",
      "Iteration 30616 => Loss: 6.70151836437600945118\n",
      "Iteration 30617 => Loss: 6.70151791424549614362\n",
      "Iteration 30618 => Loss: 6.70151746417563742853\n",
      "Iteration 30619 => Loss: 6.70151701416642975317\n",
      "Iteration 30620 => Loss: 6.70151656421785624218\n",
      "Iteration 30621 => Loss: 6.70151611432991778372\n",
      "Iteration 30622 => Loss: 6.70151566450259394969\n",
      "Iteration 30623 => Loss: 6.70151521473589717459\n",
      "Iteration 30624 => Loss: 6.70151476502979015493\n",
      "Iteration 30625 => Loss: 6.70151431538428887791\n",
      "Iteration 30626 => Loss: 6.70151386579938357357\n",
      "Iteration 30627 => Loss: 6.70151341627505825471\n",
      "Iteration 30628 => Loss: 6.70151296681129249322\n",
      "Iteration 30629 => Loss: 6.70151251740810849356\n",
      "Iteration 30630 => Loss: 6.70151206806546895223\n",
      "Iteration 30631 => Loss: 6.70151161878338808009\n",
      "Iteration 30632 => Loss: 6.70151116956184189632\n",
      "Iteration 30633 => Loss: 6.70151072040082862458\n",
      "Iteration 30634 => Loss: 6.70151027130034560031\n",
      "Iteration 30635 => Loss: 6.70150982226037594813\n",
      "Iteration 30636 => Loss: 6.70150937328091167444\n",
      "Iteration 30637 => Loss: 6.70150892436195189106\n",
      "Iteration 30638 => Loss: 6.70150847550348416348\n",
      "Iteration 30639 => Loss: 6.70150802670549694540\n",
      "Iteration 30640 => Loss: 6.70150757796798401955\n",
      "Iteration 30641 => Loss: 6.70150712929094005688\n",
      "Iteration 30642 => Loss: 6.70150668067435795194\n",
      "Iteration 30643 => Loss: 6.70150623211822082936\n",
      "Iteration 30644 => Loss: 6.70150578362252868914\n",
      "Iteration 30645 => Loss: 6.70150533518727709037\n",
      "Iteration 30646 => Loss: 6.70150488681245004585\n",
      "Iteration 30647 => Loss: 6.70150443849803334473\n",
      "Iteration 30648 => Loss: 6.70150399024403942150\n",
      "Iteration 30649 => Loss: 6.70150354205044340716\n",
      "Iteration 30650 => Loss: 6.70150309391723464358\n",
      "Iteration 30651 => Loss: 6.70150264584441934801\n",
      "Iteration 30652 => Loss: 6.70150219783197886869\n",
      "Iteration 30653 => Loss: 6.70150174987990965292\n",
      "Iteration 30654 => Loss: 6.70150130198820370708\n",
      "Iteration 30655 => Loss: 6.70150085415684504397\n",
      "Iteration 30656 => Loss: 6.70150040638583721631\n",
      "Iteration 30657 => Loss: 6.70149995867516690140\n",
      "Iteration 30658 => Loss: 6.70149951102482344112\n",
      "Iteration 30659 => Loss: 6.70149906343479973003\n",
      "Iteration 30660 => Loss: 6.70149861590508866271\n",
      "Iteration 30661 => Loss: 6.70149816843568668645\n",
      "Iteration 30662 => Loss: 6.70149772102656893225\n",
      "Iteration 30663 => Loss: 6.70149727367775760456\n",
      "Iteration 30664 => Loss: 6.70149682638921273536\n",
      "Iteration 30665 => Loss: 6.70149637916094942369\n",
      "Iteration 30666 => Loss: 6.70149593199294546508\n",
      "Iteration 30667 => Loss: 6.70149548488519819500\n",
      "Iteration 30668 => Loss: 6.70149503783770050802\n",
      "Iteration 30669 => Loss: 6.70149459085043819329\n",
      "Iteration 30670 => Loss: 6.70149414392341213897\n",
      "Iteration 30671 => Loss: 6.70149369705659925245\n",
      "Iteration 30672 => Loss: 6.70149325025001285638\n",
      "Iteration 30673 => Loss: 6.70149280350363607539\n",
      "Iteration 30674 => Loss: 6.70149235681744759319\n",
      "Iteration 30675 => Loss: 6.70149191019145806791\n",
      "Iteration 30676 => Loss: 6.70149146362564795965\n",
      "Iteration 30677 => Loss: 6.70149101712001371567\n",
      "Iteration 30678 => Loss: 6.70149057067454645420\n",
      "Iteration 30679 => Loss: 6.70149012428923374074\n",
      "Iteration 30680 => Loss: 6.70148967796407468711\n",
      "Iteration 30681 => Loss: 6.70148923169906129971\n",
      "Iteration 30682 => Loss: 6.70148878549417492678\n",
      "Iteration 30683 => Loss: 6.70148833934941556834\n",
      "Iteration 30684 => Loss: 6.70148789326478322437\n",
      "Iteration 30685 => Loss: 6.70148744724025213770\n",
      "Iteration 30686 => Loss: 6.70148700127582230834\n",
      "Iteration 30687 => Loss: 6.70148655537149107175\n",
      "Iteration 30688 => Loss: 6.70148610952724599343\n",
      "Iteration 30689 => Loss: 6.70148566374307552707\n",
      "Iteration 30690 => Loss: 6.70148521801898144901\n",
      "Iteration 30691 => Loss: 6.70148477235493977844\n",
      "Iteration 30692 => Loss: 6.70148432675095229172\n",
      "Iteration 30693 => Loss: 6.70148388120701632431\n",
      "Iteration 30694 => Loss: 6.70148343572311055993\n",
      "Iteration 30695 => Loss: 6.70148299029923411041\n",
      "Iteration 30696 => Loss: 6.70148254493538519938\n",
      "Iteration 30697 => Loss: 6.70148209963154162239\n",
      "Iteration 30698 => Loss: 6.70148165438770782032\n",
      "Iteration 30699 => Loss: 6.70148120920386602961\n",
      "Iteration 30700 => Loss: 6.70148076408001713844\n",
      "Iteration 30701 => Loss: 6.70148031901614338324\n",
      "Iteration 30702 => Loss: 6.70147987401224742854\n",
      "Iteration 30703 => Loss: 6.70147942906831683985\n",
      "Iteration 30704 => Loss: 6.70147898418433474177\n",
      "Iteration 30705 => Loss: 6.70147853936030823974\n",
      "Iteration 30706 => Loss: 6.70147809459621690564\n",
      "Iteration 30707 => Loss: 6.70147764989206962127\n",
      "Iteration 30708 => Loss: 6.70147720524782997131\n",
      "Iteration 30709 => Loss: 6.70147676066351127844\n",
      "Iteration 30710 => Loss: 6.70147631613910110815\n",
      "Iteration 30711 => Loss: 6.70147587167459146684\n",
      "Iteration 30712 => Loss: 6.70147542726997347273\n",
      "Iteration 30713 => Loss: 6.70147498292523380314\n",
      "Iteration 30714 => Loss: 6.70147453864038222804\n",
      "Iteration 30715 => Loss: 6.70147409441538854935\n",
      "Iteration 30716 => Loss: 6.70147365025025631979\n",
      "Iteration 30717 => Loss: 6.70147320614497221669\n",
      "Iteration 30718 => Loss: 6.70147276209953535187\n",
      "Iteration 30719 => Loss: 6.70147231811393773171\n",
      "Iteration 30720 => Loss: 6.70147187418816159266\n",
      "Iteration 30721 => Loss: 6.70147143032220249381\n",
      "Iteration 30722 => Loss: 6.70147098651605599429\n",
      "Iteration 30723 => Loss: 6.70147054276971676501\n",
      "Iteration 30724 => Loss: 6.70147009908317059512\n",
      "Iteration 30725 => Loss: 6.70146965545640771467\n",
      "Iteration 30726 => Loss: 6.70146921188942723546\n",
      "Iteration 30727 => Loss: 6.70146876838221494666\n",
      "Iteration 30728 => Loss: 6.70146832493476640735\n",
      "Iteration 30729 => Loss: 6.70146788154707184759\n",
      "Iteration 30730 => Loss: 6.70146743821912327377\n",
      "Iteration 30731 => Loss: 6.70146699495091446863\n",
      "Iteration 30732 => Loss: 6.70146655174243655040\n",
      "Iteration 30733 => Loss: 6.70146610859368241364\n",
      "Iteration 30734 => Loss: 6.70146566550463340661\n",
      "Iteration 30735 => Loss: 6.70146522247529752292\n",
      "Iteration 30736 => Loss: 6.70146477950565699899\n",
      "Iteration 30737 => Loss: 6.70146433659571538755\n",
      "Iteration 30738 => Loss: 6.70146389374545403683\n",
      "Iteration 30739 => Loss: 6.70146345095485873600\n",
      "Iteration 30740 => Loss: 6.70146300822393481411\n",
      "Iteration 30741 => Loss: 6.70146256555266628396\n",
      "Iteration 30742 => Loss: 6.70146212294105492191\n",
      "Iteration 30743 => Loss: 6.70146168038907408260\n",
      "Iteration 30744 => Loss: 6.70146123789673175963\n",
      "Iteration 30745 => Loss: 6.70146079546401640670\n",
      "Iteration 30746 => Loss: 6.70146035309092180654\n",
      "Iteration 30747 => Loss: 6.70145991077743463649\n",
      "Iteration 30748 => Loss: 6.70145946852354867929\n",
      "Iteration 30749 => Loss: 6.70145902632925594133\n",
      "Iteration 30750 => Loss: 6.70145858419454931720\n",
      "Iteration 30751 => Loss: 6.70145814211941637240\n",
      "Iteration 30752 => Loss: 6.70145770010385355420\n",
      "Iteration 30753 => Loss: 6.70145725814785819807\n",
      "Iteration 30754 => Loss: 6.70145681625141698134\n",
      "Iteration 30755 => Loss: 6.70145637441451924587\n",
      "Iteration 30756 => Loss: 6.70145593263716143895\n",
      "Iteration 30757 => Loss: 6.70145549091931957975\n",
      "Iteration 30758 => Loss: 6.70145504926101676091\n",
      "Iteration 30759 => Loss: 6.70145460766222367255\n",
      "Iteration 30760 => Loss: 6.70145416612292788017\n",
      "Iteration 30761 => Loss: 6.70145372464313560101\n",
      "Iteration 30762 => Loss: 6.70145328322283173605\n",
      "Iteration 30763 => Loss: 6.70145284186201095622\n",
      "Iteration 30764 => Loss: 6.70145240056066260337\n",
      "Iteration 30765 => Loss: 6.70145195931877601936\n",
      "Iteration 30766 => Loss: 6.70145151813635209237\n",
      "Iteration 30767 => Loss: 6.70145107701337305883\n",
      "Iteration 30768 => Loss: 6.70145063594983803057\n",
      "Iteration 30769 => Loss: 6.70145019494573812580\n",
      "Iteration 30770 => Loss: 6.70144975400105646912\n",
      "Iteration 30771 => Loss: 6.70144931311580105415\n",
      "Iteration 30772 => Loss: 6.70144887228994967643\n",
      "Iteration 30773 => Loss: 6.70144843152350411231\n",
      "Iteration 30774 => Loss: 6.70144799081644393368\n",
      "Iteration 30775 => Loss: 6.70144755016877535780\n",
      "Iteration 30776 => Loss: 6.70144710958048150928\n",
      "Iteration 30777 => Loss: 6.70144666905155705905\n",
      "Iteration 30778 => Loss: 6.70144622858199667803\n",
      "Iteration 30779 => Loss: 6.70144578817178704355\n",
      "Iteration 30780 => Loss: 6.70144534782092637926\n",
      "Iteration 30781 => Loss: 6.70144490752939780975\n",
      "Iteration 30782 => Loss: 6.70144446729719867051\n",
      "Iteration 30783 => Loss: 6.70144402712432718516\n",
      "Iteration 30784 => Loss: 6.70144358701076381379\n",
      "Iteration 30785 => Loss: 6.70144314695650411551\n",
      "Iteration 30786 => Loss: 6.70144270696154453759\n",
      "Iteration 30787 => Loss: 6.70144226702587619826\n",
      "Iteration 30788 => Loss: 6.70144182714948133395\n",
      "Iteration 30789 => Loss: 6.70144138733236616190\n",
      "Iteration 30790 => Loss: 6.70144094757452180033\n",
      "Iteration 30791 => Loss: 6.70144050787592782115\n",
      "Iteration 30792 => Loss: 6.70144006823658333616\n",
      "Iteration 30793 => Loss: 6.70143962865648212812\n",
      "Iteration 30794 => Loss: 6.70143918913561531525\n",
      "Iteration 30795 => Loss: 6.70143874967397046305\n",
      "Iteration 30796 => Loss: 6.70143831027154135427\n",
      "Iteration 30797 => Loss: 6.70143787092832710073\n",
      "Iteration 30798 => Loss: 6.70143743164430905068\n",
      "Iteration 30799 => Loss: 6.70143699241949253320\n",
      "Iteration 30800 => Loss: 6.70143655325385712018\n",
      "Iteration 30801 => Loss: 6.70143611414740281162\n",
      "Iteration 30802 => Loss: 6.70143567510011095578\n",
      "Iteration 30803 => Loss: 6.70143523611198421719\n",
      "Iteration 30804 => Loss: 6.70143479718300927317\n",
      "Iteration 30805 => Loss: 6.70143435831318345919\n",
      "Iteration 30806 => Loss: 6.70143391950249078803\n",
      "Iteration 30807 => Loss: 6.70143348075093836513\n",
      "Iteration 30808 => Loss: 6.70143304205850220967\n",
      "Iteration 30809 => Loss: 6.70143260342517343986\n",
      "Iteration 30810 => Loss: 6.70143216485095383206\n",
      "Iteration 30811 => Loss: 6.70143172633583095177\n",
      "Iteration 30812 => Loss: 6.70143128787980302263\n",
      "Iteration 30813 => Loss: 6.70143084948285316926\n",
      "Iteration 30814 => Loss: 6.70143041114498227984\n",
      "Iteration 30815 => Loss: 6.70142997286617614350\n",
      "Iteration 30816 => Loss: 6.70142953464642143757\n",
      "Iteration 30817 => Loss: 6.70142909648572615566\n",
      "Iteration 30818 => Loss: 6.70142865838406631696\n",
      "Iteration 30819 => Loss: 6.70142822034144103327\n",
      "Iteration 30820 => Loss: 6.70142778235784764007\n",
      "Iteration 30821 => Loss: 6.70142734443326482108\n",
      "Iteration 30822 => Loss: 6.70142690656770145807\n",
      "Iteration 30823 => Loss: 6.70142646876113534660\n",
      "Iteration 30824 => Loss: 6.70142603101355849304\n",
      "Iteration 30825 => Loss: 6.70142559332497356195\n",
      "Iteration 30826 => Loss: 6.70142515569536634246\n",
      "Iteration 30827 => Loss: 6.70142471812473594639\n",
      "Iteration 30828 => Loss: 6.70142428061305572840\n",
      "Iteration 30829 => Loss: 6.70142384316034434022\n",
      "Iteration 30830 => Loss: 6.70142340576656803108\n",
      "Iteration 30831 => Loss: 6.70142296843173923548\n",
      "Iteration 30832 => Loss: 6.70142253115583397260\n",
      "Iteration 30833 => Loss: 6.70142209393885668334\n",
      "Iteration 30834 => Loss: 6.70142165678078605140\n",
      "Iteration 30835 => Loss: 6.70142121968163095858\n",
      "Iteration 30836 => Loss: 6.70142078264137541765\n",
      "Iteration 30837 => Loss: 6.70142034566000877049\n",
      "Iteration 30838 => Loss: 6.70141990873752568803\n",
      "Iteration 30839 => Loss: 6.70141947187391728846\n",
      "Iteration 30840 => Loss: 6.70141903506918001909\n",
      "Iteration 30841 => Loss: 6.70141859832329611635\n",
      "Iteration 30842 => Loss: 6.70141816163626469205\n",
      "Iteration 30843 => Loss: 6.70141772500808485802\n",
      "Iteration 30844 => Loss: 6.70141728843873263344\n",
      "Iteration 30845 => Loss: 6.70141685192820890649\n",
      "Iteration 30846 => Loss: 6.70141641547650923627\n",
      "Iteration 30847 => Loss: 6.70141597908361497105\n",
      "Iteration 30848 => Loss: 6.70141554274952611081\n",
      "Iteration 30849 => Loss: 6.70141510647423910285\n",
      "Iteration 30850 => Loss: 6.70141467025773795996\n",
      "Iteration 30851 => Loss: 6.70141423410001557670\n",
      "Iteration 30852 => Loss: 6.70141379800106395948\n",
      "Iteration 30853 => Loss: 6.70141336196087777921\n",
      "Iteration 30854 => Loss: 6.70141292597944548959\n",
      "Iteration 30855 => Loss: 6.70141249005676442607\n",
      "Iteration 30856 => Loss: 6.70141205419282837141\n",
      "Iteration 30857 => Loss: 6.70141161838762222658\n",
      "Iteration 30858 => Loss: 6.70141118264113888614\n",
      "Iteration 30859 => Loss: 6.70141074695336946832\n",
      "Iteration 30860 => Loss: 6.70141031132431397310\n",
      "Iteration 30861 => Loss: 6.70140987575396174236\n",
      "Iteration 30862 => Loss: 6.70140944024229678888\n",
      "Iteration 30863 => Loss: 6.70140900478931733630\n",
      "Iteration 30864 => Loss: 6.70140856939501894374\n",
      "Iteration 30865 => Loss: 6.70140813405938295944\n",
      "Iteration 30866 => Loss: 6.70140769878242092972\n",
      "Iteration 30867 => Loss: 6.70140726356410354470\n",
      "Iteration 30868 => Loss: 6.70140682840443346890\n",
      "Iteration 30869 => Loss: 6.70140639330340004420\n",
      "Iteration 30870 => Loss: 6.70140595826100060606\n",
      "Iteration 30871 => Loss: 6.70140552327721739090\n",
      "Iteration 30872 => Loss: 6.70140508835205750415\n",
      "Iteration 30873 => Loss: 6.70140465348549074776\n",
      "Iteration 30874 => Loss: 6.70140421867752689167\n",
      "Iteration 30875 => Loss: 6.70140378392815883046\n",
      "Iteration 30876 => Loss: 6.70140334923737057693\n",
      "Iteration 30877 => Loss: 6.70140291460515769018\n",
      "Iteration 30878 => Loss: 6.70140248003150951206\n",
      "Iteration 30879 => Loss: 6.70140204551642248987\n",
      "Iteration 30880 => Loss: 6.70140161105988152457\n",
      "Iteration 30881 => Loss: 6.70140117666188928069\n",
      "Iteration 30882 => Loss: 6.70140074232242799468\n",
      "Iteration 30883 => Loss: 6.70140030804148967292\n",
      "Iteration 30884 => Loss: 6.70139987381908230901\n",
      "Iteration 30885 => Loss: 6.70139943965518725122\n",
      "Iteration 30886 => Loss: 6.70139900554978940050\n",
      "Iteration 30887 => Loss: 6.70139857150288609233\n",
      "Iteration 30888 => Loss: 6.70139813751447110945\n",
      "Iteration 30889 => Loss: 6.70139770358454267551\n",
      "Iteration 30890 => Loss: 6.70139726971307414516\n",
      "Iteration 30891 => Loss: 6.70139683590007884106\n",
      "Iteration 30892 => Loss: 6.70139640214553633513\n",
      "Iteration 30893 => Loss: 6.70139596844944396281\n",
      "Iteration 30894 => Loss: 6.70139553481178928962\n",
      "Iteration 30895 => Loss: 6.70139510123256965102\n",
      "Iteration 30896 => Loss: 6.70139466771178060611\n",
      "Iteration 30897 => Loss: 6.70139423424939639773\n",
      "Iteration 30898 => Loss: 6.70139380084542857219\n",
      "Iteration 30899 => Loss: 6.70139336749986203046\n",
      "Iteration 30900 => Loss: 6.70139293421268700257\n",
      "Iteration 30901 => Loss: 6.70139250098389904764\n",
      "Iteration 30902 => Loss: 6.70139206781348573116\n",
      "Iteration 30903 => Loss: 6.70139163470144616497\n",
      "Iteration 30904 => Loss: 6.70139120164776702637\n",
      "Iteration 30905 => Loss: 6.70139076865243854542\n",
      "Iteration 30906 => Loss: 6.70139033571546249846\n",
      "Iteration 30907 => Loss: 6.70138990283682112192\n",
      "Iteration 30908 => Loss: 6.70138947001650819857\n",
      "Iteration 30909 => Loss: 6.70138903725452639293\n",
      "Iteration 30910 => Loss: 6.70138860455085172418\n",
      "Iteration 30911 => Loss: 6.70138817190548241598\n",
      "Iteration 30912 => Loss: 6.70138773931841225107\n",
      "Iteration 30913 => Loss: 6.70138730678964211762\n",
      "Iteration 30914 => Loss: 6.70138687431914714665\n",
      "Iteration 30915 => Loss: 6.70138644190692911451\n",
      "Iteration 30916 => Loss: 6.70138600955297381034\n",
      "Iteration 30917 => Loss: 6.70138557725729278047\n",
      "Iteration 30918 => Loss: 6.70138514501985316230\n",
      "Iteration 30919 => Loss: 6.70138471284065850853\n",
      "Iteration 30920 => Loss: 6.70138428071969638466\n",
      "Iteration 30921 => Loss: 6.70138384865696501436\n",
      "Iteration 30922 => Loss: 6.70138341665245818035\n",
      "Iteration 30923 => Loss: 6.70138298470616255997\n",
      "Iteration 30924 => Loss: 6.70138255281807015962\n",
      "Iteration 30925 => Loss: 6.70138212098818009110\n",
      "Iteration 30926 => Loss: 6.70138168921646926179\n",
      "Iteration 30927 => Loss: 6.70138125750294477712\n",
      "Iteration 30928 => Loss: 6.70138082584759597893\n",
      "Iteration 30929 => Loss: 6.70138039425041043273\n",
      "Iteration 30930 => Loss: 6.70137996271138192128\n",
      "Iteration 30931 => Loss: 6.70137953123050333915\n",
      "Iteration 30932 => Loss: 6.70137909980776846908\n",
      "Iteration 30933 => Loss: 6.70137866844316665293\n",
      "Iteration 30934 => Loss: 6.70137823713669256165\n",
      "Iteration 30935 => Loss: 6.70137780588833553708\n",
      "Iteration 30936 => Loss: 6.70137737469809291468\n",
      "Iteration 30937 => Loss: 6.70137694356594604272\n",
      "Iteration 30938 => Loss: 6.70137651249190202662\n",
      "Iteration 30939 => Loss: 6.70137608147593688557\n",
      "Iteration 30940 => Loss: 6.70137565051805950134\n",
      "Iteration 30941 => Loss: 6.70137521961824589312\n",
      "Iteration 30942 => Loss: 6.70137478877650227815\n",
      "Iteration 30943 => Loss: 6.70137435799281533377\n",
      "Iteration 30944 => Loss: 6.70137392726716907276\n",
      "Iteration 30945 => Loss: 6.70137349659956882419\n",
      "Iteration 30946 => Loss: 6.70137306599000037721\n",
      "Iteration 30947 => Loss: 6.70137263543845485003\n",
      "Iteration 30948 => Loss: 6.70137220494492691358\n",
      "Iteration 30949 => Loss: 6.70137177450941301515\n",
      "Iteration 30950 => Loss: 6.70137134413189361481\n",
      "Iteration 30951 => Loss: 6.70137091381236871257\n",
      "Iteration 30952 => Loss: 6.70137048355083120299\n",
      "Iteration 30953 => Loss: 6.70137005334727309247\n",
      "Iteration 30954 => Loss: 6.70136962320168461105\n",
      "Iteration 30955 => Loss: 6.70136919311405332422\n",
      "Iteration 30956 => Loss: 6.70136876308437923200\n",
      "Iteration 30957 => Loss: 6.70136833311265167623\n",
      "Iteration 30958 => Loss: 6.70136790319886621603\n",
      "Iteration 30959 => Loss: 6.70136747334300775236\n",
      "Iteration 30960 => Loss: 6.70136704354507362069\n",
      "Iteration 30961 => Loss: 6.70136661380505493923\n",
      "Iteration 30962 => Loss: 6.70136618412294637892\n",
      "Iteration 30963 => Loss: 6.70136575449873284072\n",
      "Iteration 30964 => Loss: 6.70136532493241254826\n",
      "Iteration 30965 => Loss: 6.70136489542397306707\n",
      "Iteration 30966 => Loss: 6.70136446597341439713\n",
      "Iteration 30967 => Loss: 6.70136403658072143941\n",
      "Iteration 30968 => Loss: 6.70136360724589241755\n",
      "Iteration 30969 => Loss: 6.70136317796891756160\n",
      "Iteration 30970 => Loss: 6.70136274874978177252\n",
      "Iteration 30971 => Loss: 6.70136231958848860302\n",
      "Iteration 30972 => Loss: 6.70136189048502295407\n",
      "Iteration 30973 => Loss: 6.70136146143937594388\n",
      "Iteration 30974 => Loss: 6.70136103245154757246\n",
      "Iteration 30975 => Loss: 6.70136060352152718167\n",
      "Iteration 30976 => Loss: 6.70136017464930056065\n",
      "Iteration 30977 => Loss: 6.70135974583486238032\n",
      "Iteration 30978 => Loss: 6.70135931707821086434\n",
      "Iteration 30979 => Loss: 6.70135888837933535456\n",
      "Iteration 30980 => Loss: 6.70135845973822519284\n",
      "Iteration 30981 => Loss: 6.70135803115487416193\n",
      "Iteration 30982 => Loss: 6.70135760262927338005\n",
      "Iteration 30983 => Loss: 6.70135717416141929448\n",
      "Iteration 30984 => Loss: 6.70135674575130124708\n",
      "Iteration 30985 => Loss: 6.70135631739891213243\n",
      "Iteration 30986 => Loss: 6.70135588910423862785\n",
      "Iteration 30987 => Loss: 6.70135546086728073334\n",
      "Iteration 30988 => Loss: 6.70135503268803223165\n",
      "Iteration 30989 => Loss: 6.70135460456647535921\n",
      "Iteration 30990 => Loss: 6.70135417650261011602\n",
      "Iteration 30991 => Loss: 6.70135374849642939665\n",
      "Iteration 30992 => Loss: 6.70135332054791277301\n",
      "Iteration 30993 => Loss: 6.70135289265707090323\n",
      "Iteration 30994 => Loss: 6.70135246482388335920\n",
      "Iteration 30995 => Loss: 6.70135203704834836458\n",
      "Iteration 30996 => Loss: 6.70135160933045703757\n",
      "Iteration 30997 => Loss: 6.70135118167020138458\n",
      "Iteration 30998 => Loss: 6.70135075406756719474\n",
      "Iteration 30999 => Loss: 6.70135032652255535623\n",
      "Iteration 31000 => Loss: 6.70134989903515965182\n",
      "Iteration 31001 => Loss: 6.70134947160536054156\n",
      "Iteration 31002 => Loss: 6.70134904423316690725\n",
      "Iteration 31003 => Loss: 6.70134861691855476806\n",
      "Iteration 31004 => Loss: 6.70134818966152590036\n",
      "Iteration 31005 => Loss: 6.70134776246206875783\n",
      "Iteration 31006 => Loss: 6.70134733532017889956\n",
      "Iteration 31007 => Loss: 6.70134690823584300290\n",
      "Iteration 31008 => Loss: 6.70134648120905840329\n",
      "Iteration 31009 => Loss: 6.70134605423981710715\n",
      "Iteration 31010 => Loss: 6.70134562732810046271\n",
      "Iteration 31011 => Loss: 6.70134520047392268083\n",
      "Iteration 31012 => Loss: 6.70134477367725978070\n",
      "Iteration 31013 => Loss: 6.70134434693811087413\n",
      "Iteration 31014 => Loss: 6.70134392025645819757\n",
      "Iteration 31015 => Loss: 6.70134349363230619190\n",
      "Iteration 31016 => Loss: 6.70134306706563886991\n",
      "Iteration 31017 => Loss: 6.70134264055645090252\n",
      "Iteration 31018 => Loss: 6.70134221410473340796\n",
      "Iteration 31019 => Loss: 6.70134178771048549805\n",
      "Iteration 31020 => Loss: 6.70134136137369473829\n",
      "Iteration 31021 => Loss: 6.70134093509434691782\n",
      "Iteration 31022 => Loss: 6.70134050887244381300\n",
      "Iteration 31023 => Loss: 6.70134008270797298934\n",
      "Iteration 31024 => Loss: 6.70133965660093178229\n",
      "Iteration 31025 => Loss: 6.70133923055129976376\n",
      "Iteration 31026 => Loss: 6.70133880455908581553\n",
      "Iteration 31027 => Loss: 6.70133837862427572674\n",
      "Iteration 31028 => Loss: 6.70133795274685795107\n",
      "Iteration 31029 => Loss: 6.70133752692682715946\n",
      "Iteration 31030 => Loss: 6.70133710116417091740\n",
      "Iteration 31031 => Loss: 6.70133667545889100126\n",
      "Iteration 31032 => Loss: 6.70133624981097320017\n",
      "Iteration 31033 => Loss: 6.70133582422041396143\n",
      "Iteration 31034 => Loss: 6.70133539868719729782\n",
      "Iteration 31035 => Loss: 6.70133497321132676205\n",
      "Iteration 31036 => Loss: 6.70133454779278547875\n",
      "Iteration 31037 => Loss: 6.70133412243157433608\n",
      "Iteration 31038 => Loss: 6.70133369712768534043\n",
      "Iteration 31039 => Loss: 6.70133327188108918193\n",
      "Iteration 31040 => Loss: 6.70133284669180717685\n",
      "Iteration 31041 => Loss: 6.70133242155981445620\n",
      "Iteration 31042 => Loss: 6.70133199648511013180\n",
      "Iteration 31043 => Loss: 6.70133157146768443369\n",
      "Iteration 31044 => Loss: 6.70133114650753025643\n",
      "Iteration 31045 => Loss: 6.70133072160464315914\n",
      "Iteration 31046 => Loss: 6.70133029675900449007\n",
      "Iteration 31047 => Loss: 6.70132987197061780194\n",
      "Iteration 31048 => Loss: 6.70132944723947421295\n",
      "Iteration 31049 => Loss: 6.70132902256555773590\n",
      "Iteration 31050 => Loss: 6.70132859794887369986\n",
      "Iteration 31051 => Loss: 6.70132817338939901219\n",
      "Iteration 31052 => Loss: 6.70132774888713544925\n",
      "Iteration 31053 => Loss: 6.70132732444207768197\n",
      "Iteration 31054 => Loss: 6.70132690005421238766\n",
      "Iteration 31055 => Loss: 6.70132647572352890819\n",
      "Iteration 31056 => Loss: 6.70132605145002369085\n",
      "Iteration 31057 => Loss: 6.70132562723369229474\n",
      "Iteration 31058 => Loss: 6.70132520307452406172\n",
      "Iteration 31059 => Loss: 6.70132477897251810361\n",
      "Iteration 31060 => Loss: 6.70132435492764955143\n",
      "Iteration 31061 => Loss: 6.70132393093992373423\n",
      "Iteration 31062 => Loss: 6.70132350700933265841\n",
      "Iteration 31063 => Loss: 6.70132308313586477766\n",
      "Iteration 31064 => Loss: 6.70132265931951298654\n",
      "Iteration 31065 => Loss: 6.70132223556027017963\n",
      "Iteration 31066 => Loss: 6.70132181185813458058\n",
      "Iteration 31067 => Loss: 6.70132138821308487309\n",
      "Iteration 31068 => Loss: 6.70132096462512460988\n",
      "Iteration 31069 => Loss: 6.70132054109424490917\n",
      "Iteration 31070 => Loss: 6.70132011762043688918\n",
      "Iteration 31071 => Loss: 6.70131969420368811541\n",
      "Iteration 31072 => Loss: 6.70131927084399503514\n",
      "Iteration 31073 => Loss: 6.70131884754135409565\n",
      "Iteration 31074 => Loss: 6.70131842429575108611\n",
      "Iteration 31075 => Loss: 6.70131800110717890107\n",
      "Iteration 31076 => Loss: 6.70131757797563132328\n",
      "Iteration 31077 => Loss: 6.70131715490110035915\n",
      "Iteration 31078 => Loss: 6.70131673188358067961\n",
      "Iteration 31079 => Loss: 6.70131630892305985014\n",
      "Iteration 31080 => Loss: 6.70131588601954053530\n",
      "Iteration 31081 => Loss: 6.70131546317299520155\n",
      "Iteration 31082 => Loss: 6.70131504038344250063\n",
      "Iteration 31083 => Loss: 6.70131461765084601723\n",
      "Iteration 31084 => Loss: 6.70131419497522440309\n",
      "Iteration 31085 => Loss: 6.70131377235655278923\n",
      "Iteration 31086 => Loss: 6.70131334979483117564\n",
      "Iteration 31087 => Loss: 6.70131292729004801600\n",
      "Iteration 31088 => Loss: 6.70131250484220508667\n",
      "Iteration 31089 => Loss: 6.70131208245128018319\n",
      "Iteration 31090 => Loss: 6.70131166011726975285\n",
      "Iteration 31091 => Loss: 6.70131123784016669021\n",
      "Iteration 31092 => Loss: 6.70131081561997277163\n",
      "Iteration 31093 => Loss: 6.70131039345667200990\n",
      "Iteration 31094 => Loss: 6.70130997135025285871\n",
      "Iteration 31095 => Loss: 6.70130954930072331166\n",
      "Iteration 31096 => Loss: 6.70130912730805672339\n",
      "Iteration 31097 => Loss: 6.70130870537225487027\n",
      "Iteration 31098 => Loss: 6.70130828349330176508\n",
      "Iteration 31099 => Loss: 6.70130786167120895414\n",
      "Iteration 31100 => Loss: 6.70130743990594979209\n",
      "Iteration 31101 => Loss: 6.70130701819752871984\n",
      "Iteration 31102 => Loss: 6.70130659654592975016\n",
      "Iteration 31103 => Loss: 6.70130617495114222493\n",
      "Iteration 31104 => Loss: 6.70130575341317147320\n",
      "Iteration 31105 => Loss: 6.70130533193200772502\n",
      "Iteration 31106 => Loss: 6.70130491050763321681\n",
      "Iteration 31107 => Loss: 6.70130448914004617222\n",
      "Iteration 31108 => Loss: 6.70130406782923859765\n",
      "Iteration 31109 => Loss: 6.70130364657520072313\n",
      "Iteration 31110 => Loss: 6.70130322537792988413\n",
      "Iteration 31111 => Loss: 6.70130280423741275797\n",
      "Iteration 31112 => Loss: 6.70130238315364312740\n",
      "Iteration 31113 => Loss: 6.70130196212662010424\n",
      "Iteration 31114 => Loss: 6.70130154115632503675\n",
      "Iteration 31115 => Loss: 6.70130112024275792493\n",
      "Iteration 31116 => Loss: 6.70130069938591166334\n",
      "Iteration 31117 => Loss: 6.70130027858577026478\n",
      "Iteration 31118 => Loss: 6.70129985784233461743\n",
      "Iteration 31119 => Loss: 6.70129943715559583950\n",
      "Iteration 31120 => Loss: 6.70129901652554238467\n",
      "Iteration 31121 => Loss: 6.70129859595216981205\n",
      "Iteration 31122 => Loss: 6.70129817543546657532\n",
      "Iteration 31123 => Loss: 6.70129775497543622720\n",
      "Iteration 31124 => Loss: 6.70129733457205301050\n",
      "Iteration 31125 => Loss: 6.70129691422532491885\n",
      "Iteration 31126 => Loss: 6.70129649393523685319\n",
      "Iteration 31127 => Loss: 6.70129607370178170811\n",
      "Iteration 31128 => Loss: 6.70129565352495237818\n",
      "Iteration 31129 => Loss: 6.70129523340474708704\n",
      "Iteration 31130 => Loss: 6.70129481334114718294\n",
      "Iteration 31131 => Loss: 6.70129439333415533042\n",
      "Iteration 31132 => Loss: 6.70129397338375909499\n",
      "Iteration 31133 => Loss: 6.70129355348994693031\n",
      "Iteration 31134 => Loss: 6.70129313365271794822\n",
      "Iteration 31135 => Loss: 6.70129271387206504329\n",
      "Iteration 31136 => Loss: 6.70129229414797222830\n",
      "Iteration 31137 => Loss: 6.70129187448044394415\n",
      "Iteration 31138 => Loss: 6.70129145486945887455\n",
      "Iteration 31139 => Loss: 6.70129103531502146041\n",
      "Iteration 31140 => Loss: 6.70129061581711660267\n",
      "Iteration 31141 => Loss: 6.70129019637574163681\n",
      "Iteration 31142 => Loss: 6.70128977699088590470\n",
      "Iteration 31143 => Loss: 6.70128935766253785999\n",
      "Iteration 31144 => Loss: 6.70128893839070016725\n",
      "Iteration 31145 => Loss: 6.70128851917535861560\n",
      "Iteration 31146 => Loss: 6.70128810001650432326\n",
      "Iteration 31147 => Loss: 6.70128768091412663210\n",
      "Iteration 31148 => Loss: 6.70128726186823264754\n",
      "Iteration 31149 => Loss: 6.70128684287879572423\n",
      "Iteration 31150 => Loss: 6.70128642394582652031\n",
      "Iteration 31151 => Loss: 6.70128600506930105496\n",
      "Iteration 31152 => Loss: 6.70128558624922465725\n",
      "Iteration 31153 => Loss: 6.70128516748558400451\n",
      "Iteration 31154 => Loss: 6.70128474877836843859\n",
      "Iteration 31155 => Loss: 6.70128433012757440679\n",
      "Iteration 31156 => Loss: 6.70128391153319746820\n",
      "Iteration 31157 => Loss: 6.70128349299522074745\n",
      "Iteration 31158 => Loss: 6.70128307451364335634\n",
      "Iteration 31159 => Loss: 6.70128265608846085399\n",
      "Iteration 31160 => Loss: 6.70128223771966080591\n",
      "Iteration 31161 => Loss: 6.70128181940723077759\n",
      "Iteration 31162 => Loss: 6.70128140115116810449\n",
      "Iteration 31163 => Loss: 6.70128098295147101027\n",
      "Iteration 31164 => Loss: 6.70128056480812528406\n",
      "Iteration 31165 => Loss: 6.70128014672111582684\n",
      "Iteration 31166 => Loss: 6.70127972869045507309\n",
      "Iteration 31167 => Loss: 6.70127931071611815383\n",
      "Iteration 31168 => Loss: 6.70127889279810684542\n",
      "Iteration 31169 => Loss: 6.70127847493640516063\n",
      "Iteration 31170 => Loss: 6.70127805713101842855\n",
      "Iteration 31171 => Loss: 6.70127763938191911564\n",
      "Iteration 31172 => Loss: 6.70127722168912054457\n",
      "Iteration 31173 => Loss: 6.70127680405259873453\n",
      "Iteration 31174 => Loss: 6.70127638647235812641\n",
      "Iteration 31175 => Loss: 6.70127596894838806207\n",
      "Iteration 31176 => Loss: 6.70127555148068054791\n",
      "Iteration 31177 => Loss: 6.70127513406922492578\n",
      "Iteration 31178 => Loss: 6.70127471671401231390\n",
      "Iteration 31179 => Loss: 6.70127429941504093591\n",
      "Iteration 31180 => Loss: 6.70127388217230191003\n",
      "Iteration 31181 => Loss: 6.70127346498578280176\n",
      "Iteration 31182 => Loss: 6.70127304785548627564\n",
      "Iteration 31183 => Loss: 6.70127263078139367991\n",
      "Iteration 31184 => Loss: 6.70127221376349879733\n",
      "Iteration 31185 => Loss: 6.70127179680180340426\n",
      "Iteration 31186 => Loss: 6.70127137989628707260\n",
      "Iteration 31187 => Loss: 6.70127096304695157869\n",
      "Iteration 31188 => Loss: 6.70127054625379425801\n",
      "Iteration 31189 => Loss: 6.70127012951679112973\n",
      "Iteration 31190 => Loss: 6.70126971283594130568\n",
      "Iteration 31191 => Loss: 6.70126929621123945680\n",
      "Iteration 31192 => Loss: 6.70126887964268291853\n",
      "Iteration 31193 => Loss: 6.70126846313026280910\n",
      "Iteration 31194 => Loss: 6.70126804667395603587\n",
      "Iteration 31195 => Loss: 6.70126763027377592152\n",
      "Iteration 31196 => Loss: 6.70126721392970026159\n",
      "Iteration 31197 => Loss: 6.70126679764172461518\n",
      "Iteration 31198 => Loss: 6.70126638140985786407\n",
      "Iteration 31199 => Loss: 6.70126596523406803385\n",
      "Iteration 31200 => Loss: 6.70126554911435690087\n",
      "Iteration 31201 => Loss: 6.70126513305072091242\n",
      "Iteration 31202 => Loss: 6.70126471704314941036\n",
      "Iteration 31203 => Loss: 6.70126430109163706561\n",
      "Iteration 31204 => Loss: 6.70126388519616789097\n",
      "Iteration 31205 => Loss: 6.70126346935674721550\n",
      "Iteration 31206 => Loss: 6.70126305357335727564\n",
      "Iteration 31207 => Loss: 6.70126263784599540685\n",
      "Iteration 31208 => Loss: 6.70126222217465006281\n",
      "Iteration 31209 => Loss: 6.70126180655931946717\n",
      "Iteration 31210 => Loss: 6.70126139099999651449\n",
      "Iteration 31211 => Loss: 6.70126097549666255304\n",
      "Iteration 31212 => Loss: 6.70126056004931847099\n",
      "Iteration 31213 => Loss: 6.70126014465796426833\n",
      "Iteration 31214 => Loss: 6.70125972932257418790\n",
      "Iteration 31215 => Loss: 6.70125931404315799966\n",
      "Iteration 31216 => Loss: 6.70125889881969349915\n",
      "Iteration 31217 => Loss: 6.70125848365218512726\n",
      "Iteration 31218 => Loss: 6.70125806854062133766\n",
      "Iteration 31219 => Loss: 6.70125765348498880769\n",
      "Iteration 31220 => Loss: 6.70125723848528753734\n",
      "Iteration 31221 => Loss: 6.70125682354151397391\n",
      "Iteration 31222 => Loss: 6.70125640865364413656\n",
      "Iteration 31223 => Loss: 6.70125599382168601892\n",
      "Iteration 31224 => Loss: 6.70125557904563251554\n",
      "Iteration 31225 => Loss: 6.70125516432545964562\n",
      "Iteration 31226 => Loss: 6.70125474966117007369\n",
      "Iteration 31227 => Loss: 6.70125433505276024704\n",
      "Iteration 31228 => Loss: 6.70125392050022306023\n",
      "Iteration 31229 => Loss: 6.70125350600354519059\n",
      "Iteration 31230 => Loss: 6.70125309156271509181\n",
      "Iteration 31231 => Loss: 6.70125267717773720477\n",
      "Iteration 31232 => Loss: 6.70125226284859021320\n",
      "Iteration 31233 => Loss: 6.70125184857528211069\n",
      "Iteration 31234 => Loss: 6.70125143435779424550\n",
      "Iteration 31235 => Loss: 6.70125102019612128856\n",
      "Iteration 31236 => Loss: 6.70125060609026323988\n",
      "Iteration 31237 => Loss: 6.70125019204019345409\n",
      "Iteration 31238 => Loss: 6.70124977804592436570\n",
      "Iteration 31239 => Loss: 6.70124936410743643478\n",
      "Iteration 31240 => Loss: 6.70124895022473676676\n",
      "Iteration 31241 => Loss: 6.70124853639780049264\n",
      "Iteration 31242 => Loss: 6.70124812262662938878\n",
      "Iteration 31243 => Loss: 6.70124770891121457339\n",
      "Iteration 31244 => Loss: 6.70124729525154538834\n",
      "Iteration 31245 => Loss: 6.70124688164761384002\n",
      "Iteration 31246 => Loss: 6.70124646809941815206\n",
      "Iteration 31247 => Loss: 6.70124605460695921266\n",
      "Iteration 31248 => Loss: 6.70124564117020415921\n",
      "Iteration 31249 => Loss: 6.70124522778916187349\n",
      "Iteration 31250 => Loss: 6.70124481446383235550\n",
      "Iteration 31251 => Loss: 6.70124440119419073625\n",
      "Iteration 31252 => Loss: 6.70124398798023257484\n",
      "Iteration 31253 => Loss: 6.70124357482196142399\n",
      "Iteration 31254 => Loss: 6.70124316171935685560\n",
      "Iteration 31255 => Loss: 6.70124274867243396869\n",
      "Iteration 31256 => Loss: 6.70124233568115368342\n",
      "Iteration 31257 => Loss: 6.70124192274552843429\n",
      "Iteration 31258 => Loss: 6.70124150986555022769\n",
      "Iteration 31259 => Loss: 6.70124109704120041187\n",
      "Iteration 31260 => Loss: 6.70124068427248253954\n",
      "Iteration 31261 => Loss: 6.70124027155938772893\n",
      "Iteration 31262 => Loss: 6.70123985890189999282\n",
      "Iteration 31263 => Loss: 6.70123944630002466027\n",
      "Iteration 31264 => Loss: 6.70123903375374752045\n",
      "Iteration 31265 => Loss: 6.70123862126305436249\n",
      "Iteration 31266 => Loss: 6.70123820882794785092\n",
      "Iteration 31267 => Loss: 6.70123779644842088032\n",
      "Iteration 31268 => Loss: 6.70123738412445657531\n",
      "Iteration 31269 => Loss: 6.70123697185605049498\n",
      "Iteration 31270 => Loss: 6.70123655964320086298\n",
      "Iteration 31271 => Loss: 6.70123614748590057388\n",
      "Iteration 31272 => Loss: 6.70123573538413985773\n",
      "Iteration 31273 => Loss: 6.70123532333789917459\n",
      "Iteration 31274 => Loss: 6.70123491134718740625\n",
      "Iteration 31275 => Loss: 6.70123449941199389457\n",
      "Iteration 31276 => Loss: 6.70123408753230709323\n",
      "Iteration 31277 => Loss: 6.70123367570812078498\n",
      "Iteration 31278 => Loss: 6.70123326393942964074\n",
      "Iteration 31279 => Loss: 6.70123285222621767332\n",
      "Iteration 31280 => Loss: 6.70123244056849465267\n",
      "Iteration 31281 => Loss: 6.70123202896623482161\n",
      "Iteration 31282 => Loss: 6.70123161741944262104\n",
      "Iteration 31283 => Loss: 6.70123120592810561647\n",
      "Iteration 31284 => Loss: 6.70123079449221670245\n",
      "Iteration 31285 => Loss: 6.70123038311176788540\n",
      "Iteration 31286 => Loss: 6.70122997178675117169\n",
      "Iteration 31287 => Loss: 6.70122956051715767956\n",
      "Iteration 31288 => Loss: 6.70122914930298918534\n",
      "Iteration 31289 => Loss: 6.70122873814423236638\n",
      "Iteration 31290 => Loss: 6.70122832704087567635\n",
      "Iteration 31291 => Loss: 6.70122791599291645070\n",
      "Iteration 31292 => Loss: 6.70122750500034669585\n",
      "Iteration 31293 => Loss: 6.70122709406315753000\n",
      "Iteration 31294 => Loss: 6.70122668318133829501\n",
      "Iteration 31295 => Loss: 6.70122627235489343178\n",
      "Iteration 31296 => Loss: 6.70122586158380162402\n",
      "Iteration 31297 => Loss: 6.70122545086806287173\n",
      "Iteration 31298 => Loss: 6.70122504020766829314\n",
      "Iteration 31299 => Loss: 6.70122462960261167098\n",
      "Iteration 31300 => Loss: 6.70122421905287701804\n",
      "Iteration 31301 => Loss: 6.70122380855846788705\n",
      "Iteration 31302 => Loss: 6.70122339811937894893\n",
      "Iteration 31303 => Loss: 6.70122298773559510465\n",
      "Iteration 31304 => Loss: 6.70122257740710214335\n",
      "Iteration 31305 => Loss: 6.70122216713390628229\n",
      "Iteration 31306 => Loss: 6.70122175691599863967\n",
      "Iteration 31307 => Loss: 6.70122134675336500464\n",
      "Iteration 31308 => Loss: 6.70122093664599738361\n",
      "Iteration 31309 => Loss: 6.70122052659389755291\n",
      "Iteration 31310 => Loss: 6.70122011659704686082\n",
      "Iteration 31311 => Loss: 6.70121970665544175461\n",
      "Iteration 31312 => Loss: 6.70121929676907956974\n",
      "Iteration 31313 => Loss: 6.70121888693795675351\n",
      "Iteration 31314 => Loss: 6.70121847716204754875\n",
      "Iteration 31315 => Loss: 6.70121806744136350176\n",
      "Iteration 31316 => Loss: 6.70121765777588063173\n",
      "Iteration 31317 => Loss: 6.70121724816561048499\n",
      "Iteration 31318 => Loss: 6.70121683861053263342\n",
      "Iteration 31319 => Loss: 6.70121642911063020165\n",
      "Iteration 31320 => Loss: 6.70121601966592184141\n",
      "Iteration 31321 => Loss: 6.70121561027638268371\n",
      "Iteration 31322 => Loss: 6.70121520094200828765\n",
      "Iteration 31323 => Loss: 6.70121479166279065964\n",
      "Iteration 31324 => Loss: 6.70121438243873068785\n",
      "Iteration 31325 => Loss: 6.70121397326980261511\n",
      "Iteration 31326 => Loss: 6.70121356415601798773\n",
      "Iteration 31327 => Loss: 6.70121315509735726579\n",
      "Iteration 31328 => Loss: 6.70121274609381778475\n",
      "Iteration 31329 => Loss: 6.70121233714538888648\n",
      "Iteration 31330 => Loss: 6.70121192825207323551\n",
      "Iteration 31331 => Loss: 6.70121151941385129192\n",
      "Iteration 31332 => Loss: 6.70121111063071772662\n",
      "Iteration 31333 => Loss: 6.70121070190267520417\n",
      "Iteration 31334 => Loss: 6.70121029322970063191\n",
      "Iteration 31335 => Loss: 6.70120988461180289164\n",
      "Iteration 31336 => Loss: 6.70120947604896421979\n",
      "Iteration 31337 => Loss: 6.70120906754117662274\n",
      "Iteration 31338 => Loss: 6.70120865908843210690\n",
      "Iteration 31339 => Loss: 6.70120825069072978408\n",
      "Iteration 31340 => Loss: 6.70120784234806432522\n",
      "Iteration 31341 => Loss: 6.70120743406041530221\n",
      "Iteration 31342 => Loss: 6.70120702582778804413\n",
      "Iteration 31343 => Loss: 6.70120661765016834011\n",
      "Iteration 31344 => Loss: 6.70120620952755263744\n",
      "Iteration 31345 => Loss: 6.70120580145992938981\n",
      "Iteration 31346 => Loss: 6.70120539344729504450\n",
      "Iteration 31347 => Loss: 6.70120498548963627883\n",
      "Iteration 31348 => Loss: 6.70120457758695042827\n",
      "Iteration 31349 => Loss: 6.70120416973923749282\n",
      "Iteration 31350 => Loss: 6.70120376194647437984\n",
      "Iteration 31351 => Loss: 6.70120335420866197751\n",
      "Iteration 31352 => Loss: 6.70120294652579495676\n",
      "Iteration 31353 => Loss: 6.70120253889786532397\n",
      "Iteration 31354 => Loss: 6.70120213132485442742\n",
      "Iteration 31355 => Loss: 6.70120172380677114887\n",
      "Iteration 31356 => Loss: 6.70120131634359950112\n",
      "Iteration 31357 => Loss: 6.70120090893533415510\n",
      "Iteration 31358 => Loss: 6.70120050158196267631\n",
      "Iteration 31359 => Loss: 6.70120009428349483471\n",
      "Iteration 31360 => Loss: 6.70119968703989421499\n",
      "Iteration 31361 => Loss: 6.70119927985117858071\n",
      "Iteration 31362 => Loss: 6.70119887271733283285\n",
      "Iteration 31363 => Loss: 6.70119846563834453690\n",
      "Iteration 31364 => Loss: 6.70119805861420925197\n",
      "Iteration 31365 => Loss: 6.70119765164492076082\n",
      "Iteration 31366 => Loss: 6.70119724473047906343\n",
      "Iteration 31367 => Loss: 6.70119683787086550808\n",
      "Iteration 31368 => Loss: 6.70119643106607032479\n",
      "Iteration 31369 => Loss: 6.70119602431610061899\n",
      "Iteration 31370 => Loss: 6.70119561762093329804\n",
      "Iteration 31371 => Loss: 6.70119521098057191466\n",
      "Iteration 31372 => Loss: 6.70119480439499959346\n",
      "Iteration 31373 => Loss: 6.70119439786422521621\n",
      "Iteration 31374 => Loss: 6.70119399138822569029\n",
      "Iteration 31375 => Loss: 6.70119358496699302208\n",
      "Iteration 31376 => Loss: 6.70119317860053076430\n",
      "Iteration 31377 => Loss: 6.70119277228882648245\n",
      "Iteration 31378 => Loss: 6.70119236603187395929\n",
      "Iteration 31379 => Loss: 6.70119195982966608938\n",
      "Iteration 31380 => Loss: 6.70119155368219132640\n",
      "Iteration 31381 => Loss: 6.70119114758944167676\n",
      "Iteration 31382 => Loss: 6.70119074155141714044\n",
      "Iteration 31383 => Loss: 6.70119033556810617114\n",
      "Iteration 31384 => Loss: 6.70118992963949811070\n",
      "Iteration 31385 => Loss: 6.70118952376559384732\n",
      "Iteration 31386 => Loss: 6.70118911794637650559\n",
      "Iteration 31387 => Loss: 6.70118871218184875005\n",
      "Iteration 31388 => Loss: 6.70118830647199192896\n",
      "Iteration 31389 => Loss: 6.70118790081680781867\n",
      "Iteration 31390 => Loss: 6.70118749521628487287\n",
      "Iteration 31391 => Loss: 6.70118708967041598612\n",
      "Iteration 31392 => Loss: 6.70118668417920115843\n",
      "Iteration 31393 => Loss: 6.70118627874261729715\n",
      "Iteration 31394 => Loss: 6.70118587336066884319\n",
      "Iteration 31395 => Loss: 6.70118546803334691475\n",
      "Iteration 31396 => Loss: 6.70118506276064529459\n",
      "Iteration 31397 => Loss: 6.70118465754254533095\n",
      "Iteration 31398 => Loss: 6.70118425237905679381\n",
      "Iteration 31399 => Loss: 6.70118384727016014324\n",
      "Iteration 31400 => Loss: 6.70118344221585449105\n",
      "Iteration 31401 => Loss: 6.70118303721612829094\n",
      "Iteration 31402 => Loss: 6.70118263227098154289\n",
      "Iteration 31403 => Loss: 6.70118222738039026609\n",
      "Iteration 31404 => Loss: 6.70118182254436423051\n",
      "Iteration 31405 => Loss: 6.70118141776289100164\n",
      "Iteration 31406 => Loss: 6.70118101303595814500\n",
      "Iteration 31407 => Loss: 6.70118060836357010146\n",
      "Iteration 31408 => Loss: 6.70118020374570555475\n",
      "Iteration 31409 => Loss: 6.70117979918236184034\n",
      "Iteration 31410 => Loss: 6.70117939467353274097\n",
      "Iteration 31411 => Loss: 6.70117899021921381575\n",
      "Iteration 31412 => Loss: 6.70117858581939529472\n",
      "Iteration 31413 => Loss: 6.70117818147406829610\n",
      "Iteration 31414 => Loss: 6.70117777718322660263\n",
      "Iteration 31415 => Loss: 6.70117737294686488525\n",
      "Iteration 31416 => Loss: 6.70117696876497337399\n",
      "Iteration 31417 => Loss: 6.70117656463755118068\n",
      "Iteration 31418 => Loss: 6.70117616056457432450\n",
      "Iteration 31419 => Loss: 6.70117575654605257540\n",
      "Iteration 31420 => Loss: 6.70117535258197261072\n",
      "Iteration 31421 => Loss: 6.70117494867233087774\n",
      "Iteration 31422 => Loss: 6.70117454481710694836\n",
      "Iteration 31423 => Loss: 6.70117414101630437528\n",
      "Iteration 31424 => Loss: 6.70117373726991782945\n",
      "Iteration 31425 => Loss: 6.70117333357792865911\n",
      "Iteration 31426 => Loss: 6.70117292994034663423\n",
      "Iteration 31427 => Loss: 6.70117252635715043851\n",
      "Iteration 31428 => Loss: 6.70117212282833563108\n",
      "Iteration 31429 => Loss: 6.70117171935389599469\n",
      "Iteration 31430 => Loss: 6.70117131593383508203\n",
      "Iteration 31431 => Loss: 6.70117091256812447142\n",
      "Iteration 31432 => Loss: 6.70117050925676771556\n",
      "Iteration 31433 => Loss: 6.70117010599976037355\n",
      "Iteration 31434 => Loss: 6.70116970279709089908\n",
      "Iteration 31435 => Loss: 6.70116929964875485126\n",
      "Iteration 31436 => Loss: 6.70116889655474246013\n",
      "Iteration 31437 => Loss: 6.70116849351504484389\n",
      "Iteration 31438 => Loss: 6.70116809052966022620\n",
      "Iteration 31439 => Loss: 6.70116768759857261983\n",
      "Iteration 31440 => Loss: 6.70116728472177847209\n",
      "Iteration 31441 => Loss: 6.70116688189927867114\n",
      "Iteration 31442 => Loss: 6.70116647913105811796\n",
      "Iteration 31443 => Loss: 6.70116607641710881893\n",
      "Iteration 31444 => Loss: 6.70116567375742722135\n",
      "Iteration 31445 => Loss: 6.70116527115200355524\n",
      "Iteration 31446 => Loss: 6.70116486860082893884\n",
      "Iteration 31447 => Loss: 6.70116446610389537852\n",
      "Iteration 31448 => Loss: 6.70116406366120642701\n",
      "Iteration 31449 => Loss: 6.70116366127274254438\n",
      "Iteration 31450 => Loss: 6.70116325893850106610\n",
      "Iteration 31451 => Loss: 6.70116285665847222219\n",
      "Iteration 31452 => Loss: 6.70116245443264890724\n",
      "Iteration 31453 => Loss: 6.70116205226103023307\n",
      "Iteration 31454 => Loss: 6.70116165014360198882\n",
      "Iteration 31455 => Loss: 6.70116124808035973359\n",
      "Iteration 31456 => Loss: 6.70116084607129902651\n",
      "Iteration 31457 => Loss: 6.70116044411640476852\n",
      "Iteration 31458 => Loss: 6.70116004221567251875\n",
      "Iteration 31459 => Loss: 6.70115964036909783630\n",
      "Iteration 31460 => Loss: 6.70115923857667450392\n",
      "Iteration 31461 => Loss: 6.70115883683838919893\n",
      "Iteration 31462 => Loss: 6.70115843515423925680\n",
      "Iteration 31463 => Loss: 6.70115803352421934846\n",
      "Iteration 31464 => Loss: 6.70115763194831792759\n",
      "Iteration 31465 => Loss: 6.70115723042652611241\n",
      "Iteration 31466 => Loss: 6.70115682895884123838\n",
      "Iteration 31467 => Loss: 6.70115642754525353553\n",
      "Iteration 31468 => Loss: 6.70115602618575678662\n",
      "Iteration 31469 => Loss: 6.70115562488034299804\n",
      "Iteration 31470 => Loss: 6.70115522362900595255\n",
      "Iteration 31471 => Loss: 6.70115482243173854471\n",
      "Iteration 31472 => Loss: 6.70115442128852745185\n",
      "Iteration 31473 => Loss: 6.70115402019937622669\n",
      "Iteration 31474 => Loss: 6.70115361916427065836\n",
      "Iteration 31475 => Loss: 6.70115321818320364144\n",
      "Iteration 31476 => Loss: 6.70115281725616451780\n",
      "Iteration 31477 => Loss: 6.70115241638316128103\n",
      "Iteration 31478 => Loss: 6.70115201556416462125\n",
      "Iteration 31479 => Loss: 6.70115161479918519660\n",
      "Iteration 31480 => Loss: 6.70115121408819813809\n",
      "Iteration 31481 => Loss: 6.70115081343122653834\n",
      "Iteration 31482 => Loss: 6.70115041282822865298\n",
      "Iteration 31483 => Loss: 6.70115001227921336380\n",
      "Iteration 31484 => Loss: 6.70114961178417800625\n",
      "Iteration 31485 => Loss: 6.70114921134309859951\n",
      "Iteration 31486 => Loss: 6.70114881095599024263\n",
      "Iteration 31487 => Loss: 6.70114841062282717843\n",
      "Iteration 31488 => Loss: 6.70114801034361295962\n",
      "Iteration 31489 => Loss: 6.70114761011833603987\n",
      "Iteration 31490 => Loss: 6.70114720994699197831\n",
      "Iteration 31491 => Loss: 6.70114680982956212318\n",
      "Iteration 31492 => Loss: 6.70114640976605890899\n",
      "Iteration 31493 => Loss: 6.70114600975645480219\n",
      "Iteration 31494 => Loss: 6.70114560980075335550\n",
      "Iteration 31495 => Loss: 6.70114520989895279257\n",
      "Iteration 31496 => Loss: 6.70114481005103357347\n",
      "Iteration 31497 => Loss: 6.70114441025698592824\n",
      "Iteration 31498 => Loss: 6.70114401051682673227\n",
      "Iteration 31499 => Loss: 6.70114361083052401113\n",
      "Iteration 31500 => Loss: 6.70114321119808131755\n",
      "Iteration 31501 => Loss: 6.70114281161948888155\n",
      "Iteration 31502 => Loss: 6.70114241209473959771\n",
      "Iteration 31503 => Loss: 6.70114201262382458424\n",
      "Iteration 31504 => Loss: 6.70114161320674295297\n",
      "Iteration 31505 => Loss: 6.70114121384348049304\n",
      "Iteration 31506 => Loss: 6.70114081453402743449\n",
      "Iteration 31507 => Loss: 6.70114041527838288914\n",
      "Iteration 31508 => Loss: 6.70114001607654419246\n",
      "Iteration 31509 => Loss: 6.70113961692849091634\n",
      "Iteration 31510 => Loss: 6.70113921783423105438\n",
      "Iteration 31511 => Loss: 6.70113881879374506667\n",
      "Iteration 31512 => Loss: 6.70113841980702851231\n",
      "Iteration 31513 => Loss: 6.70113802087407162134\n",
      "Iteration 31514 => Loss: 6.70113762199487972282\n",
      "Iteration 31515 => Loss: 6.70113722316943150048\n",
      "Iteration 31516 => Loss: 6.70113682439772784249\n",
      "Iteration 31517 => Loss: 6.70113642567974920894\n",
      "Iteration 31518 => Loss: 6.70113602701550981067\n",
      "Iteration 31519 => Loss: 6.70113562840498033779\n",
      "Iteration 31520 => Loss: 6.70113522984817056027\n",
      "Iteration 31521 => Loss: 6.70113483134506981997\n",
      "Iteration 31522 => Loss: 6.70113443289566301786\n",
      "Iteration 31523 => Loss: 6.70113403449994571304\n",
      "Iteration 31524 => Loss: 6.70113363615791346461\n",
      "Iteration 31525 => Loss: 6.70113323786956005534\n",
      "Iteration 31526 => Loss: 6.70113283963487038619\n",
      "Iteration 31527 => Loss: 6.70113244145385245076\n",
      "Iteration 31528 => Loss: 6.70113204332648226824\n",
      "Iteration 31529 => Loss: 6.70113164525275895045\n",
      "Iteration 31530 => Loss: 6.70113124723267894467\n",
      "Iteration 31531 => Loss: 6.70113084926623070459\n",
      "Iteration 31532 => Loss: 6.70113045135340801295\n",
      "Iteration 31533 => Loss: 6.70113005349420731704\n",
      "Iteration 31534 => Loss: 6.70112965568861351784\n",
      "Iteration 31535 => Loss: 6.70112925793662839169\n",
      "Iteration 31536 => Loss: 6.70112886023823861592\n",
      "Iteration 31537 => Loss: 6.70112846259343708510\n",
      "Iteration 31538 => Loss: 6.70112806500221935835\n",
      "Iteration 31539 => Loss: 6.70112766746457921840\n",
      "Iteration 31540 => Loss: 6.70112726998050511895\n",
      "Iteration 31541 => Loss: 6.70112687254998906639\n",
      "Iteration 31542 => Loss: 6.70112647517303550160\n",
      "Iteration 31543 => Loss: 6.70112607784962488466\n",
      "Iteration 31544 => Loss: 6.70112568057975011016\n",
      "Iteration 31545 => Loss: 6.70112528336340584900\n",
      "Iteration 31546 => Loss: 6.70112488620059210120\n",
      "Iteration 31547 => Loss: 6.70112448909129376773\n",
      "Iteration 31548 => Loss: 6.70112409203550551950\n",
      "Iteration 31549 => Loss: 6.70112369503321936293\n",
      "Iteration 31550 => Loss: 6.70112329808443529799\n",
      "Iteration 31551 => Loss: 6.70112290118913733750\n",
      "Iteration 31552 => Loss: 6.70112250434731215876\n",
      "Iteration 31553 => Loss: 6.70112210755897308445\n",
      "Iteration 31554 => Loss: 6.70112171082409524558\n",
      "Iteration 31555 => Loss: 6.70112131414267686580\n",
      "Iteration 31556 => Loss: 6.70112091751472327417\n",
      "Iteration 31557 => Loss: 6.70112052094020427262\n",
      "Iteration 31558 => Loss: 6.70112012441912341387\n",
      "Iteration 31559 => Loss: 6.70111972795147536885\n",
      "Iteration 31560 => Loss: 6.70111933153725392032\n",
      "Iteration 31561 => Loss: 6.70111893517644752194\n",
      "Iteration 31562 => Loss: 6.70111853886904551558\n",
      "Iteration 31563 => Loss: 6.70111814261504967760\n",
      "Iteration 31564 => Loss: 6.70111774641445201439\n",
      "Iteration 31565 => Loss: 6.70111735026724453235\n",
      "Iteration 31566 => Loss: 6.70111695417340946790\n",
      "Iteration 31567 => Loss: 6.70111655813295570283\n",
      "Iteration 31568 => Loss: 6.70111616214586458540\n",
      "Iteration 31569 => Loss: 6.70111576621213167471\n",
      "Iteration 31570 => Loss: 6.70111537033175341804\n",
      "Iteration 31571 => Loss: 6.70111497450471649273\n",
      "Iteration 31572 => Loss: 6.70111457873102533966\n",
      "Iteration 31573 => Loss: 6.70111418301065597802\n",
      "Iteration 31574 => Loss: 6.70111378734361284870\n",
      "Iteration 31575 => Loss: 6.70111339172988884627\n",
      "Iteration 31576 => Loss: 6.70111299616947597713\n",
      "Iteration 31577 => Loss: 6.70111260066235558952\n",
      "Iteration 31578 => Loss: 6.70111220520853567706\n",
      "Iteration 31579 => Loss: 6.70111180980800380524\n",
      "Iteration 31580 => Loss: 6.70111141446074842776\n",
      "Iteration 31581 => Loss: 6.70111101916677043278\n",
      "Iteration 31582 => Loss: 6.70111062392605294491\n",
      "Iteration 31583 => Loss: 6.70111022873859862869\n",
      "Iteration 31584 => Loss: 6.70110983360439327328\n",
      "Iteration 31585 => Loss: 6.70110943852343687865\n",
      "Iteration 31586 => Loss: 6.70110904349571168126\n",
      "Iteration 31587 => Loss: 6.70110864852121768109\n",
      "Iteration 31588 => Loss: 6.70110825359995487815\n",
      "Iteration 31589 => Loss: 6.70110785873189929163\n",
      "Iteration 31590 => Loss: 6.70110746391704648062\n",
      "Iteration 31591 => Loss: 6.70110706915541154416\n",
      "Iteration 31592 => Loss: 6.70110667444695806694\n",
      "Iteration 31593 => Loss: 6.70110627979169315438\n",
      "Iteration 31594 => Loss: 6.70110588518961503013\n",
      "Iteration 31595 => Loss: 6.70110549064070326608\n",
      "Iteration 31596 => Loss: 6.70110509614495608588\n",
      "Iteration 31597 => Loss: 6.70110470170237082499\n",
      "Iteration 31598 => Loss: 6.70110430731293593709\n",
      "Iteration 31599 => Loss: 6.70110391297663898769\n",
      "Iteration 31600 => Loss: 6.70110351869349241127\n",
      "Iteration 31601 => Loss: 6.70110312446346423343\n",
      "Iteration 31602 => Loss: 6.70110273028656333594\n",
      "Iteration 31603 => Loss: 6.70110233616277373159\n",
      "Iteration 31604 => Loss: 6.70110194209209630856\n",
      "Iteration 31605 => Loss: 6.70110154807451774417\n",
      "Iteration 31606 => Loss: 6.70110115411003270935\n",
      "Iteration 31607 => Loss: 6.70110076019863942776\n",
      "Iteration 31608 => Loss: 6.70110036634032102398\n",
      "Iteration 31609 => Loss: 6.70109997253507483350\n",
      "Iteration 31610 => Loss: 6.70109957878289641542\n",
      "Iteration 31611 => Loss: 6.70109918508377067070\n",
      "Iteration 31612 => Loss: 6.70109879143770292842\n",
      "Iteration 31613 => Loss: 6.70109839784467453683\n",
      "Iteration 31614 => Loss: 6.70109800430468727228\n",
      "Iteration 31615 => Loss: 6.70109761081772692393\n",
      "Iteration 31616 => Loss: 6.70109721738378727451\n",
      "Iteration 31617 => Loss: 6.70109682400286388315\n",
      "Iteration 31618 => Loss: 6.70109643067495142077\n",
      "Iteration 31619 => Loss: 6.70109603740003390016\n",
      "Iteration 31620 => Loss: 6.70109564417811576220\n",
      "Iteration 31621 => Loss: 6.70109525100918368423\n",
      "Iteration 31622 => Loss: 6.70109485789323322535\n",
      "Iteration 31623 => Loss: 6.70109446483024573382\n",
      "Iteration 31624 => Loss: 6.70109407182023275595\n",
      "Iteration 31625 => Loss: 6.70109367886317652818\n",
      "Iteration 31626 => Loss: 6.70109328595906728054\n",
      "Iteration 31627 => Loss: 6.70109289310790412486\n",
      "Iteration 31628 => Loss: 6.70109250030968262024\n",
      "Iteration 31629 => Loss: 6.70109210756438233858\n",
      "Iteration 31630 => Loss: 6.70109171487201038531\n",
      "Iteration 31631 => Loss: 6.70109132223254722049\n",
      "Iteration 31632 => Loss: 6.70109092964599728504\n",
      "Iteration 31633 => Loss: 6.70109053711234903261\n",
      "Iteration 31634 => Loss: 6.70109014463159180508\n",
      "Iteration 31635 => Loss: 6.70108975220372293791\n",
      "Iteration 31636 => Loss: 6.70108935982873621384\n",
      "Iteration 31637 => Loss: 6.70108896750662186292\n",
      "Iteration 31638 => Loss: 6.70108857523736300976\n",
      "Iteration 31639 => Loss: 6.70108818302097386521\n",
      "Iteration 31640 => Loss: 6.70108779085743311299\n",
      "Iteration 31641 => Loss: 6.70108739874673453585\n",
      "Iteration 31642 => Loss: 6.70108700668887724561\n",
      "Iteration 31643 => Loss: 6.70108661468384436688\n",
      "Iteration 31644 => Loss: 6.70108622273163234695\n",
      "Iteration 31645 => Loss: 6.70108583083224118582\n",
      "Iteration 31646 => Loss: 6.70108543898565933716\n",
      "Iteration 31647 => Loss: 6.70108504719187525467\n",
      "Iteration 31648 => Loss: 6.70108465545088805015\n",
      "Iteration 31649 => Loss: 6.70108426376268084823\n",
      "Iteration 31650 => Loss: 6.70108387212725986615\n",
      "Iteration 31651 => Loss: 6.70108348054461355758\n",
      "Iteration 31652 => Loss: 6.70108308901472504715\n",
      "Iteration 31653 => Loss: 6.70108269753760588117\n",
      "Iteration 31654 => Loss: 6.70108230611323918424\n",
      "Iteration 31655 => Loss: 6.70108191474160808099\n",
      "Iteration 31656 => Loss: 6.70108152342271790047\n",
      "Iteration 31657 => Loss: 6.70108113215655620820\n",
      "Iteration 31658 => Loss: 6.70108074094311412239\n",
      "Iteration 31659 => Loss: 6.70108034978239697210\n",
      "Iteration 31660 => Loss: 6.70107995867438699378\n",
      "Iteration 31661 => Loss: 6.70107956761907708199\n",
      "Iteration 31662 => Loss: 6.70107917661646013130\n",
      "Iteration 31663 => Loss: 6.70107878566653258900\n",
      "Iteration 31664 => Loss: 6.70107839476928379696\n",
      "Iteration 31665 => Loss: 6.70107800392470842610\n",
      "Iteration 31666 => Loss: 6.70107761313280470006\n",
      "Iteration 31667 => Loss: 6.70107722239355219074\n",
      "Iteration 31668 => Loss: 6.70107683170695889174\n",
      "Iteration 31669 => Loss: 6.70107644107300526315\n",
      "Iteration 31670 => Loss: 6.70107605049169130496\n",
      "Iteration 31671 => Loss: 6.70107565996300635902\n",
      "Iteration 31672 => Loss: 6.70107526948694864899\n",
      "Iteration 31673 => Loss: 6.70107487906350840490\n",
      "Iteration 31674 => Loss: 6.70107448869267230407\n",
      "Iteration 31675 => Loss: 6.70107409837444834011\n",
      "Iteration 31676 => Loss: 6.70107370810881253220\n",
      "Iteration 31677 => Loss: 6.70107331789576488035\n",
      "Iteration 31678 => Loss: 6.70107292773530449637\n",
      "Iteration 31679 => Loss: 6.70107253762741361669\n",
      "Iteration 31680 => Loss: 6.70107214757208691225\n",
      "Iteration 31681 => Loss: 6.70107175756932260668\n",
      "Iteration 31682 => Loss: 6.70107136761910915368\n",
      "Iteration 31683 => Loss: 6.70107097772144566505\n",
      "Iteration 31684 => Loss: 6.70107058787631881813\n",
      "Iteration 31685 => Loss: 6.70107019808372594838\n",
      "Iteration 31686 => Loss: 6.70106980834365284494\n",
      "Iteration 31687 => Loss: 6.70106941865610217235\n",
      "Iteration 31688 => Loss: 6.70106902902106060793\n",
      "Iteration 31689 => Loss: 6.70106863943852282262\n",
      "Iteration 31690 => Loss: 6.70106824990848171097\n",
      "Iteration 31691 => Loss: 6.70106786043092839122\n",
      "Iteration 31692 => Loss: 6.70106747100585575794\n",
      "Iteration 31693 => Loss: 6.70106708163325848204\n",
      "Iteration 31694 => Loss: 6.70106669231313123447\n",
      "Iteration 31695 => Loss: 6.70106630304546246890\n",
      "Iteration 31696 => Loss: 6.70106591383025040898\n",
      "Iteration 31697 => Loss: 6.70106552466748439656\n",
      "Iteration 31698 => Loss: 6.70106513555715821440\n",
      "Iteration 31699 => Loss: 6.70106474649926120435\n",
      "Iteration 31700 => Loss: 6.70106435749379336642\n",
      "Iteration 31701 => Loss: 6.70106396854074315428\n",
      "Iteration 31702 => Loss: 6.70106357964010257433\n",
      "Iteration 31703 => Loss: 6.70106319079186985022\n",
      "Iteration 31704 => Loss: 6.70106280199603876468\n",
      "Iteration 31705 => Loss: 6.70106241325258800146\n",
      "Iteration 31706 => Loss: 6.70106202456152200142\n",
      "Iteration 31707 => Loss: 6.70106163592283810004\n",
      "Iteration 31708 => Loss: 6.70106124733651586922\n",
      "Iteration 31709 => Loss: 6.70106085880256241438\n",
      "Iteration 31710 => Loss: 6.70106047032095997196\n",
      "Iteration 31711 => Loss: 6.70106008189170765377\n",
      "Iteration 31712 => Loss: 6.70105969351479568985\n",
      "Iteration 31713 => Loss: 6.70105930519021963931\n",
      "Iteration 31714 => Loss: 6.70105891691796440313\n",
      "Iteration 31715 => Loss: 6.70105852869802998129\n",
      "Iteration 31716 => Loss: 6.70105814053040926837\n",
      "Iteration 31717 => Loss: 6.70105775241509515894\n",
      "Iteration 31718 => Loss: 6.70105736435208054758\n",
      "Iteration 31719 => Loss: 6.70105697634136188157\n",
      "Iteration 31720 => Loss: 6.70105658838292406188\n",
      "Iteration 31721 => Loss: 6.70105620047675998308\n",
      "Iteration 31722 => Loss: 6.70105581262286698063\n",
      "Iteration 31723 => Loss: 6.70105542482124061365\n",
      "Iteration 31724 => Loss: 6.70105503707186667128\n",
      "Iteration 31725 => Loss: 6.70105464937474604170\n",
      "Iteration 31726 => Loss: 6.70105426172986629041\n",
      "Iteration 31727 => Loss: 6.70105387413721942380\n",
      "Iteration 31728 => Loss: 6.70105348659680277734\n",
      "Iteration 31729 => Loss: 6.70105309910860746925\n",
      "Iteration 31730 => Loss: 6.70105271167262284138\n",
      "Iteration 31731 => Loss: 6.70105232428885244644\n",
      "Iteration 31732 => Loss: 6.70105193695727407999\n",
      "Iteration 31733 => Loss: 6.70105154967789129472\n",
      "Iteration 31734 => Loss: 6.70105116245069343250\n",
      "Iteration 31735 => Loss: 6.70105077527568582241\n",
      "Iteration 31736 => Loss: 6.70105038815283915454\n",
      "Iteration 31737 => Loss: 6.70105000108215431709\n",
      "Iteration 31738 => Loss: 6.70104961406363219822\n",
      "Iteration 31739 => Loss: 6.70104922709726302799\n",
      "Iteration 31740 => Loss: 6.70104884018303259552\n",
      "Iteration 31741 => Loss: 6.70104845332093912447\n",
      "Iteration 31742 => Loss: 6.70104806651097906212\n",
      "Iteration 31743 => Loss: 6.70104767975313997397\n",
      "Iteration 31744 => Loss: 6.70104729304742186002\n",
      "Iteration 31745 => Loss: 6.70104690639380251582\n",
      "Iteration 31746 => Loss: 6.70104651979229082315\n",
      "Iteration 31747 => Loss: 6.70104613324287257115\n",
      "Iteration 31748 => Loss: 6.70104574674553887803\n",
      "Iteration 31749 => Loss: 6.70104536030029329652\n",
      "Iteration 31750 => Loss: 6.70104497390710829308\n",
      "Iteration 31751 => Loss: 6.70104458756599985492\n",
      "Iteration 31752 => Loss: 6.70104420127695465936\n",
      "Iteration 31753 => Loss: 6.70104381503995405467\n",
      "Iteration 31754 => Loss: 6.70104342885500425808\n",
      "Iteration 31755 => Loss: 6.70104304272208928239\n",
      "Iteration 31756 => Loss: 6.70104265664120379853\n",
      "Iteration 31757 => Loss: 6.70104227061234514196\n",
      "Iteration 31758 => Loss: 6.70104188463550531907\n",
      "Iteration 31759 => Loss: 6.70104149871067633626\n",
      "Iteration 31760 => Loss: 6.70104111283784664721\n",
      "Iteration 31761 => Loss: 6.70104072701701447556\n",
      "Iteration 31762 => Loss: 6.70104034124817182771\n",
      "Iteration 31763 => Loss: 6.70103995553131426277\n",
      "Iteration 31764 => Loss: 6.70103956986643556348\n",
      "Iteration 31765 => Loss: 6.70103918425351974264\n",
      "Iteration 31766 => Loss: 6.70103879869256058299\n",
      "Iteration 31767 => Loss: 6.70103841318355808454\n",
      "Iteration 31768 => Loss: 6.70103802772650958275\n",
      "Iteration 31769 => Loss: 6.70103764232140175494\n",
      "Iteration 31770 => Loss: 6.70103725696821950208\n",
      "Iteration 31771 => Loss: 6.70103687166696815325\n",
      "Iteration 31772 => Loss: 6.70103648641763527394\n",
      "Iteration 31773 => Loss: 6.70103610122021198237\n",
      "Iteration 31774 => Loss: 6.70103571607469739035\n",
      "Iteration 31775 => Loss: 6.70103533098107906341\n",
      "Iteration 31776 => Loss: 6.70103494593936055423\n",
      "Iteration 31777 => Loss: 6.70103456094951432931\n",
      "Iteration 31778 => Loss: 6.70103417601155282313\n",
      "Iteration 31779 => Loss: 6.70103379112546182483\n",
      "Iteration 31780 => Loss: 6.70103340629123422900\n",
      "Iteration 31781 => Loss: 6.70103302150885493660\n",
      "Iteration 31782 => Loss: 6.70103263677833460576\n",
      "Iteration 31783 => Loss: 6.70103225209965458475\n",
      "Iteration 31784 => Loss: 6.70103186747280332725\n",
      "Iteration 31785 => Loss: 6.70103148289778793867\n",
      "Iteration 31786 => Loss: 6.70103109837459065545\n",
      "Iteration 31787 => Loss: 6.70103071390321236578\n",
      "Iteration 31788 => Loss: 6.70103032948363530608\n",
      "Iteration 31789 => Loss: 6.70102994511587102267\n",
      "Iteration 31790 => Loss: 6.70102956079988665294\n",
      "Iteration 31791 => Loss: 6.70102917653569019052\n",
      "Iteration 31792 => Loss: 6.70102879232327897085\n",
      "Iteration 31793 => Loss: 6.70102840816264055945\n",
      "Iteration 31794 => Loss: 6.70102802405376518635\n",
      "Iteration 31795 => Loss: 6.70102763999664929884\n",
      "Iteration 31796 => Loss: 6.70102725599128223877\n",
      "Iteration 31797 => Loss: 6.70102687203766311796\n",
      "Iteration 31798 => Loss: 6.70102648813578039011\n",
      "Iteration 31799 => Loss: 6.70102610428562872613\n",
      "Iteration 31800 => Loss: 6.70102572048720457332\n",
      "Iteration 31801 => Loss: 6.70102533674049549717\n",
      "Iteration 31802 => Loss: 6.70102495304549439226\n",
      "Iteration 31803 => Loss: 6.70102456940219770587\n",
      "Iteration 31804 => Loss: 6.70102418581059477987\n",
      "Iteration 31805 => Loss: 6.70102380227067850882\n",
      "Iteration 31806 => Loss: 6.70102341878244978091\n",
      "Iteration 31807 => Loss: 6.70102303534589349709\n",
      "Iteration 31808 => Loss: 6.70102265196100521649\n",
      "Iteration 31809 => Loss: 6.70102226862777516914\n",
      "Iteration 31810 => Loss: 6.70102188534620601956\n",
      "Iteration 31811 => Loss: 6.70102150211628622145\n",
      "Iteration 31812 => Loss: 6.70102111893800067577\n",
      "Iteration 31813 => Loss: 6.70102073581134582980\n",
      "Iteration 31814 => Loss: 6.70102035273632168355\n",
      "Iteration 31815 => Loss: 6.70101996971291669070\n",
      "Iteration 31816 => Loss: 6.70101958674112374581\n",
      "Iteration 31817 => Loss: 6.70101920382093840800\n",
      "Iteration 31818 => Loss: 6.70101882095234646641\n",
      "Iteration 31819 => Loss: 6.70101843813534969740\n",
      "Iteration 31820 => Loss: 6.70101805536993300194\n",
      "Iteration 31821 => Loss: 6.70101767265609904456\n",
      "Iteration 31822 => Loss: 6.70101728999383716712\n",
      "Iteration 31823 => Loss: 6.70101690738313937601\n",
      "Iteration 31824 => Loss: 6.70101652482398879584\n",
      "Iteration 31825 => Loss: 6.70101614231639430841\n",
      "Iteration 31826 => Loss: 6.70101575986034525556\n",
      "Iteration 31827 => Loss: 6.70101537745583364369\n",
      "Iteration 31828 => Loss: 6.70101499510284615013\n",
      "Iteration 31829 => Loss: 6.70101461280137833398\n",
      "Iteration 31830 => Loss: 6.70101423055143108343\n",
      "Iteration 31831 => Loss: 6.70101384835299018761\n",
      "Iteration 31832 => Loss: 6.70101346620605475835\n",
      "Iteration 31833 => Loss: 6.70101308411060880843\n",
      "Iteration 31834 => Loss: 6.70101270206665056151\n",
      "Iteration 31835 => Loss: 6.70101232007417024761\n",
      "Iteration 31836 => Loss: 6.70101193813316164949\n",
      "Iteration 31837 => Loss: 6.70101155624363098440\n",
      "Iteration 31838 => Loss: 6.70101117440555338334\n",
      "Iteration 31839 => Loss: 6.70101079261893062267\n",
      "Iteration 31840 => Loss: 6.70101041088375115606\n",
      "Iteration 31841 => Loss: 6.70101002920000965446\n",
      "Iteration 31842 => Loss: 6.70100964756769901243\n",
      "Iteration 31843 => Loss: 6.70100926598682011814\n",
      "Iteration 31844 => Loss: 6.70100888445735076715\n",
      "Iteration 31845 => Loss: 6.70100850297929540034\n",
      "Iteration 31846 => Loss: 6.70100812155264957681\n",
      "Iteration 31847 => Loss: 6.70100774017739464483\n",
      "Iteration 31848 => Loss: 6.70100735885353238075\n",
      "Iteration 31849 => Loss: 6.70100697758105479096\n",
      "Iteration 31850 => Loss: 6.70100659635994944097\n",
      "Iteration 31851 => Loss: 6.70100621519021899530\n",
      "Iteration 31852 => Loss: 6.70100583407184746676\n",
      "Iteration 31853 => Loss: 6.70100545300483041444\n",
      "Iteration 31854 => Loss: 6.70100507198916606200\n",
      "Iteration 31855 => Loss: 6.70100469102484286310\n",
      "Iteration 31856 => Loss: 6.70100431011185460051\n",
      "Iteration 31857 => Loss: 6.70100392925019150425\n",
      "Iteration 31858 => Loss: 6.70100354843985179798\n",
      "Iteration 31859 => Loss: 6.70100316768083192898\n",
      "Iteration 31860 => Loss: 6.70100278697311058096\n",
      "Iteration 31861 => Loss: 6.70100240631669219482\n",
      "Iteration 31862 => Loss: 6.70100202571157055331\n",
      "Iteration 31863 => Loss: 6.70100164515773055740\n",
      "Iteration 31864 => Loss: 6.70100126465517131891\n",
      "Iteration 31865 => Loss: 6.70100088420389106147\n",
      "Iteration 31866 => Loss: 6.70100050380386846882\n",
      "Iteration 31867 => Loss: 6.70100012345510442913\n",
      "Iteration 31868 => Loss: 6.70099974315760160692\n",
      "Iteration 31869 => Loss: 6.70099936291134046229\n",
      "Iteration 31870 => Loss: 6.70099898271631388980\n",
      "Iteration 31871 => Loss: 6.70099860257252188944\n",
      "Iteration 31872 => Loss: 6.70099822247995025037\n",
      "Iteration 31873 => Loss: 6.70099784243859808441\n",
      "Iteration 31874 => Loss: 6.70099746244845384524\n",
      "Iteration 31875 => Loss: 6.70099708250951930921\n",
      "Iteration 31876 => Loss: 6.70099670262178026547\n",
      "Iteration 31877 => Loss: 6.70099632278522605588\n",
      "Iteration 31878 => Loss: 6.70099594299986378587\n",
      "Iteration 31879 => Loss: 6.70099556326566947462\n",
      "Iteration 31880 => Loss: 6.70099518358264489848\n",
      "Iteration 31881 => Loss: 6.70099480395078650474\n",
      "Iteration 31882 => Loss: 6.70099442437008274709\n",
      "Iteration 31883 => Loss: 6.70099404484052207920\n",
      "Iteration 31884 => Loss: 6.70099366536211338286\n",
      "Iteration 31885 => Loss: 6.70099328593483090089\n",
      "Iteration 31886 => Loss: 6.70099290655867640965\n",
      "Iteration 31887 => Loss: 6.70099252723364546824\n",
      "Iteration 31888 => Loss: 6.70099214795972741854\n",
      "Iteration 31889 => Loss: 6.70099176873692137235\n",
      "Iteration 31890 => Loss: 6.70099138956520778976\n",
      "Iteration 31891 => Loss: 6.70099101044459377619\n",
      "Iteration 31892 => Loss: 6.70099063137506156806\n",
      "Iteration 31893 => Loss: 6.70099025235661560629\n",
      "Iteration 31894 => Loss: 6.70098987338923635093\n",
      "Iteration 31895 => Loss: 6.70098949447292469017\n",
      "Iteration 31896 => Loss: 6.70098911560767529494\n",
      "Iteration 31897 => Loss: 6.70098873679347661891\n",
      "Iteration 31898 => Loss: 6.70098835803032066849\n",
      "Iteration 31899 => Loss: 6.70098797931820300278\n",
      "Iteration 31900 => Loss: 6.70098760065712006906\n",
      "Iteration 31901 => Loss: 6.70098722204705854466\n",
      "Iteration 31902 => Loss: 6.70098684348801310051\n",
      "Iteration 31903 => Loss: 6.70098646497998196025\n",
      "Iteration 31904 => Loss: 6.70098608652295180121\n",
      "Iteration 31905 => Loss: 6.70098570811692617610\n",
      "Iteration 31906 => Loss: 6.70098532976188643318\n",
      "Iteration 31907 => Loss: 6.70098495145783168425\n",
      "Iteration 31908 => Loss: 6.70098457320474683030\n",
      "Iteration 31909 => Loss: 6.70098419500263631221\n",
      "Iteration 31910 => Loss: 6.70098381685149124820\n",
      "Iteration 31911 => Loss: 6.70098343875129121017\n",
      "Iteration 31912 => Loss: 6.70098306070204863261\n",
      "Iteration 31913 => Loss: 6.70098268270374752831\n",
      "Iteration 31914 => Loss: 6.70098230475638434456\n",
      "Iteration 31915 => Loss: 6.70098192685994931139\n",
      "Iteration 31916 => Loss: 6.70098154901443443521\n",
      "Iteration 31917 => Loss: 6.70098117121983172240\n",
      "Iteration 31918 => Loss: 6.70098079347613673207\n",
      "Iteration 31919 => Loss: 6.70098041578334946422\n",
      "Iteration 31920 => Loss: 6.70098003814145037893\n",
      "Iteration 31921 => Loss: 6.70097966055043592348\n",
      "Iteration 31922 => Loss: 6.70097928301030432152\n",
      "Iteration 31923 => Loss: 6.70097890552104846762\n",
      "Iteration 31924 => Loss: 6.70097852808265859181\n",
      "Iteration 31925 => Loss: 6.70097815069513025321\n",
      "Iteration 31926 => Loss: 6.70097777335845634639\n",
      "Iteration 31927 => Loss: 6.70097739607262443684\n",
      "Iteration 31928 => Loss: 6.70097701883763097186\n",
      "Iteration 31929 => Loss: 6.70097664165346884602\n",
      "Iteration 31930 => Loss: 6.70097626452013539478\n",
      "Iteration 31931 => Loss: 6.70097588743762262453\n",
      "Iteration 31932 => Loss: 6.70097551040591632443\n",
      "Iteration 31933 => Loss: 6.70097513342502271172\n",
      "Iteration 31934 => Loss: 6.70097475649491958194\n",
      "Iteration 31935 => Loss: 6.70097437961561048780\n",
      "Iteration 31936 => Loss: 6.70097400278708565935\n",
      "Iteration 31937 => Loss: 6.70097362600933976751\n",
      "Iteration 31938 => Loss: 6.70097324928236215413\n",
      "Iteration 31939 => Loss: 6.70097287260615637194\n",
      "Iteration 31940 => Loss: 6.70097249598069844012\n",
      "Iteration 31941 => Loss: 6.70097211940599901681\n",
      "Iteration 31942 => Loss: 6.70097174288203500936\n",
      "Iteration 31943 => Loss: 6.70097136640881174685\n",
      "Iteration 31944 => Loss: 6.70097098998631768296\n",
      "Iteration 31945 => Loss: 6.70097061361454304773\n",
      "Iteration 31946 => Loss: 6.70097023729348961751\n",
      "Iteration 31947 => Loss: 6.70096986102314318146\n",
      "Iteration 31948 => Loss: 6.70096948480350018684\n",
      "Iteration 31949 => Loss: 6.70096910863454642282\n",
      "Iteration 31950 => Loss: 6.70096873251629077117\n",
      "Iteration 31951 => Loss: 6.70096835644871724469\n",
      "Iteration 31952 => Loss: 6.70096798043181607341\n",
      "Iteration 31953 => Loss: 6.70096760446558459279\n",
      "Iteration 31954 => Loss: 6.70096722855001303287\n",
      "Iteration 31955 => Loss: 6.70096685268509517641\n",
      "Iteration 31956 => Loss: 6.70096647687082391798\n",
      "Iteration 31957 => Loss: 6.70096610110719481668\n",
      "Iteration 31958 => Loss: 6.70096572539420431980\n",
      "Iteration 31959 => Loss: 6.70096534973183466377\n",
      "Iteration 31960 => Loss: 6.70096497412009117767\n",
      "Iteration 31961 => Loss: 6.70096459855895432156\n",
      "Iteration 31962 => Loss: 6.70096422304843031270\n",
      "Iteration 31963 => Loss: 6.70096384758849783481\n",
      "Iteration 31964 => Loss: 6.70096347217917109873\n",
      "Iteration 31965 => Loss: 6.70096309682042701183\n",
      "Iteration 31966 => Loss: 6.70096272151225491598\n",
      "Iteration 31967 => Loss: 6.70096234625466813384\n",
      "Iteration 31968 => Loss: 6.70096197104763380281\n",
      "Iteration 31969 => Loss: 6.70096159589116435740\n",
      "Iteration 31970 => Loss: 6.70096122078525890942\n",
      "Iteration 31971 => Loss: 6.70096084572988104355\n",
      "Iteration 31972 => Loss: 6.70096047072505118791\n",
      "Iteration 31973 => Loss: 6.70096009577074713803\n",
      "Iteration 31974 => Loss: 6.70095972086697333481\n",
      "Iteration 31975 => Loss: 6.70095934601371467920\n",
      "Iteration 31976 => Loss: 6.70095897121097472393\n",
      "Iteration 31977 => Loss: 6.70095859645873570543\n",
      "Iteration 31978 => Loss: 6.70095822175698963008\n",
      "Iteration 31979 => Loss: 6.70095784710574093879\n",
      "Iteration 31980 => Loss: 6.70095747250497097980\n",
      "Iteration 31981 => Loss: 6.70095709795468330583\n",
      "Iteration 31982 => Loss: 6.70095672345486281785\n",
      "Iteration 31983 => Loss: 6.70095634900551129221\n",
      "Iteration 31984 => Loss: 6.70095597460661185352\n",
      "Iteration 31985 => Loss: 6.70095560025816627814\n",
      "Iteration 31986 => Loss: 6.70095522596015591432\n",
      "Iteration 31987 => Loss: 6.70095485171259408474\n",
      "Iteration 31988 => Loss: 6.70095447751545592041\n",
      "Iteration 31989 => Loss: 6.70095410336873964496\n",
      "Iteration 31990 => Loss: 6.70095372927244259387\n",
      "Iteration 31991 => Loss: 6.70095335522655055627\n",
      "Iteration 31992 => Loss: 6.70095298123106708488\n",
      "Iteration 31993 => Loss: 6.70095260728598152156\n",
      "Iteration 31994 => Loss: 6.70095223339127876727\n",
      "Iteration 31995 => Loss: 6.70095185954695971020\n",
      "Iteration 31996 => Loss: 6.70095148575301724492\n",
      "Iteration 31997 => Loss: 6.70095111200943893692\n",
      "Iteration 31998 => Loss: 6.70095073831622833893\n",
      "Iteration 31999 => Loss: 6.70095036467336946373\n",
      "Iteration 32000 => Loss: 6.70094999108086408768\n",
      "Iteration 32001 => Loss: 6.70094961753869711174\n",
      "Iteration 32002 => Loss: 6.70094924404686143049\n",
      "Iteration 32003 => Loss: 6.70094887060535882028\n",
      "Iteration 32004 => Loss: 6.70094849721417507027\n",
      "Iteration 32005 => Loss: 6.70094812387330662773\n",
      "Iteration 32006 => Loss: 6.70094775058274727542\n",
      "Iteration 32007 => Loss: 6.70094737734248457883\n",
      "Iteration 32008 => Loss: 6.70094700415252120251\n",
      "Iteration 32009 => Loss: 6.70094663101284382378\n",
      "Iteration 32010 => Loss: 6.70094625792344267268\n",
      "Iteration 32011 => Loss: 6.70094588488431774920\n",
      "Iteration 32012 => Loss: 6.70094551189546638881\n",
      "Iteration 32013 => Loss: 6.70094513895686549887\n",
      "Iteration 32014 => Loss: 6.70094476606852218481\n",
      "Iteration 32015 => Loss: 6.70094439323043289392\n",
      "Iteration 32016 => Loss: 6.70094402044257275719\n",
      "Iteration 32017 => Loss: 6.70094364770495154460\n",
      "Iteration 32018 => Loss: 6.70094327501755326892\n",
      "Iteration 32019 => Loss: 6.70094290238037704199\n",
      "Iteration 32020 => Loss: 6.70094252979341220566\n",
      "Iteration 32021 => Loss: 6.70094215725665165451\n",
      "Iteration 32022 => Loss: 6.70094178477009716488\n",
      "Iteration 32023 => Loss: 6.70094141233373274957\n",
      "Iteration 32024 => Loss: 6.70094103994754863862\n",
      "Iteration 32025 => Loss: 6.70094066761154838474\n",
      "Iteration 32026 => Loss: 6.70094029532572044161\n",
      "Iteration 32027 => Loss: 6.70093992309005592745\n",
      "Iteration 32028 => Loss: 6.70093955090455217771\n",
      "Iteration 32029 => Loss: 6.70093917876919853427\n",
      "Iteration 32030 => Loss: 6.70093880668399233258\n",
      "Iteration 32031 => Loss: 6.70093843464892646722\n",
      "Iteration 32032 => Loss: 6.70093806266398583915\n",
      "Iteration 32033 => Loss: 6.70093769072917311291\n",
      "Iteration 32034 => Loss: 6.70093731884447940672\n",
      "Iteration 32035 => Loss: 6.70093694700989495061\n",
      "Iteration 32036 => Loss: 6.70093657522542329730\n",
      "Iteration 32037 => Loss: 6.70093620349103868961\n",
      "Iteration 32038 => Loss: 6.70093583180675089750\n",
      "Iteration 32039 => Loss: 6.70093546017254837466\n",
      "Iteration 32040 => Loss: 6.70093508858842223930\n",
      "Iteration 32041 => Loss: 6.70093471705436982688\n",
      "Iteration 32042 => Loss: 6.70093434557037781474\n",
      "Iteration 32043 => Loss: 6.70093397413644709104\n",
      "Iteration 32044 => Loss: 6.70093360275256078040\n",
      "Iteration 32045 => Loss: 6.70093323141872154736\n",
      "Iteration 32046 => Loss: 6.70093286013492317466\n",
      "Iteration 32047 => Loss: 6.70093248890115322780\n",
      "Iteration 32048 => Loss: 6.70093211771740016047\n",
      "Iteration 32049 => Loss: 6.70093174658366841356\n",
      "Iteration 32050 => Loss: 6.70093137549995354618\n",
      "Iteration 32051 => Loss: 6.70093100446624223565\n",
      "Iteration 32052 => Loss: 6.70093063348251849476\n",
      "Iteration 32053 => Loss: 6.70093026254878587622\n",
      "Iteration 32054 => Loss: 6.70092989166504704457\n",
      "Iteration 32055 => Loss: 6.70092952083127624263\n",
      "Iteration 32056 => Loss: 6.70092915004747968766\n",
      "Iteration 32057 => Loss: 6.70092877931363695154\n",
      "Iteration 32058 => Loss: 6.70092840862976135696\n",
      "Iteration 32059 => Loss: 6.70092803799583514035\n",
      "Iteration 32060 => Loss: 6.70092766741184764356\n",
      "Iteration 32061 => Loss: 6.70092729687779264935\n",
      "Iteration 32062 => Loss: 6.70092692639367193408\n",
      "Iteration 32063 => Loss: 6.70092655595947217506\n",
      "Iteration 32064 => Loss: 6.70092618557518981959\n",
      "Iteration 32065 => Loss: 6.70092581524081598587\n",
      "Iteration 32066 => Loss: 6.70092544495634534485\n",
      "Iteration 32067 => Loss: 6.70092507472177256744\n",
      "Iteration 32068 => Loss: 6.70092470453708433098\n",
      "Iteration 32069 => Loss: 6.70092433440227797092\n",
      "Iteration 32070 => Loss: 6.70092396431735881634\n",
      "Iteration 32071 => Loss: 6.70092359428229311646\n",
      "Iteration 32072 => Loss: 6.70092322429709863485\n",
      "Iteration 32073 => Loss: 6.70092285436175938429\n",
      "Iteration 32074 => Loss: 6.70092248447627092389\n",
      "Iteration 32075 => Loss: 6.70092211464061549009\n",
      "Iteration 32076 => Loss: 6.70092174485480196466\n",
      "Iteration 32077 => Loss: 6.70092137511882057765\n",
      "Iteration 32078 => Loss: 6.70092100543265356549\n",
      "Iteration 32079 => Loss: 6.70092063579630359271\n",
      "Iteration 32080 => Loss: 6.70092026620976444207\n",
      "Iteration 32081 => Loss: 6.70091989667302545541\n",
      "Iteration 32082 => Loss: 6.70091952718608396822\n",
      "Iteration 32083 => Loss: 6.70091915774892665780\n",
      "Iteration 32084 => Loss: 6.70091878836155618870\n",
      "Iteration 32085 => Loss: 6.70091841902395835007\n",
      "Iteration 32086 => Loss: 6.70091804973612958918\n",
      "Iteration 32087 => Loss: 6.70091768049805747154\n",
      "Iteration 32088 => Loss: 6.70091731130974732622\n",
      "Iteration 32089 => Loss: 6.70091694217117517240\n",
      "Iteration 32090 => Loss: 6.70091657308235344459\n",
      "Iteration 32091 => Loss: 6.70091620404326882010\n",
      "Iteration 32092 => Loss: 6.70091583505390264719\n",
      "Iteration 32093 => Loss: 6.70091546611426558400\n",
      "Iteration 32094 => Loss: 6.70091509722433720242\n",
      "Iteration 32095 => Loss: 6.70091472838412194335\n",
      "Iteration 32096 => Loss: 6.70091435959360559593\n",
      "Iteration 32097 => Loss: 6.70091399085278283110\n",
      "Iteration 32098 => Loss: 6.70091362216164654342\n",
      "Iteration 32099 => Loss: 6.70091325352019762107\n",
      "Iteration 32100 => Loss: 6.70091288492842007685\n",
      "Iteration 32101 => Loss: 6.70091251638631213439\n",
      "Iteration 32102 => Loss: 6.70091214789386313555\n",
      "Iteration 32103 => Loss: 6.70091177945106775127\n",
      "Iteration 32104 => Loss: 6.70091141105791976429\n",
      "Iteration 32105 => Loss: 6.70091104271441739826\n",
      "Iteration 32106 => Loss: 6.70091067442054377779\n",
      "Iteration 32107 => Loss: 6.70091030617629890287\n",
      "Iteration 32108 => Loss: 6.70090993798167566808\n",
      "Iteration 32109 => Loss: 6.70090956983667052071\n",
      "Iteration 32110 => Loss: 6.70090920174126569719\n",
      "Iteration 32111 => Loss: 6.70090883369546563841\n",
      "Iteration 32112 => Loss: 6.70090846569925790988\n",
      "Iteration 32113 => Loss: 6.70090809775264251158\n",
      "Iteration 32114 => Loss: 6.70090772985559901542\n",
      "Iteration 32115 => Loss: 6.70090736200814252044\n",
      "Iteration 32116 => Loss: 6.70090699421024371674\n",
      "Iteration 32117 => Loss: 6.70090662646190704521\n",
      "Iteration 32118 => Loss: 6.70090625876312451226\n",
      "Iteration 32119 => Loss: 6.70090589111388812427\n",
      "Iteration 32120 => Loss: 6.70090552351420232213\n",
      "Iteration 32121 => Loss: 6.70090515596404134868\n",
      "Iteration 32122 => Loss: 6.70090478846340875663\n",
      "Iteration 32123 => Loss: 6.70090442101229921690\n",
      "Iteration 32124 => Loss: 6.70090405361070118317\n",
      "Iteration 32125 => Loss: 6.70090368625860932639\n",
      "Iteration 32126 => Loss: 6.70090331895602453471\n",
      "Iteration 32127 => Loss: 6.70090295170292904459\n",
      "Iteration 32128 => Loss: 6.70090258449932107965\n",
      "Iteration 32129 => Loss: 6.70090221734519619901\n",
      "Iteration 32130 => Loss: 6.70090185024054463270\n",
      "Iteration 32131 => Loss: 6.70090148318535661076\n",
      "Iteration 32132 => Loss: 6.70090111617963124502\n",
      "Iteration 32133 => Loss: 6.70090074922336143004\n",
      "Iteration 32134 => Loss: 6.70090038231653739587\n",
      "Iteration 32135 => Loss: 6.70090001545916003067\n",
      "Iteration 32136 => Loss: 6.70089964865120979454\n",
      "Iteration 32137 => Loss: 6.70089928189268757563\n",
      "Iteration 32138 => Loss: 6.70089891518358804490\n",
      "Iteration 32139 => Loss: 6.70089854852390498507\n",
      "Iteration 32140 => Loss: 6.70089818191362596167\n",
      "Iteration 32141 => Loss: 6.70089781535274475743\n",
      "Iteration 32142 => Loss: 6.70089744884126314872\n",
      "Iteration 32143 => Loss: 6.70089708237916248379\n",
      "Iteration 32144 => Loss: 6.70089671596645253260\n",
      "Iteration 32145 => Loss: 6.70089634960311375522\n",
      "Iteration 32146 => Loss: 6.70089598328913726988\n",
      "Iteration 32147 => Loss: 6.70089561702452574110\n",
      "Iteration 32148 => Loss: 6.70089525080926584621\n",
      "Iteration 32149 => Loss: 6.70089488464335758522\n",
      "Iteration 32150 => Loss: 6.70089451852678585908\n",
      "Iteration 32151 => Loss: 6.70089415245954889144\n",
      "Iteration 32152 => Loss: 6.70089378644163780052\n",
      "Iteration 32153 => Loss: 6.70089342047304992178\n",
      "Iteration 32154 => Loss: 6.70089305455377726162\n",
      "Iteration 32155 => Loss: 6.70089268868381182642\n",
      "Iteration 32156 => Loss: 6.70089232286314384623\n",
      "Iteration 32157 => Loss: 6.70089195709177332105\n",
      "Iteration 32158 => Loss: 6.70089159136969758634\n",
      "Iteration 32159 => Loss: 6.70089122569689088493\n",
      "Iteration 32160 => Loss: 6.70089086007336920403\n",
      "Iteration 32161 => Loss: 6.70089049449910945100\n",
      "Iteration 32162 => Loss: 6.70089012897410718494\n",
      "Iteration 32163 => Loss: 6.70088976349836773494\n",
      "Iteration 32164 => Loss: 6.70088939807187333741\n",
      "Iteration 32165 => Loss: 6.70088903269461866330\n",
      "Iteration 32166 => Loss: 6.70088866736659749535\n",
      "Iteration 32167 => Loss: 6.70088830208780539266\n",
      "Iteration 32168 => Loss: 6.70088793685823436164\n",
      "Iteration 32169 => Loss: 6.70088757167787907321\n",
      "Iteration 32170 => Loss: 6.70088720654672975741\n",
      "Iteration 32171 => Loss: 6.70088684146478374970\n",
      "Iteration 32172 => Loss: 6.70088647643203483284\n",
      "Iteration 32173 => Loss: 6.70088611144846968415\n",
      "Iteration 32174 => Loss: 6.70088574651408919181\n",
      "Iteration 32175 => Loss: 6.70088538162888447403\n",
      "Iteration 32176 => Loss: 6.70088501679284487267\n",
      "Iteration 32177 => Loss: 6.70088465200596594684\n",
      "Iteration 32178 => Loss: 6.70088428726823970294\n",
      "Iteration 32179 => Loss: 6.70088392257967324639\n",
      "Iteration 32180 => Loss: 6.70088355794023815548\n",
      "Iteration 32181 => Loss: 6.70088319334993975929\n",
      "Iteration 32182 => Loss: 6.70088282880877628145\n",
      "Iteration 32183 => Loss: 6.70088246431673351111\n",
      "Iteration 32184 => Loss: 6.70088209987380256649\n",
      "Iteration 32185 => Loss: 6.70088173547998255941\n",
      "Iteration 32186 => Loss: 6.70088137113525572630\n",
      "Iteration 32187 => Loss: 6.70088100683963805437\n",
      "Iteration 32188 => Loss: 6.70088064259310112192\n",
      "Iteration 32189 => Loss: 6.70088027839564848165\n",
      "Iteration 32190 => Loss: 6.70087991424727125178\n",
      "Iteration 32191 => Loss: 6.70087955014796321507\n",
      "Iteration 32192 => Loss: 6.70087918609771282519\n",
      "Iteration 32193 => Loss: 6.70087882209652985210\n",
      "Iteration 32194 => Loss: 6.70087845814438498593\n",
      "Iteration 32195 => Loss: 6.70087809424128622027\n",
      "Iteration 32196 => Loss: 6.70087773038722378516\n",
      "Iteration 32197 => Loss: 6.70087736658219235153\n",
      "Iteration 32198 => Loss: 6.70087700282618570213\n",
      "Iteration 32199 => Loss: 6.70087663911919317883\n",
      "Iteration 32200 => Loss: 6.70087627546120678801\n",
      "Iteration 32201 => Loss: 6.70087591185222386514\n",
      "Iteration 32202 => Loss: 6.70087554829223996933\n",
      "Iteration 32203 => Loss: 6.70087518478124355426\n",
      "Iteration 32204 => Loss: 6.70087482131922929085\n",
      "Iteration 32205 => Loss: 6.70087445790619451458\n",
      "Iteration 32206 => Loss: 6.70087409454212501458\n",
      "Iteration 32207 => Loss: 6.70087373122702789630\n",
      "Iteration 32208 => Loss: 6.70087336796088006707\n",
      "Iteration 32209 => Loss: 6.70087300474368596781\n",
      "Iteration 32210 => Loss: 6.70087264157543049947\n",
      "Iteration 32211 => Loss: 6.70087227845611721477\n",
      "Iteration 32212 => Loss: 6.70087191538573101468\n",
      "Iteration 32213 => Loss: 6.70087155236427367555\n",
      "Iteration 32214 => Loss: 6.70087118939172832199\n",
      "Iteration 32215 => Loss: 6.70087082646809495401\n",
      "Iteration 32216 => Loss: 6.70087046359336380164\n",
      "Iteration 32217 => Loss: 6.70087010076753220034\n",
      "Iteration 32218 => Loss: 6.70086973799059304469\n",
      "Iteration 32219 => Loss: 6.70086937526253390018\n",
      "Iteration 32220 => Loss: 6.70086901258335654319\n",
      "Iteration 32221 => Loss: 6.70086864995304498649\n",
      "Iteration 32222 => Loss: 6.70086828737160367098\n",
      "Iteration 32223 => Loss: 6.70086792483901838580\n",
      "Iteration 32224 => Loss: 6.70086756235528557823\n",
      "Iteration 32225 => Loss: 6.70086719992039547833\n",
      "Iteration 32226 => Loss: 6.70086683753434542155\n",
      "Iteration 32227 => Loss: 6.70086647519712297338\n",
      "Iteration 32228 => Loss: 6.70086611290872102842\n",
      "Iteration 32229 => Loss: 6.70086575066915024479\n",
      "Iteration 32230 => Loss: 6.70086538847838841804\n",
      "Iteration 32231 => Loss: 6.70086502633642400184\n",
      "Iteration 32232 => Loss: 6.70086466424326321345\n",
      "Iteration 32233 => Loss: 6.70086430219889717108\n",
      "Iteration 32234 => Loss: 6.70086394020330633481\n",
      "Iteration 32235 => Loss: 6.70086357825650669184\n",
      "Iteration 32236 => Loss: 6.70086321635847870226\n",
      "Iteration 32237 => Loss: 6.70086285450920726703\n",
      "Iteration 32238 => Loss: 6.70086249270870304429\n",
      "Iteration 32239 => Loss: 6.70086213095694116504\n",
      "Iteration 32240 => Loss: 6.70086176925393584014\n",
      "Iteration 32241 => Loss: 6.70086140759967019420\n",
      "Iteration 32242 => Loss: 6.70086104599413268090\n",
      "Iteration 32243 => Loss: 6.70086068443732418842\n",
      "Iteration 32244 => Loss: 6.70086032292923405862\n",
      "Iteration 32245 => Loss: 6.70085996146986051514\n",
      "Iteration 32246 => Loss: 6.70085960005918401805\n",
      "Iteration 32247 => Loss: 6.70085923869721433732\n",
      "Iteration 32248 => Loss: 6.70085887738394525570\n",
      "Iteration 32249 => Loss: 6.70085851611935101602\n",
      "Iteration 32250 => Loss: 6.70085815490344316459\n",
      "Iteration 32251 => Loss: 6.70085779373620393784\n",
      "Iteration 32252 => Loss: 6.70085743261763777667\n",
      "Iteration 32253 => Loss: 6.70085707154773135841\n",
      "Iteration 32254 => Loss: 6.70085671052647580126\n",
      "Iteration 32255 => Loss: 6.70085634955387376976\n",
      "Iteration 32256 => Loss: 6.70085598862991371760\n",
      "Iteration 32257 => Loss: 6.70085562775457965756\n",
      "Iteration 32258 => Loss: 6.70085526692788047143\n",
      "Iteration 32259 => Loss: 6.70085490614979839563\n",
      "Iteration 32260 => Loss: 6.70085454542033165382\n",
      "Iteration 32261 => Loss: 6.70085418473947935780\n",
      "Iteration 32262 => Loss: 6.70085382410722374402\n",
      "Iteration 32263 => Loss: 6.70085346352356037158\n",
      "Iteration 32264 => Loss: 6.70085310298849190502\n",
      "Iteration 32265 => Loss: 6.70085274250199969259\n",
      "Iteration 32266 => Loss: 6.70085238206408995154\n",
      "Iteration 32267 => Loss: 6.70085202167474491830\n",
      "Iteration 32268 => Loss: 6.70085166133396192834\n",
      "Iteration 32269 => Loss: 6.70085130104173654075\n",
      "Iteration 32270 => Loss: 6.70085094079805898559\n",
      "Iteration 32271 => Loss: 6.70085058060292571014\n",
      "Iteration 32272 => Loss: 6.70085022045633138532\n",
      "Iteration 32273 => Loss: 6.70084986035826091211\n",
      "Iteration 32274 => Loss: 6.70084950030871961957\n",
      "Iteration 32275 => Loss: 6.70084914030768796778\n",
      "Iteration 32276 => Loss: 6.70084878035517039763\n",
      "Iteration 32277 => Loss: 6.70084842045115980369\n",
      "Iteration 32278 => Loss: 6.70084806059564108693\n",
      "Iteration 32279 => Loss: 6.70084770078861868825\n",
      "Iteration 32280 => Loss: 6.70084734103007306771\n",
      "Iteration 32281 => Loss: 6.70084698132000866622\n",
      "Iteration 32282 => Loss: 6.70084662165841571380\n",
      "Iteration 32283 => Loss: 6.70084626204528976956\n",
      "Iteration 32284 => Loss: 6.70084590248061839901\n",
      "Iteration 32285 => Loss: 6.70084554296439716126\n",
      "Iteration 32286 => Loss: 6.70084518349661539816\n",
      "Iteration 32287 => Loss: 6.70084482407727843878\n",
      "Iteration 32288 => Loss: 6.70084446470637562498\n",
      "Iteration 32289 => Loss: 6.70084410538389807499\n",
      "Iteration 32290 => Loss: 6.70084374610983601883\n",
      "Iteration 32291 => Loss: 6.70084338688418412744\n",
      "Iteration 32292 => Loss: 6.70084302770693973628\n",
      "Iteration 32293 => Loss: 6.70084266857810018081\n",
      "Iteration 32294 => Loss: 6.70084230949765125018\n",
      "Iteration 32295 => Loss: 6.70084195046558672715\n",
      "Iteration 32296 => Loss: 6.70084159148190039446\n",
      "Iteration 32297 => Loss: 6.70084123254658425850\n",
      "Iteration 32298 => Loss: 6.70084087365964098382\n",
      "Iteration 32299 => Loss: 6.70084051482105191866\n",
      "Iteration 32300 => Loss: 6.70084015603081795120\n",
      "Iteration 32301 => Loss: 6.70083979728892931149\n",
      "Iteration 32302 => Loss: 6.70083943859538688770\n",
      "Iteration 32303 => Loss: 6.70083907995018179804\n",
      "Iteration 32304 => Loss: 6.70083872135329539077\n",
      "Iteration 32305 => Loss: 6.70083836280473388314\n",
      "Iteration 32306 => Loss: 6.70083800430448661700\n",
      "Iteration 32307 => Loss: 6.70083764585254204604\n",
      "Iteration 32308 => Loss: 6.70083728744890905205\n",
      "Iteration 32309 => Loss: 6.70083692909355832512\n",
      "Iteration 32310 => Loss: 6.70083657078650496430\n",
      "Iteration 32311 => Loss: 6.70083621252773120602\n",
      "Iteration 32312 => Loss: 6.70083585431723083303\n",
      "Iteration 32313 => Loss: 6.70083549615500650987\n",
      "Iteration 32314 => Loss: 6.70083513804103692024\n",
      "Iteration 32315 => Loss: 6.70083477997532739323\n",
      "Iteration 32316 => Loss: 6.70083442195786460616\n",
      "Iteration 32317 => Loss: 6.70083406398864678266\n",
      "Iteration 32318 => Loss: 6.70083370606766770550\n",
      "Iteration 32319 => Loss: 6.70083334819490872292\n",
      "Iteration 32320 => Loss: 6.70083299037038404578\n",
      "Iteration 32321 => Loss: 6.70083263259406702872\n",
      "Iteration 32322 => Loss: 6.70083227486597010625\n",
      "Iteration 32323 => Loss: 6.70083191718607018572\n",
      "Iteration 32324 => Loss: 6.70083155955437437257\n",
      "Iteration 32325 => Loss: 6.70083120197085513325\n",
      "Iteration 32326 => Loss: 6.70083084443553111953\n",
      "Iteration 32327 => Loss: 6.70083048694839078507\n",
      "Iteration 32328 => Loss: 6.70083012950941370178\n",
      "Iteration 32329 => Loss: 6.70082977211860431055\n",
      "Iteration 32330 => Loss: 6.70082941477594840052\n",
      "Iteration 32331 => Loss: 6.70082905748145041258\n",
      "Iteration 32332 => Loss: 6.70082870023508991864\n",
      "Iteration 32333 => Loss: 6.70082834303687757682\n",
      "Iteration 32334 => Loss: 6.70082798588679295904\n",
      "Iteration 32335 => Loss: 6.70082762878483428892\n",
      "Iteration 32336 => Loss: 6.70082727173099979012\n",
      "Iteration 32337 => Loss: 6.70082691472527525178\n",
      "Iteration 32338 => Loss: 6.70082655776765356848\n",
      "Iteration 32339 => Loss: 6.70082620085813651656\n",
      "Iteration 32340 => Loss: 6.70082584399670722064\n",
      "Iteration 32341 => Loss: 6.70082548718337633886\n",
      "Iteration 32342 => Loss: 6.70082513041811544952\n",
      "Iteration 32343 => Loss: 6.70082477370093521074\n",
      "Iteration 32344 => Loss: 6.70082441703182052350\n",
      "Iteration 32345 => Loss: 6.70082406041076250602\n",
      "Iteration 32346 => Loss: 6.70082370383776471101\n",
      "Iteration 32347 => Loss: 6.70082334731280848672\n",
      "Iteration 32348 => Loss: 6.70082299083590182676\n",
      "Iteration 32349 => Loss: 6.70082263440703052026\n",
      "Iteration 32350 => Loss: 6.70082227802618390911\n",
      "Iteration 32351 => Loss: 6.70082192169336110510\n",
      "Iteration 32352 => Loss: 6.70082156540855233828\n",
      "Iteration 32353 => Loss: 6.70082120917176027319\n",
      "Iteration 32354 => Loss: 6.70082085298296359355\n",
      "Iteration 32355 => Loss: 6.70082049684216674024\n",
      "Iteration 32356 => Loss: 6.70082014074935550241\n",
      "Iteration 32357 => Loss: 6.70081978470453076824\n",
      "Iteration 32358 => Loss: 6.70081942870768010323\n",
      "Iteration 32359 => Loss: 6.70081907275880617192\n",
      "Iteration 32360 => Loss: 6.70081871685789831616\n",
      "Iteration 32361 => Loss: 6.70081836100493077879\n",
      "Iteration 32362 => Loss: 6.70081800519993286969\n",
      "Iteration 32363 => Loss: 6.70081764944287616714\n",
      "Iteration 32364 => Loss: 6.70081729373375445391\n",
      "Iteration 32365 => Loss: 6.70081693807256328910\n",
      "Iteration 32366 => Loss: 6.70081658245930089635\n",
      "Iteration 32367 => Loss: 6.70081622689395306480\n",
      "Iteration 32368 => Loss: 6.70081587137652423536\n",
      "Iteration 32369 => Loss: 6.70081551590699842080\n",
      "Iteration 32370 => Loss: 6.70081516048537384478\n",
      "Iteration 32371 => Loss: 6.70081480511163452007\n",
      "Iteration 32372 => Loss: 6.70081444978578488758\n",
      "Iteration 32373 => Loss: 6.70081409450782050641\n",
      "Iteration 32374 => Loss: 6.70081373927772983023\n",
      "Iteration 32375 => Loss: 6.70081338409550308910\n",
      "Iteration 32376 => Loss: 6.70081302896113939482\n",
      "Iteration 32377 => Loss: 6.70081267387462453655\n",
      "Iteration 32378 => Loss: 6.70081231883596739607\n",
      "Iteration 32379 => Loss: 6.70081196384514132802\n",
      "Iteration 32380 => Loss: 6.70081160890215254966\n",
      "Iteration 32381 => Loss: 6.70081125400699839645\n",
      "Iteration 32382 => Loss: 6.70081089915966643389\n",
      "Iteration 32383 => Loss: 6.70081054436014245113\n",
      "Iteration 32384 => Loss: 6.70081018960843444177\n",
      "Iteration 32385 => Loss: 6.70080983490452375406\n",
      "Iteration 32386 => Loss: 6.70080948024841127619\n",
      "Iteration 32387 => Loss: 6.70080912564009345544\n",
      "Iteration 32388 => Loss: 6.70080877107955075189\n",
      "Iteration 32389 => Loss: 6.70080841656679648821\n",
      "Iteration 32390 => Loss: 6.70080806210180135452\n",
      "Iteration 32391 => Loss: 6.70080770768457600894\n",
      "Iteration 32392 => Loss: 6.70080735331510535246\n",
      "Iteration 32393 => Loss: 6.70080699899338938508\n",
      "Iteration 32394 => Loss: 6.70080664471941744864\n",
      "Iteration 32395 => Loss: 6.70080629049318599044\n",
      "Iteration 32396 => Loss: 6.70080593631468435234\n",
      "Iteration 32397 => Loss: 6.70080558218391342251\n",
      "Iteration 32398 => Loss: 6.70080522810084655561\n",
      "Iteration 32399 => Loss: 6.70080487406551039697\n",
      "Iteration 32400 => Loss: 6.70080452007786941948\n",
      "Iteration 32401 => Loss: 6.70080416613793250491\n",
      "Iteration 32402 => Loss: 6.70080381224568721876\n",
      "Iteration 32403 => Loss: 6.70080345840113000833\n",
      "Iteration 32404 => Loss: 6.70080310460425110364\n",
      "Iteration 32405 => Loss: 6.70080275085504695198\n",
      "Iteration 32406 => Loss: 6.70080239715351133611\n",
      "Iteration 32407 => Loss: 6.70080204349963981514\n",
      "Iteration 32408 => Loss: 6.70080168989342173091\n",
      "Iteration 32409 => Loss: 6.70080133633485619526\n",
      "Iteration 32410 => Loss: 6.70080098282392633280\n",
      "Iteration 32411 => Loss: 6.70080062936063036716\n",
      "Iteration 32412 => Loss: 6.70080027594496741017\n",
      "Iteration 32413 => Loss: 6.70079992257692591551\n",
      "Iteration 32414 => Loss: 6.70079956925650233046\n",
      "Iteration 32415 => Loss: 6.70079921598368422053\n",
      "Iteration 32416 => Loss: 6.70079886275847513843\n",
      "Iteration 32417 => Loss: 6.70079850958086087331\n",
      "Iteration 32418 => Loss: 6.70079815645083520792\n",
      "Iteration 32419 => Loss: 6.70079780336840169497\n",
      "Iteration 32420 => Loss: 6.70079745033353990635\n",
      "Iteration 32421 => Loss: 6.70079709734624806572\n",
      "Iteration 32422 => Loss: 6.70079674440652350853\n",
      "Iteration 32423 => Loss: 6.70079639151435824118\n",
      "Iteration 32424 => Loss: 6.70079603866974693460\n",
      "Iteration 32425 => Loss: 6.70079568587268159519\n",
      "Iteration 32426 => Loss: 6.70079533312314712390\n",
      "Iteration 32427 => Loss: 6.70079498042115861978\n",
      "Iteration 32428 => Loss: 6.70079462776668766111\n",
      "Iteration 32429 => Loss: 6.70079427515973780061\n",
      "Iteration 32430 => Loss: 6.70079392260030370920\n",
      "Iteration 32431 => Loss: 6.70079357008837828147\n",
      "Iteration 32432 => Loss: 6.70079321762395174744\n",
      "Iteration 32433 => Loss: 6.70079286520702233076\n",
      "Iteration 32434 => Loss: 6.70079251283757759694\n",
      "Iteration 32435 => Loss: 6.70079216051561576961\n",
      "Iteration 32436 => Loss: 6.70079180824112530246\n",
      "Iteration 32437 => Loss: 6.70079145601411418909\n",
      "Iteration 32438 => Loss: 6.70079110383455844868\n",
      "Iteration 32439 => Loss: 6.70079075170245719306\n",
      "Iteration 32440 => Loss: 6.70079039961781219858\n",
      "Iteration 32441 => Loss: 6.70079004758060481350\n",
      "Iteration 32442 => Loss: 6.70078969559084036689\n",
      "Iteration 32443 => Loss: 6.70078934364850020700\n",
      "Iteration 32444 => Loss: 6.70078899175358611018\n",
      "Iteration 32445 => Loss: 6.70078863990608919465\n",
      "Iteration 32446 => Loss: 6.70078828810601034860\n",
      "Iteration 32447 => Loss: 6.70078793635333358480\n",
      "Iteration 32448 => Loss: 6.70078758464805179784\n",
      "Iteration 32449 => Loss: 6.70078723299015965864\n",
      "Iteration 32450 => Loss: 6.70078688137966071992\n",
      "Iteration 32451 => Loss: 6.70078652981654077081\n",
      "Iteration 32452 => Loss: 6.70078617830078915318\n",
      "Iteration 32453 => Loss: 6.70078582683241119611\n",
      "Iteration 32454 => Loss: 6.70078547541138291876\n",
      "Iteration 32455 => Loss: 6.70078512403771942019\n",
      "Iteration 32456 => Loss: 6.70078477271139583138\n",
      "Iteration 32457 => Loss: 6.70078442143242103413\n",
      "Iteration 32458 => Loss: 6.70078407020077637668\n",
      "Iteration 32459 => Loss: 6.70078371901645919451\n",
      "Iteration 32460 => Loss: 6.70078336787947126396\n",
      "Iteration 32461 => Loss: 6.70078301678979126876\n",
      "Iteration 32462 => Loss: 6.70078266574742453798\n",
      "Iteration 32463 => Loss: 6.70078231475235508441\n",
      "Iteration 32464 => Loss: 6.70078196380458734893\n",
      "Iteration 32465 => Loss: 6.70078161290411156159\n",
      "Iteration 32466 => Loss: 6.70078126205091351153\n",
      "Iteration 32467 => Loss: 6.70078091124499408693\n",
      "Iteration 32468 => Loss: 6.70078056048635062325\n",
      "Iteration 32469 => Loss: 6.70078020977496979782\n",
      "Iteration 32470 => Loss: 6.70077985911084716975\n",
      "Iteration 32471 => Loss: 6.70077950849397119271\n",
      "Iteration 32472 => Loss: 6.70077915792435518938\n",
      "Iteration 32473 => Loss: 6.70077880740196096809\n",
      "Iteration 32474 => Loss: 6.70077845692681250966\n",
      "Iteration 32475 => Loss: 6.70077810649888494510\n",
      "Iteration 32476 => Loss: 6.70077775611817738621\n",
      "Iteration 32477 => Loss: 6.70077740578468805666\n",
      "Iteration 32478 => Loss: 6.70077705549840008104\n",
      "Iteration 32479 => Loss: 6.70077670525931612389\n",
      "Iteration 32480 => Loss: 6.70077635506742286253\n",
      "Iteration 32481 => Loss: 6.70077600492272562605\n",
      "Iteration 32482 => Loss: 6.70077565482520931539\n",
      "Iteration 32483 => Loss: 6.70077530477486238425\n",
      "Iteration 32484 => Loss: 6.70077495477169193805\n",
      "Iteration 32485 => Loss: 6.70077460481568110140\n",
      "Iteration 32486 => Loss: 6.70077425490682987430\n",
      "Iteration 32487 => Loss: 6.70077390504512138136\n",
      "Iteration 32488 => Loss: 6.70077355523056006348\n",
      "Iteration 32489 => Loss: 6.70077320546313615068\n",
      "Iteration 32490 => Loss: 6.70077285574285586023\n",
      "Iteration 32491 => Loss: 6.70077250606968632951\n",
      "Iteration 32492 => Loss: 6.70077215644363377578\n",
      "Iteration 32493 => Loss: 6.70077180686470175175\n",
      "Iteration 32494 => Loss: 6.70077145733287338203\n",
      "Iteration 32495 => Loss: 6.70077110784813978483\n",
      "Iteration 32496 => Loss: 6.70077075841050096017\n",
      "Iteration 32497 => Loss: 6.70077040901995069078\n",
      "Iteration 32498 => Loss: 6.70077005967648364759\n",
      "Iteration 32499 => Loss: 6.70076971038008917247\n",
      "Iteration 32500 => Loss: 6.70076936113075571910\n",
      "Iteration 32501 => Loss: 6.70076901192849039290\n",
      "Iteration 32502 => Loss: 6.70076866277327898302\n",
      "Iteration 32503 => Loss: 6.70076831366511349586\n",
      "Iteration 32504 => Loss: 6.70076796460399393141\n",
      "Iteration 32505 => Loss: 6.70076761558991140788\n",
      "Iteration 32506 => Loss: 6.70076726662285171443\n",
      "Iteration 32507 => Loss: 6.70076691770281751559\n",
      "Iteration 32508 => Loss: 6.70076656882980437047\n",
      "Iteration 32509 => Loss: 6.70076622000379984456\n",
      "Iteration 32510 => Loss: 6.70076587122480038516\n",
      "Iteration 32511 => Loss: 6.70076552249280155138\n",
      "Iteration 32512 => Loss: 6.70076517380778291511\n",
      "Iteration 32513 => Loss: 6.70076482516976401627\n",
      "Iteration 32514 => Loss: 6.70076447657871643315\n",
      "Iteration 32515 => Loss: 6.70076412803463838941\n",
      "Iteration 32516 => Loss: 6.70076377953753610228\n",
      "Iteration 32517 => Loss: 6.70076343108739003185\n",
      "Iteration 32518 => Loss: 6.70076308268419840175\n",
      "Iteration 32519 => Loss: 6.70076273432794966567\n",
      "Iteration 32520 => Loss: 6.70076238601864204725\n",
      "Iteration 32521 => Loss: 6.70076203775627465831\n",
      "Iteration 32522 => Loss: 6.70076168954082795892\n",
      "Iteration 32523 => Loss: 6.70076134137231083088\n",
      "Iteration 32524 => Loss: 6.70076099325071172785\n",
      "Iteration 32525 => Loss: 6.70076064517601288628\n",
      "Iteration 32526 => Loss: 6.70076029714822318795\n",
      "Iteration 32527 => Loss: 6.70075994916732931017\n",
      "Iteration 32528 => Loss: 6.70075960123332414753\n",
      "Iteration 32529 => Loss: 6.70075925334620237095\n",
      "Iteration 32530 => Loss: 6.70075890550596220407\n",
      "Iteration 32531 => Loss: 6.70075855771258854787\n",
      "Iteration 32532 => Loss: 6.70075820996608673141\n",
      "Iteration 32533 => Loss: 6.70075786226643366206\n",
      "Iteration 32534 => Loss: 6.70075751461364355066\n",
      "Iteration 32535 => Loss: 6.70075716700769419276\n",
      "Iteration 32536 => Loss: 6.70075681944858914107\n",
      "Iteration 32537 => Loss: 6.70075647193630796750\n",
      "Iteration 32538 => Loss: 6.70075612447086133017\n",
      "Iteration 32539 => Loss: 6.70075577705223768277\n",
      "Iteration 32540 => Loss: 6.70075542968042903169\n",
      "Iteration 32541 => Loss: 6.70075508235542560698\n",
      "Iteration 32542 => Loss: 6.70075473507722296773\n",
      "Iteration 32543 => Loss: 6.70075438784581756124\n",
      "Iteration 32544 => Loss: 6.70075404066120050572\n",
      "Iteration 32545 => Loss: 6.70075369352336736029\n",
      "Iteration 32546 => Loss: 6.70075334643231190768\n",
      "Iteration 32547 => Loss: 6.70075299938802437794\n",
      "Iteration 32548 => Loss: 6.70075265239050299471\n",
      "Iteration 32549 => Loss: 6.70075230543974331709\n",
      "Iteration 32550 => Loss: 6.70075195853573113425\n",
      "Iteration 32551 => Loss: 6.70075161167846733434\n",
      "Iteration 32552 => Loss: 6.70075126486794214742\n",
      "Iteration 32553 => Loss: 6.70075091810414402715\n",
      "Iteration 32554 => Loss: 6.70075057138707474991\n",
      "Iteration 32555 => Loss: 6.70075022471672809843\n",
      "Iteration 32556 => Loss: 6.70074987809309607911\n",
      "Iteration 32557 => Loss: 6.70074953151616892200\n",
      "Iteration 32558 => Loss: 6.70074918498593863347\n",
      "Iteration 32559 => Loss: 6.70074883850241054262\n",
      "Iteration 32560 => Loss: 6.70074849206557310310\n",
      "Iteration 32561 => Loss: 6.70074814567541388044\n",
      "Iteration 32562 => Loss: 6.70074779933193376280\n",
      "Iteration 32563 => Loss: 6.70074745303511942751\n",
      "Iteration 32564 => Loss: 6.70074710678496821004\n",
      "Iteration 32565 => Loss: 6.70074676058147566948\n",
      "Iteration 32566 => Loss: 6.70074641442462937135\n",
      "Iteration 32567 => Loss: 6.70074606831443642108\n",
      "Iteration 32568 => Loss: 6.70074572225087461419\n",
      "Iteration 32569 => Loss: 6.70074537623395460884\n",
      "Iteration 32570 => Loss: 6.70074503026365508873\n",
      "Iteration 32571 => Loss: 6.70074468433996806027\n",
      "Iteration 32572 => Loss: 6.70074433846290329342\n",
      "Iteration 32573 => Loss: 6.70074399263243858371\n",
      "Iteration 32574 => Loss: 6.70074364684858458929\n",
      "Iteration 32575 => Loss: 6.70074330111131644117\n",
      "Iteration 32576 => Loss: 6.70074295542064124476\n",
      "Iteration 32577 => Loss: 6.70074260977654567739\n",
      "Iteration 32578 => Loss: 6.70074226417902796271\n",
      "Iteration 32579 => Loss: 6.70074191862807300168\n",
      "Iteration 32580 => Loss: 6.70074157312368789974\n",
      "Iteration 32581 => Loss: 6.70074122766585489330\n",
      "Iteration 32582 => Loss: 6.70074088225457220602\n",
      "Iteration 32583 => Loss: 6.70074053688983628518\n",
      "Iteration 32584 => Loss: 6.70074019157164091354\n",
      "Iteration 32585 => Loss: 6.70073984629997276841\n",
      "Iteration 32586 => Loss: 6.70073950107483451433\n",
      "Iteration 32587 => Loss: 6.70073915589620572320\n",
      "Iteration 32588 => Loss: 6.70073881076409882951\n",
      "Iteration 32589 => Loss: 6.70073846567849606970\n",
      "Iteration 32590 => Loss: 6.70073812063939833195\n",
      "Iteration 32591 => Loss: 6.70073777564678518814\n",
      "Iteration 32592 => Loss: 6.70073743070066729643\n",
      "Iteration 32593 => Loss: 6.70073708580102689325\n",
      "Iteration 32594 => Loss: 6.70073674094786042588\n",
      "Iteration 32595 => Loss: 6.70073639614116522978\n",
      "Iteration 32596 => Loss: 6.70073605138093153499\n",
      "Iteration 32597 => Loss: 6.70073570666715934152\n",
      "Iteration 32598 => Loss: 6.70073536199983532669\n",
      "Iteration 32599 => Loss: 6.70073501737895060870\n",
      "Iteration 32600 => Loss: 6.70073467280450962846\n",
      "Iteration 32601 => Loss: 6.70073432827649373422\n",
      "Iteration 32602 => Loss: 6.70073398379490825505\n",
      "Iteration 32603 => Loss: 6.70073363935973898009\n",
      "Iteration 32604 => Loss: 6.70073329497097969210\n",
      "Iteration 32605 => Loss: 6.70073295062863572014\n",
      "Iteration 32606 => Loss: 6.70073260633268130704\n",
      "Iteration 32607 => Loss: 6.70073226208313066365\n",
      "Iteration 32608 => Loss: 6.70073191787996425006\n",
      "Iteration 32609 => Loss: 6.70073157372317496083\n",
      "Iteration 32610 => Loss: 6.70073122961276190779\n",
      "Iteration 32611 => Loss: 6.70073088554872331457\n",
      "Iteration 32612 => Loss: 6.70073054153104230579\n",
      "Iteration 32613 => Loss: 6.70073019755972154599\n",
      "Iteration 32614 => Loss: 6.70072985363474948883\n",
      "Iteration 32615 => Loss: 6.70072950975611902891\n",
      "Iteration 32616 => Loss: 6.70072916592382927803\n",
      "Iteration 32617 => Loss: 6.70072882213786158445\n",
      "Iteration 32618 => Loss: 6.70072847839823193539\n",
      "Iteration 32619 => Loss: 6.70072813470491457366\n",
      "Iteration 32620 => Loss: 6.70072779105791038745\n",
      "Iteration 32621 => Loss: 6.70072744745721671222\n",
      "Iteration 32622 => Loss: 6.70072710390281756077\n",
      "Iteration 32623 => Loss: 6.70072676039471826215\n",
      "Iteration 32624 => Loss: 6.70072641693290371734\n",
      "Iteration 32625 => Loss: 6.70072607351737303816\n",
      "Iteration 32626 => Loss: 6.70072573014811201375\n",
      "Iteration 32627 => Loss: 6.70072538682512419683\n",
      "Iteration 32628 => Loss: 6.70072504354839626473\n",
      "Iteration 32629 => Loss: 6.70072470031792555289\n",
      "Iteration 32630 => Loss: 6.70072435713371206134\n",
      "Iteration 32631 => Loss: 6.70072401399573358560\n",
      "Iteration 32632 => Loss: 6.70072367090399723111\n",
      "Iteration 32633 => Loss: 6.70072332785848701064\n",
      "Iteration 32634 => Loss: 6.70072298485920914146\n",
      "Iteration 32635 => Loss: 6.70072264190614941270\n",
      "Iteration 32636 => Loss: 6.70072229899930338348\n",
      "Iteration 32637 => Loss: 6.70072195613866306019\n",
      "Iteration 32638 => Loss: 6.70072161332421778468\n",
      "Iteration 32639 => Loss: 6.70072127055597199785\n",
      "Iteration 32640 => Loss: 6.70072092783392125881\n",
      "Iteration 32641 => Loss: 6.70072058515803625767\n",
      "Iteration 32642 => Loss: 6.70072024252833742253\n",
      "Iteration 32643 => Loss: 6.70071989994480521347\n",
      "Iteration 32644 => Loss: 6.70071955740743963048\n",
      "Iteration 32645 => Loss: 6.70071921491622557454\n",
      "Iteration 32646 => Loss: 6.70071887247116571018\n",
      "Iteration 32647 => Loss: 6.70071853007224582655\n",
      "Iteration 32648 => Loss: 6.70071818771946592364\n",
      "Iteration 32649 => Loss: 6.70071784541282067238\n",
      "Iteration 32650 => Loss: 6.70071750315230563189\n",
      "Iteration 32651 => Loss: 6.70071716093790037405\n",
      "Iteration 32652 => Loss: 6.70071681876961289248\n",
      "Iteration 32653 => Loss: 6.70071647664743252903\n",
      "Iteration 32654 => Loss: 6.70071613457135661918\n",
      "Iteration 32655 => Loss: 6.70071579254137628112\n",
      "Iteration 32656 => Loss: 6.70071545055747996855\n",
      "Iteration 32657 => Loss: 6.70071510861966412875\n",
      "Iteration 32658 => Loss: 6.70071476672792698537\n",
      "Iteration 32659 => Loss: 6.70071442488226143297\n",
      "Iteration 32660 => Loss: 6.70071408308265681342\n",
      "Iteration 32661 => Loss: 6.70071374132910957400\n",
      "Iteration 32662 => Loss: 6.70071339962161527382\n",
      "Iteration 32663 => Loss: 6.70071305796016591927\n",
      "Iteration 32664 => Loss: 6.70071271634475795764\n",
      "Iteration 32665 => Loss: 6.70071237477537628990\n",
      "Iteration 32666 => Loss: 6.70071203325202890966\n",
      "Iteration 32667 => Loss: 6.70071169177469716516\n",
      "Iteration 32668 => Loss: 6.70071135034337928005\n",
      "Iteration 32669 => Loss: 6.70071100895806814890\n",
      "Iteration 32670 => Loss: 6.70071066761876554807\n",
      "Iteration 32671 => Loss: 6.70071032632545016128\n",
      "Iteration 32672 => Loss: 6.70070998507812554124\n",
      "Iteration 32673 => Loss: 6.70070964387678991159\n",
      "Iteration 32674 => Loss: 6.70070930272142373241\n",
      "Iteration 32675 => Loss: 6.70070896161203322094\n",
      "Iteration 32676 => Loss: 6.70070862054860594270\n",
      "Iteration 32677 => Loss: 6.70070827953114012132\n",
      "Iteration 32678 => Loss: 6.70070793855961976959\n",
      "Iteration 32679 => Loss: 6.70070759763404844023\n",
      "Iteration 32680 => Loss: 6.70070725675441813962\n",
      "Iteration 32681 => Loss: 6.70070691592072265053\n",
      "Iteration 32682 => Loss: 6.70070657513295397933\n",
      "Iteration 32683 => Loss: 6.70070623439110324426\n",
      "Iteration 32684 => Loss: 6.70070589369517133349\n",
      "Iteration 32685 => Loss: 6.70070555304514048345\n",
      "Iteration 32686 => Loss: 6.70070521244102312863\n",
      "Iteration 32687 => Loss: 6.70070487188279351187\n",
      "Iteration 32688 => Loss: 6.70070453137046140313\n",
      "Iteration 32689 => Loss: 6.70070419090400726247\n",
      "Iteration 32690 => Loss: 6.70070385048343197809\n",
      "Iteration 32691 => Loss: 6.70070351010872844455\n",
      "Iteration 32692 => Loss: 6.70070316977989133278\n",
      "Iteration 32693 => Loss: 6.70070282949691620189\n",
      "Iteration 32694 => Loss: 6.70070248925978795285\n",
      "Iteration 32695 => Loss: 6.70070214906851457926\n",
      "Iteration 32696 => Loss: 6.70070180892308009390\n",
      "Iteration 32697 => Loss: 6.70070146882347650319\n",
      "Iteration 32698 => Loss: 6.70070112876969847804\n",
      "Iteration 32699 => Loss: 6.70070078876175223570\n",
      "Iteration 32700 => Loss: 6.70070044879961645989\n",
      "Iteration 32701 => Loss: 6.70070010888329470333\n",
      "Iteration 32702 => Loss: 6.70069976901276653791\n",
      "Iteration 32703 => Loss: 6.70069942918804528631\n",
      "Iteration 32704 => Loss: 6.70069908940911318496\n",
      "Iteration 32705 => Loss: 6.70069874967596845750\n",
      "Iteration 32706 => Loss: 6.70069840998859778125\n",
      "Iteration 32707 => Loss: 6.70069807034700737347\n",
      "Iteration 32708 => Loss: 6.70069773075117147698\n",
      "Iteration 32709 => Loss: 6.70069739120110163810\n",
      "Iteration 32710 => Loss: 6.70069705169679163959\n",
      "Iteration 32711 => Loss: 6.70069671223822727057\n",
      "Iteration 32712 => Loss: 6.70069637282540409018\n",
      "Iteration 32713 => Loss: 6.70069603345831410479\n",
      "Iteration 32714 => Loss: 6.70069569413695820259\n",
      "Iteration 32715 => Loss: 6.70069535486132572544\n",
      "Iteration 32716 => Loss: 6.70069501563141223244\n",
      "Iteration 32717 => Loss: 6.70069467644719996002\n",
      "Iteration 32718 => Loss: 6.70069433730870311905\n",
      "Iteration 32719 => Loss: 6.70069399821590039323\n",
      "Iteration 32720 => Loss: 6.70069365916879355893\n",
      "Iteration 32721 => Loss: 6.70069332016737195801\n",
      "Iteration 32722 => Loss: 6.70069298121163203774\n",
      "Iteration 32723 => Loss: 6.70069264230156402817\n",
      "Iteration 32724 => Loss: 6.70069230343716526477\n",
      "Iteration 32725 => Loss: 6.70069196461842686574\n",
      "Iteration 32726 => Loss: 6.70069162584534350202\n",
      "Iteration 32727 => Loss: 6.70069128711791162090\n",
      "Iteration 32728 => Loss: 6.70069094843612500512\n",
      "Iteration 32729 => Loss: 6.70069060979997654925\n",
      "Iteration 32730 => Loss: 6.70069027120945648335\n",
      "Iteration 32731 => Loss: 6.70068993266456214286\n",
      "Iteration 32732 => Loss: 6.70068959416528375783\n",
      "Iteration 32733 => Loss: 6.70068925571162576915\n",
      "Iteration 32734 => Loss: 6.70068891730356863690\n",
      "Iteration 32735 => Loss: 6.70068857894111502560\n",
      "Iteration 32736 => Loss: 6.70068824062425782984\n",
      "Iteration 32737 => Loss: 6.70068790235298106239\n",
      "Iteration 32738 => Loss: 6.70068756412729449323\n",
      "Iteration 32739 => Loss: 6.70068722594717414154\n",
      "Iteration 32740 => Loss: 6.70068688781262711274\n",
      "Iteration 32741 => Loss: 6.70068654972364541322\n",
      "Iteration 32742 => Loss: 6.70068621168022549028\n",
      "Iteration 32743 => Loss: 6.70068587368235135671\n",
      "Iteration 32744 => Loss: 6.70068553573002745338\n",
      "Iteration 32745 => Loss: 6.70068519782323424039\n",
      "Iteration 32746 => Loss: 6.70068485996198415222\n",
      "Iteration 32747 => Loss: 6.70068452214625942531\n",
      "Iteration 32748 => Loss: 6.70068418437604584881\n",
      "Iteration 32749 => Loss: 6.70068384665135496903\n",
      "Iteration 32750 => Loss: 6.70068350897217435147\n",
      "Iteration 32751 => Loss: 6.70068317133849156164\n",
      "Iteration 32752 => Loss: 6.70068283375030748772\n",
      "Iteration 32753 => Loss: 6.70068249620760614249\n",
      "Iteration 32754 => Loss: 6.70068215871039551956\n",
      "Iteration 32755 => Loss: 6.70068182125865963172\n",
      "Iteration 32756 => Loss: 6.70068148385239581444\n",
      "Iteration 32757 => Loss: 6.70068114649159962681\n",
      "Iteration 32758 => Loss: 6.70068080917626218707\n",
      "Iteration 32759 => Loss: 6.70068047190637905430\n",
      "Iteration 32760 => Loss: 6.70068013468194045856\n",
      "Iteration 32761 => Loss: 6.70067979750294373531\n",
      "Iteration 32762 => Loss: 6.70067946036938622001\n",
      "Iteration 32763 => Loss: 6.70067912328125192545\n",
      "Iteration 32764 => Loss: 6.70067878623854351616\n",
      "Iteration 32765 => Loss: 6.70067844924125211037\n",
      "Iteration 32766 => Loss: 6.70067811228936527357\n",
      "Iteration 32767 => Loss: 6.70067777538288655848\n",
      "Iteration 32768 => Loss: 6.70067743852180974784\n",
      "Iteration 32769 => Loss: 6.70067710170611796627\n",
      "Iteration 32770 => Loss: 6.70067676493581299013\n",
      "Iteration 32771 => Loss: 6.70067642821089304306\n",
      "Iteration 32772 => Loss: 6.70067609153134124966\n",
      "Iteration 32773 => Loss: 6.70067575489716382720\n",
      "Iteration 32774 => Loss: 6.70067541830834390026\n",
      "Iteration 32775 => Loss: 6.70067508176487613980\n",
      "Iteration 32776 => Loss: 6.70067474526676498670\n",
      "Iteration 32777 => Loss: 6.70067440881399356556\n",
      "Iteration 32778 => Loss: 6.70067407240655921186\n",
      "Iteration 32779 => Loss: 6.70067373604445215562\n",
      "Iteration 32780 => Loss: 6.70067339972767150869\n",
      "Iteration 32781 => Loss: 6.70067306345621638286\n",
      "Iteration 32782 => Loss: 6.70067272723006457369\n",
      "Iteration 32783 => Loss: 6.70067239104922052206\n",
      "Iteration 32784 => Loss: 6.70067205491368245163\n",
      "Iteration 32785 => Loss: 6.70067171882342726974\n",
      "Iteration 32786 => Loss: 6.70067138277847629269\n",
      "Iteration 32787 => Loss: 6.70067104677879843422\n",
      "Iteration 32788 => Loss: 6.70067071082439813523\n",
      "Iteration 32789 => Loss: 6.70067037491526651394\n",
      "Iteration 32790 => Loss: 6.70067003905139912945\n",
      "Iteration 32791 => Loss: 6.70066970323279331723\n",
      "Iteration 32792 => Loss: 6.70066936745943397824\n",
      "Iteration 32793 => Loss: 6.70066903173132200067\n",
      "Iteration 32794 => Loss: 6.70066869604845294361\n",
      "Iteration 32795 => Loss: 6.70066836041081259623\n",
      "Iteration 32796 => Loss: 6.70066802481839740580\n",
      "Iteration 32797 => Loss: 6.70066768927120559596\n",
      "Iteration 32798 => Loss: 6.70066735376922562040\n",
      "Iteration 32799 => Loss: 6.70066701831246014365\n",
      "Iteration 32800 => Loss: 6.70066668290089761939\n",
      "Iteration 32801 => Loss: 6.70066634753453271856\n",
      "Iteration 32802 => Loss: 6.70066601221335478300\n",
      "Iteration 32803 => Loss: 6.70066567693735937183\n",
      "Iteration 32804 => Loss: 6.70066534170654826141\n",
      "Iteration 32805 => Loss: 6.70066500652090990542\n",
      "Iteration 32806 => Loss: 6.70066467138042565210\n",
      "Iteration 32807 => Loss: 6.70066433628511504139\n",
      "Iteration 32808 => Loss: 6.70066400123495320429\n",
      "Iteration 32809 => Loss: 6.70066366622993836444\n",
      "Iteration 32810 => Loss: 6.70066333127007318637\n",
      "Iteration 32811 => Loss: 6.70066299635533280110\n",
      "Iteration 32812 => Loss: 6.70066266148573230765\n",
      "Iteration 32813 => Loss: 6.70066232666125127793\n",
      "Iteration 32814 => Loss: 6.70066199188188438285\n",
      "Iteration 32815 => Loss: 6.70066165714763339878\n",
      "Iteration 32816 => Loss: 6.70066132245848500304\n",
      "Iteration 32817 => Loss: 6.70066098781443653110\n",
      "Iteration 32818 => Loss: 6.70066065321548798295\n",
      "Iteration 32819 => Loss: 6.70066031866161271324\n",
      "Iteration 32820 => Loss: 6.70065998415282759737\n",
      "Iteration 32821 => Loss: 6.70065964968911931265\n",
      "Iteration 32822 => Loss: 6.70065931527047986549\n",
      "Iteration 32823 => Loss: 6.70065898089689504502\n",
      "Iteration 32824 => Loss: 6.70065864656837373303\n",
      "Iteration 32825 => Loss: 6.70065831228490171867\n",
      "Iteration 32826 => Loss: 6.70065797804647722558\n",
      "Iteration 32827 => Loss: 6.70065764385308959561\n",
      "Iteration 32828 => Loss: 6.70065730970473527606\n",
      "Iteration 32829 => Loss: 6.70065697560140538513\n",
      "Iteration 32830 => Loss: 6.70065664154309637013\n",
      "Iteration 32831 => Loss: 6.70065630752980112561\n",
      "Iteration 32832 => Loss: 6.70065597356151876340\n",
      "Iteration 32833 => Loss: 6.70065563963823684901\n",
      "Iteration 32834 => Loss: 6.70065530575994916518\n",
      "Iteration 32835 => Loss: 6.70065497192665304738\n",
      "Iteration 32836 => Loss: 6.70065463813833517293\n",
      "Iteration 32837 => Loss: 6.70065430439500087090\n",
      "Iteration 32838 => Loss: 6.70065397069664037133\n",
      "Iteration 32839 => Loss: 6.70065363704324301608\n",
      "Iteration 32840 => Loss: 6.70065330343480702879\n",
      "Iteration 32841 => Loss: 6.70065296987132352768\n",
      "Iteration 32842 => Loss: 6.70065263635278629550\n",
      "Iteration 32843 => Loss: 6.70065230287919000318\n",
      "Iteration 32844 => Loss: 6.70065196945053642708\n",
      "Iteration 32845 => Loss: 6.70065163606680513908\n",
      "Iteration 32846 => Loss: 6.70065130272799702738\n",
      "Iteration 32847 => Loss: 6.70065096943411298014\n",
      "Iteration 32848 => Loss: 6.70065063618513345745\n",
      "Iteration 32849 => Loss: 6.70065030298105845930\n",
      "Iteration 32850 => Loss: 6.70064996982189420294\n",
      "Iteration 32851 => Loss: 6.70064963670761049030\n",
      "Iteration 32852 => Loss: 6.70064930363822330861\n",
      "Iteration 32853 => Loss: 6.70064897061371045339\n",
      "Iteration 32854 => Loss: 6.70064863763407636554\n",
      "Iteration 32855 => Loss: 6.70064830469930416967\n",
      "Iteration 32856 => Loss: 6.70064797180940718846\n",
      "Iteration 32857 => Loss: 6.70064763896435522383\n",
      "Iteration 32858 => Loss: 6.70064730616416071030\n",
      "Iteration 32859 => Loss: 6.70064697340880766063\n",
      "Iteration 32860 => Loss: 6.70064664069829607485\n",
      "Iteration 32861 => Loss: 6.70064630803261618297\n",
      "Iteration 32862 => Loss: 6.70064597541176620865\n",
      "Iteration 32863 => Loss: 6.70064564283573282921\n",
      "Iteration 32864 => Loss: 6.70064531030452048554\n",
      "Iteration 32865 => Loss: 6.70064497781810519683\n",
      "Iteration 32866 => Loss: 6.70064464537650028575\n",
      "Iteration 32867 => Loss: 6.70064431297968887691\n",
      "Iteration 32868 => Loss: 6.70064398062767274666\n",
      "Iteration 32869 => Loss: 6.70064364832043679598\n",
      "Iteration 32870 => Loss: 6.70064331605798013669\n",
      "Iteration 32871 => Loss: 6.70064298384029921607\n",
      "Iteration 32872 => Loss: 6.70064265166737804691\n",
      "Iteration 32873 => Loss: 6.70064231953922284646\n",
      "Iteration 32874 => Loss: 6.70064198745582384475\n",
      "Iteration 32875 => Loss: 6.70064165541716771912\n",
      "Iteration 32876 => Loss: 6.70064132342325358138\n",
      "Iteration 32877 => Loss: 6.70064099147407699064\n",
      "Iteration 32878 => Loss: 6.70064065956962728876\n",
      "Iteration 32879 => Loss: 6.70064032770991158117\n",
      "Iteration 32880 => Loss: 6.70063999589490588704\n",
      "Iteration 32881 => Loss: 6.70063966412461109456\n",
      "Iteration 32882 => Loss: 6.70063933239902898009\n",
      "Iteration 32883 => Loss: 6.70063900071814266823\n",
      "Iteration 32884 => Loss: 6.70063866908195127081\n",
      "Iteration 32885 => Loss: 6.70063833749044679422\n",
      "Iteration 32886 => Loss: 6.70063800594363012664\n",
      "Iteration 32887 => Loss: 6.70063767444148172814\n",
      "Iteration 32888 => Loss: 6.70063734298401048051\n",
      "Iteration 32889 => Loss: 6.70063701157119151475\n",
      "Iteration 32890 => Loss: 6.70063668020304348261\n",
      "Iteration 32891 => Loss: 6.70063634887954240327\n",
      "Iteration 32892 => Loss: 6.70063601760068205948\n",
      "Iteration 32893 => Loss: 6.70063568636646778032\n",
      "Iteration 32894 => Loss: 6.70063535517688357857\n",
      "Iteration 32895 => Loss: 6.70063502403192945422\n",
      "Iteration 32896 => Loss: 6.70063469293159474915\n",
      "Iteration 32897 => Loss: 6.70063436187587946335\n",
      "Iteration 32898 => Loss: 6.70063403086477293868\n",
      "Iteration 32899 => Loss: 6.70063369989827073425\n",
      "Iteration 32900 => Loss: 6.70063336897636574463\n",
      "Iteration 32901 => Loss: 6.70063303809905264075\n",
      "Iteration 32902 => Loss: 6.70063270726632609353\n",
      "Iteration 32903 => Loss: 6.70063237647817278031\n",
      "Iteration 32904 => Loss: 6.70063204573460335922\n",
      "Iteration 32905 => Loss: 6.70063171503559651399\n",
      "Iteration 32906 => Loss: 6.70063138438115135642\n",
      "Iteration 32907 => Loss: 6.70063105377125811657\n",
      "Iteration 32908 => Loss: 6.70063072320591857078\n",
      "Iteration 32909 => Loss: 6.70063039268512650182\n",
      "Iteration 32910 => Loss: 6.70063006220886769881\n",
      "Iteration 32911 => Loss: 6.70062973177713772088\n",
      "Iteration 32912 => Loss: 6.70062940138994278527\n",
      "Iteration 32913 => Loss: 6.70062907104725535845\n",
      "Iteration 32914 => Loss: 6.70062874074909142763\n",
      "Iteration 32915 => Loss: 6.70062841049543234107\n",
      "Iteration 32916 => Loss: 6.70062808028627365786\n",
      "Iteration 32917 => Loss: 6.70062775012160827259\n",
      "Iteration 32918 => Loss: 6.70062742000143618526\n",
      "Iteration 32919 => Loss: 6.70062708992575117861\n",
      "Iteration 32920 => Loss: 6.70062675989453548908\n",
      "Iteration 32921 => Loss: 6.70062642990779888663\n",
      "Iteration 32922 => Loss: 6.70062609996552094316\n",
      "Iteration 32923 => Loss: 6.70062577006770876409\n",
      "Iteration 32924 => Loss: 6.70062544021435080310\n",
      "Iteration 32925 => Loss: 6.70062511040543551388\n",
      "Iteration 32926 => Loss: 6.70062478064096644914\n",
      "Iteration 32927 => Loss: 6.70062445092093295074\n",
      "Iteration 32928 => Loss: 6.70062412124532880142\n",
      "Iteration 32929 => Loss: 6.70062379161414245488\n",
      "Iteration 32930 => Loss: 6.70062346202738190470\n",
      "Iteration 32931 => Loss: 6.70062313248502849916\n",
      "Iteration 32932 => Loss: 6.70062280298708490278\n",
      "Iteration 32933 => Loss: 6.70062247353354312196\n",
      "Iteration 32934 => Loss: 6.70062214412438894584\n",
      "Iteration 32935 => Loss: 6.70062181475961970989\n",
      "Iteration 32936 => Loss: 6.70062148543924251953\n",
      "Iteration 32937 => Loss: 6.70062115616323250578\n",
      "Iteration 32938 => Loss: 6.70062082693159588587\n",
      "Iteration 32939 => Loss: 6.70062049774432466620\n",
      "Iteration 32940 => Loss: 6.70062016860141440588\n",
      "Iteration 32941 => Loss: 6.70061983950285000589\n",
      "Iteration 32942 => Loss: 6.70061951044863324256\n",
      "Iteration 32943 => Loss: 6.70061918143875434595\n",
      "Iteration 32944 => Loss: 6.70061885247320976333\n",
      "Iteration 32945 => Loss: 6.70061852355200393561\n",
      "Iteration 32946 => Loss: 6.70061819467511199377\n",
      "Iteration 32947 => Loss: 6.70061786584253660237\n",
      "Iteration 32948 => Loss: 6.70061753705426887961\n",
      "Iteration 32949 => Loss: 6.70061720831030793732\n",
      "Iteration 32950 => Loss: 6.70061687961064489372\n",
      "Iteration 32951 => Loss: 6.70061655095527264336\n",
      "Iteration 32952 => Loss: 6.70061622234418940991\n",
      "Iteration 32953 => Loss: 6.70061589377738631157\n",
      "Iteration 32954 => Loss: 6.70061556525485890745\n",
      "Iteration 32955 => Loss: 6.70061523677659653941\n",
      "Iteration 32956 => Loss: 6.70061490834259565474\n",
      "Iteration 32957 => Loss: 6.70061457995285536526\n",
      "Iteration 32958 => Loss: 6.70061425160736678919\n",
      "Iteration 32959 => Loss: 6.70061392330612193291\n",
      "Iteration 32960 => Loss: 6.70061359504910747376\n",
      "Iteration 32961 => Loss: 6.70061326683633584622\n",
      "Iteration 32962 => Loss: 6.70061293866778662220\n",
      "Iteration 32963 => Loss: 6.70061261054345713717\n",
      "Iteration 32964 => Loss: 6.70061228246334561476\n",
      "Iteration 32965 => Loss: 6.70061195442744228501\n",
      "Iteration 32966 => Loss: 6.70061162643573915432\n",
      "Iteration 32967 => Loss: 6.70061129848823622268\n",
      "Iteration 32968 => Loss: 6.70061097058492194378\n",
      "Iteration 32969 => Loss: 6.70061064272579276491\n",
      "Iteration 32970 => Loss: 6.70061031491084424516\n",
      "Iteration 32971 => Loss: 6.70060998714006750276\n",
      "Iteration 32972 => Loss: 6.70060965941345720864\n",
      "Iteration 32973 => Loss: 6.70060933173100359284\n",
      "Iteration 32974 => Loss: 6.70060900409271553713\n",
      "Iteration 32975 => Loss: 6.70060867649857083705\n",
      "Iteration 32976 => Loss: 6.70060834894856593991\n",
      "Iteration 32977 => Loss: 6.70060802144270617475\n",
      "Iteration 32978 => Loss: 6.70060769398097644256\n",
      "Iteration 32979 => Loss: 6.70060736656337319062\n",
      "Iteration 32980 => Loss: 6.70060703918988043171\n",
      "Iteration 32981 => Loss: 6.70060671186051060033\n",
      "Iteration 32982 => Loss: 6.70060638457523971567\n",
      "Iteration 32983 => Loss: 6.70060605733408287676\n",
      "Iteration 32984 => Loss: 6.70060573013701432643\n",
      "Iteration 32985 => Loss: 6.70060540298403672921\n",
      "Iteration 32986 => Loss: 6.70060507587514031513\n",
      "Iteration 32987 => Loss: 6.70060474881032508421\n",
      "Iteration 32988 => Loss: 6.70060442178958215464\n",
      "Iteration 32989 => Loss: 6.70060409481290353284\n",
      "Iteration 32990 => Loss: 6.70060376788028122519\n",
      "Iteration 32991 => Loss: 6.70060344099171878440\n",
      "Iteration 32992 => Loss: 6.70060311414720199963\n",
      "Iteration 32993 => Loss: 6.70060278734673264722\n",
      "Iteration 32994 => Loss: 6.70060246059029740451\n",
      "Iteration 32995 => Loss: 6.70060213387789005424\n",
      "Iteration 32996 => Loss: 6.70060180720950437916\n",
      "Iteration 32997 => Loss: 6.70060148058514659652\n",
      "Iteration 32998 => Loss: 6.70060115400479183734\n",
      "Iteration 32999 => Loss: 6.70060082746844543067\n",
      "Iteration 33000 => Loss: 6.70060050097610471198\n",
      "Iteration 33001 => Loss: 6.70060017452775547042\n",
      "Iteration 33002 => Loss: 6.70059984812339948235\n",
      "Iteration 33003 => Loss: 6.70059952176302342508\n",
      "Iteration 33004 => Loss: 6.70059919544662196955\n",
      "Iteration 33005 => Loss: 6.70059886917419689212\n",
      "Iteration 33006 => Loss: 6.70059854294573042921\n",
      "Iteration 33007 => Loss: 6.70059821676123146261\n",
      "Iteration 33008 => Loss: 6.70059789062067689969\n",
      "Iteration 33009 => Loss: 6.70059756452407562222\n",
      "Iteration 33010 => Loss: 6.70059723847141608388\n",
      "Iteration 33011 => Loss: 6.70059691246268940290\n",
      "Iteration 33012 => Loss: 6.70059658649789202656\n",
      "Iteration 33013 => Loss: 6.70059626057702306667\n",
      "Iteration 33014 => Loss: 6.70059593470006653604\n",
      "Iteration 33015 => Loss: 6.70059560886702954008\n",
      "Iteration 33016 => Loss: 6.70059528307788898616\n",
      "Iteration 33017 => Loss: 6.70059495733265730877\n",
      "Iteration 33018 => Loss: 6.70059463163131407981\n",
      "Iteration 33019 => Loss: 6.70059430597386107564\n",
      "Iteration 33020 => Loss: 6.70059398036028763812\n",
      "Iteration 33021 => Loss: 6.70059365479059376725\n",
      "Iteration 33022 => Loss: 6.70059332926477058123\n",
      "Iteration 33023 => Loss: 6.70059300378281097466\n",
      "Iteration 33024 => Loss: 6.70059267834471050662\n",
      "Iteration 33025 => Loss: 6.70059235295045940717\n",
      "Iteration 33026 => Loss: 6.70059202760006389354\n",
      "Iteration 33027 => Loss: 6.70059170229350264947\n",
      "Iteration 33028 => Loss: 6.70059137703077567494\n",
      "Iteration 33029 => Loss: 6.70059105181188119360\n",
      "Iteration 33030 => Loss: 6.70059072663680854731\n",
      "Iteration 33031 => Loss: 6.70059040150555595972\n",
      "Iteration 33032 => Loss: 6.70059007641811010814\n",
      "Iteration 33033 => Loss: 6.70058975137446743986\n",
      "Iteration 33034 => Loss: 6.70058942637463506031\n",
      "Iteration 33035 => Loss: 6.70058910141858987686\n",
      "Iteration 33036 => Loss: 6.70058877650633277767\n",
      "Iteration 33037 => Loss: 6.70058845163785399279\n",
      "Iteration 33038 => Loss: 6.70058812681315885129\n",
      "Iteration 33039 => Loss: 6.70058780203222603689\n",
      "Iteration 33040 => Loss: 6.70058747729506087865\n",
      "Iteration 33041 => Loss: 6.70058715260165804750\n",
      "Iteration 33042 => Loss: 6.70058682795200510895\n",
      "Iteration 33043 => Loss: 6.70058650334609584576\n",
      "Iteration 33044 => Loss: 6.70058617878393203426\n",
      "Iteration 33045 => Loss: 6.70058585426549502273\n",
      "Iteration 33046 => Loss: 6.70058552979079902201\n",
      "Iteration 33047 => Loss: 6.70058520535981561039\n",
      "Iteration 33048 => Loss: 6.70058488097255722238\n",
      "Iteration 33049 => Loss: 6.70058455662900076533\n",
      "Iteration 33050 => Loss: 6.70058423232915689738\n",
      "Iteration 33051 => Loss: 6.70058390807301051950\n",
      "Iteration 33052 => Loss: 6.70058358386055541445\n",
      "Iteration 33053 => Loss: 6.70058325969178714132\n",
      "Iteration 33054 => Loss: 6.70058293556670925284\n",
      "Iteration 33055 => Loss: 6.70058261148529688001\n",
      "Iteration 33056 => Loss: 6.70058228744756068096\n",
      "Iteration 33057 => Loss: 6.70058196345348289213\n",
      "Iteration 33058 => Loss: 6.70058163950307417167\n",
      "Iteration 33059 => Loss: 6.70058131559630965057\n",
      "Iteration 33060 => Loss: 6.70058099173318932884\n",
      "Iteration 33061 => Loss: 6.70058066791371498283\n",
      "Iteration 33062 => Loss: 6.70058034413787773076\n",
      "Iteration 33063 => Loss: 6.70058002040565803270\n",
      "Iteration 33064 => Loss: 6.70057969671707098769\n",
      "Iteration 33065 => Loss: 6.70057937307209616762\n",
      "Iteration 33066 => Loss: 6.70057904947073623703\n",
      "Iteration 33067 => Loss: 6.70057872591298142595\n",
      "Iteration 33068 => Loss: 6.70057840239882462896\n",
      "Iteration 33069 => Loss: 6.70057807892826318152\n",
      "Iteration 33070 => Loss: 6.70057775550128464914\n",
      "Iteration 33071 => Loss: 6.70057743211789436089\n",
      "Iteration 33072 => Loss: 6.70057710877807632954\n",
      "Iteration 33073 => Loss: 6.70057678548182433786\n",
      "Iteration 33074 => Loss: 6.70057646222914637946\n",
      "Iteration 33075 => Loss: 6.70057613902001580897\n",
      "Iteration 33076 => Loss: 6.70057581585444328454\n",
      "Iteration 33077 => Loss: 6.70057549273241814802\n",
      "Iteration 33078 => Loss: 6.70057516965393418218\n",
      "Iteration 33079 => Loss: 6.70057484661898339340\n",
      "Iteration 33080 => Loss: 6.70057452362756045261\n",
      "Iteration 33081 => Loss: 6.70057420067966802435\n",
      "Iteration 33082 => Loss: 6.70057387777528656869\n",
      "Iteration 33083 => Loss: 6.70057355491440898021\n",
      "Iteration 33084 => Loss: 6.70057323209705035794\n",
      "Iteration 33085 => Loss: 6.70057290932318228016\n",
      "Iteration 33086 => Loss: 6.70057258659281274049\n",
      "Iteration 33087 => Loss: 6.70057226390593108079\n",
      "Iteration 33088 => Loss: 6.70057194126252841926\n",
      "Iteration 33089 => Loss: 6.70057161866260653227\n",
      "Iteration 33090 => Loss: 6.70057129610615120896\n",
      "Iteration 33091 => Loss: 6.70057097359316156115\n",
      "Iteration 33092 => Loss: 6.70057065112362781889\n",
      "Iteration 33093 => Loss: 6.70057032869755531124\n",
      "Iteration 33094 => Loss: 6.70057000631492361009\n",
      "Iteration 33095 => Loss: 6.70056968397573271545\n",
      "Iteration 33096 => Loss: 6.70056936167997907461\n",
      "Iteration 33097 => Loss: 6.70056903942765025306\n",
      "Iteration 33098 => Loss: 6.70056871721874980352\n",
      "Iteration 33099 => Loss: 6.70056839505326351514\n",
      "Iteration 33100 => Loss: 6.70056807293119405244\n",
      "Iteration 33101 => Loss: 6.70056775085252986912\n",
      "Iteration 33102 => Loss: 6.70056742881726297156\n",
      "Iteration 33103 => Loss: 6.70056710682538447799\n",
      "Iteration 33104 => Loss: 6.70056678487690771107\n",
      "Iteration 33105 => Loss: 6.70056646297180247274\n",
      "Iteration 33106 => Loss: 6.70056614111007764478\n",
      "Iteration 33107 => Loss: 6.70056581929172345724\n",
      "Iteration 33108 => Loss: 6.70056549751673635740\n",
      "Iteration 33109 => Loss: 6.70056517578510657529\n",
      "Iteration 33110 => Loss: 6.70056485409682700549\n",
      "Iteration 33111 => Loss: 6.70056453245190297707\n",
      "Iteration 33112 => Loss: 6.70056421085031583829\n",
      "Iteration 33113 => Loss: 6.70056388929206026006\n",
      "Iteration 33114 => Loss: 6.70056356777714601236\n",
      "Iteration 33115 => Loss: 6.70056324630554556165\n",
      "Iteration 33116 => Loss: 6.70056292487726956608\n",
      "Iteration 33117 => Loss: 6.70056260349230381479\n",
      "Iteration 33118 => Loss: 6.70056228215064297871\n",
      "Iteration 33119 => Loss: 6.70056196085228261694\n",
      "Iteration 33120 => Loss: 6.70056163959721917678\n",
      "Iteration 33121 => Loss: 6.70056131838544377644\n",
      "Iteration 33122 => Loss: 6.70056099721695286320\n",
      "Iteration 33123 => Loss: 6.70056067609173933164\n",
      "Iteration 33124 => Loss: 6.70056035500979962904\n",
      "Iteration 33125 => Loss: 6.70056003397112487363\n",
      "Iteration 33126 => Loss: 6.70055971297570973633\n",
      "Iteration 33127 => Loss: 6.70055939202354888806\n",
      "Iteration 33128 => Loss: 6.70055907111463522341\n",
      "Iteration 33129 => Loss: 6.70055875024896074876\n",
      "Iteration 33130 => Loss: 6.70055842942652990502\n",
      "Iteration 33131 => Loss: 6.70055810864732315224\n",
      "Iteration 33132 => Loss: 6.70055778791134670769\n",
      "Iteration 33133 => Loss: 6.70055746721858813686\n",
      "Iteration 33134 => Loss: 6.70055714656904033433\n",
      "Iteration 33135 => Loss: 6.70055682596270152374\n",
      "Iteration 33136 => Loss: 6.70055650539956193512\n",
      "Iteration 33137 => Loss: 6.70055618487962778573\n",
      "Iteration 33138 => Loss: 6.70055586440287509475\n",
      "Iteration 33139 => Loss: 6.70055554396930741490\n",
      "Iteration 33140 => Loss: 6.70055522357891764074\n",
      "Iteration 33141 => Loss: 6.70055490323170488409\n",
      "Iteration 33142 => Loss: 6.70055458292765404593\n",
      "Iteration 33143 => Loss: 6.70055426266676867897\n",
      "Iteration 33144 => Loss: 6.70055394244903457235\n",
      "Iteration 33145 => Loss: 6.70055362227445083789\n",
      "Iteration 33146 => Loss: 6.70055330214301037017\n",
      "Iteration 33147 => Loss: 6.70055298205471139283\n",
      "Iteration 33148 => Loss: 6.70055266200954324773\n",
      "Iteration 33149 => Loss: 6.70055234200749350038\n",
      "Iteration 33150 => Loss: 6.70055202204857280890\n",
      "Iteration 33151 => Loss: 6.70055170213276785063\n",
      "Iteration 33152 => Loss: 6.70055138226006352653\n",
      "Iteration 33153 => Loss: 6.70055106243046960657\n",
      "Iteration 33154 => Loss: 6.70055074264396921535\n",
      "Iteration 33155 => Loss: 6.70055042290055791199\n",
      "Iteration 33156 => Loss: 6.70055010320023658466\n",
      "Iteration 33157 => Loss: 6.70054978354299013432\n",
      "Iteration 33158 => Loss: 6.70054946392882389006\n",
      "Iteration 33159 => Loss: 6.70054914435771742376\n",
      "Iteration 33160 => Loss: 6.70054882482967517632\n",
      "Iteration 33161 => Loss: 6.70054850534468826595\n",
      "Iteration 33162 => Loss: 6.70054818590276290990\n",
      "Iteration 33163 => Loss: 6.70054786650387779190\n",
      "Iteration 33164 => Loss: 6.70054754714802403015\n",
      "Iteration 33165 => Loss: 6.70054722783520695373\n",
      "Iteration 33166 => Loss: 6.70054690856542567445\n",
      "Iteration 33167 => Loss: 6.70054658933866065240\n",
      "Iteration 33168 => Loss: 6.70054627015490300579\n",
      "Iteration 33169 => Loss: 6.70054595101416339276\n",
      "Iteration 33170 => Loss: 6.70054563191642671427\n",
      "Iteration 33171 => Loss: 6.70054531286169030579\n",
      "Iteration 33172 => Loss: 6.70054499384994617373\n",
      "Iteration 33173 => Loss: 6.70054467488118810081\n",
      "Iteration 33174 => Loss: 6.70054435595540720527\n",
      "Iteration 33175 => Loss: 6.70054403707260703982\n",
      "Iteration 33176 => Loss: 6.70054371823277161724\n",
      "Iteration 33177 => Loss: 6.70054339943590360207\n",
      "Iteration 33178 => Loss: 6.70054308068198700710\n",
      "Iteration 33179 => Loss: 6.70054276197103249046\n",
      "Iteration 33180 => Loss: 6.70054244330302317678\n",
      "Iteration 33181 => Loss: 6.70054212467794574337\n",
      "Iteration 33182 => Loss: 6.70054180609581173655\n",
      "Iteration 33183 => Loss: 6.70054148755660428094\n",
      "Iteration 33184 => Loss: 6.70054116906031715928\n",
      "Iteration 33185 => Loss: 6.70054085060694770704\n",
      "Iteration 33186 => Loss: 6.70054053219649681239\n",
      "Iteration 33187 => Loss: 6.70054021382894227088\n",
      "Iteration 33188 => Loss: 6.70053989550429474065\n",
      "Iteration 33189 => Loss: 6.70053957722253912266\n",
      "Iteration 33190 => Loss: 6.70053925898366653513\n",
      "Iteration 33191 => Loss: 6.70053894078768319531\n",
      "Iteration 33192 => Loss: 6.70053862263458110959\n",
      "Iteration 33193 => Loss: 6.70053830452434517895\n",
      "Iteration 33194 => Loss: 6.70053798645697007430\n",
      "Iteration 33195 => Loss: 6.70053766843245934837\n",
      "Iteration 33196 => Loss: 6.70053735045079701393\n",
      "Iteration 33197 => Loss: 6.70053703251199017643\n",
      "Iteration 33198 => Loss: 6.70053671461602196047\n",
      "Iteration 33199 => Loss: 6.70053639676288881333\n",
      "Iteration 33200 => Loss: 6.70053607895259428773\n",
      "Iteration 33201 => Loss: 6.70053576118511262649\n",
      "Iteration 33202 => Loss: 6.70053544346045359958\n",
      "Iteration 33203 => Loss: 6.70053512577861365429\n",
      "Iteration 33204 => Loss: 6.70053480813957857976\n",
      "Iteration 33205 => Loss: 6.70053449054334304691\n",
      "Iteration 33206 => Loss: 6.70053417298990527939\n",
      "Iteration 33207 => Loss: 6.70053385547926172450\n",
      "Iteration 33208 => Loss: 6.70053353801139639501\n",
      "Iteration 33209 => Loss: 6.70053322058631461999\n",
      "Iteration 33210 => Loss: 6.70053290320400662949\n",
      "Iteration 33211 => Loss: 6.70053258586446354172\n",
      "Iteration 33212 => Loss: 6.70053226856768358033\n",
      "Iteration 33213 => Loss: 6.70053195131365342263\n",
      "Iteration 33214 => Loss: 6.70053163410237839770\n",
      "Iteration 33215 => Loss: 6.70053131693384695922\n",
      "Iteration 33216 => Loss: 6.70053099980805821900\n",
      "Iteration 33217 => Loss: 6.70053068272499885438\n",
      "Iteration 33218 => Loss: 6.70053036568466620082\n",
      "Iteration 33219 => Loss: 6.70053004868705315289\n",
      "Iteration 33220 => Loss: 6.70052973173215171698\n",
      "Iteration 33221 => Loss: 6.70052941481996633399\n",
      "Iteration 33222 => Loss: 6.70052909795048368125\n",
      "Iteration 33223 => Loss: 6.70052878112370287056\n",
      "Iteration 33224 => Loss: 6.70052846433960436201\n",
      "Iteration 33225 => Loss: 6.70052814759819703738\n",
      "Iteration 33226 => Loss: 6.70052783089947379125\n",
      "Iteration 33227 => Loss: 6.70052751424342840636\n",
      "Iteration 33228 => Loss: 6.70052719763004223097\n",
      "Iteration 33229 => Loss: 6.70052688105932769957\n",
      "Iteration 33230 => Loss: 6.70052656453126704861\n",
      "Iteration 33231 => Loss: 6.70052624804586560714\n",
      "Iteration 33232 => Loss: 6.70052593160310472342\n",
      "Iteration 33233 => Loss: 6.70052561520298262110\n",
      "Iteration 33234 => Loss: 6.70052529884550018835\n",
      "Iteration 33235 => Loss: 6.70052498253063966160\n",
      "Iteration 33236 => Loss: 6.70052466625841347536\n",
      "Iteration 33237 => Loss: 6.70052435002879764880\n",
      "Iteration 33238 => Loss: 6.70052403384179395829\n",
      "Iteration 33239 => Loss: 6.70052371769740240381\n",
      "Iteration 33240 => Loss: 6.70052340159560522181\n",
      "Iteration 33241 => Loss: 6.70052308553640063593\n",
      "Iteration 33242 => Loss: 6.70052276951979575159\n",
      "Iteration 33243 => Loss: 6.70052245354576481162\n",
      "Iteration 33244 => Loss: 6.70052213761431580963\n",
      "Iteration 33245 => Loss: 6.70052182172543453476\n",
      "Iteration 33246 => Loss: 6.70052150587912187518\n",
      "Iteration 33247 => Loss: 6.70052119007536983730\n",
      "Iteration 33248 => Loss: 6.70052087431417131569\n",
      "Iteration 33249 => Loss: 6.70052055859552631034\n",
      "Iteration 33250 => Loss: 6.70052024291941528134\n",
      "Iteration 33251 => Loss: 6.70051992728584444592\n",
      "Iteration 33252 => Loss: 6.70051961169481113956\n",
      "Iteration 33253 => Loss: 6.70051929614629848686\n",
      "Iteration 33254 => Loss: 6.70051898064030293511\n",
      "Iteration 33255 => Loss: 6.70051866517682981339\n",
      "Iteration 33256 => Loss: 6.70051834975586668719\n",
      "Iteration 33257 => Loss: 6.70051803437740378655\n",
      "Iteration 33258 => Loss: 6.70051771904143045333\n",
      "Iteration 33259 => Loss: 6.70051740374795468114\n",
      "Iteration 33260 => Loss: 6.70051708849696492365\n",
      "Iteration 33261 => Loss: 6.70051677328845407544\n",
      "Iteration 33262 => Loss: 6.70051645812241947198\n",
      "Iteration 33263 => Loss: 6.70051614299885045511\n",
      "Iteration 33264 => Loss: 6.70051582791774613668\n",
      "Iteration 33265 => Loss: 6.70051551287910118759\n",
      "Iteration 33266 => Loss: 6.70051519788290583790\n",
      "Iteration 33267 => Loss: 6.70051488292915742306\n",
      "Iteration 33268 => Loss: 6.70051456801785061401\n",
      "Iteration 33269 => Loss: 6.70051425314897031171\n",
      "Iteration 33270 => Loss: 6.70051393832252717431\n",
      "Iteration 33271 => Loss: 6.70051362353850166187\n",
      "Iteration 33272 => Loss: 6.70051330879689022169\n",
      "Iteration 33273 => Loss: 6.70051299409769640647\n",
      "Iteration 33274 => Loss: 6.70051267944090689355\n",
      "Iteration 33275 => Loss: 6.70051236482652257109\n",
      "Iteration 33276 => Loss: 6.70051205025452745190\n",
      "Iteration 33277 => Loss: 6.70051173572491709507\n",
      "Iteration 33278 => Loss: 6.70051142123769416514\n",
      "Iteration 33279 => Loss: 6.70051110679284711580\n",
      "Iteration 33280 => Loss: 6.70051079239037417068\n",
      "Iteration 33281 => Loss: 6.70051047803026289529\n",
      "Iteration 33282 => Loss: 6.70051016371251240145\n",
      "Iteration 33283 => Loss: 6.70050984943712002462\n",
      "Iteration 33284 => Loss: 6.70050953520407421848\n",
      "Iteration 33285 => Loss: 6.70050922101337320669\n",
      "Iteration 33286 => Loss: 6.70050890686500277837\n",
      "Iteration 33287 => Loss: 6.70050859275897536804\n",
      "Iteration 33288 => Loss: 6.70050827869526077762\n",
      "Iteration 33289 => Loss: 6.70050796467387055344\n",
      "Iteration 33290 => Loss: 6.70050765069480025460\n",
      "Iteration 33291 => Loss: 6.70050733675803211753\n",
      "Iteration 33292 => Loss: 6.70050702286357235948\n",
      "Iteration 33293 => Loss: 6.70050670901140410507\n",
      "Iteration 33294 => Loss: 6.70050639520153090700\n",
      "Iteration 33295 => Loss: 6.70050608143394121896\n",
      "Iteration 33296 => Loss: 6.70050576770863859366\n",
      "Iteration 33297 => Loss: 6.70050545402560171482\n",
      "Iteration 33298 => Loss: 6.70050514038483857604\n",
      "Iteration 33299 => Loss: 6.70050482678633674283\n",
      "Iteration 33300 => Loss: 6.70050451323009177429\n",
      "Iteration 33301 => Loss: 6.70050419971610367043\n",
      "Iteration 33302 => Loss: 6.70050388624435910856\n",
      "Iteration 33303 => Loss: 6.70050357281485009509\n",
      "Iteration 33304 => Loss: 6.70050325942758018272\n",
      "Iteration 33305 => Loss: 6.70050294608253604878\n",
      "Iteration 33306 => Loss: 6.70050263277971591691\n",
      "Iteration 33307 => Loss: 6.70050231951911801076\n",
      "Iteration 33308 => Loss: 6.70050200630073522490\n",
      "Iteration 33309 => Loss: 6.70050169312454446668\n",
      "Iteration 33310 => Loss: 6.70050137999056794058\n",
      "Iteration 33311 => Loss: 6.70050106689878166577\n",
      "Iteration 33312 => Loss: 6.70050075384917942500\n",
      "Iteration 33313 => Loss: 6.70050044084176388282\n",
      "Iteration 33314 => Loss: 6.70050012787652704560\n",
      "Iteration 33315 => Loss: 6.70049981495345914340\n",
      "Iteration 33316 => Loss: 6.70049950207256461709\n",
      "Iteration 33317 => Loss: 6.70049918923382659131\n",
      "Iteration 33318 => Loss: 6.70049887643724328967\n",
      "Iteration 33319 => Loss: 6.70049856368280316588\n",
      "Iteration 33320 => Loss: 6.70049825097051776623\n",
      "Iteration 33321 => Loss: 6.70049793830036843900\n",
      "Iteration 33322 => Loss: 6.70049762567234363786\n",
      "Iteration 33323 => Loss: 6.70049731308645490913\n",
      "Iteration 33324 => Loss: 6.70049700054268093652\n",
      "Iteration 33325 => Loss: 6.70049668804102260822\n",
      "Iteration 33326 => Loss: 6.70049637558147725969\n",
      "Iteration 33327 => Loss: 6.70049606316403689732\n",
      "Iteration 33328 => Loss: 6.70049575078868997480\n",
      "Iteration 33329 => Loss: 6.70049543845543382758\n",
      "Iteration 33330 => Loss: 6.70049512616427112022\n",
      "Iteration 33331 => Loss: 6.70049481391519119455\n",
      "Iteration 33332 => Loss: 6.70049450170817983974\n",
      "Iteration 33333 => Loss: 6.70049418954324149666\n",
      "Iteration 33334 => Loss: 6.70049387742036817173\n",
      "Iteration 33335 => Loss: 6.70049356533955009496\n",
      "Iteration 33336 => Loss: 6.70049325330079081908\n",
      "Iteration 33337 => Loss: 6.70049294130407169234\n",
      "Iteration 33338 => Loss: 6.70049262934939804381\n",
      "Iteration 33339 => Loss: 6.70049231743676276807\n",
      "Iteration 33340 => Loss: 6.70049200556615254243\n",
      "Iteration 33341 => Loss: 6.70049169373756914325\n",
      "Iteration 33342 => Loss: 6.70049138195100812965\n",
      "Iteration 33343 => Loss: 6.70049107020645706712\n",
      "Iteration 33344 => Loss: 6.70049075850391417930\n",
      "Iteration 33345 => Loss: 6.70049044684337147260\n",
      "Iteration 33346 => Loss: 6.70049013522482628247\n",
      "Iteration 33347 => Loss: 6.70048982364826883895\n",
      "Iteration 33348 => Loss: 6.70048951211370269476\n",
      "Iteration 33349 => Loss: 6.70048920062110653362\n",
      "Iteration 33350 => Loss: 6.70048888917049545455\n",
      "Iteration 33351 => Loss: 6.70048857776184458857\n",
      "Iteration 33352 => Loss: 6.70048826639515393566\n",
      "Iteration 33353 => Loss: 6.70048795507042882491\n",
      "Iteration 33354 => Loss: 6.70048764378765060457\n",
      "Iteration 33355 => Loss: 6.70048733254681305738\n",
      "Iteration 33356 => Loss: 6.70048702134792328877\n",
      "Iteration 33357 => Loss: 6.70048671019096353518\n",
      "Iteration 33358 => Loss: 6.70048639907593379661\n",
      "Iteration 33359 => Loss: 6.70048608800282430309\n",
      "Iteration 33360 => Loss: 6.70048577697163327827\n",
      "Iteration 33361 => Loss: 6.70048546598235450489\n",
      "Iteration 33362 => Loss: 6.70048515503497998935\n",
      "Iteration 33363 => Loss: 6.70048484412950706712\n",
      "Iteration 33364 => Loss: 6.70048453326593129731\n",
      "Iteration 33365 => Loss: 6.70048422244423669270\n",
      "Iteration 33366 => Loss: 6.70048391166443302325\n",
      "Iteration 33367 => Loss: 6.70048360092650430175\n",
      "Iteration 33368 => Loss: 6.70048329023044875186\n",
      "Iteration 33369 => Loss: 6.70048297957625216270\n",
      "Iteration 33370 => Loss: 6.70048266896392608061\n",
      "Iteration 33371 => Loss: 6.70048235839344563658\n",
      "Iteration 33372 => Loss: 6.70048204786482326512\n",
      "Iteration 33373 => Loss: 6.70048173737804031447\n",
      "Iteration 33374 => Loss: 6.70048142693310300189\n",
      "Iteration 33375 => Loss: 6.70048111652998468202\n",
      "Iteration 33376 => Loss: 6.70048080616870045390\n",
      "Iteration 33377 => Loss: 6.70048049584923610666\n",
      "Iteration 33378 => Loss: 6.70048018557158808761\n",
      "Iteration 33379 => Loss: 6.70047987533575017949\n",
      "Iteration 33380 => Loss: 6.70047956514171616504\n",
      "Iteration 33381 => Loss: 6.70047925498948426792\n",
      "Iteration 33382 => Loss: 6.70047894487903938909\n",
      "Iteration 33383 => Loss: 6.70047863481038685762\n",
      "Iteration 33384 => Loss: 6.70047832478351423902\n",
      "Iteration 33385 => Loss: 6.70047801479841087513\n",
      "Iteration 33386 => Loss: 6.70047770485508831229\n",
      "Iteration 33387 => Loss: 6.70047739495352523420\n",
      "Iteration 33388 => Loss: 6.70047708509372519359\n",
      "Iteration 33389 => Loss: 6.70047677527567930866\n",
      "Iteration 33390 => Loss: 6.70047646549937958582\n",
      "Iteration 33391 => Loss: 6.70047615576481980781\n",
      "Iteration 33392 => Loss: 6.70047584607200086282\n",
      "Iteration 33393 => Loss: 6.70047553642090409909\n",
      "Iteration 33394 => Loss: 6.70047522681154639201\n",
      "Iteration 33395 => Loss: 6.70047491724389487899\n",
      "Iteration 33396 => Loss: 6.70047460771796465906\n",
      "Iteration 33397 => Loss: 6.70047429823375217950\n",
      "Iteration 33398 => Loss: 6.70047398879123168314\n",
      "Iteration 33399 => Loss: 6.70047367939040228180\n",
      "Iteration 33400 => Loss: 6.70047337003127552180\n",
      "Iteration 33401 => Loss: 6.70047306071383097503\n",
      "Iteration 33402 => Loss: 6.70047275143807130604\n",
      "Iteration 33403 => Loss: 6.70047244220398052761\n",
      "Iteration 33404 => Loss: 6.70047213301156752152\n",
      "Iteration 33405 => Loss: 6.70047182386081097150\n",
      "Iteration 33406 => Loss: 6.70047151475171087753\n",
      "Iteration 33407 => Loss: 6.70047120568426990417\n",
      "Iteration 33408 => Loss: 6.70047089665847028783\n",
      "Iteration 33409 => Loss: 6.70047058767431558124\n",
      "Iteration 33410 => Loss: 6.70047027873179334989\n",
      "Iteration 33411 => Loss: 6.70046996983090092925\n",
      "Iteration 33412 => Loss: 6.70046966097163476661\n",
      "Iteration 33413 => Loss: 6.70046935215399397379\n",
      "Iteration 33414 => Loss: 6.70046904337795456996\n",
      "Iteration 33415 => Loss: 6.70046873464352987781\n",
      "Iteration 33416 => Loss: 6.70046842595070657467\n",
      "Iteration 33417 => Loss: 6.70046811729947400238\n",
      "Iteration 33418 => Loss: 6.70046780868984193091\n",
      "Iteration 33419 => Loss: 6.70046750012178815581\n",
      "Iteration 33420 => Loss: 6.70046719159531800614\n",
      "Iteration 33421 => Loss: 6.70046688311041460651\n",
      "Iteration 33422 => Loss: 6.70046657466708595052\n",
      "Iteration 33423 => Loss: 6.70046626626531693915\n",
      "Iteration 33424 => Loss: 6.70046595790510579604\n",
      "Iteration 33425 => Loss: 6.70046564958645163301\n",
      "Iteration 33426 => Loss: 6.70046534130934023921\n",
      "Iteration 33427 => Loss: 6.70046503307375918013\n",
      "Iteration 33428 => Loss: 6.70046472487972710752\n",
      "Iteration 33429 => Loss: 6.70046441672721826421\n",
      "Iteration 33430 => Loss: 6.70046410861623176203\n",
      "Iteration 33431 => Loss: 6.70046380054676316007\n",
      "Iteration 33432 => Loss: 6.70046349251880979381\n",
      "Iteration 33433 => Loss: 6.70046318453236366963\n",
      "Iteration 33434 => Loss: 6.70046287658741146487\n",
      "Iteration 33435 => Loss: 6.70046256868396294948\n",
      "Iteration 33436 => Loss: 6.70046226082200568896\n",
      "Iteration 33437 => Loss: 6.70046195300152724883\n",
      "Iteration 33438 => Loss: 6.70046164522252940543\n",
      "Iteration 33439 => Loss: 6.70046133748500771787\n",
      "Iteration 33440 => Loss: 6.70046102978895063984\n",
      "Iteration 33441 => Loss: 6.70046072213435461862\n",
      "Iteration 33442 => Loss: 6.70046041452121787785\n",
      "Iteration 33443 => Loss: 6.70046010694952975939\n",
      "Iteration 33444 => Loss: 6.70045979941928671053\n",
      "Iteration 33445 => Loss: 6.70045949193048429038\n",
      "Iteration 33446 => Loss: 6.70045918448311450533\n",
      "Iteration 33447 => Loss: 6.70045887707717202630\n",
      "Iteration 33448 => Loss: 6.70045856971265685331\n",
      "Iteration 33449 => Loss: 6.70045826238956099274\n",
      "Iteration 33450 => Loss: 6.70045795510787023375\n",
      "Iteration 33451 => Loss: 6.70045764786759168175\n",
      "Iteration 33452 => Loss: 6.70045734066871112589\n",
      "Iteration 33453 => Loss: 6.70045703351122501346\n",
      "Iteration 33454 => Loss: 6.70045672639512801538\n",
      "Iteration 33455 => Loss: 6.70045641932041302624\n",
      "Iteration 33456 => Loss: 6.70045611228708537510\n",
      "Iteration 33457 => Loss: 6.70045580529511841661\n",
      "Iteration 33458 => Loss: 6.70045549834452813798\n",
      "Iteration 33459 => Loss: 6.70045519143529322292\n",
      "Iteration 33460 => Loss: 6.70045488456741988870\n",
      "Iteration 33461 => Loss: 6.70045457774089303626\n",
      "Iteration 33462 => Loss: 6.70045427095571177745\n",
      "Iteration 33463 => Loss: 6.70045396421187344771\n",
      "Iteration 33464 => Loss: 6.70045365750936472438\n",
      "Iteration 33465 => Loss: 6.70045335084818471927\n",
      "Iteration 33466 => Loss: 6.70045304422832721514\n",
      "Iteration 33467 => Loss: 6.70045273764978421838\n",
      "Iteration 33468 => Loss: 6.70045243111255484081\n",
      "Iteration 33469 => Loss: 6.70045212461663641790\n",
      "Iteration 33470 => Loss: 6.70045181816201029790\n",
      "Iteration 33471 => Loss: 6.70045151174868713895\n",
      "Iteration 33472 => Loss: 6.70045120537664828930\n",
      "Iteration 33473 => Loss: 6.70045089904588841989\n",
      "Iteration 33474 => Loss: 6.70045059275641996521\n",
      "Iteration 33475 => Loss: 6.70045028650821183902\n",
      "Iteration 33476 => Loss: 6.70044998030127736399\n",
      "Iteration 33477 => Loss: 6.70044967413559966474\n",
      "Iteration 33478 => Loss: 6.70044936801118140579\n",
      "Iteration 33479 => Loss: 6.70044906192801192901\n",
      "Iteration 33480 => Loss: 6.70044875588608501715\n",
      "Iteration 33481 => Loss: 6.70044844988540155839\n",
      "Iteration 33482 => Loss: 6.70044814392594911823\n",
      "Iteration 33483 => Loss: 6.70044783800773036120\n",
      "Iteration 33484 => Loss: 6.70044753213072308284\n",
      "Iteration 33485 => Loss: 6.70044722629494060584\n",
      "Iteration 33486 => Loss: 6.70044692050037316022\n",
      "Iteration 33487 => Loss: 6.70044661474699942971\n",
      "Iteration 33488 => Loss: 6.70044630903483096063\n",
      "Iteration 33489 => Loss: 6.70044600336386242390\n",
      "Iteration 33490 => Loss: 6.70044569773407960867\n",
      "Iteration 33491 => Loss: 6.70044539214548162676\n",
      "Iteration 33492 => Loss: 6.70044508659806048456\n",
      "Iteration 33493 => Loss: 6.70044478109181174119\n",
      "Iteration 33494 => Loss: 6.70044447562673095575\n",
      "Iteration 33495 => Loss: 6.70044417020281279918\n",
      "Iteration 33496 => Loss: 6.70044386482004039607\n",
      "Iteration 33497 => Loss: 6.70044355947843151000\n",
      "Iteration 33498 => Loss: 6.70044325417796393651\n",
      "Iteration 33499 => Loss: 6.70044294891863412289\n",
      "Iteration 33500 => Loss: 6.70044264370043052281\n",
      "Iteration 33501 => Loss: 6.70044233852336912349\n",
      "Iteration 33502 => Loss: 6.70044203338742594411\n",
      "Iteration 33503 => Loss: 6.70044172829259743196\n",
      "Iteration 33504 => Loss: 6.70044142323887736978\n",
      "Iteration 33505 => Loss: 6.70044111822626842212\n",
      "Iteration 33506 => Loss: 6.70044081325475460176\n",
      "Iteration 33507 => Loss: 6.70044050832434212595\n",
      "Iteration 33508 => Loss: 6.70044020343501678383\n",
      "Iteration 33509 => Loss: 6.70043989858677502269\n",
      "Iteration 33510 => Loss: 6.70043959377961151347\n",
      "Iteration 33511 => Loss: 6.70043928901352003891\n",
      "Iteration 33512 => Loss: 6.70043898428849793447\n",
      "Iteration 33513 => Loss: 6.70043867960453543020\n",
      "Iteration 33514 => Loss: 6.70043837496162897338\n",
      "Iteration 33515 => Loss: 6.70043807035977767583\n",
      "Iteration 33516 => Loss: 6.70043776579895844492\n",
      "Iteration 33517 => Loss: 6.70043746127919170874\n",
      "Iteration 33518 => Loss: 6.70043715680045615102\n",
      "Iteration 33519 => Loss: 6.70043685236274999539\n",
      "Iteration 33520 => Loss: 6.70043654796606258373\n",
      "Iteration 33521 => Loss: 6.70043624361039391601\n",
      "Iteration 33522 => Loss: 6.70043593929574310408\n",
      "Iteration 33523 => Loss: 6.70043563502209327254\n",
      "Iteration 33524 => Loss: 6.70043533078944353321\n",
      "Iteration 33525 => Loss: 6.70043502659779033337\n",
      "Iteration 33526 => Loss: 6.70043472244712834396\n",
      "Iteration 33527 => Loss: 6.70043441833745045955\n",
      "Iteration 33528 => Loss: 6.70043411426874957471\n",
      "Iteration 33529 => Loss: 6.70043381024102480126\n",
      "Iteration 33530 => Loss: 6.70043350625426281653\n",
      "Iteration 33531 => Loss: 6.70043320230846894958\n",
      "Iteration 33532 => Loss: 6.70043289840363343046\n",
      "Iteration 33533 => Loss: 6.70043259453973938378\n",
      "Iteration 33534 => Loss: 6.70043229071680013220\n",
      "Iteration 33535 => Loss: 6.70043198693479791217\n",
      "Iteration 33536 => Loss: 6.70043168319372917097\n",
      "Iteration 33537 => Loss: 6.70043137949359213223\n",
      "Iteration 33538 => Loss: 6.70043107583437791419\n",
      "Iteration 33539 => Loss: 6.70043077221607852323\n",
      "Iteration 33540 => Loss: 6.70043046863868863028\n",
      "Iteration 33541 => Loss: 6.70043016510221356441\n",
      "Iteration 33542 => Loss: 6.70042986160664000295\n",
      "Iteration 33543 => Loss: 6.70042955815195817593\n",
      "Iteration 33544 => Loss: 6.70042925473817074788\n",
      "Iteration 33545 => Loss: 6.70042895136527061339\n",
      "Iteration 33546 => Loss: 6.70042864803324356160\n",
      "Iteration 33547 => Loss: 6.70042834474209225704\n",
      "Iteration 33548 => Loss: 6.70042804149180781792\n",
      "Iteration 33549 => Loss: 6.70042773828238580336\n",
      "Iteration 33550 => Loss: 6.70042743511382443700\n",
      "Iteration 33551 => Loss: 6.70042713198611838976\n",
      "Iteration 33552 => Loss: 6.70042682889925877987\n",
      "Iteration 33553 => Loss: 6.70042652585323406100\n",
      "Iteration 33554 => Loss: 6.70042622284804867405\n",
      "Iteration 33555 => Loss: 6.70042591988368929634\n",
      "Iteration 33556 => Loss: 6.70042561696015503969\n",
      "Iteration 33557 => Loss: 6.70042531407744146321\n",
      "Iteration 33558 => Loss: 6.70042501123554234965\n",
      "Iteration 33559 => Loss: 6.70042470843445503448\n",
      "Iteration 33560 => Loss: 6.70042440567416441866\n",
      "Iteration 33561 => Loss: 6.70042410295466872583\n",
      "Iteration 33562 => Loss: 6.70042380027597239689\n",
      "Iteration 33563 => Loss: 6.70042349763805411555\n",
      "Iteration 33564 => Loss: 6.70042319504091832272\n",
      "Iteration 33565 => Loss: 6.70042289248455702477\n",
      "Iteration 33566 => Loss: 6.70042258996897643897\n",
      "Iteration 33567 => Loss: 6.70042228749414459088\n",
      "Iteration 33568 => Loss: 6.70042198506007746772\n",
      "Iteration 33569 => Loss: 6.70042168266676352317\n",
      "Iteration 33570 => Loss: 6.70042138031419387545\n",
      "Iteration 33571 => Loss: 6.70042107800236852455\n",
      "Iteration 33572 => Loss: 6.70042077573128302959\n",
      "Iteration 33573 => Loss: 6.70042047350092406788\n",
      "Iteration 33574 => Loss: 6.70042017131128897489\n",
      "Iteration 33575 => Loss: 6.70041986916238041516\n",
      "Iteration 33576 => Loss: 6.70041956705418240148\n",
      "Iteration 33577 => Loss: 6.70041926498669582202\n",
      "Iteration 33578 => Loss: 6.70041896295990913046\n",
      "Iteration 33579 => Loss: 6.70041866097382676770\n",
      "Iteration 33580 => Loss: 6.70041835902843097017\n",
      "Iteration 33581 => Loss: 6.70041805712372706694\n",
      "Iteration 33582 => Loss: 6.70041775525970173533\n",
      "Iteration 33583 => Loss: 6.70041745343634875809\n",
      "Iteration 33584 => Loss: 6.70041715165367879337\n",
      "Iteration 33585 => Loss: 6.70041684991166075491\n",
      "Iteration 33586 => Loss: 6.70041654821030263633\n",
      "Iteration 33587 => Loss: 6.70041624654960710217\n",
      "Iteration 33588 => Loss: 6.70041594492956527063\n",
      "Iteration 33589 => Loss: 6.70041564335015049636\n",
      "Iteration 33590 => Loss: 6.70041534181137965476\n",
      "Iteration 33591 => Loss: 6.70041504031324919310\n",
      "Iteration 33592 => Loss: 6.70041473885573335423\n",
      "Iteration 33593 => Loss: 6.70041443743884901352\n",
      "Iteration 33594 => Loss: 6.70041413606258018376\n",
      "Iteration 33595 => Loss: 6.70041383472691709500\n",
      "Iteration 33596 => Loss: 6.70041353343186152358\n",
      "Iteration 33597 => Loss: 6.70041323217740014684\n",
      "Iteration 33598 => Loss: 6.70041293096353740566\n",
      "Iteration 33599 => Loss: 6.70041262979026708280\n",
      "Iteration 33600 => Loss: 6.70041232865757940829\n",
      "Iteration 33601 => Loss: 6.70041202756545839492\n",
      "Iteration 33602 => Loss: 6.70041172651391736537\n",
      "Iteration 33603 => Loss: 6.70041142550294122060\n",
      "Iteration 33604 => Loss: 6.70041112453253084880\n",
      "Iteration 33605 => Loss: 6.70041082360267914453\n",
      "Iteration 33606 => Loss: 6.70041052271336923241\n",
      "Iteration 33607 => Loss: 6.70041022186460466514\n",
      "Iteration 33608 => Loss: 6.70040992105638189003\n",
      "Iteration 33609 => Loss: 6.70040962028869557798\n",
      "Iteration 33610 => Loss: 6.70040931956152885363\n",
      "Iteration 33611 => Loss: 6.70040901887489592781\n",
      "Iteration 33612 => Loss: 6.70040871822877637243\n",
      "Iteration 33613 => Loss: 6.70040841762316485841\n",
      "Iteration 33614 => Loss: 6.70040811705806582665\n",
      "Iteration 33615 => Loss: 6.70040781653346595448\n",
      "Iteration 33616 => Loss: 6.70040751604935724828\n",
      "Iteration 33617 => Loss: 6.70040721560574592530\n",
      "Iteration 33618 => Loss: 6.70040691520262132741\n",
      "Iteration 33619 => Loss: 6.70040661483996213832\n",
      "Iteration 33620 => Loss: 6.70040631451779056249\n",
      "Iteration 33621 => Loss: 6.70040601423608173093\n",
      "Iteration 33622 => Loss: 6.70040571399483919635\n",
      "Iteration 33623 => Loss: 6.70040541379405407696\n",
      "Iteration 33624 => Loss: 6.70040511363371571463\n",
      "Iteration 33625 => Loss: 6.70040481351382855024\n",
      "Iteration 33626 => Loss: 6.70040451343437926113\n",
      "Iteration 33627 => Loss: 6.70040421339536784728\n",
      "Iteration 33628 => Loss: 6.70040391339678631510\n",
      "Iteration 33629 => Loss: 6.70040361343863022370\n",
      "Iteration 33630 => Loss: 6.70040331352089690853\n",
      "Iteration 33631 => Loss: 6.70040301364356949421\n",
      "Iteration 33632 => Loss: 6.70040271380665863887\n",
      "Iteration 33633 => Loss: 6.70040241401014480260\n",
      "Iteration 33634 => Loss: 6.70040211425403242629\n",
      "Iteration 33635 => Loss: 6.70040181453830996361\n",
      "Iteration 33636 => Loss: 6.70040151486297475003\n",
      "Iteration 33637 => Loss: 6.70040121522801523923\n",
      "Iteration 33638 => Loss: 6.70040091563343764847\n",
      "Iteration 33639 => Loss: 6.70040061607923398412\n",
      "Iteration 33640 => Loss: 6.70040031656538914717\n",
      "Iteration 33641 => Loss: 6.70040001709190669033\n",
      "Iteration 33642 => Loss: 6.70039971765877861998\n",
      "Iteration 33643 => Loss: 6.70039941826599605434\n",
      "Iteration 33644 => Loss: 6.70039911891356165796\n",
      "Iteration 33645 => Loss: 6.70039881960146210815\n",
      "Iteration 33646 => Loss: 6.70039852032969296403\n",
      "Iteration 33647 => Loss: 6.70039822109825866647\n",
      "Iteration 33648 => Loss: 6.70039792190713345832\n",
      "Iteration 33649 => Loss: 6.70039762275633066224\n",
      "Iteration 33650 => Loss: 6.70039732364583695556\n",
      "Iteration 33651 => Loss: 6.70039702457565411464\n",
      "Iteration 33652 => Loss: 6.70039672554576881680\n",
      "Iteration 33653 => Loss: 6.70039642655616773936\n",
      "Iteration 33654 => Loss: 6.70039612760686953408\n",
      "Iteration 33655 => Loss: 6.70039582869784933195\n",
      "Iteration 33656 => Loss: 6.70039552982910269208\n",
      "Iteration 33657 => Loss: 6.70039523100063938443\n",
      "Iteration 33658 => Loss: 6.70039493221243542820\n",
      "Iteration 33659 => Loss: 6.70039463346448993519\n",
      "Iteration 33660 => Loss: 6.70039433475680823449\n",
      "Iteration 33661 => Loss: 6.70039403608936989798\n",
      "Iteration 33662 => Loss: 6.70039373746218291927\n",
      "Iteration 33663 => Loss: 6.70039343887523219934\n",
      "Iteration 33664 => Loss: 6.70039314032852040270\n",
      "Iteration 33665 => Loss: 6.70039284182203687124\n",
      "Iteration 33666 => Loss: 6.70039254335577183497\n",
      "Iteration 33667 => Loss: 6.70039224492973417568\n",
      "Iteration 33668 => Loss: 6.70039194654390346528\n",
      "Iteration 33669 => Loss: 6.70039164819827792741\n",
      "Iteration 33670 => Loss: 6.70039134989286022659\n",
      "Iteration 33671 => Loss: 6.70039105162763615198\n",
      "Iteration 33672 => Loss: 6.70039075340261014446\n",
      "Iteration 33673 => Loss: 6.70039045521776177594\n",
      "Iteration 33674 => Loss: 6.70039015707309548731\n",
      "Iteration 33675 => Loss: 6.70038985896860150859\n",
      "Iteration 33676 => Loss: 6.70038956090428428070\n",
      "Iteration 33677 => Loss: 6.70038926288012337551\n",
      "Iteration 33678 => Loss: 6.70038896489612323393\n",
      "Iteration 33679 => Loss: 6.70038866695228207959\n",
      "Iteration 33680 => Loss: 6.70038836904858481347\n",
      "Iteration 33681 => Loss: 6.70038807118502788285\n",
      "Iteration 33682 => Loss: 6.70038777336161750497\n",
      "Iteration 33683 => Loss: 6.70038747557832525814\n",
      "Iteration 33684 => Loss: 6.70038717783516712956\n",
      "Iteration 33685 => Loss: 6.70038688013212890837\n",
      "Iteration 33686 => Loss: 6.70038658246921237094\n",
      "Iteration 33687 => Loss: 6.70038628484639353644\n",
      "Iteration 33688 => Loss: 6.70038598726368928027\n",
      "Iteration 33689 => Loss: 6.70038568972107828614\n",
      "Iteration 33690 => Loss: 6.70038539221856410677\n",
      "Iteration 33691 => Loss: 6.70038509475613786037\n",
      "Iteration 33692 => Loss: 6.70038479733379066516\n",
      "Iteration 33693 => Loss: 6.70038449995152607386\n",
      "Iteration 33694 => Loss: 6.70038420260933165196\n",
      "Iteration 33695 => Loss: 6.70038390530721095217\n",
      "Iteration 33696 => Loss: 6.70038360804513999369\n",
      "Iteration 33697 => Loss: 6.70038331082313831644\n",
      "Iteration 33698 => Loss: 6.70038301364117838688\n",
      "Iteration 33699 => Loss: 6.70038271649926642226\n",
      "Iteration 33700 => Loss: 6.70038241939738998809\n",
      "Iteration 33701 => Loss: 6.70038212233555530162\n",
      "Iteration 33702 => Loss: 6.70038182531374104656\n",
      "Iteration 33703 => Loss: 6.70038152833195965741\n",
      "Iteration 33704 => Loss: 6.70038123139019248242\n",
      "Iteration 33705 => Loss: 6.70038093448844040978\n",
      "Iteration 33706 => Loss: 6.70038063762668745227\n",
      "Iteration 33707 => Loss: 6.70038034080494426803\n",
      "Iteration 33708 => Loss: 6.70038004402320019892\n",
      "Iteration 33709 => Loss: 6.70037974728144369863\n",
      "Iteration 33710 => Loss: 6.70037945057966499718\n",
      "Iteration 33711 => Loss: 6.70037915391787830544\n",
      "Iteration 33712 => Loss: 6.70037885729606674801\n",
      "Iteration 33713 => Loss: 6.70037856071422321946\n",
      "Iteration 33714 => Loss: 6.70037826417233794984\n",
      "Iteration 33715 => Loss: 6.70037796767042248547\n",
      "Iteration 33716 => Loss: 6.70037767120845195734\n",
      "Iteration 33717 => Loss: 6.70037737478643524724\n",
      "Iteration 33718 => Loss: 6.70037707840435459161\n",
      "Iteration 33719 => Loss: 6.70037678206221798405\n",
      "Iteration 33720 => Loss: 6.70037648576001299006\n",
      "Iteration 33721 => Loss: 6.70037618949772717514\n",
      "Iteration 33722 => Loss: 6.70037589327536409201\n",
      "Iteration 33723 => Loss: 6.70037559709292906973\n",
      "Iteration 33724 => Loss: 6.70037530095039723932\n",
      "Iteration 33725 => Loss: 6.70037500484776682441\n",
      "Iteration 33726 => Loss: 6.70037470878503516047\n",
      "Iteration 33727 => Loss: 6.70037441276220757658\n",
      "Iteration 33728 => Loss: 6.70037411677926275644\n",
      "Iteration 33729 => Loss: 6.70037382083620425277\n",
      "Iteration 33730 => Loss: 6.70037352493302140743\n",
      "Iteration 33731 => Loss: 6.70037322906971333225\n",
      "Iteration 33732 => Loss: 6.70037293324627292179\n",
      "Iteration 33733 => Loss: 6.70037263746269307063\n",
      "Iteration 33734 => Loss: 6.70037234171897200241\n",
      "Iteration 33735 => Loss: 6.70037204601509817081\n",
      "Iteration 33736 => Loss: 6.70037175035107690491\n",
      "Iteration 33737 => Loss: 6.70037145472689310566\n",
      "Iteration 33738 => Loss: 6.70037115914253877946\n",
      "Iteration 33739 => Loss: 6.70037086359803080171\n",
      "Iteration 33740 => Loss: 6.70037056809333364527\n",
      "Iteration 33741 => Loss: 6.70037027262845974462\n",
      "Iteration 33742 => Loss: 6.70036997720340199436\n",
      "Iteration 33743 => Loss: 6.70036968181814529544\n",
      "Iteration 33744 => Loss: 6.70036938647269764147\n",
      "Iteration 33745 => Loss: 6.70036909116704748612\n",
      "Iteration 33746 => Loss: 6.70036879590118772398\n",
      "Iteration 33747 => Loss: 6.70036850067511657869\n",
      "Iteration 33748 => Loss: 6.70036820548882783299\n",
      "Iteration 33749 => Loss: 6.70036791034231704600\n",
      "Iteration 33750 => Loss: 6.70036761523557000686\n",
      "Iteration 33751 => Loss: 6.70036732016860181460\n",
      "Iteration 33752 => Loss: 6.70036702514138493569\n",
      "Iteration 33753 => Loss: 6.70036673015391670560\n",
      "Iteration 33754 => Loss: 6.70036643520620422976\n",
      "Iteration 33755 => Loss: 6.70036614029823507366\n",
      "Iteration 33756 => Loss: 6.70036584543001367820\n",
      "Iteration 33757 => Loss: 6.70036555060152050345\n",
      "Iteration 33758 => Loss: 6.70036525581275022034\n",
      "Iteration 33759 => Loss: 6.70036496106370904613\n",
      "Iteration 33760 => Loss: 6.70036466635437921724\n",
      "Iteration 33761 => Loss: 6.70036437168476695092\n",
      "Iteration 33762 => Loss: 6.70036407705485714814\n",
      "Iteration 33763 => Loss: 6.70036378246465158526\n",
      "Iteration 33764 => Loss: 6.70036348791414226866\n",
      "Iteration 33765 => Loss: 6.70036319340331942840\n",
      "Iteration 33766 => Loss: 6.70036289893218661717\n",
      "Iteration 33767 => Loss: 6.70036260450072962414\n",
      "Iteration 33768 => Loss: 6.70036231010894933746\n",
      "Iteration 33769 => Loss: 6.70036201575684131626\n",
      "Iteration 33770 => Loss: 6.70036172144439667875\n",
      "Iteration 33771 => Loss: 6.70036142717160831950\n",
      "Iteration 33772 => Loss: 6.70036113293847179762\n",
      "Iteration 33773 => Loss: 6.70036083874498711310\n",
      "Iteration 33774 => Loss: 6.70036054459114271964\n",
      "Iteration 33775 => Loss: 6.70036025047693950540\n",
      "Iteration 33776 => Loss: 6.70035995640236237136\n",
      "Iteration 33777 => Loss: 6.70035966236741220570\n",
      "Iteration 33778 => Loss: 6.70035936837208900840\n",
      "Iteration 33779 => Loss: 6.70035907441637856863\n",
      "Iteration 33780 => Loss: 6.70035878050027644548\n",
      "Iteration 33781 => Loss: 6.70035848662378263896\n",
      "Iteration 33782 => Loss: 6.70035819278688737910\n",
      "Iteration 33783 => Loss: 6.70035789898958888955\n",
      "Iteration 33784 => Loss: 6.70035760523187029492\n",
      "Iteration 33785 => Loss: 6.70035731151374669423\n",
      "Iteration 33786 => Loss: 6.70035701783520121211\n",
      "Iteration 33787 => Loss: 6.70035672419622141405\n",
      "Iteration 33788 => Loss: 6.70035643059681174094\n",
      "Iteration 33789 => Loss: 6.70035613703696508736\n",
      "Iteration 33790 => Loss: 6.70035584351668411784\n",
      "Iteration 33791 => Loss: 6.70035555003594573975\n",
      "Iteration 33792 => Loss: 6.70035525659475617033\n",
      "Iteration 33793 => Loss: 6.70035496319309853419\n",
      "Iteration 33794 => Loss: 6.70035466983098970672\n",
      "Iteration 33795 => Loss: 6.70035437650841281254\n",
      "Iteration 33796 => Loss: 6.70035408322535097625\n",
      "Iteration 33797 => Loss: 6.70035378998181752053\n",
      "Iteration 33798 => Loss: 6.70035349677779112909\n",
      "Iteration 33799 => Loss: 6.70035320361328157190\n",
      "Iteration 33800 => Loss: 6.70035291048827197358\n",
      "Iteration 33801 => Loss: 6.70035261740276322229\n",
      "Iteration 33802 => Loss: 6.70035232435674288354\n",
      "Iteration 33803 => Loss: 6.70035203135021806276\n",
      "Iteration 33804 => Loss: 6.70035173838316833184\n",
      "Iteration 33805 => Loss: 6.70035144545559901985\n",
      "Iteration 33806 => Loss: 6.70035115256750213319\n",
      "Iteration 33807 => Loss: 6.70035085971887056644\n",
      "Iteration 33808 => Loss: 6.70035056690969721416\n",
      "Iteration 33809 => Loss: 6.70035027413998740542\n",
      "Iteration 33810 => Loss: 6.70034998140972160030\n",
      "Iteration 33811 => Loss: 6.70034968871890335151\n",
      "Iteration 33812 => Loss: 6.70034939606753532360\n",
      "Iteration 33813 => Loss: 6.70034910345558998301\n",
      "Iteration 33814 => Loss: 6.70034881088307709973\n",
      "Iteration 33815 => Loss: 6.70034851834998868014\n",
      "Iteration 33816 => Loss: 6.70034822585631850700\n",
      "Iteration 33817 => Loss: 6.70034793340206569212\n",
      "Iteration 33818 => Loss: 6.70034764098722046555\n",
      "Iteration 33819 => Loss: 6.70034734861177216914\n",
      "Iteration 33820 => Loss: 6.70034705627572790831\n",
      "Iteration 33821 => Loss: 6.70034676397906459044\n",
      "Iteration 33822 => Loss: 6.70034647172179642638\n",
      "Iteration 33823 => Loss: 6.70034617950391808705\n",
      "Iteration 33824 => Loss: 6.70034588732540381528\n",
      "Iteration 33825 => Loss: 6.70034559518626426922\n",
      "Iteration 33826 => Loss: 6.70034530308648879071\n",
      "Iteration 33827 => Loss: 6.70034501102607915612\n",
      "Iteration 33828 => Loss: 6.70034471900502026642\n",
      "Iteration 33829 => Loss: 6.70034442702331389796\n",
      "Iteration 33830 => Loss: 6.70034413508095028078\n",
      "Iteration 33831 => Loss: 6.70034384317792763852\n",
      "Iteration 33832 => Loss: 6.70034355131423886576\n",
      "Iteration 33833 => Loss: 6.70034325948987419252\n",
      "Iteration 33834 => Loss: 6.70034296770483805972\n",
      "Iteration 33835 => Loss: 6.70034267595911980919\n",
      "Iteration 33836 => Loss: 6.70034238425271322370\n",
      "Iteration 33837 => Loss: 6.70034209258561563871\n",
      "Iteration 33838 => Loss: 6.70034180095781373154\n",
      "Iteration 33839 => Loss: 6.70034150936931194309\n",
      "Iteration 33840 => Loss: 6.70034121782011027335\n",
      "Iteration 33841 => Loss: 6.70034092631018651787\n",
      "Iteration 33842 => Loss: 6.70034063483953890028\n",
      "Iteration 33843 => Loss: 6.70034034340817985509\n",
      "Iteration 33844 => Loss: 6.70034005201607918423\n",
      "Iteration 33845 => Loss: 6.70033976066325021037\n",
      "Iteration 33846 => Loss: 6.70033946934968405174\n",
      "Iteration 33847 => Loss: 6.70033917807536116840\n",
      "Iteration 33848 => Loss: 6.70033888684029754756\n",
      "Iteration 33849 => Loss: 6.70033859564447542567\n",
      "Iteration 33850 => Loss: 6.70033830448788947365\n",
      "Iteration 33851 => Loss: 6.70033801337053880331\n",
      "Iteration 33852 => Loss: 6.70033772229241719742\n",
      "Iteration 33853 => Loss: 6.70033743125351399783\n",
      "Iteration 33854 => Loss: 6.70033714025383186907\n",
      "Iteration 33855 => Loss: 6.70033684929336015301\n",
      "Iteration 33856 => Loss: 6.70033655837209796147\n",
      "Iteration 33857 => Loss: 6.70033626749003641265\n",
      "Iteration 33858 => Loss: 6.70033597664717373021\n",
      "Iteration 33859 => Loss: 6.70033568584349925601\n",
      "Iteration 33860 => Loss: 6.70033539507900766097\n",
      "Iteration 33861 => Loss: 6.70033510435369983327\n",
      "Iteration 33862 => Loss: 6.70033481366756689113\n",
      "Iteration 33863 => Loss: 6.70033452302060350547\n",
      "Iteration 33864 => Loss: 6.70033423241280967630\n",
      "Iteration 33865 => Loss: 6.70033394184416852823\n",
      "Iteration 33866 => Loss: 6.70033365131468894305\n",
      "Iteration 33867 => Loss: 6.70033336082435404535\n",
      "Iteration 33868 => Loss: 6.70033307037315850607\n",
      "Iteration 33869 => Loss: 6.70033277996109966068\n",
      "Iteration 33870 => Loss: 6.70033248958818283825\n",
      "Iteration 33871 => Loss: 6.70033219925438761067\n",
      "Iteration 33872 => Loss: 6.70033190895972197154\n",
      "Iteration 33873 => Loss: 6.70033161870416993366\n",
      "Iteration 33874 => Loss: 6.70033132848772705614\n",
      "Iteration 33875 => Loss: 6.70033103831039333897\n",
      "Iteration 33876 => Loss: 6.70033074817216078856\n",
      "Iteration 33877 => Loss: 6.70033045807302496399\n",
      "Iteration 33878 => Loss: 6.70033016801297609533\n",
      "Iteration 33879 => Loss: 6.70032987799201951162\n",
      "Iteration 33880 => Loss: 6.70032958801013656114\n",
      "Iteration 33881 => Loss: 6.70032929806733523748\n",
      "Iteration 33882 => Loss: 6.70032900816359866525\n",
      "Iteration 33883 => Loss: 6.70032871829893039717\n",
      "Iteration 33884 => Loss: 6.70032842847331355784\n",
      "Iteration 33885 => Loss: 6.70032813868676146996\n",
      "Iteration 33886 => Loss: 6.70032784893924748815\n",
      "Iteration 33887 => Loss: 6.70032755923078759963\n",
      "Iteration 33888 => Loss: 6.70032726956135782359\n",
      "Iteration 33889 => Loss: 6.70032697993096792999\n",
      "Iteration 33890 => Loss: 6.70032669033959571436\n",
      "Iteration 33891 => Loss: 6.70032640078725805211\n",
      "Iteration 33892 => Loss: 6.70032611127392829786\n",
      "Iteration 33893 => Loss: 6.70032582179961533342\n",
      "Iteration 33894 => Loss: 6.70032553236430850063\n",
      "Iteration 33895 => Loss: 6.70032524296800158226\n",
      "Iteration 33896 => Loss: 6.70032495361069280193\n",
      "Iteration 33897 => Loss: 6.70032466429236706063\n",
      "Iteration 33898 => Loss: 6.70032437501303945737\n",
      "Iteration 33899 => Loss: 6.70032408577268157046\n",
      "Iteration 33900 => Loss: 6.70032379657131205164\n",
      "Iteration 33901 => Loss: 6.70032350740890425556\n",
      "Iteration 33902 => Loss: 6.70032321828545818221\n",
      "Iteration 33903 => Loss: 6.70032292920097027888\n",
      "Iteration 33904 => Loss: 6.70032264015544409830\n",
      "Iteration 33905 => Loss: 6.70032235114886365324\n",
      "Iteration 33906 => Loss: 6.70032206218122361463\n",
      "Iteration 33907 => Loss: 6.70032177325252131794\n",
      "Iteration 33908 => Loss: 6.70032148436275765135\n",
      "Iteration 33909 => Loss: 6.70032119551191929219\n",
      "Iteration 33910 => Loss: 6.70032090670000801680\n",
      "Iteration 33911 => Loss: 6.70032061792701050251\n",
      "Iteration 33912 => Loss: 6.70032032919292230844\n",
      "Iteration 33913 => Loss: 6.70032004049774521093\n",
      "Iteration 33914 => Loss: 6.70031975184146855185\n",
      "Iteration 33915 => Loss: 6.70031946322408611394\n",
      "Iteration 33916 => Loss: 6.70031917464559878539\n",
      "Iteration 33917 => Loss: 6.70031888610599857259\n",
      "Iteration 33918 => Loss: 6.70031859760527481740\n",
      "Iteration 33919 => Loss: 6.70031830914342219074\n",
      "Iteration 33920 => Loss: 6.70031802072044868623\n",
      "Iteration 33921 => Loss: 6.70031773233633920483\n",
      "Iteration 33922 => Loss: 6.70031744399108752930\n",
      "Iteration 33923 => Loss: 6.70031715568469010691\n",
      "Iteration 33924 => Loss: 6.70031686741714072042\n",
      "Iteration 33925 => Loss: 6.70031657918843759347\n",
      "Iteration 33926 => Loss: 6.70031629099857006793\n",
      "Iteration 33927 => Loss: 6.70031600284753814378\n",
      "Iteration 33928 => Loss: 6.70031571473533116290\n",
      "Iteration 33929 => Loss: 6.70031542666195889524\n",
      "Iteration 33930 => Loss: 6.70031513862739469545\n",
      "Iteration 33931 => Loss: 6.70031485063164389260\n",
      "Iteration 33932 => Loss: 6.70031456267470648669\n",
      "Iteration 33933 => Loss: 6.70031427475656471415\n",
      "Iteration 33934 => Loss: 6.70031398687722479224\n",
      "Iteration 33935 => Loss: 6.70031369903666806920\n",
      "Iteration 33936 => Loss: 6.70031341123490520317\n",
      "Iteration 33937 => Loss: 6.70031312347192287149\n",
      "Iteration 33938 => Loss: 6.70031283574771574507\n",
      "Iteration 33939 => Loss: 6.70031254806227583032\n",
      "Iteration 33940 => Loss: 6.70031226041560934448\n",
      "Iteration 33941 => Loss: 6.70031197280770207669\n",
      "Iteration 33942 => Loss: 6.70031168523854780972\n",
      "Iteration 33943 => Loss: 6.70031139770813410905\n",
      "Iteration 33944 => Loss: 6.70031111021647607373\n",
      "Iteration 33945 => Loss: 6.70031082276355949290\n",
      "Iteration 33946 => Loss: 6.70031053534937459659\n",
      "Iteration 33947 => Loss: 6.70031024797391605574\n",
      "Iteration 33948 => Loss: 6.70030996063718120581\n",
      "Iteration 33949 => Loss: 6.70030967333916294137\n",
      "Iteration 33950 => Loss: 6.70030938607986925604\n",
      "Iteration 33951 => Loss: 6.70030909885926995173\n",
      "Iteration 33952 => Loss: 6.70030881167738368021\n",
      "Iteration 33953 => Loss: 6.70030852453418734882\n",
      "Iteration 33954 => Loss: 6.70030823742968717482\n",
      "Iteration 33955 => Loss: 6.70030795036387694097\n",
      "Iteration 33956 => Loss: 6.70030766333674510093\n",
      "Iteration 33957 => Loss: 6.70030737634829343108\n",
      "Iteration 33958 => Loss: 6.70030708939851304962\n",
      "Iteration 33959 => Loss: 6.70030680248739418658\n",
      "Iteration 33960 => Loss: 6.70030651561493861834\n",
      "Iteration 33961 => Loss: 6.70030622878114723306\n",
      "Iteration 33962 => Loss: 6.70030594198599871447\n",
      "Iteration 33963 => Loss: 6.70030565522949661528\n",
      "Iteration 33964 => Loss: 6.70030536851163560641\n",
      "Iteration 33965 => Loss: 6.70030508183241391151\n",
      "Iteration 33966 => Loss: 6.70030479519181554338\n",
      "Iteration 33967 => Loss: 6.70030450858984316653\n",
      "Iteration 33968 => Loss: 6.70030422202648967556\n",
      "Iteration 33969 => Loss: 6.70030393550175418227\n",
      "Iteration 33970 => Loss: 6.70030364901562958124\n",
      "Iteration 33971 => Loss: 6.70030336256810343798\n",
      "Iteration 33972 => Loss: 6.70030307615918285791\n",
      "Iteration 33973 => Loss: 6.70030278978885363017\n",
      "Iteration 33974 => Loss: 6.70030250345711131388\n",
      "Iteration 33975 => Loss: 6.70030221716394525089\n",
      "Iteration 33976 => Loss: 6.70030193090936965206\n",
      "Iteration 33977 => Loss: 6.70030164469336408928\n",
      "Iteration 33978 => Loss: 6.70030135851591879259\n",
      "Iteration 33979 => Loss: 6.70030107237703997924\n",
      "Iteration 33980 => Loss: 6.70030078627672320835\n",
      "Iteration 33981 => Loss: 6.70030050021494805179\n",
      "Iteration 33982 => Loss: 6.70030021419172872044\n",
      "Iteration 33983 => Loss: 6.70029992820704656253\n",
      "Iteration 33984 => Loss: 6.70029964226090335444\n",
      "Iteration 33985 => Loss: 6.70029935635329199073\n",
      "Iteration 33986 => Loss: 6.70029907048420447779\n",
      "Iteration 33987 => Loss: 6.70029878465363726292\n",
      "Iteration 33988 => Loss: 6.70029849886158856975\n",
      "Iteration 33989 => Loss: 6.70029821310804329926\n",
      "Iteration 33990 => Loss: 6.70029792739301033322\n",
      "Iteration 33991 => Loss: 6.70029764171647812532\n",
      "Iteration 33992 => Loss: 6.70029735607843779377\n",
      "Iteration 33993 => Loss: 6.70029707047889644400\n",
      "Iteration 33994 => Loss: 6.70029678491783098337\n",
      "Iteration 33995 => Loss: 6.70029649939524141189\n",
      "Iteration 33996 => Loss: 6.70029621391112772955\n",
      "Iteration 33997 => Loss: 6.70029592846548371909\n",
      "Iteration 33998 => Loss: 6.70029564305830493964\n",
      "Iteration 33999 => Loss: 6.70029535768958783848\n",
      "Iteration 34000 => Loss: 6.70029507235932264564\n",
      "Iteration 34001 => Loss: 6.70029478706750047934\n",
      "Iteration 34002 => Loss: 6.70029450181412755683\n",
      "Iteration 34003 => Loss: 6.70029421659918966725\n",
      "Iteration 34004 => Loss: 6.70029393142268236971\n",
      "Iteration 34005 => Loss: 6.70029364628460832876\n",
      "Iteration 34006 => Loss: 6.70029336118495155716\n",
      "Iteration 34007 => Loss: 6.70029307612372004854\n",
      "Iteration 34008 => Loss: 6.70029279110089337479\n",
      "Iteration 34009 => Loss: 6.70029250611647420044\n",
      "Iteration 34010 => Loss: 6.70029222117045897278\n",
      "Iteration 34011 => Loss: 6.70029193626283881002\n",
      "Iteration 34012 => Loss: 6.70029165139361104764\n",
      "Iteration 34013 => Loss: 6.70029136656276858020\n",
      "Iteration 34014 => Loss: 6.70029108177030074955\n",
      "Iteration 34015 => Loss: 6.70029079701621910203\n",
      "Iteration 34016 => Loss: 6.70029051230050320953\n",
      "Iteration 34017 => Loss: 6.70029022762315662476\n",
      "Iteration 34018 => Loss: 6.70028994298415891961\n",
      "Iteration 34019 => Loss: 6.70028965838352608131\n",
      "Iteration 34020 => Loss: 6.70028937382124567534\n",
      "Iteration 34021 => Loss: 6.70028908929730437904\n",
      "Iteration 34022 => Loss: 6.70028880481170130423\n",
      "Iteration 34023 => Loss: 6.70028852036443822726\n",
      "Iteration 34024 => Loss: 6.70028823595550537817\n",
      "Iteration 34025 => Loss: 6.70028795158488765793\n",
      "Iteration 34026 => Loss: 6.70028766725260194193\n",
      "Iteration 34027 => Loss: 6.70028738295861714391\n",
      "Iteration 34028 => Loss: 6.70028709870295280382\n",
      "Iteration 34029 => Loss: 6.70028681448557783540\n",
      "Iteration 34030 => Loss: 6.70028653030650556133\n",
      "Iteration 34031 => Loss: 6.70028624616573509343\n",
      "Iteration 34032 => Loss: 6.70028596206324955631\n",
      "Iteration 34033 => Loss: 6.70028567799903918001\n",
      "Iteration 34034 => Loss: 6.70028539397311995174\n",
      "Iteration 34035 => Loss: 6.70028510998545989708\n",
      "Iteration 34036 => Loss: 6.70028482603606878598\n",
      "Iteration 34037 => Loss: 6.70028454212494750664\n",
      "Iteration 34038 => Loss: 6.70028425825207918365\n",
      "Iteration 34039 => Loss: 6.70028397441746115248\n",
      "Iteration 34040 => Loss: 6.70028369062109163679\n",
      "Iteration 34041 => Loss: 6.70028340686296086659\n",
      "Iteration 34042 => Loss: 6.70028312314307061826\n",
      "Iteration 34043 => Loss: 6.70028283946141645089\n",
      "Iteration 34044 => Loss: 6.70028255581797793639\n",
      "Iteration 34045 => Loss: 6.70028227221276484471\n",
      "Iteration 34046 => Loss: 6.70028198864576651772\n",
      "Iteration 34047 => Loss: 6.70028170511697940270\n",
      "Iteration 34048 => Loss: 6.70028142162639728241\n",
      "Iteration 34049 => Loss: 6.70028113817401749230\n",
      "Iteration 34050 => Loss: 6.70028085475983559149\n",
      "Iteration 34051 => Loss: 6.70028057138383381641\n",
      "Iteration 34052 => Loss: 6.70028028804602993063\n",
      "Iteration 34053 => Loss: 6.70028000474639373607\n",
      "Iteration 34054 => Loss: 6.70027972148493855542\n",
      "Iteration 34055 => Loss: 6.70027943826165461871\n",
      "Iteration 34056 => Loss: 6.70027915507653126781\n",
      "Iteration 34057 => Loss: 6.70027887192956850271\n",
      "Iteration 34058 => Loss: 6.70027858882075744162\n",
      "Iteration 34059 => Loss: 6.70027830575009897274\n",
      "Iteration 34060 => Loss: 6.70027802271758243791\n",
      "Iteration 34061 => Loss: 6.70027773972320428442\n",
      "Iteration 34062 => Loss: 6.70027745676695563048\n",
      "Iteration 34063 => Loss: 6.70027717384884091700\n",
      "Iteration 34064 => Loss: 6.70027689096885215037\n",
      "Iteration 34065 => Loss: 6.70027660812697600790\n",
      "Iteration 34066 => Loss: 6.70027632532321426595\n",
      "Iteration 34067 => Loss: 6.70027604255756603635\n",
      "Iteration 34068 => Loss: 6.70027575983001000282\n",
      "Iteration 34069 => Loss: 6.70027547714055504713\n",
      "Iteration 34070 => Loss: 6.70027519448919672840\n",
      "Iteration 34071 => Loss: 6.70027491187592350030\n",
      "Iteration 34072 => Loss: 6.70027462930073269831\n",
      "Iteration 34073 => Loss: 6.70027434676361632881\n",
      "Iteration 34074 => Loss: 6.70027406426457350364\n",
      "Iteration 34075 => Loss: 6.70027378180360155824\n",
      "Iteration 34076 => Loss: 6.70027349938068450541\n",
      "Iteration 34077 => Loss: 6.70027321699582767422\n",
      "Iteration 34078 => Loss: 6.70027293464902129472\n",
      "Iteration 34079 => Loss: 6.70027265234026270235\n",
      "Iteration 34080 => Loss: 6.70027237006954123899\n",
      "Iteration 34081 => Loss: 6.70027208783685956917\n",
      "Iteration 34082 => Loss: 6.70027180564220437020\n",
      "Iteration 34083 => Loss: 6.70027152348558097117\n",
      "Iteration 34084 => Loss: 6.70027124136697427303\n",
      "Iteration 34085 => Loss: 6.70027095928638249944\n",
      "Iteration 34086 => Loss: 6.70027067724380565039\n",
      "Iteration 34087 => Loss: 6.70027039523923306774\n",
      "Iteration 34088 => Loss: 6.70027011327265675789\n",
      "Iteration 34089 => Loss: 6.70026983134408116172\n",
      "Iteration 34090 => Loss: 6.70026954945348318660\n",
      "Iteration 34091 => Loss: 6.70026926760088681334\n",
      "Iteration 34092 => Loss: 6.70026898578625651481\n",
      "Iteration 34093 => Loss: 6.70026870400960383733\n",
      "Iteration 34094 => Loss: 6.70026842227093144544\n",
      "Iteration 34095 => Loss: 6.70026814057020736470\n",
      "Iteration 34096 => Loss: 6.70026785890744847052\n",
      "Iteration 34097 => Loss: 6.70026757728265121017\n",
      "Iteration 34098 => Loss: 6.70026729569579515555\n",
      "Iteration 34099 => Loss: 6.70026701414688563574\n",
      "Iteration 34100 => Loss: 6.70026673263591110441\n",
      "Iteration 34101 => Loss: 6.70026645116287511428\n",
      "Iteration 34102 => Loss: 6.70026616972776434267\n",
      "Iteration 34103 => Loss: 6.70026588833057434869\n",
      "Iteration 34104 => Loss: 6.70026560697130779687\n",
      "Iteration 34105 => Loss: 6.70026532564995935815\n",
      "Iteration 34106 => Loss: 6.70026504436650771623\n",
      "Iteration 34107 => Loss: 6.70026476312096441745\n",
      "Iteration 34108 => Loss: 6.70026448191331969184\n",
      "Iteration 34109 => Loss: 6.70026420074356288126\n",
      "Iteration 34110 => Loss: 6.70026391961170197931\n",
      "Iteration 34111 => Loss: 6.70026363851771744606\n",
      "Iteration 34112 => Loss: 6.70026335746161194606\n",
      "Iteration 34113 => Loss: 6.70026307644338015024\n",
      "Iteration 34114 => Loss: 6.70026279546301051226\n",
      "Iteration 34115 => Loss: 6.70026251452050480850\n",
      "Iteration 34116 => Loss: 6.70026223361586303895\n",
      "Iteration 34117 => Loss: 6.70026195274906566368\n",
      "Iteration 34118 => Loss: 6.70026167192011889995\n",
      "Iteration 34119 => Loss: 6.70026139112900942507\n",
      "Iteration 34120 => Loss: 6.70026111037574345630\n",
      "Iteration 34121 => Loss: 6.70026082966030411825\n",
      "Iteration 34122 => Loss: 6.70026054898268963456\n",
      "Iteration 34123 => Loss: 6.70026026834290355794\n",
      "Iteration 34124 => Loss: 6.70025998774092190757\n",
      "Iteration 34125 => Loss: 6.70025970717676599975\n",
      "Iteration 34126 => Loss: 6.70025942665040830093\n",
      "Iteration 34127 => Loss: 6.70025914616184969930\n",
      "Iteration 34128 => Loss: 6.70025886571108930667\n",
      "Iteration 34129 => Loss: 6.70025858529811912945\n",
      "Iteration 34130 => Loss: 6.70025830492293561491\n",
      "Iteration 34131 => Loss: 6.70025802458553521035\n",
      "Iteration 34132 => Loss: 6.70025774428590636944\n",
      "Iteration 34133 => Loss: 6.70025746402404820401\n",
      "Iteration 34134 => Loss: 6.70025718379995716134\n",
      "Iteration 34135 => Loss: 6.70025690361362524783\n",
      "Iteration 34136 => Loss: 6.70025662346504002898\n",
      "Iteration 34137 => Loss: 6.70025634335422104471\n",
      "Iteration 34138 => Loss: 6.70025606328114076149\n",
      "Iteration 34139 => Loss: 6.70025578324579562661\n",
      "Iteration 34140 => Loss: 6.70025550324818919279\n",
      "Iteration 34141 => Loss: 6.70025522328831346641\n",
      "Iteration 34142 => Loss: 6.70025494336616223023\n",
      "Iteration 34143 => Loss: 6.70025466348172749065\n",
      "Iteration 34144 => Loss: 6.70025438363500747130\n",
      "Iteration 34145 => Loss: 6.70025410382600039583\n",
      "Iteration 34146 => Loss: 6.70025382405469471792\n",
      "Iteration 34147 => Loss: 6.70025354432108510849\n",
      "Iteration 34148 => Loss: 6.70025326462517778481\n",
      "Iteration 34149 => Loss: 6.70025298496695054240\n",
      "Iteration 34150 => Loss: 6.70025270534641403941\n",
      "Iteration 34151 => Loss: 6.70025242576355051227\n",
      "Iteration 34152 => Loss: 6.70025214621836706641\n",
      "Iteration 34153 => Loss: 6.70025186671085215551\n",
      "Iteration 34154 => Loss: 6.70025158724099600960\n",
      "Iteration 34155 => Loss: 6.70025130780879862868\n",
      "Iteration 34156 => Loss: 6.70025102841426267730\n",
      "Iteration 34157 => Loss: 6.70025074905737039188\n",
      "Iteration 34158 => Loss: 6.70025046973811466700\n",
      "Iteration 34159 => Loss: 6.70025019045650438443\n",
      "Iteration 34160 => Loss: 6.70024991121252799786\n",
      "Iteration 34161 => Loss: 6.70024963200617484915\n",
      "Iteration 34162 => Loss: 6.70024935283745026737\n",
      "Iteration 34163 => Loss: 6.70024907370634181802\n",
      "Iteration 34164 => Loss: 6.70024879461284506021\n",
      "Iteration 34165 => Loss: 6.70024851555695821759\n",
      "Iteration 34166 => Loss: 6.70024823653867152018\n",
      "Iteration 34167 => Loss: 6.70024795755798319163\n",
      "Iteration 34168 => Loss: 6.70024767861488790288\n",
      "Iteration 34169 => Loss: 6.70024739970937766032\n",
      "Iteration 34170 => Loss: 6.70024712084145868118\n",
      "Iteration 34171 => Loss: 6.70024684201110698467\n",
      "Iteration 34172 => Loss: 6.70024656321833322892\n",
      "Iteration 34173 => Loss: 6.70024628446312942032\n",
      "Iteration 34174 => Loss: 6.70024600574548490073\n",
      "Iteration 34175 => Loss: 6.70024572706539611744\n",
      "Iteration 34176 => Loss: 6.70024544842285418866\n",
      "Iteration 34177 => Loss: 6.70024516981786533165\n",
      "Iteration 34178 => Loss: 6.70024489125042332915\n",
      "Iteration 34179 => Loss: 6.70024461272051219396\n",
      "Iteration 34180 => Loss: 6.70024433422813014971\n",
      "Iteration 34181 => Loss: 6.70024405577328341366\n",
      "Iteration 34182 => Loss: 6.70024377735595333405\n",
      "Iteration 34183 => Loss: 6.70024349897613813454\n",
      "Iteration 34184 => Loss: 6.70024322063384136783\n",
      "Iteration 34185 => Loss: 6.70024294232904171764\n",
      "Iteration 34186 => Loss: 6.70024266406175073030\n",
      "Iteration 34187 => Loss: 6.70024238583195685948\n",
      "Iteration 34188 => Loss: 6.70024210763965388793\n",
      "Iteration 34189 => Loss: 6.70024182948483293387\n",
      "Iteration 34190 => Loss: 6.70024155136749577366\n",
      "Iteration 34191 => Loss: 6.70024127328763619005\n",
      "Iteration 34192 => Loss: 6.70024099524524618943\n",
      "Iteration 34193 => Loss: 6.70024071724032133091\n",
      "Iteration 34194 => Loss: 6.70024043927286250266\n",
      "Iteration 34195 => Loss: 6.70024016134285993473\n",
      "Iteration 34196 => Loss: 6.70023988345030652169\n",
      "Iteration 34197 => Loss: 6.70023960559519160540\n",
      "Iteration 34198 => Loss: 6.70023932777753383760\n",
      "Iteration 34199 => Loss: 6.70023904999729857934\n",
      "Iteration 34200 => Loss: 6.70023877225449471240\n",
      "Iteration 34201 => Loss: 6.70023849454911424317\n",
      "Iteration 34202 => Loss: 6.70023821688116871798\n",
      "Iteration 34203 => Loss: 6.70023793925062705057\n",
      "Iteration 34204 => Loss: 6.70023766165750078727\n",
      "Iteration 34205 => Loss: 6.70023738410178371083\n",
      "Iteration 34206 => Loss: 6.70023710658345983404\n",
      "Iteration 34207 => Loss: 6.70023682910253803868\n",
      "Iteration 34208 => Loss: 6.70023655165899700847\n",
      "Iteration 34209 => Loss: 6.70023627425285273063\n",
      "Iteration 34210 => Loss: 6.70023599688407767161\n",
      "Iteration 34211 => Loss: 6.70023571955268426592\n",
      "Iteration 34212 => Loss: 6.70023544225866274360\n",
      "Iteration 34213 => Loss: 6.70023516500201044011\n",
      "Iteration 34214 => Loss: 6.70023488778271225641\n",
      "Iteration 34215 => Loss: 6.70023461060076908069\n",
      "Iteration 34216 => Loss: 6.70023433345617558388\n",
      "Iteration 34217 => Loss: 6.70023405634893443050\n",
      "Iteration 34218 => Loss: 6.70023377927902696882\n",
      "Iteration 34219 => Loss: 6.70023350224645231066\n",
      "Iteration 34220 => Loss: 6.70023322525121223237\n",
      "Iteration 34221 => Loss: 6.70023294829330140487\n",
      "Iteration 34222 => Loss: 6.70023267137270206462\n",
      "Iteration 34223 => Loss: 6.70023239448942398155\n",
      "Iteration 34224 => Loss: 6.70023211764345028030\n",
      "Iteration 34225 => Loss: 6.70023184083478717810\n",
      "Iteration 34226 => Loss: 6.70023156406342312863\n",
      "Iteration 34227 => Loss: 6.70023128732935102647\n",
      "Iteration 34228 => Loss: 6.70023101063257442433\n",
      "Iteration 34229 => Loss: 6.70023073397307555865\n",
      "Iteration 34230 => Loss: 6.70023045735085975849\n",
      "Iteration 34231 => Loss: 6.70023018076591903025\n",
      "Iteration 34232 => Loss: 6.70022990421824626850\n",
      "Iteration 34233 => Loss: 6.70022962770783703235\n",
      "Iteration 34234 => Loss: 6.70022935123469309815\n",
      "Iteration 34235 => Loss: 6.70022907479880824866\n",
      "Iteration 34236 => Loss: 6.70022879840016116759\n",
      "Iteration 34237 => Loss: 6.70022852203876251309\n",
      "Iteration 34238 => Loss: 6.70022824571460429155\n",
      "Iteration 34239 => Loss: 6.70022796942768650297\n",
      "Iteration 34240 => Loss: 6.70022769317799404831\n",
      "Iteration 34241 => Loss: 6.70022741696552337487\n",
      "Iteration 34242 => Loss: 6.70022714079027803535\n",
      "Iteration 34243 => Loss: 6.70022686465224293073\n",
      "Iteration 34244 => Loss: 6.70022658855141717282\n",
      "Iteration 34245 => Loss: 6.70022631248779898527\n",
      "Iteration 34246 => Loss: 6.70022603646137770994\n",
      "Iteration 34247 => Loss: 6.70022576047215423500\n",
      "Iteration 34248 => Loss: 6.70022548452011701414\n",
      "Iteration 34249 => Loss: 6.70022520860525983011\n",
      "Iteration 34250 => Loss: 6.70022493272759600558\n",
      "Iteration 34251 => Loss: 6.70022465688709178977\n",
      "Iteration 34252 => Loss: 6.70022438108376316990\n",
      "Iteration 34253 => Loss: 6.70022410531760570507\n",
      "Iteration 34254 => Loss: 6.70022382958860074353\n",
      "Iteration 34255 => Loss: 6.70022355389675094983\n",
      "Iteration 34256 => Loss: 6.70022327824204744218\n",
      "Iteration 34257 => Loss: 6.70022300262449554964\n",
      "Iteration 34258 => Loss: 6.70022272704408372590\n",
      "Iteration 34259 => Loss: 6.70022245150080042464\n",
      "Iteration 34260 => Loss: 6.70022217599465097493\n",
      "Iteration 34261 => Loss: 6.70022190052562116591\n",
      "Iteration 34262 => Loss: 6.70022162509371010941\n",
      "Iteration 34263 => Loss: 6.70022134969891780543\n",
      "Iteration 34264 => Loss: 6.70022107434123270764\n",
      "Iteration 34265 => Loss: 6.70022079902065215151\n",
      "Iteration 34266 => Loss: 6.70022052373717347251\n",
      "Iteration 34267 => Loss: 6.70022024849079222975\n",
      "Iteration 34268 => Loss: 6.70021997328149332418\n",
      "Iteration 34269 => Loss: 6.70021969810927853217\n",
      "Iteration 34270 => Loss: 6.70021942297414874190\n",
      "Iteration 34271 => Loss: 6.70021914787609507158\n",
      "Iteration 34272 => Loss: 6.70021887281510419854\n",
      "Iteration 34273 => Loss: 6.70021859779117523459\n",
      "Iteration 34274 => Loss: 6.70021832280431706153\n",
      "Iteration 34275 => Loss: 6.70021804785450658670\n",
      "Iteration 34276 => Loss: 6.70021777294174913919\n",
      "Iteration 34277 => Loss: 6.70021749806602517907\n",
      "Iteration 34278 => Loss: 6.70021722322735602262\n",
      "Iteration 34279 => Loss: 6.70021694842571502448\n",
      "Iteration 34280 => Loss: 6.70021667366109774377\n",
      "Iteration 34281 => Loss: 6.70021639893351572681\n",
      "Iteration 34282 => Loss: 6.70021612424294854549\n",
      "Iteration 34283 => Loss: 6.70021584958939264709\n",
      "Iteration 34284 => Loss: 6.70021557497284891980\n",
      "Iteration 34285 => Loss: 6.70021530039330670547\n",
      "Iteration 34286 => Loss: 6.70021502585076600411\n",
      "Iteration 34287 => Loss: 6.70021475134522503936\n",
      "Iteration 34288 => Loss: 6.70021447687666782400\n",
      "Iteration 34289 => Loss: 6.70021420244509791075\n",
      "Iteration 34290 => Loss: 6.70021392805051174690\n",
      "Iteration 34291 => Loss: 6.70021365369289068070\n",
      "Iteration 34292 => Loss: 6.70021337937224448211\n",
      "Iteration 34293 => Loss: 6.70021310508856338117\n",
      "Iteration 34294 => Loss: 6.70021283084183760792\n",
      "Iteration 34295 => Loss: 6.70021255663207337960\n",
      "Iteration 34296 => Loss: 6.70021228245925204448\n",
      "Iteration 34297 => Loss: 6.70021200832338248432\n",
      "Iteration 34298 => Loss: 6.70021173422445048828\n",
      "Iteration 34299 => Loss: 6.70021146016245783272\n",
      "Iteration 34300 => Loss: 6.70021118613738231318\n",
      "Iteration 34301 => Loss: 6.70021091214923902868\n",
      "Iteration 34302 => Loss: 6.70021063819801110384\n",
      "Iteration 34303 => Loss: 6.70021036428371097315\n",
      "Iteration 34304 => Loss: 6.70021009040631110310\n",
      "Iteration 34305 => Loss: 6.70020981656581682273\n",
      "Iteration 34306 => Loss: 6.70020954276222013846\n",
      "Iteration 34307 => Loss: 6.70020926899552282663\n",
      "Iteration 34308 => Loss: 6.70020899526571955818\n",
      "Iteration 34309 => Loss: 6.70020872157279256953\n",
      "Iteration 34310 => Loss: 6.70020844791675163066\n",
      "Iteration 34311 => Loss: 6.70020817429758253070\n",
      "Iteration 34312 => Loss: 6.70020790071528615783\n",
      "Iteration 34313 => Loss: 6.70020762716985363028\n",
      "Iteration 34314 => Loss: 6.70020735366128050714\n",
      "Iteration 34315 => Loss: 6.70020708018956323571\n",
      "Iteration 34316 => Loss: 6.70020680675469826326\n",
      "Iteration 34317 => Loss: 6.70020653335667670802\n",
      "Iteration 34318 => Loss: 6.70020625999549679364\n",
      "Iteration 34319 => Loss: 6.70020598667114875013\n",
      "Iteration 34320 => Loss: 6.70020571338363435387\n",
      "Iteration 34321 => Loss: 6.70020544013294916397\n",
      "Iteration 34322 => Loss: 6.70020516691907808138\n",
      "Iteration 34323 => Loss: 6.70020489374201932975\n",
      "Iteration 34324 => Loss: 6.70020462060178090269\n",
      "Iteration 34325 => Loss: 6.70020434749834858934\n",
      "Iteration 34326 => Loss: 6.70020407443171173156\n",
      "Iteration 34327 => Loss: 6.70020380140187121754\n",
      "Iteration 34328 => Loss: 6.70020352840882171819\n",
      "Iteration 34329 => Loss: 6.70020325545255435173\n",
      "Iteration 34330 => Loss: 6.70020298253307533543\n",
      "Iteration 34331 => Loss: 6.70020270965036779387\n",
      "Iteration 34332 => Loss: 6.70020243680443350343\n",
      "Iteration 34333 => Loss: 6.70020216399525736506\n",
      "Iteration 34334 => Loss: 6.70020189122285181327\n",
      "Iteration 34335 => Loss: 6.70020161848719730813\n",
      "Iteration 34336 => Loss: 6.70020134578829473782\n",
      "Iteration 34337 => Loss: 6.70020107312614676687\n",
      "Iteration 34338 => Loss: 6.70020080050072674993\n",
      "Iteration 34339 => Loss: 6.70020052791204978604\n",
      "Iteration 34340 => Loss: 6.70020025536010077616\n",
      "Iteration 34341 => Loss: 6.70019998284488860207\n",
      "Iteration 34342 => Loss: 6.70019971036638750661\n",
      "Iteration 34343 => Loss: 6.70019943792460637155\n",
      "Iteration 34344 => Loss: 6.70019916551953453876\n",
      "Iteration 34345 => Loss: 6.70019889315117023187\n",
      "Iteration 34346 => Loss: 6.70019862081951078636\n",
      "Iteration 34347 => Loss: 6.70019834852453577412\n",
      "Iteration 34348 => Loss: 6.70019807626626029418\n",
      "Iteration 34349 => Loss: 6.70019780404467901747\n",
      "Iteration 34350 => Loss: 6.70019753185977240406\n",
      "Iteration 34351 => Loss: 6.70019725971154489486\n",
      "Iteration 34352 => Loss: 6.70019698759998583171\n",
      "Iteration 34353 => Loss: 6.70019671552509965551\n",
      "Iteration 34354 => Loss: 6.70019644348687215540\n",
      "Iteration 34355 => Loss: 6.70019617148530244322\n",
      "Iteration 34356 => Loss: 6.70019589952038607805\n",
      "Iteration 34357 => Loss: 6.70019562759211861902\n",
      "Iteration 34358 => Loss: 6.70019535570048585527\n",
      "Iteration 34359 => Loss: 6.70019508384549755675\n",
      "Iteration 34360 => Loss: 6.70019481202713507173\n",
      "Iteration 34361 => Loss: 6.70019454024540817016\n",
      "Iteration 34362 => Loss: 6.70019426850029908849\n",
      "Iteration 34363 => Loss: 6.70019399679180960305\n",
      "Iteration 34364 => Loss: 6.70019372511992727937\n",
      "Iteration 34365 => Loss: 6.70019345348466011103\n",
      "Iteration 34366 => Loss: 6.70019318188599122266\n",
      "Iteration 34367 => Loss: 6.70019291032392505514\n",
      "Iteration 34368 => Loss: 6.70019263879844739762\n",
      "Iteration 34369 => Loss: 6.70019236730956535553\n",
      "Iteration 34370 => Loss: 6.70019209585725938894\n",
      "Iteration 34371 => Loss: 6.70019182444152949785\n",
      "Iteration 34372 => Loss: 6.70019155306237923497\n",
      "Iteration 34373 => Loss: 6.70019128171979083675\n",
      "Iteration 34374 => Loss: 6.70019101041377229677\n",
      "Iteration 34375 => Loss: 6.70019073914430940420\n",
      "Iteration 34376 => Loss: 6.70019046791140127084\n",
      "Iteration 34377 => Loss: 6.70019019671503901492\n",
      "Iteration 34378 => Loss: 6.70018992555522174825\n",
      "Iteration 34379 => Loss: 6.70018965443194591813\n",
      "Iteration 34380 => Loss: 6.70018938334520530731\n",
      "Iteration 34381 => Loss: 6.70018911229498392856\n",
      "Iteration 34382 => Loss: 6.70018884128128799915\n",
      "Iteration 34383 => Loss: 6.70018857030412462450\n",
      "Iteration 34384 => Loss: 6.70018829936346271836\n",
      "Iteration 34385 => Loss: 6.70018802845931737977\n",
      "Iteration 34386 => Loss: 6.70018775759166995698\n",
      "Iteration 34387 => Loss: 6.70018748676052666724\n",
      "Iteration 34388 => Loss: 6.70018721596587418787\n",
      "Iteration 34389 => Loss: 6.70018694520771518341\n",
      "Iteration 34390 => Loss: 6.70018667448603810755\n",
      "Iteration 34391 => Loss: 6.70018640380084118391\n",
      "Iteration 34392 => Loss: 6.70018613315211819526\n",
      "Iteration 34393 => Loss: 6.70018586253986647705\n",
      "Iteration 34394 => Loss: 6.70018559196408336476\n",
      "Iteration 34395 => Loss: 6.70018532142475464752\n",
      "Iteration 34396 => Loss: 6.70018505092188298988\n",
      "Iteration 34397 => Loss: 6.70018478045546395094\n",
      "Iteration 34398 => Loss: 6.70018451002548864892\n",
      "Iteration 34399 => Loss: 6.70018423963195353110\n",
      "Iteration 34400 => Loss: 6.70018396927484971570\n",
      "Iteration 34401 => Loss: 6.70018369895418608451\n",
      "Iteration 34402 => Loss: 6.70018342866994309759\n",
      "Iteration 34403 => Loss: 6.70018315842211720224\n",
      "Iteration 34404 => Loss: 6.70018288821071106298\n",
      "Iteration 34405 => Loss: 6.70018261803571935076\n",
      "Iteration 34406 => Loss: 6.70018234789713229560\n",
      "Iteration 34407 => Loss: 6.70018207779494368026\n",
      "Iteration 34408 => Loss: 6.70018180772914551113\n",
      "Iteration 34409 => Loss: 6.70018153769974578182\n",
      "Iteration 34410 => Loss: 6.70018126770673294601\n",
      "Iteration 34411 => Loss: 6.70018099775010345098\n",
      "Iteration 34412 => Loss: 6.70018072782984575042\n",
      "Iteration 34413 => Loss: 6.70018045794595984432\n",
      "Iteration 34414 => Loss: 6.70018018809844129180\n",
      "Iteration 34415 => Loss: 6.70017991828728831649\n",
      "Iteration 34416 => Loss: 6.70017964851249203662\n",
      "Iteration 34417 => Loss: 6.70017937877404357039\n",
      "Iteration 34418 => Loss: 6.70017910907194291781\n",
      "Iteration 34419 => Loss: 6.70017883940619007888\n",
      "Iteration 34420 => Loss: 6.70017856977677617181\n",
      "Iteration 34421 => Loss: 6.70017830018368965028\n",
      "Iteration 34422 => Loss: 6.70017803062692607341\n",
      "Iteration 34423 => Loss: 6.70017776110649254662\n",
      "Iteration 34424 => Loss: 6.70017749162237485905\n",
      "Iteration 34425 => Loss: 6.70017722217457034617\n",
      "Iteration 34426 => Loss: 6.70017695276307279073\n",
      "Iteration 34427 => Loss: 6.70017668338787775184\n",
      "Iteration 34428 => Loss: 6.70017641404898345314\n",
      "Iteration 34429 => Loss: 6.70017614474638367739\n",
      "Iteration 34430 => Loss: 6.70017587548006776643\n",
      "Iteration 34431 => Loss: 6.70017560625004104935\n",
      "Iteration 34432 => Loss: 6.70017533705628931529\n",
      "Iteration 34433 => Loss: 6.70017506789881256424\n",
      "Iteration 34434 => Loss: 6.70017479877760635532\n",
      "Iteration 34435 => Loss: 6.70017452969266535945\n",
      "Iteration 34436 => Loss: 6.70017426064397447760\n",
      "Iteration 34437 => Loss: 6.70017399163154614428\n",
      "Iteration 34438 => Loss: 6.70017372265536703679\n",
      "Iteration 34439 => Loss: 6.70017345371543093790\n",
      "Iteration 34440 => Loss: 6.70017318481173607125\n",
      "Iteration 34441 => Loss: 6.70017291594426822599\n",
      "Iteration 34442 => Loss: 6.70017264711303628388\n",
      "Iteration 34443 => Loss: 6.70017237831803136316\n",
      "Iteration 34444 => Loss: 6.70017210955924458204\n",
      "Iteration 34445 => Loss: 6.70017184083667149963\n",
      "Iteration 34446 => Loss: 6.70017157215031478046\n",
      "Iteration 34447 => Loss: 6.70017130350015932549\n",
      "Iteration 34448 => Loss: 6.70017103488620691110\n",
      "Iteration 34449 => Loss: 6.70017076630844687912\n",
      "Iteration 34450 => Loss: 6.70017049776687567686\n",
      "Iteration 34451 => Loss: 6.70017022926149241613\n",
      "Iteration 34452 => Loss: 6.70016996079229443239\n",
      "Iteration 34453 => Loss: 6.70016969235927106752\n",
      "Iteration 34454 => Loss: 6.70016942396241699242\n",
      "Iteration 34455 => Loss: 6.70016915560172865440\n",
      "Iteration 34456 => Loss: 6.70016888727720516528\n",
      "Iteration 34457 => Loss: 6.70016861898883142601\n",
      "Iteration 34458 => Loss: 6.70016835073662164746\n",
      "Iteration 34459 => Loss: 6.70016808252054918427\n",
      "Iteration 34460 => Loss: 6.70016781434062469458\n",
      "Iteration 34461 => Loss: 6.70016754619683485572\n",
      "Iteration 34462 => Loss: 6.70016727808917433862\n",
      "Iteration 34463 => Loss: 6.70016701001764047874\n",
      "Iteration 34464 => Loss: 6.70016674198224038150\n",
      "Iteration 34465 => Loss: 6.70016647398295095428\n",
      "Iteration 34466 => Loss: 6.70016620601977042071\n",
      "Iteration 34467 => Loss: 6.70016593809269878079\n",
      "Iteration 34468 => Loss: 6.70016567020173692271\n",
      "Iteration 34469 => Loss: 6.70016540234686264199\n",
      "Iteration 34470 => Loss: 6.70016513452809370222\n",
      "Iteration 34471 => Loss: 6.70016486674541145163\n",
      "Iteration 34472 => Loss: 6.70016459899880878481\n",
      "Iteration 34473 => Loss: 6.70016433128828392540\n",
      "Iteration 34474 => Loss: 6.70016406361383687340\n",
      "Iteration 34475 => Loss: 6.70016379597545252977\n",
      "Iteration 34476 => Loss: 6.70016352837314244084\n",
      "Iteration 34477 => Loss: 6.70016326080688351396\n",
      "Iteration 34478 => Loss: 6.70016299327668374275\n",
      "Iteration 34479 => Loss: 6.70016272578253335723\n",
      "Iteration 34480 => Loss: 6.70016245832441814656\n",
      "Iteration 34481 => Loss: 6.70016219090235942701\n",
      "Iteration 34482 => Loss: 6.70016192351632788871\n",
      "Iteration 34483 => Loss: 6.70016165616632175528\n",
      "Iteration 34484 => Loss: 6.70016138885234724398\n",
      "Iteration 34485 => Loss: 6.70016112157439369668\n",
      "Iteration 34486 => Loss: 6.70016085433245667247\n",
      "Iteration 34487 => Loss: 6.70016058712652906593\n",
      "Iteration 34488 => Loss: 6.70016031995660465981\n",
      "Iteration 34489 => Loss: 6.70016005282267901322\n",
      "Iteration 34490 => Loss: 6.70015978572475390251\n",
      "Iteration 34491 => Loss: 6.70015951866282311045\n",
      "Iteration 34492 => Loss: 6.70015925163687686705\n",
      "Iteration 34493 => Loss: 6.70015898464690895509\n",
      "Iteration 34494 => Loss: 6.70015871769292115090\n",
      "Iteration 34495 => Loss: 6.70015845077490812542\n",
      "Iteration 34496 => Loss: 6.70015818389285833234\n",
      "Iteration 34497 => Loss: 6.70015791704676821894\n",
      "Iteration 34498 => Loss: 6.70015765023664222610\n",
      "Iteration 34499 => Loss: 6.70015738346246703117\n",
      "Iteration 34500 => Loss: 6.70015711672424174594\n",
      "Iteration 34501 => Loss: 6.70015685002195393594\n",
      "Iteration 34502 => Loss: 6.70015658335560360115\n",
      "Iteration 34503 => Loss: 6.70015631672518896522\n",
      "Iteration 34504 => Loss: 6.70015605013070025819\n",
      "Iteration 34505 => Loss: 6.70015578357214458549\n",
      "Iteration 34506 => Loss: 6.70015551704949618994\n",
      "Iteration 34507 => Loss: 6.70015525056277017057\n",
      "Iteration 34508 => Loss: 6.70015498411195142836\n",
      "Iteration 34509 => Loss: 6.70015471769703196969\n",
      "Iteration 34510 => Loss: 6.70015445131801268275\n",
      "Iteration 34511 => Loss: 6.70015418497489623206\n",
      "Iteration 34512 => Loss: 6.70015391866766130136\n",
      "Iteration 34513 => Loss: 6.70015365239631233152\n",
      "Iteration 34514 => Loss: 6.70015338616085021073\n",
      "Iteration 34515 => Loss: 6.70015311996125451088\n",
      "Iteration 34516 => Loss: 6.70015285379753144923\n",
      "Iteration 34517 => Loss: 6.70015258766967836124\n",
      "Iteration 34518 => Loss: 6.70015232157768370058\n",
      "Iteration 34519 => Loss: 6.70015205552154391455\n",
      "Iteration 34520 => Loss: 6.70015178950125900315\n",
      "Iteration 34521 => Loss: 6.70015152351681830822\n",
      "Iteration 34522 => Loss: 6.70015125756821205982\n",
      "Iteration 34523 => Loss: 6.70015099165544913973\n",
      "Iteration 34524 => Loss: 6.70015072577852155433\n",
      "Iteration 34525 => Loss: 6.70015045993741242825\n",
      "Iteration 34526 => Loss: 6.70015019413213419597\n",
      "Iteration 34527 => Loss: 6.70014992836266198850\n",
      "Iteration 34528 => Loss: 6.70014966262901001670\n",
      "Iteration 34529 => Loss: 6.70014939693116495789\n",
      "Iteration 34530 => Loss: 6.70014913126912325936\n",
      "Iteration 34531 => Loss: 6.70014886564287692750\n",
      "Iteration 34532 => Loss: 6.70014860005242862684\n",
      "Iteration 34533 => Loss: 6.70014833449776237018\n",
      "Iteration 34534 => Loss: 6.70014806897888082204\n",
      "Iteration 34535 => Loss: 6.70014780349578131791\n",
      "Iteration 34536 => Loss: 6.70014753804845231144\n",
      "Iteration 34537 => Loss: 6.70014727263689291448\n",
      "Iteration 34538 => Loss: 6.70014700726110490336\n",
      "Iteration 34539 => Loss: 6.70014674192106429729\n",
      "Iteration 34540 => Loss: 6.70014647661678619528\n",
      "Iteration 34541 => Loss: 6.70014621134825461013\n",
      "Iteration 34542 => Loss: 6.70014594611547309455\n",
      "Iteration 34543 => Loss: 6.70014568091842654951\n",
      "Iteration 34544 => Loss: 6.70014541575711497501\n",
      "Iteration 34545 => Loss: 6.70014515063153570651\n",
      "Iteration 34546 => Loss: 6.70014488554168696766\n",
      "Iteration 34547 => Loss: 6.70014462048755543577\n",
      "Iteration 34548 => Loss: 6.70014435546913400543\n",
      "Iteration 34549 => Loss: 6.70014409048642622935\n",
      "Iteration 34550 => Loss: 6.70014382553942944298\n",
      "Iteration 34551 => Loss: 6.70014356062813387638\n",
      "Iteration 34552 => Loss: 6.70014329575253153592\n",
      "Iteration 34553 => Loss: 6.70014303091262330980\n",
      "Iteration 34554 => Loss: 6.70014276610840386894\n",
      "Iteration 34555 => Loss: 6.70014250133986077884\n",
      "Iteration 34556 => Loss: 6.70014223660700469765\n",
      "Iteration 34557 => Loss: 6.70014197190981430907\n",
      "Iteration 34558 => Loss: 6.70014170724829227765\n",
      "Iteration 34559 => Loss: 6.70014144262243416250\n",
      "Iteration 34560 => Loss: 6.70014117803223641090\n",
      "Iteration 34561 => Loss: 6.70014091347769014106\n",
      "Iteration 34562 => Loss: 6.70014064895879268846\n",
      "Iteration 34563 => Loss: 6.70014038447553517130\n",
      "Iteration 34564 => Loss: 6.70014012002792558320\n",
      "Iteration 34565 => Loss: 6.70013985561594349605\n",
      "Iteration 34566 => Loss: 6.70013959123959068620\n",
      "Iteration 34567 => Loss: 6.70013932689887159455\n",
      "Iteration 34568 => Loss: 6.70013906259376135210\n",
      "Iteration 34569 => Loss: 6.70013879832427505789\n",
      "Iteration 34570 => Loss: 6.70013853409039583653\n",
      "Iteration 34571 => Loss: 6.70013826989212191165\n",
      "Iteration 34572 => Loss: 6.70013800572944884237\n",
      "Iteration 34573 => Loss: 6.70013774160237485233\n",
      "Iteration 34574 => Loss: 6.70013747751088395432\n",
      "Iteration 34575 => Loss: 6.70013721345498414195\n",
      "Iteration 34576 => Loss: 6.70013694943466298071\n",
      "Iteration 34577 => Loss: 6.70013668544992313514\n",
      "Iteration 34578 => Loss: 6.70013642150075394710\n",
      "Iteration 34579 => Loss: 6.70013615758715275206\n",
      "Iteration 34580 => Loss: 6.70013589370911244458\n",
      "Iteration 34581 => Loss: 6.70013562986662147836\n",
      "Iteration 34582 => Loss: 6.70013536605969761695\n",
      "Iteration 34583 => Loss: 6.70013510228831599136\n",
      "Iteration 34584 => Loss: 6.70013483855247660159\n",
      "Iteration 34585 => Loss: 6.70013457485217589493\n",
      "Iteration 34586 => Loss: 6.70013431118740676595\n",
      "Iteration 34587 => Loss: 6.70013404755817543190\n",
      "Iteration 34588 => Loss: 6.70013378396445968832\n",
      "Iteration 34589 => Loss: 6.70013352040626397610\n",
      "Iteration 34590 => Loss: 6.70013325688358296617\n",
      "Iteration 34591 => Loss: 6.70013299339641843488\n",
      "Iteration 34592 => Loss: 6.70013272994474995414\n",
      "Iteration 34593 => Loss: 6.70013246652858818209\n",
      "Iteration 34594 => Loss: 6.70013220314791624332\n",
      "Iteration 34595 => Loss: 6.70013193980274124328\n",
      "Iteration 34596 => Loss: 6.70013167649304097750\n",
      "Iteration 34597 => Loss: 6.70013141321882610413\n",
      "Iteration 34598 => Loss: 6.70013114998009218226\n",
      "Iteration 34599 => Loss: 6.70013088677683121830\n",
      "Iteration 34600 => Loss: 6.70013062360903077774\n",
      "Iteration 34601 => Loss: 6.70013036047669174877\n",
      "Iteration 34602 => Loss: 6.70013009737981501956\n",
      "Iteration 34603 => Loss: 6.70012983431838815562\n",
      "Iteration 34604 => Loss: 6.70012957129240849241\n",
      "Iteration 34605 => Loss: 6.70012930830187070086\n",
      "Iteration 34606 => Loss: 6.70012904534677034007\n",
      "Iteration 34607 => Loss: 6.70012878242711096277\n",
      "Iteration 34608 => Loss: 6.70012851954287302902\n",
      "Iteration 34609 => Loss: 6.70012825669406009155\n",
      "Iteration 34610 => Loss: 6.70012799388066859763\n",
      "Iteration 34611 => Loss: 6.70012773110268344823\n",
      "Iteration 34612 => Loss: 6.70012746836011441331\n",
      "Iteration 34613 => Loss: 6.70012720565294639385\n",
      "Iteration 34614 => Loss: 6.70012694298117938985\n",
      "Iteration 34615 => Loss: 6.70012668034480896040\n",
      "Iteration 34616 => Loss: 6.70012641774382888826\n",
      "Iteration 34617 => Loss: 6.70012615517823562072\n",
      "Iteration 34618 => Loss: 6.70012589264801849964\n",
      "Iteration 34619 => Loss: 6.70012563015318107773\n",
      "Iteration 34620 => Loss: 6.70012536769371092049\n",
      "Iteration 34621 => Loss: 6.70012510526961158064\n",
      "Iteration 34622 => Loss: 6.70012484288086973550\n",
      "Iteration 34623 => Loss: 6.70012458052748804960\n",
      "Iteration 34624 => Loss: 6.70012431820945941752\n",
      "Iteration 34625 => Loss: 6.70012405592677584565\n",
      "Iteration 34626 => Loss: 6.70012379367943555764\n",
      "Iteration 34627 => Loss: 6.70012353146743677712\n",
      "Iteration 34628 => Loss: 6.70012326929076174054\n",
      "Iteration 34629 => Loss: 6.70012300714942288238\n",
      "Iteration 34630 => Loss: 6.70012274504340066272\n",
      "Iteration 34631 => Loss: 6.70012248297270485153\n",
      "Iteration 34632 => Loss: 6.70012222093732212613\n",
      "Iteration 34633 => Loss: 6.70012195893724360474\n",
      "Iteration 34634 => Loss: 6.70012169697247195188\n",
      "Iteration 34635 => Loss: 6.70012143504300095032\n",
      "Iteration 34636 => Loss: 6.70012117314882793551\n",
      "Iteration 34637 => Loss: 6.70012091128994224931\n",
      "Iteration 34638 => Loss: 6.70012064946633678630\n",
      "Iteration 34639 => Loss: 6.70012038767801776373\n",
      "Iteration 34640 => Loss: 6.70012012592496830621\n",
      "Iteration 34641 => Loss: 6.70011986420720528912\n",
      "Iteration 34642 => Loss: 6.70011960252469407351\n",
      "Iteration 34643 => Loss: 6.70011934087745242294\n",
      "Iteration 34644 => Loss: 6.70011907926546523839\n",
      "Iteration 34645 => Loss: 6.70011881768873074350\n",
      "Iteration 34646 => Loss: 6.70011855614724094465\n",
      "Iteration 34647 => Loss: 6.70011829464099761822\n",
      "Iteration 34648 => Loss: 6.70011803316998921787\n",
      "Iteration 34649 => Loss: 6.70011777173421751996\n",
      "Iteration 34650 => Loss: 6.70011751033367275454\n",
      "Iteration 34651 => Loss: 6.70011724896834515164\n",
      "Iteration 34652 => Loss: 6.70011698763824536940\n",
      "Iteration 34653 => Loss: 6.70011672634335653242\n",
      "Iteration 34654 => Loss: 6.70011646508368130526\n",
      "Iteration 34655 => Loss: 6.70011620385920725340\n",
      "Iteration 34656 => Loss: 6.70011594266993348867\n",
      "Iteration 34657 => Loss: 6.70011568151585823472\n",
      "Iteration 34658 => Loss: 6.70011542039696816886\n",
      "Iteration 34659 => Loss: 6.70011515931326773199\n",
      "Iteration 34660 => Loss: 6.70011489826474448961\n",
      "Iteration 34661 => Loss: 6.70011463725139844172\n",
      "Iteration 34662 => Loss: 6.70011437627322958832\n",
      "Iteration 34663 => Loss: 6.70011411533022016584\n",
      "Iteration 34664 => Loss: 6.70011385442237550336\n",
      "Iteration 34665 => Loss: 6.70011359354968494273\n",
      "Iteration 34666 => Loss: 6.70011333271215292484\n",
      "Iteration 34667 => Loss: 6.70011307190976435066\n",
      "Iteration 34668 => Loss: 6.70011281114252366109\n",
      "Iteration 34669 => Loss: 6.70011255041041486891\n",
      "Iteration 34670 => Loss: 6.70011228971344952043\n",
      "Iteration 34671 => Loss: 6.70011202905160718757\n",
      "Iteration 34672 => Loss: 6.70011176842488698213\n",
      "Iteration 34673 => Loss: 6.70011150783328801595\n",
      "Iteration 34674 => Loss: 6.70011124727680407176\n",
      "Iteration 34675 => Loss: 6.70011098675542982051\n",
      "Iteration 34676 => Loss: 6.70011072626915993311\n",
      "Iteration 34677 => Loss: 6.70011046581799796229\n",
      "Iteration 34678 => Loss: 6.70011020540192614448\n",
      "Iteration 34679 => Loss: 6.70010994502093648606\n",
      "Iteration 34680 => Loss: 6.70010968467504763879\n",
      "Iteration 34681 => Loss: 6.70010942436422851642\n",
      "Iteration 34682 => Loss: 6.70010916408849244164\n",
      "Iteration 34683 => Loss: 6.70010890384783053264\n",
      "Iteration 34684 => Loss: 6.70010864364223301948\n",
      "Iteration 34685 => Loss: 6.70010838347169990215\n",
      "Iteration 34686 => Loss: 6.70010812333622229886\n",
      "Iteration 34687 => Loss: 6.70010786323580109780\n",
      "Iteration 34688 => Loss: 6.70010760317042475265\n",
      "Iteration 34689 => Loss: 6.70010734314009859247\n",
      "Iteration 34690 => Loss: 6.70010708314480130099\n",
      "Iteration 34691 => Loss: 6.70010682318454975359\n",
      "Iteration 34692 => Loss: 6.70010656325932085764\n",
      "Iteration 34693 => Loss: 6.70010630336911727767\n",
      "Iteration 34694 => Loss: 6.70010604351393812550\n",
      "Iteration 34695 => Loss: 6.70010578369376741392\n",
      "Iteration 34696 => Loss: 6.70010552390861224836\n",
      "Iteration 34697 => Loss: 6.70010526415846552339\n",
      "Iteration 34698 => Loss: 6.70010500444331391634\n",
      "Iteration 34699 => Loss: 6.70010474476316542081\n",
      "Iteration 34700 => Loss: 6.70010448511800404958\n",
      "Iteration 34701 => Loss: 6.70010422550782802631\n",
      "Iteration 34702 => Loss: 6.70010396593264090370\n",
      "Iteration 34703 => Loss: 6.70010370639243379998\n",
      "Iteration 34704 => Loss: 6.70010344688719250428\n",
      "Iteration 34705 => Loss: 6.70010318741691879296\n",
      "Iteration 34706 => Loss: 6.70010292798161266603\n",
      "Iteration 34707 => Loss: 6.70010266858126080081\n",
      "Iteration 34708 => Loss: 6.70010240921587207907\n",
      "Iteration 34709 => Loss: 6.70010214988542873726\n",
      "Iteration 34710 => Loss: 6.70010189058992455813\n",
      "Iteration 34711 => Loss: 6.70010163132936487074\n",
      "Iteration 34712 => Loss: 6.70010137210374168149\n",
      "Iteration 34713 => Loss: 6.70010111291304522041\n",
      "Iteration 34714 => Loss: 6.70010085375727992840\n",
      "Iteration 34715 => Loss: 6.70010059463643514732\n",
      "Iteration 34716 => Loss: 6.70010033555050998899\n",
      "Iteration 34717 => Loss: 6.70010007649948757802\n",
      "Iteration 34718 => Loss: 6.70009981748337590801\n",
      "Iteration 34719 => Loss: 6.70009955850216609718\n",
      "Iteration 34720 => Loss: 6.70009929955586081007\n",
      "Iteration 34721 => Loss: 6.70009904064444317129\n",
      "Iteration 34722 => Loss: 6.70009878176790785176\n",
      "Iteration 34723 => Loss: 6.70009852292626550962\n",
      "Iteration 34724 => Loss: 6.70009826411949394043\n",
      "Iteration 34725 => Loss: 6.70009800534760113777\n",
      "Iteration 34726 => Loss: 6.70009774661057999623\n",
      "Iteration 34727 => Loss: 6.70009748790841985766\n",
      "Iteration 34728 => Loss: 6.70009722924112782749\n",
      "Iteration 34729 => Loss: 6.70009697060868347762\n",
      "Iteration 34730 => Loss: 6.70009671201109124894\n",
      "Iteration 34731 => Loss: 6.70009645344834936509\n",
      "Iteration 34732 => Loss: 6.70009619492043917433\n",
      "Iteration 34733 => Loss: 6.70009593642737488750\n",
      "Iteration 34734 => Loss: 6.70009567796913341198\n",
      "Iteration 34735 => Loss: 6.70009541954573162315\n",
      "Iteration 34736 => Loss: 6.70009516115714021112\n",
      "Iteration 34737 => Loss: 6.70009490280337249857\n",
      "Iteration 34738 => Loss: 6.70009464448442138007\n",
      "Iteration 34739 => Loss: 6.70009438620027175659\n",
      "Iteration 34740 => Loss: 6.70009412795092895720\n",
      "Iteration 34741 => Loss: 6.70009386973638854101\n",
      "Iteration 34742 => Loss: 6.70009361155663807352\n",
      "Iteration 34743 => Loss: 6.70009335341167755473\n",
      "Iteration 34744 => Loss: 6.70009309530149987921\n",
      "Iteration 34745 => Loss: 6.70009283722611126421\n",
      "Iteration 34746 => Loss: 6.70009257918549394617\n",
      "Iteration 34747 => Loss: 6.70009232117964703690\n",
      "Iteration 34748 => Loss: 6.70009206320856165462\n",
      "Iteration 34749 => Loss: 6.70009180527224668111\n",
      "Iteration 34750 => Loss: 6.70009154737068612917\n",
      "Iteration 34751 => Loss: 6.70009128950387289336\n",
      "Iteration 34752 => Loss: 6.70009103167180963823\n",
      "Iteration 34753 => Loss: 6.70009077387448837015\n",
      "Iteration 34754 => Loss: 6.70009051611190731279\n",
      "Iteration 34755 => Loss: 6.70009025838406113706\n",
      "Iteration 34756 => Loss: 6.70009000069094096119\n",
      "Iteration 34757 => Loss: 6.70008974303254412064\n",
      "Iteration 34758 => Loss: 6.70008948540887150358\n",
      "Iteration 34759 => Loss: 6.70008922781991067552\n",
      "Iteration 34760 => Loss: 6.70008897026565808375\n",
      "Iteration 34761 => Loss: 6.70008871274611461644\n",
      "Iteration 34762 => Loss: 6.70008845526127405634\n",
      "Iteration 34763 => Loss: 6.70008819781112308078\n",
      "Iteration 34764 => Loss: 6.70008794039566524248\n",
      "Iteration 34765 => Loss: 6.70008768301489965324\n",
      "Iteration 34766 => Loss: 6.70008742566880854952\n",
      "Iteration 34767 => Loss: 6.70008716835739992490\n",
      "Iteration 34768 => Loss: 6.70008691108065956854\n",
      "Iteration 34769 => Loss: 6.70008665383859014497\n",
      "Iteration 34770 => Loss: 6.70008639663118898966\n",
      "Iteration 34771 => Loss: 6.70008613945843745086\n",
      "Iteration 34772 => Loss: 6.70008588232034263399\n",
      "Iteration 34773 => Loss: 6.70008562521690098635\n",
      "Iteration 34774 => Loss: 6.70008536814810007343\n",
      "Iteration 34775 => Loss: 6.70008511111394433613\n",
      "Iteration 34776 => Loss: 6.70008485411441867541\n",
      "Iteration 34777 => Loss: 6.70008459714952664399\n",
      "Iteration 34778 => Loss: 6.70008434021925669555\n",
      "Iteration 34779 => Loss: 6.70008408332360883009\n",
      "Iteration 34780 => Loss: 6.70008382646258038307\n",
      "Iteration 34781 => Loss: 6.70008356963616336088\n",
      "Iteration 34782 => Loss: 6.70008331284435154629\n",
      "Iteration 34783 => Loss: 6.70008305608714582746\n",
      "Iteration 34784 => Loss: 6.70008279936454087533\n",
      "Iteration 34785 => Loss: 6.70008254267652336722\n",
      "Iteration 34786 => Loss: 6.70008228602310040856\n",
      "Iteration 34787 => Loss: 6.70008202940425423577\n",
      "Iteration 34788 => Loss: 6.70008177281999550701\n",
      "Iteration 34789 => Loss: 6.70008151627030645869\n",
      "Iteration 34790 => Loss: 6.70008125975518442630\n",
      "Iteration 34791 => Loss: 6.70008100327463118617\n",
      "Iteration 34792 => Loss: 6.70008074682863696836\n",
      "Iteration 34793 => Loss: 6.70008049041719733196\n",
      "Iteration 34794 => Loss: 6.70008023404031671788\n",
      "Iteration 34795 => Loss: 6.70007997769798091525\n",
      "Iteration 34796 => Loss: 6.70007972139018193047\n",
      "Iteration 34797 => Loss: 6.70007946511692686897\n",
      "Iteration 34798 => Loss: 6.70007920887820240807\n",
      "Iteration 34799 => Loss: 6.70007895267400321870\n",
      "Iteration 34800 => Loss: 6.70007869650432930086\n",
      "Iteration 34801 => Loss: 6.70007844036917177277\n",
      "Iteration 34802 => Loss: 6.70007818426853329896\n",
      "Iteration 34803 => Loss: 6.70007792820240144493\n",
      "Iteration 34804 => Loss: 6.70007767217077798705\n",
      "Iteration 34805 => Loss: 6.70007741617364782627\n",
      "Iteration 34806 => Loss: 6.70007716021101984438\n",
      "Iteration 34807 => Loss: 6.70007690428287894235\n",
      "Iteration 34808 => Loss: 6.70007664838922778472\n",
      "Iteration 34809 => Loss: 6.70007639253005482516\n",
      "Iteration 34810 => Loss: 6.70007613670536006367\n",
      "Iteration 34811 => Loss: 6.70007588091513905937\n",
      "Iteration 34812 => Loss: 6.70007562515938559500\n",
      "Iteration 34813 => Loss: 6.70007536943809878238\n",
      "Iteration 34814 => Loss: 6.70007511375126263431\n",
      "Iteration 34815 => Loss: 6.70007485809888425621\n",
      "Iteration 34816 => Loss: 6.70007460248095298994\n",
      "Iteration 34817 => Loss: 6.70007434689746705914\n",
      "Iteration 34818 => Loss: 6.70007409134842735199\n",
      "Iteration 34819 => Loss: 6.70007383583381699310\n",
      "Iteration 34820 => Loss: 6.70007358035363687065\n",
      "Iteration 34821 => Loss: 6.70007332490788432011\n",
      "Iteration 34822 => Loss: 6.70007306949654957151\n",
      "Iteration 34823 => Loss: 6.70007281411963528939\n",
      "Iteration 34824 => Loss: 6.70007255877713525649\n",
      "Iteration 34825 => Loss: 6.70007230346903881468\n",
      "Iteration 34826 => Loss: 6.70007204819534418760\n",
      "Iteration 34827 => Loss: 6.70007179295605048708\n",
      "Iteration 34828 => Loss: 6.70007153775115327221\n",
      "Iteration 34829 => Loss: 6.70007128258063922033\n",
      "Iteration 34830 => Loss: 6.70007102744450833143\n",
      "Iteration 34831 => Loss: 6.70007077234275794098\n",
      "Iteration 34832 => Loss: 6.70007051727538716079\n",
      "Iteration 34833 => Loss: 6.70007026224238710910\n",
      "Iteration 34834 => Loss: 6.70007000724374979228\n",
      "Iteration 34835 => Loss: 6.70006975227946899309\n",
      "Iteration 34836 => Loss: 6.70006949734954737607\n",
      "Iteration 34837 => Loss: 6.70006924245397872397\n",
      "Iteration 34838 => Loss: 6.70006898759276214861\n",
      "Iteration 34839 => Loss: 6.70006873276588432731\n",
      "Iteration 34840 => Loss: 6.70006847797334259553\n",
      "Iteration 34841 => Loss: 6.70006822321513784146\n",
      "Iteration 34842 => Loss: 6.70006796849125585425\n",
      "Iteration 34843 => Loss: 6.70006771380169929841\n",
      "Iteration 34844 => Loss: 6.70006745914646906215\n",
      "Iteration 34845 => Loss: 6.70006720452554827006\n",
      "Iteration 34846 => Loss: 6.70006694993894136303\n",
      "Iteration 34847 => Loss: 6.70006669538663679475\n",
      "Iteration 34848 => Loss: 6.70006644086863012433\n",
      "Iteration 34849 => Loss: 6.70006618638492401629\n",
      "Iteration 34850 => Loss: 6.70006593193550692433\n",
      "Iteration 34851 => Loss: 6.70006567752038240116\n",
      "Iteration 34852 => Loss: 6.70006542313953534773\n",
      "Iteration 34853 => Loss: 6.70006516879296842859\n",
      "Iteration 34854 => Loss: 6.70006491448067187378\n",
      "Iteration 34855 => Loss: 6.70006466020265101236\n",
      "Iteration 34856 => Loss: 6.70006440595888541623\n",
      "Iteration 34857 => Loss: 6.70006415174937774992\n",
      "Iteration 34858 => Loss: 6.70006389757412978980\n",
      "Iteration 34859 => Loss: 6.70006364343313709497\n",
      "Iteration 34860 => Loss: 6.70006338932638723094\n",
      "Iteration 34861 => Loss: 6.70006313525387398045\n",
      "Iteration 34862 => Loss: 6.70006288121559023807\n",
      "Iteration 34863 => Loss: 6.70006262721155287920\n",
      "Iteration 34864 => Loss: 6.70006237324174414027\n",
      "Iteration 34865 => Loss: 6.70006211930614270500\n",
      "Iteration 34866 => Loss: 6.70006186540476722513\n",
      "Iteration 34867 => Loss: 6.70006161153760437799\n",
      "Iteration 34868 => Loss: 6.70006135770464616996\n",
      "Iteration 34869 => Loss: 6.70006110390590059467\n",
      "Iteration 34870 => Loss: 6.70006085014134988853\n",
      "Iteration 34871 => Loss: 6.70006059641099227520\n",
      "Iteration 34872 => Loss: 6.70006034271482864284\n",
      "Iteration 34873 => Loss: 6.70006008905284833332\n",
      "Iteration 34874 => Loss: 6.70005983542504335304\n",
      "Iteration 34875 => Loss: 6.70005958183142524831\n",
      "Iteration 34876 => Loss: 6.70005932827197003832\n",
      "Iteration 34877 => Loss: 6.70005907474669193391\n",
      "Iteration 34878 => Loss: 6.70005882125556873063\n",
      "Iteration 34879 => Loss: 6.70005856779860398120\n",
      "Iteration 34880 => Loss: 6.70005831437579502108\n",
      "Iteration 34881 => Loss: 6.70005806098713296848\n",
      "Iteration 34882 => Loss: 6.70005780763261427069\n",
      "Iteration 34883 => Loss: 6.70005755431223715135\n",
      "Iteration 34884 => Loss: 6.70005730102599184050\n",
      "Iteration 34885 => Loss: 6.70005704777388366722\n",
      "Iteration 34886 => Loss: 6.70005679455589575610\n",
      "Iteration 34887 => Loss: 6.70005654137202899534\n",
      "Iteration 34888 => Loss: 6.70005628822228160857\n",
      "Iteration 34889 => Loss: 6.70005603510664027311\n",
      "Iteration 34890 => Loss: 6.70005578202511475894\n",
      "Iteration 34891 => Loss: 6.70005552897768819065\n",
      "Iteration 34892 => Loss: 6.70005527596436234461\n",
      "Iteration 34893 => Loss: 6.70005502298512478632\n",
      "Iteration 34894 => Loss: 6.70005477003997551577\n",
      "Iteration 34895 => Loss: 6.70005451712891364480\n",
      "Iteration 34896 => Loss: 6.70005426425193562068\n",
      "Iteration 34897 => Loss: 6.70005401140902723256\n",
      "Iteration 34898 => Loss: 6.70005375860018759226\n",
      "Iteration 34899 => Loss: 6.70005350582542025251\n",
      "Iteration 34900 => Loss: 6.70005325308471544332\n",
      "Iteration 34901 => Loss: 6.70005300037805895386\n",
      "Iteration 34902 => Loss: 6.70005274770545611318\n",
      "Iteration 34903 => Loss: 6.70005249506690425676\n",
      "Iteration 34904 => Loss: 6.70005224246239716734\n",
      "Iteration 34905 => Loss: 6.70005198989192329861\n",
      "Iteration 34906 => Loss: 6.70005173735549419689\n",
      "Iteration 34907 => Loss: 6.70005148485308055228\n",
      "Iteration 34908 => Loss: 6.70005123238469924019\n",
      "Iteration 34909 => Loss: 6.70005097995033516156\n",
      "Iteration 34910 => Loss: 6.70005072754999186913\n",
      "Iteration 34911 => Loss: 6.70005047518365248749\n",
      "Iteration 34912 => Loss: 6.70005022285132323390\n",
      "Iteration 34913 => Loss: 6.70004997055299877928\n",
      "Iteration 34914 => Loss: 6.70004971828867024186\n",
      "Iteration 34915 => Loss: 6.70004946605832607531\n",
      "Iteration 34916 => Loss: 6.70004921386197516142\n",
      "Iteration 34917 => Loss: 6.70004896169961039476\n",
      "Iteration 34918 => Loss: 6.70004870957122289354\n",
      "Iteration 34919 => Loss: 6.70004845747681088142\n",
      "Iteration 34920 => Loss: 6.70004820541636547659\n",
      "Iteration 34921 => Loss: 6.70004795338988756725\n",
      "Iteration 34922 => Loss: 6.70004770139737715340\n",
      "Iteration 34923 => Loss: 6.70004744943881647146\n",
      "Iteration 34924 => Loss: 6.70004719751420285689\n",
      "Iteration 34925 => Loss: 6.70004694562353719789\n",
      "Iteration 34926 => Loss: 6.70004669376681771809\n",
      "Iteration 34927 => Loss: 6.70004644194403109481\n",
      "Iteration 34928 => Loss: 6.70004619015517999259\n",
      "Iteration 34929 => Loss: 6.70004593840025997054\n",
      "Iteration 34930 => Loss: 6.70004568667926037051\n",
      "Iteration 34931 => Loss: 6.70004543499218474523\n",
      "Iteration 34932 => Loss: 6.70004518333901710747\n",
      "Iteration 34933 => Loss: 6.70004493171976722721\n",
      "Iteration 34934 => Loss: 6.70004468013442000540\n",
      "Iteration 34935 => Loss: 6.70004442858297100116\n",
      "Iteration 34936 => Loss: 6.70004417706542110267\n",
      "Iteration 34937 => Loss: 6.70004392558176409267\n",
      "Iteration 34938 => Loss: 6.70004367413199286574\n",
      "Iteration 34939 => Loss: 6.70004342271610919823\n",
      "Iteration 34940 => Loss: 6.70004317133410065566\n",
      "Iteration 34941 => Loss: 6.70004291998596190894\n",
      "Iteration 34942 => Loss: 6.70004266867169384625\n",
      "Iteration 34943 => Loss: 6.70004241739129646760\n",
      "Iteration 34944 => Loss: 6.70004216614474934488\n",
      "Iteration 34945 => Loss: 6.70004191493206402441\n",
      "Iteration 34946 => Loss: 6.70004166375323162441\n",
      "Iteration 34947 => Loss: 6.70004141260824415127\n",
      "Iteration 34948 => Loss: 6.70004116149709272321\n",
      "Iteration 34949 => Loss: 6.70004091041978266929\n",
      "Iteration 34950 => Loss: 6.70004065937630688410\n",
      "Iteration 34951 => Loss: 6.70004040836666003855\n",
      "Iteration 34952 => Loss: 6.70004015739083591541\n",
      "Iteration 34953 => Loss: 6.70003990644882652106\n",
      "Iteration 34954 => Loss: 6.70003965554064162546\n",
      "Iteration 34955 => Loss: 6.70003940466625103056\n",
      "Iteration 34956 => Loss: 6.70003915382568138170\n",
      "Iteration 34957 => Loss: 6.70003890301890336900\n",
      "Iteration 34958 => Loss: 6.70003865224592498606\n",
      "Iteration 34959 => Loss: 6.70003840150673646292\n",
      "Iteration 34960 => Loss: 6.70003815080133158233\n",
      "Iteration 34961 => Loss: 6.70003790012971833789\n",
      "Iteration 34962 => Loss: 6.70003764949187363698\n",
      "Iteration 34963 => Loss: 6.70003739888781346679\n",
      "Iteration 34964 => Loss: 6.70003714831751118197\n",
      "Iteration 34965 => Loss: 6.70003689778097832885\n",
      "Iteration 34966 => Loss: 6.70003664727820513747\n",
      "Iteration 34967 => Loss: 6.70003639680918716692\n",
      "Iteration 34968 => Loss: 6.70003614637391731179\n",
      "Iteration 34969 => Loss: 6.70003589597239646025\n",
      "Iteration 34970 => Loss: 6.70003564560461484234\n",
      "Iteration 34971 => Loss: 6.70003539527057423442\n",
      "Iteration 34972 => Loss: 6.70003514497026753105\n",
      "Iteration 34973 => Loss: 6.70003489470368496228\n",
      "Iteration 34974 => Loss: 6.70003464447082119904\n",
      "Iteration 34975 => Loss: 6.70003439427168245857\n",
      "Iteration 34976 => Loss: 6.70003414410625897091\n",
      "Iteration 34977 => Loss: 6.70003389397454274246\n",
      "Iteration 34978 => Loss: 6.70003364387653554957\n",
      "Iteration 34979 => Loss: 6.70003339381222051685\n",
      "Iteration 34980 => Loss: 6.70003314378160919063\n",
      "Iteration 34981 => Loss: 6.70003289378468469550\n",
      "Iteration 34982 => Loss: 6.70003264382145058420\n",
      "Iteration 34983 => Loss: 6.70003239389189708675\n",
      "Iteration 34984 => Loss: 6.70003214399602509133\n",
      "Iteration 34985 => Loss: 6.70003189413382571615\n",
      "Iteration 34986 => Loss: 6.70003164430529185580\n",
      "Iteration 34987 => Loss: 6.70003139451042262209\n",
      "Iteration 34988 => Loss: 6.70003114474921801502\n",
      "Iteration 34989 => Loss: 6.70003089502166648828\n",
      "Iteration 34990 => Loss: 6.70003064532776093642\n",
      "Iteration 34991 => Loss: 6.70003039566750580036\n",
      "Iteration 34992 => Loss: 6.70003014604089752737\n",
      "Iteration 34993 => Loss: 6.70002989644792101842\n",
      "Iteration 34994 => Loss: 6.70002964688857538533\n",
      "Iteration 34995 => Loss: 6.70002939736285973993\n",
      "Iteration 34996 => Loss: 6.70002914787076964132\n",
      "Iteration 34997 => Loss: 6.70002889841229798407\n",
      "Iteration 34998 => Loss: 6.70002864898744121547\n",
      "Iteration 34999 => Loss: 6.70002839959619311827\n",
      "Iteration 35000 => Loss: 6.70002815023854658705\n",
      "Iteration 35001 => Loss: 6.70002790091450517451\n",
      "Iteration 35002 => Loss: 6.70002765162406710431\n",
      "Iteration 35003 => Loss: 6.70002740236720839562\n",
      "Iteration 35004 => Loss: 6.70002715314394325929\n",
      "Iteration 35005 => Loss: 6.70002690395425659631\n",
      "Iteration 35006 => Loss: 6.70002665479815462390\n",
      "Iteration 35007 => Loss: 6.70002640567562046670\n",
      "Iteration 35008 => Loss: 6.70002615658666123011\n",
      "Iteration 35009 => Loss: 6.70002590753126625600\n",
      "Iteration 35010 => Loss: 6.70002565850943021530\n",
      "Iteration 35011 => Loss: 6.70002540952114689077\n",
      "Iteration 35012 => Loss: 6.70002516056642249964\n",
      "Iteration 35013 => Loss: 6.70002491164523217293\n",
      "Iteration 35014 => Loss: 6.70002466275759811509\n",
      "Iteration 35015 => Loss: 6.70002441390348746353\n",
      "Iteration 35016 => Loss: 6.70002416508292242270\n",
      "Iteration 35017 => Loss: 6.70002391629588167632\n",
      "Iteration 35018 => Loss: 6.70002366754235989532\n",
      "Iteration 35019 => Loss: 6.70002341882236240878\n",
      "Iteration 35020 => Loss: 6.70002317013588388761\n",
      "Iteration 35021 => Loss: 6.70002292148291278551\n",
      "Iteration 35022 => Loss: 6.70002267286344110886\n",
      "Iteration 35023 => Loss: 6.70002242427747596309\n",
      "Iteration 35024 => Loss: 6.70002217572501113096\n",
      "Iteration 35025 => Loss: 6.70002192720603506615\n",
      "Iteration 35026 => Loss: 6.70002167872054421593\n",
      "Iteration 35027 => Loss: 6.70002143026854479757\n",
      "Iteration 35028 => Loss: 6.70002118185001638295\n",
      "Iteration 35029 => Loss: 6.70002093346496430115\n",
      "Iteration 35030 => Loss: 6.70002068511338855217\n",
      "Iteration 35031 => Loss: 6.70002043679527314879\n",
      "Iteration 35032 => Loss: 6.70002018851061453830\n",
      "Iteration 35033 => Loss: 6.70001994025941449706\n",
      "Iteration 35034 => Loss: 6.70001969204167302507\n",
      "Iteration 35035 => Loss: 6.70001944385737324694\n",
      "Iteration 35036 => Loss: 6.70001919570651693903\n",
      "Iteration 35037 => Loss: 6.70001894758909788408\n",
      "Iteration 35038 => Loss: 6.70001869950511608209\n",
      "Iteration 35039 => Loss: 6.70001845145456176311\n",
      "Iteration 35040 => Loss: 6.70001820343743315078\n",
      "Iteration 35041 => Loss: 6.70001795545372313967\n",
      "Iteration 35042 => Loss: 6.70001770750342817706\n",
      "Iteration 35043 => Loss: 6.70001745958654382207\n",
      "Iteration 35044 => Loss: 6.70001721170306918651\n",
      "Iteration 35045 => Loss: 6.70001696385299716496\n",
      "Iteration 35046 => Loss: 6.70001671603631976382\n",
      "Iteration 35047 => Loss: 6.70001646825303787125\n",
      "Iteration 35048 => Loss: 6.70001622050314793455\n",
      "Iteration 35049 => Loss: 6.70001597278664018376\n",
      "Iteration 35050 => Loss: 6.70001572510351550704\n",
      "Iteration 35051 => Loss: 6.70001547745375702902\n",
      "Iteration 35052 => Loss: 6.70001522983737007877\n",
      "Iteration 35053 => Loss: 6.70001498225436087353\n",
      "Iteration 35054 => Loss: 6.70001473470470010341\n",
      "Iteration 35055 => Loss: 6.70001448718840197927\n",
      "Iteration 35056 => Loss: 6.70001423970546294839\n",
      "Iteration 35057 => Loss: 6.70001399225586435904\n",
      "Iteration 35058 => Loss: 6.70001374483961154027\n",
      "Iteration 35059 => Loss: 6.70001349745669827485\n",
      "Iteration 35060 => Loss: 6.70001325010712278640\n",
      "Iteration 35061 => Loss: 6.70001300279087441680\n",
      "Iteration 35062 => Loss: 6.70001275550795050151\n",
      "Iteration 35063 => Loss: 6.70001250825835104052\n",
      "Iteration 35064 => Loss: 6.70001226104207070478\n",
      "Iteration 35065 => Loss: 6.70001201385909705976\n",
      "Iteration 35066 => Loss: 6.70001176670942566460\n",
      "Iteration 35067 => Loss: 6.70001151959307517103\n",
      "Iteration 35068 => Loss: 6.70001127251001094010\n",
      "Iteration 35069 => Loss: 6.70001102546024451811\n",
      "Iteration 35070 => Loss: 6.70001077844376879966\n",
      "Iteration 35071 => Loss: 6.70001053146057934384\n",
      "Iteration 35072 => Loss: 6.70001028451067259795\n",
      "Iteration 35073 => Loss: 6.70001003759403968019\n",
      "Iteration 35074 => Loss: 6.70000979071067170878\n",
      "Iteration 35075 => Loss: 6.70000954386058200640\n",
      "Iteration 35076 => Loss: 6.70000929704374748042\n",
      "Iteration 35077 => Loss: 6.70000905026017345989\n",
      "Iteration 35078 => Loss: 6.70000880350985905665\n",
      "Iteration 35079 => Loss: 6.70000855679279183619\n",
      "Iteration 35080 => Loss: 6.70000831010897091033\n",
      "Iteration 35081 => Loss: 6.70000806345838206823\n",
      "Iteration 35082 => Loss: 6.70000781684103507985\n",
      "Iteration 35083 => Loss: 6.70000757025692461610\n",
      "Iteration 35084 => Loss: 6.70000732370603646615\n",
      "Iteration 35085 => Loss: 6.70000707718837240634\n",
      "Iteration 35086 => Loss: 6.70000683070392888396\n",
      "Iteration 35087 => Loss: 6.70000658425269346452\n",
      "Iteration 35088 => Loss: 6.70000633783466703619\n",
      "Iteration 35089 => Loss: 6.70000609144985137533\n",
      "Iteration 35090 => Loss: 6.70000584509823227108\n",
      "Iteration 35091 => Loss: 6.70000559877980972345\n",
      "Iteration 35092 => Loss: 6.70000535249457662701\n",
      "Iteration 35093 => Loss: 6.70000510624253919900\n",
      "Iteration 35094 => Loss: 6.70000486002367434679\n",
      "Iteration 35095 => Loss: 6.70000461383799184034\n",
      "Iteration 35096 => Loss: 6.70000436768548279787\n",
      "Iteration 35097 => Loss: 6.70000412156614189030\n",
      "Iteration 35098 => Loss: 6.70000387547996378856\n",
      "Iteration 35099 => Loss: 6.70000362942694849266\n",
      "Iteration 35100 => Loss: 6.70000338340708978535\n",
      "Iteration 35101 => Loss: 6.70000313742038411391\n",
      "Iteration 35102 => Loss: 6.70000289146681815566\n",
      "Iteration 35103 => Loss: 6.70000264554640079240\n",
      "Iteration 35104 => Loss: 6.70000239965911514872\n",
      "Iteration 35105 => Loss: 6.70000215380496477735\n",
      "Iteration 35106 => Loss: 6.70000190798394346103\n",
      "Iteration 35107 => Loss: 6.70000166219604942341\n",
      "Iteration 35108 => Loss: 6.70000141644127733542\n",
      "Iteration 35109 => Loss: 6.70000117071961120985\n",
      "Iteration 35110 => Loss: 6.70000092503106703390\n",
      "Iteration 35111 => Loss: 6.70000067937562260312\n",
      "Iteration 35112 => Loss: 6.70000043375327969386\n",
      "Iteration 35113 => Loss: 6.70000018816403741795\n",
      "Iteration 35114 => Loss: 6.69999994260788778178\n",
      "Iteration 35115 => Loss: 6.69999969708482279174\n",
      "Iteration 35116 => Loss: 6.69999945159484600055\n",
      "Iteration 35117 => Loss: 6.69999920613795740820\n",
      "Iteration 35118 => Loss: 6.69999896071413836296\n",
      "Iteration 35119 => Loss: 6.69999871532337731850\n",
      "Iteration 35120 => Loss: 6.69999846996569736746\n",
      "Iteration 35121 => Loss: 6.69999822464107808173\n",
      "Iteration 35122 => Loss: 6.69999797934950702682\n",
      "Iteration 35123 => Loss: 6.69999773409099663724\n",
      "Iteration 35124 => Loss: 6.69999748886553536664\n",
      "Iteration 35125 => Loss: 6.69999724367311522144\n",
      "Iteration 35126 => Loss: 6.69999699851373620163\n",
      "Iteration 35127 => Loss: 6.69999675338739120178\n",
      "Iteration 35128 => Loss: 6.69999650829407844554\n",
      "Iteration 35129 => Loss: 6.69999626323378638659\n",
      "Iteration 35130 => Loss: 6.69999601820652745943\n",
      "Iteration 35131 => Loss: 6.69999577321227501869\n",
      "Iteration 35132 => Loss: 6.69999552825104327525\n",
      "Iteration 35133 => Loss: 6.69999528332281624188\n",
      "Iteration 35134 => Loss: 6.69999503842759214223\n",
      "Iteration 35135 => Loss: 6.69999479356537275265\n",
      "Iteration 35136 => Loss: 6.69999454873614119776\n",
      "Iteration 35137 => Loss: 6.69999430393990547117\n",
      "Iteration 35138 => Loss: 6.69999405917665669108\n",
      "Iteration 35139 => Loss: 6.69999381444638775207\n",
      "Iteration 35140 => Loss: 6.69999356974909687779\n",
      "Iteration 35141 => Loss: 6.69999332508477873915\n",
      "Iteration 35142 => Loss: 6.69999308045342800710\n",
      "Iteration 35143 => Loss: 6.69999283585504290528\n",
      "Iteration 35144 => Loss: 6.69999259128961632825\n",
      "Iteration 35145 => Loss: 6.69999234675714827603\n",
      "Iteration 35146 => Loss: 6.69999210225762986681\n",
      "Iteration 35147 => Loss: 6.69999185779105577154\n",
      "Iteration 35148 => Loss: 6.69999161335742066115\n",
      "Iteration 35149 => Loss: 6.69999136895672986469\n",
      "Iteration 35150 => Loss: 6.69999112458896650679\n",
      "Iteration 35151 => Loss: 6.69999088025413858105\n",
      "Iteration 35152 => Loss: 6.69999063595222477119\n",
      "Iteration 35153 => Loss: 6.69999039168324372895\n",
      "Iteration 35154 => Loss: 6.69999014744716703262\n",
      "Iteration 35155 => Loss: 6.69998990324400356400\n",
      "Iteration 35156 => Loss: 6.69998965907374621764\n",
      "Iteration 35157 => Loss: 6.69998941493639588174\n",
      "Iteration 35158 => Loss: 6.69998917083194012179\n",
      "Iteration 35159 => Loss: 6.69998892676036827964\n",
      "Iteration 35160 => Loss: 6.69998868272170700067\n",
      "Iteration 35161 => Loss: 6.69998843871591098775\n",
      "Iteration 35162 => Loss: 6.69998819474299800447\n",
      "Iteration 35163 => Loss: 6.69998795080296360993\n",
      "Iteration 35164 => Loss: 6.69998770689580069870\n",
      "Iteration 35165 => Loss: 6.69998746302149772447\n",
      "Iteration 35166 => Loss: 6.69998721918006356901\n",
      "Iteration 35167 => Loss: 6.69998697537148668602\n",
      "Iteration 35168 => Loss: 6.69998673159576085823\n",
      "Iteration 35169 => Loss: 6.69998648785288519747\n",
      "Iteration 35170 => Loss: 6.69998624414285526285\n",
      "Iteration 35171 => Loss: 6.69998600046565950805\n",
      "Iteration 35172 => Loss: 6.69998575682130415032\n",
      "Iteration 35173 => Loss: 6.69998551320977675516\n",
      "Iteration 35174 => Loss: 6.69998526963108265164\n",
      "Iteration 35175 => Loss: 6.69998502608520318802\n",
      "Iteration 35176 => Loss: 6.69998478257214191700\n",
      "Iteration 35177 => Loss: 6.69998453909190061495\n",
      "Iteration 35178 => Loss: 6.69998429564446595919\n",
      "Iteration 35179 => Loss: 6.69998405222983528517\n",
      "Iteration 35180 => Loss: 6.69998380884800326385\n",
      "Iteration 35181 => Loss: 6.69998356549896723067\n",
      "Iteration 35182 => Loss: 6.69998332218272008021\n",
      "Iteration 35183 => Loss: 6.69998307889926536518\n",
      "Iteration 35184 => Loss: 6.69998283564858621020\n",
      "Iteration 35185 => Loss: 6.69998259243069682611\n",
      "Iteration 35186 => Loss: 6.69998234924556879122\n",
      "Iteration 35187 => Loss: 6.69998210609321542819\n",
      "Iteration 35188 => Loss: 6.69998186297362163799\n",
      "Iteration 35189 => Loss: 6.69998161988679985512\n",
      "Iteration 35190 => Loss: 6.69998137683272165788\n",
      "Iteration 35191 => Loss: 6.69998113381140480982\n",
      "Iteration 35192 => Loss: 6.69998089082282710649\n",
      "Iteration 35193 => Loss: 6.69998064786699742967\n",
      "Iteration 35194 => Loss: 6.69998040494390334487\n",
      "Iteration 35195 => Loss: 6.69998016205354485209\n",
      "Iteration 35196 => Loss: 6.69997991919591040499\n",
      "Iteration 35197 => Loss: 6.69997967637100977356\n",
      "Iteration 35198 => Loss: 6.69997943357882252968\n",
      "Iteration 35199 => Loss: 6.69997919081935489061\n",
      "Iteration 35200 => Loss: 6.69997894809259886273\n",
      "Iteration 35201 => Loss: 6.69997870539855089334\n",
      "Iteration 35202 => Loss: 6.69997846273720476518\n",
      "Iteration 35203 => Loss: 6.69997822010855959007\n",
      "Iteration 35204 => Loss: 6.69997797751260648624\n",
      "Iteration 35205 => Loss: 6.69997773494934101279\n",
      "Iteration 35206 => Loss: 6.69997749241876316972\n",
      "Iteration 35207 => Loss: 6.69997724992087206886\n",
      "Iteration 35208 => Loss: 6.69997700745564905844\n",
      "Iteration 35209 => Loss: 6.69997676502310568480\n",
      "Iteration 35210 => Loss: 6.69997652262321974348\n",
      "Iteration 35211 => Loss: 6.69997628025600455715\n",
      "Iteration 35212 => Loss: 6.69997603792144769130\n",
      "Iteration 35213 => Loss: 6.69997579561954559324\n",
      "Iteration 35214 => Loss: 6.69997555335029293389\n",
      "Iteration 35215 => Loss: 6.69997531111368616052\n",
      "Iteration 35216 => Loss: 6.69997506890972260862\n",
      "Iteration 35217 => Loss: 6.69997482673839339640\n",
      "Iteration 35218 => Loss: 6.69997458459969674749\n",
      "Iteration 35219 => Loss: 6.69997434249362378011\n",
      "Iteration 35220 => Loss: 6.69997410042018426424\n",
      "Iteration 35221 => Loss: 6.69997385837936310082\n",
      "Iteration 35222 => Loss: 6.69997361637115496080\n",
      "Iteration 35223 => Loss: 6.69997337439555362693\n",
      "Iteration 35224 => Loss: 6.69997313245256087555\n",
      "Iteration 35225 => Loss: 6.69997289054217404214\n",
      "Iteration 35226 => Loss: 6.69997264866437713948\n",
      "Iteration 35227 => Loss: 6.69997240681917993754\n",
      "Iteration 35228 => Loss: 6.69997216500656289639\n",
      "Iteration 35229 => Loss: 6.69997192322653667418\n",
      "Iteration 35230 => Loss: 6.69997168147908883640\n",
      "Iteration 35231 => Loss: 6.69997143976422293576\n",
      "Iteration 35232 => Loss: 6.69997119808191676782\n",
      "Iteration 35233 => Loss: 6.69997095643218454342\n",
      "Iteration 35234 => Loss: 6.69997071481501382806\n",
      "Iteration 35235 => Loss: 6.69997047323039485178\n",
      "Iteration 35236 => Loss: 6.69997023167833738455\n",
      "Iteration 35237 => Loss: 6.69996999015882632733\n",
      "Iteration 35238 => Loss: 6.69996974867185901559\n",
      "Iteration 35239 => Loss: 6.69996950721743456114\n",
      "Iteration 35240 => Loss: 6.69996926579554141767\n",
      "Iteration 35241 => Loss: 6.69996902440619024333\n",
      "Iteration 35242 => Loss: 6.69996878304935350457\n",
      "Iteration 35243 => Loss: 6.69996854172504452407\n",
      "Iteration 35244 => Loss: 6.69996830043325353188\n",
      "Iteration 35245 => Loss: 6.69996805917398319252\n",
      "Iteration 35246 => Loss: 6.69996781794721041337\n",
      "Iteration 35247 => Loss: 6.69996757675295562251\n",
      "Iteration 35248 => Loss: 6.69996733559119661550\n",
      "Iteration 35249 => Loss: 6.69996709446193428050\n",
      "Iteration 35250 => Loss: 6.69996685336516240028\n",
      "Iteration 35251 => Loss: 6.69996661230087831029\n",
      "Iteration 35252 => Loss: 6.69996637126908023419\n",
      "Iteration 35253 => Loss: 6.69996613026976195471\n",
      "Iteration 35254 => Loss: 6.69996588930291103736\n",
      "Iteration 35255 => Loss: 6.69996564836853636393\n",
      "Iteration 35256 => Loss: 6.69996540746662283539\n",
      "Iteration 35257 => Loss: 6.69996516659718110986\n",
      "Iteration 35258 => Loss: 6.69996492576018454201\n",
      "Iteration 35259 => Loss: 6.69996468495565089540\n",
      "Iteration 35260 => Loss: 6.69996444418355974193\n",
      "Iteration 35261 => Loss: 6.69996420344391374613\n",
      "Iteration 35262 => Loss: 6.69996396273670846711\n",
      "Iteration 35263 => Loss: 6.69996372206193768761\n",
      "Iteration 35264 => Loss: 6.69996348141960140765\n",
      "Iteration 35265 => Loss: 6.69996324080968630454\n",
      "Iteration 35266 => Loss: 6.69996300023219237829\n",
      "Iteration 35267 => Loss: 6.69996275968711874071\n",
      "Iteration 35268 => Loss: 6.69996251917445651003\n",
      "Iteration 35269 => Loss: 6.69996227869420657441\n",
      "Iteration 35270 => Loss: 6.69996203824636715751\n",
      "Iteration 35271 => Loss: 6.69996179783091871940\n",
      "Iteration 35272 => Loss: 6.69996155744786836550\n",
      "Iteration 35273 => Loss: 6.69996131709720632585\n",
      "Iteration 35274 => Loss: 6.69996107677893881771\n",
      "Iteration 35275 => Loss: 6.69996083649305429475\n",
      "Iteration 35276 => Loss: 6.69996059623954565154\n",
      "Iteration 35277 => Loss: 6.69996035601840933538\n",
      "Iteration 35278 => Loss: 6.69996011582963824083\n",
      "Iteration 35279 => Loss: 6.69995987567324835510\n",
      "Iteration 35280 => Loss: 6.69995963554920415106\n",
      "Iteration 35281 => Loss: 6.69995939545752428046\n",
      "Iteration 35282 => Loss: 6.69995915539819364426\n",
      "Iteration 35283 => Loss: 6.69995891537121668335\n",
      "Iteration 35284 => Loss: 6.69995867537657474600\n",
      "Iteration 35285 => Loss: 6.69995843541427849033\n",
      "Iteration 35286 => Loss: 6.69995819548431459367\n",
      "Iteration 35287 => Loss: 6.69995795558668039149\n",
      "Iteration 35288 => Loss: 6.69995771572137321925\n",
      "Iteration 35289 => Loss: 6.69995747588839041242\n",
      "Iteration 35290 => Loss: 6.69995723608772308921\n",
      "Iteration 35291 => Loss: 6.69995699631936680873\n",
      "Iteration 35292 => Loss: 6.69995675658332334734\n",
      "Iteration 35293 => Loss: 6.69995651687957938236\n",
      "Iteration 35294 => Loss: 6.69995627720814201922\n",
      "Iteration 35295 => Loss: 6.69995603756899615888\n",
      "Iteration 35296 => Loss: 6.69995579796214180135\n",
      "Iteration 35297 => Loss: 6.69995555838757184119\n",
      "Iteration 35298 => Loss: 6.69995531884528805477\n",
      "Iteration 35299 => Loss: 6.69995507933528511302\n",
      "Iteration 35300 => Loss: 6.69995483985754969325\n",
      "Iteration 35301 => Loss: 6.69995460041208978907\n",
      "Iteration 35302 => Loss: 6.69995436099889296599\n",
      "Iteration 35303 => Loss: 6.69995412161796277672\n",
      "Iteration 35304 => Loss: 6.69995388226927346409\n",
      "Iteration 35305 => Loss: 6.69995364295285344980\n",
      "Iteration 35306 => Loss: 6.69995340366867608850\n",
      "Iteration 35307 => Loss: 6.69995316441673693930\n",
      "Iteration 35308 => Loss: 6.69995292519703955492\n",
      "Iteration 35309 => Loss: 6.69995268600958571170\n",
      "Iteration 35310 => Loss: 6.69995244685434609977\n",
      "Iteration 35311 => Loss: 6.69995220773134469994\n",
      "Iteration 35312 => Loss: 6.69995196864056463681\n",
      "Iteration 35313 => Loss: 6.69995172958199347590\n",
      "Iteration 35314 => Loss: 6.69995149055564631624\n",
      "Iteration 35315 => Loss: 6.69995125156150095336\n",
      "Iteration 35316 => Loss: 6.69995101259956538087\n",
      "Iteration 35317 => Loss: 6.69995077366982183520\n",
      "Iteration 35318 => Loss: 6.69995053477228008632\n",
      "Iteration 35319 => Loss: 6.69995029590693036425\n",
      "Iteration 35320 => Loss: 6.69995005707376467541\n",
      "Iteration 35321 => Loss: 6.69994981827278301978\n",
      "Iteration 35322 => Loss: 6.69994957950397829194\n",
      "Iteration 35323 => Loss: 6.69994934076735049189\n",
      "Iteration 35324 => Loss: 6.69994910206289251420\n",
      "Iteration 35325 => Loss: 6.69994886339060080616\n",
      "Iteration 35326 => Loss: 6.69994862475046470962\n",
      "Iteration 35327 => Loss: 6.69994838614249044184\n",
      "Iteration 35328 => Loss: 6.69994814756666201561\n",
      "Iteration 35329 => Loss: 6.69994790902299275359\n",
      "Iteration 35330 => Loss: 6.69994767051145956316\n",
      "Iteration 35331 => Loss: 6.69994743203207043791\n",
      "Iteration 35332 => Loss: 6.69994719358481383154\n",
      "Iteration 35333 => Loss: 6.69994695516967997406\n",
      "Iteration 35334 => Loss: 6.69994671678668129999\n",
      "Iteration 35335 => Loss: 6.69994647843580715119\n",
      "Iteration 35336 => Loss: 6.69994624011704864586\n",
      "Iteration 35337 => Loss: 6.69994600183040400765\n",
      "Iteration 35338 => Loss: 6.69994576357586435478\n",
      "Iteration 35339 => Loss: 6.69994552535342968724\n",
      "Iteration 35340 => Loss: 6.69994528716310000505\n",
      "Iteration 35341 => Loss: 6.69994504900486287369\n",
      "Iteration 35342 => Loss: 6.69994481087871651681\n",
      "Iteration 35343 => Loss: 6.69994457278466271077\n",
      "Iteration 35344 => Loss: 6.69994433472268813290\n",
      "Iteration 35345 => Loss: 6.69994409669279278319\n",
      "Iteration 35346 => Loss: 6.69994385869498021435\n",
      "Iteration 35347 => Loss: 6.69994362072922733375\n",
      "Iteration 35348 => Loss: 6.69994338279554124682\n",
      "Iteration 35349 => Loss: 6.69994314489392284173\n",
      "Iteration 35350 => Loss: 6.69994290702436057217\n",
      "Iteration 35351 => Loss: 6.69994266918684555634\n",
      "Iteration 35352 => Loss: 6.69994243138138223514\n",
      "Iteration 35353 => Loss: 6.69994219360796261498\n",
      "Iteration 35354 => Loss: 6.69994195586658580766\n",
      "Iteration 35355 => Loss: 6.69994171815724026686\n",
      "Iteration 35356 => Loss: 6.69994148047993043349\n",
      "Iteration 35357 => Loss: 6.69994124283464831393\n",
      "Iteration 35358 => Loss: 6.69994100522137614462\n",
      "Iteration 35359 => Loss: 6.69994076764013524183\n",
      "Iteration 35360 => Loss: 6.69994053009090428930\n",
      "Iteration 35361 => Loss: 6.69994029257368772790\n",
      "Iteration 35362 => Loss: 6.69994005508847401131\n",
      "Iteration 35363 => Loss: 6.69993981763525781048\n",
      "Iteration 35364 => Loss: 6.69993958021404356629\n",
      "Iteration 35365 => Loss: 6.69993934282482062059\n",
      "Iteration 35366 => Loss: 6.69993910546758630886\n",
      "Iteration 35367 => Loss: 6.69993886814233441385\n",
      "Iteration 35368 => Loss: 6.69993863084906138283\n",
      "Iteration 35369 => Loss: 6.69993839358776277493\n",
      "Iteration 35370 => Loss: 6.69993815635844125467\n",
      "Iteration 35371 => Loss: 6.69993791916107905848\n",
      "Iteration 35372 => Loss: 6.69993768199568417998\n",
      "Iteration 35373 => Loss: 6.69993744486224329648\n",
      "Iteration 35374 => Loss: 6.69993720776075729617\n",
      "Iteration 35375 => Loss: 6.69993697069122529086\n",
      "Iteration 35376 => Loss: 6.69993673365362685246\n",
      "Iteration 35377 => Loss: 6.69993649664798240906\n",
      "Iteration 35378 => Loss: 6.69993625967427330892\n",
      "Iteration 35379 => Loss: 6.69993602273248800572\n",
      "Iteration 35380 => Loss: 6.69993578582263449306\n",
      "Iteration 35381 => Loss: 6.69993554894470833005\n",
      "Iteration 35382 => Loss: 6.69993531209869885856\n",
      "Iteration 35383 => Loss: 6.69993507528459986133\n",
      "Iteration 35384 => Loss: 6.69993483850242377287\n",
      "Iteration 35385 => Loss: 6.69993460175213861874\n",
      "Iteration 35386 => Loss: 6.69993436503376305069\n",
      "Iteration 35387 => Loss: 6.69993412834728463423\n",
      "Iteration 35388 => Loss: 6.69993389169270070482\n",
      "Iteration 35389 => Loss: 6.69993365507000149250\n",
      "Iteration 35390 => Loss: 6.69993341847919410270\n",
      "Iteration 35391 => Loss: 6.69993318192026787727\n",
      "Iteration 35392 => Loss: 6.69993294539321215808\n",
      "Iteration 35393 => Loss: 6.69993270889803227419\n",
      "Iteration 35394 => Loss: 6.69993247243472023200\n",
      "Iteration 35395 => Loss: 6.69993223600327159062\n",
      "Iteration 35396 => Loss: 6.69993199960367480372\n",
      "Iteration 35397 => Loss: 6.69993176323593786492\n",
      "Iteration 35398 => Loss: 6.69993152690005100425\n",
      "Iteration 35399 => Loss: 6.69993129059601510988\n",
      "Iteration 35400 => Loss: 6.69993105432382041187\n",
      "Iteration 35401 => Loss: 6.69993081808345714023\n",
      "Iteration 35402 => Loss: 6.69993058187493684130\n",
      "Iteration 35403 => Loss: 6.69993034569823819879\n",
      "Iteration 35404 => Loss: 6.69993010955336298906\n",
      "Iteration 35405 => Loss: 6.69992987344031121211\n",
      "Iteration 35406 => Loss: 6.69992963735907842704\n",
      "Iteration 35407 => Loss: 6.69992940130965486389\n",
      "Iteration 35408 => Loss: 6.69992916529203696996\n",
      "Iteration 35409 => Loss: 6.69992892930622385705\n",
      "Iteration 35410 => Loss: 6.69992869335221374882\n",
      "Iteration 35411 => Loss: 6.69992845742999154623\n",
      "Iteration 35412 => Loss: 6.69992822153956968378\n",
      "Iteration 35413 => Loss: 6.69992798568092418066\n",
      "Iteration 35414 => Loss: 6.69992774985406480681\n",
      "Iteration 35415 => Loss: 6.69992751405898445682\n",
      "Iteration 35416 => Loss: 6.69992727829567247255\n",
      "Iteration 35417 => Loss: 6.69992704256413329489\n",
      "Iteration 35418 => Loss: 6.69992680686436425930\n",
      "Iteration 35419 => Loss: 6.69992657119634849039\n",
      "Iteration 35420 => Loss: 6.69992633556009220541\n",
      "Iteration 35421 => Loss: 6.69992609995558208169\n",
      "Iteration 35422 => Loss: 6.69992586438282256012\n",
      "Iteration 35423 => Loss: 6.69992562884181719340\n",
      "Iteration 35424 => Loss: 6.69992539333253578349\n",
      "Iteration 35425 => Loss: 6.69992515785499520575\n",
      "Iteration 35426 => Loss: 6.69992492240918657842\n",
      "Iteration 35427 => Loss: 6.69992468699510190788\n",
      "Iteration 35428 => Loss: 6.69992445161274297050\n",
      "Iteration 35429 => Loss: 6.69992421626209555541\n",
      "Iteration 35430 => Loss: 6.69992398094316410351\n",
      "Iteration 35431 => Loss: 6.69992374565594683844\n",
      "Iteration 35432 => Loss: 6.69992351040042954935\n",
      "Iteration 35433 => Loss: 6.69992327517661401259\n",
      "Iteration 35434 => Loss: 6.69992303998449667546\n",
      "Iteration 35435 => Loss: 6.69992280482407132070\n",
      "Iteration 35436 => Loss: 6.69992256969533261923\n",
      "Iteration 35437 => Loss: 6.69992233459827613018\n",
      "Iteration 35438 => Loss: 6.69992209953290185354\n",
      "Iteration 35439 => Loss: 6.69992186449919735480\n",
      "Iteration 35440 => Loss: 6.69992162949716707487\n",
      "Iteration 35441 => Loss: 6.69992139452680834921\n",
      "Iteration 35442 => Loss: 6.69992115958810519061\n",
      "Iteration 35443 => Loss: 6.69992092468106470449\n",
      "Iteration 35444 => Loss: 6.69992068980567090364\n",
      "Iteration 35445 => Loss: 6.69992045496193266985\n",
      "Iteration 35446 => Loss: 6.69992022014983934497\n",
      "Iteration 35447 => Loss: 6.69991998536938737629\n",
      "Iteration 35448 => Loss: 6.69991975062056877022\n",
      "Iteration 35449 => Loss: 6.69991951590338530309\n",
      "Iteration 35450 => Loss: 6.69991928121782454042\n",
      "Iteration 35451 => Loss: 6.69991904656389447581\n",
      "Iteration 35452 => Loss: 6.69991881194157734569\n",
      "Iteration 35453 => Loss: 6.69991857735088647274\n",
      "Iteration 35454 => Loss: 6.69991834279179432343\n",
      "Iteration 35455 => Loss: 6.69991810826431777315\n",
      "Iteration 35456 => Loss: 6.69991787376844172286\n",
      "Iteration 35457 => Loss: 6.69991763930416084349\n",
      "Iteration 35458 => Loss: 6.69991740487147691141\n",
      "Iteration 35459 => Loss: 6.69991717047037926847\n",
      "Iteration 35460 => Loss: 6.69991693610087235555\n",
      "Iteration 35461 => Loss: 6.69991670176294285000\n",
      "Iteration 35462 => Loss: 6.69991646745659963358\n",
      "Iteration 35463 => Loss: 6.69991623318181872548\n",
      "Iteration 35464 => Loss: 6.69991599893860723114\n",
      "Iteration 35465 => Loss: 6.69991576472696248601\n",
      "Iteration 35466 => Loss: 6.69991553054687738467\n",
      "Iteration 35467 => Loss: 6.69991529639834393350\n",
      "Iteration 35468 => Loss: 6.69991506228136923795\n",
      "Iteration 35469 => Loss: 6.69991482819594086351\n",
      "Iteration 35470 => Loss: 6.69991459414205170475\n",
      "Iteration 35471 => Loss: 6.69991436011969998532\n",
      "Iteration 35472 => Loss: 6.69991412612888570521\n",
      "Iteration 35473 => Loss: 6.69991389216960353536\n",
      "Iteration 35474 => Loss: 6.69991365824184903488\n",
      "Iteration 35475 => Loss: 6.69991342434560888108\n",
      "Iteration 35476 => Loss: 6.69991319048089373211\n",
      "Iteration 35477 => Loss: 6.69991295664768582441\n",
      "Iteration 35478 => Loss: 6.69991272284599226339\n",
      "Iteration 35479 => Loss: 6.69991248907580061456\n",
      "Iteration 35480 => Loss: 6.69991225533710643703\n",
      "Iteration 35481 => Loss: 6.69991202162991239533\n",
      "Iteration 35482 => Loss: 6.69991178795421138403\n",
      "Iteration 35483 => Loss: 6.69991155430999452136\n",
      "Iteration 35484 => Loss: 6.69991132069726891274\n",
      "Iteration 35485 => Loss: 6.69991108711601146553\n",
      "Iteration 35486 => Loss: 6.69991085356623816693\n",
      "Iteration 35487 => Loss: 6.69991062004793125340\n",
      "Iteration 35488 => Loss: 6.69991038656109161309\n",
      "Iteration 35489 => Loss: 6.69991015310571658148\n",
      "Iteration 35490 => Loss: 6.69990991968179816496\n",
      "Iteration 35491 => Loss: 6.69990968628933014628\n",
      "Iteration 35492 => Loss: 6.69990945292831785451\n",
      "Iteration 35493 => Loss: 6.69990921959874707881\n",
      "Iteration 35494 => Loss: 6.69990898630061337826\n",
      "Iteration 35495 => Loss: 6.69990875303392829920\n",
      "Iteration 35496 => Loss: 6.69990851979866253174\n",
      "Iteration 35497 => Loss: 6.69990828659483828034\n",
      "Iteration 35498 => Loss: 6.69990805342242889964\n",
      "Iteration 35499 => Loss: 6.69990782028144238325\n",
      "Iteration 35500 => Loss: 6.69990758717187606663\n",
      "Iteration 35501 => Loss: 6.69990735409371929165\n",
      "Iteration 35502 => Loss: 6.69990712104696495288\n",
      "Iteration 35503 => Loss: 6.69990688803161660303\n",
      "Iteration 35504 => Loss: 6.69990665504766447214\n",
      "Iteration 35505 => Loss: 6.69990642209510944838\n",
      "Iteration 35506 => Loss: 6.69990618917394531451\n",
      "Iteration 35507 => Loss: 6.69990595628416674145\n",
      "Iteration 35508 => Loss: 6.69990572342576928833\n",
      "Iteration 35509 => Loss: 6.69990549059875117877\n",
      "Iteration 35510 => Loss: 6.69990525780310974824\n",
      "Iteration 35511 => Loss: 6.69990502503883167407\n",
      "Iteration 35512 => Loss: 6.69990479230591784443\n",
      "Iteration 35513 => Loss: 6.69990455960436470662\n",
      "Iteration 35514 => Loss: 6.69990432693417492516\n",
      "Iteration 35515 => Loss: 6.69990409429533251284\n",
      "Iteration 35516 => Loss: 6.69990386168783835785\n",
      "Iteration 35517 => Loss: 6.69990362911168624294\n",
      "Iteration 35518 => Loss: 6.69990339656688060899\n",
      "Iteration 35519 => Loss: 6.69990316405340458061\n",
      "Iteration 35520 => Loss: 6.69990293157125815782\n",
      "Iteration 35521 => Loss: 6.69990269912044489331\n",
      "Iteration 35522 => Loss: 6.69990246670095146442\n",
      "Iteration 35523 => Loss: 6.69990223431276810118\n",
      "Iteration 35524 => Loss: 6.69990200195591167898\n",
      "Iteration 35525 => Loss: 6.69990176963036887514\n",
      "Iteration 35526 => Loss: 6.69990153733611926157\n",
      "Iteration 35527 => Loss: 6.69990130507317438457\n",
      "Iteration 35528 => Loss: 6.69990107284153069145\n",
      "Iteration 35529 => Loss: 6.69990084064117841223\n",
      "Iteration 35530 => Loss: 6.69990060847211132966\n",
      "Iteration 35531 => Loss: 6.69990037633433832553\n",
      "Iteration 35532 => Loss: 6.69990014422783808357\n",
      "Iteration 35533 => Loss: 6.69989991215261859736\n",
      "Iteration 35534 => Loss: 6.69989968010866654424\n",
      "Iteration 35535 => Loss: 6.69989944809598636510\n",
      "Iteration 35536 => Loss: 6.69989921611456828998\n",
      "Iteration 35537 => Loss: 6.69989898416440787798\n",
      "Iteration 35538 => Loss: 6.69989875224550779365\n",
      "Iteration 35539 => Loss: 6.69989852035785293793\n",
      "Iteration 35540 => Loss: 6.69989828850145396899\n",
      "Iteration 35541 => Loss: 6.69989805667628424146\n",
      "Iteration 35542 => Loss: 6.69989782488236418345\n",
      "Iteration 35543 => Loss: 6.69989759311967336686\n",
      "Iteration 35544 => Loss: 6.69989736138821267986\n",
      "Iteration 35545 => Loss: 6.69989712968798212245\n",
      "Iteration 35546 => Loss: 6.69989689801896481924\n",
      "Iteration 35547 => Loss: 6.69989666638117054021\n",
      "Iteration 35548 => Loss: 6.69989643477459662080\n",
      "Iteration 35549 => Loss: 6.69989620319921730385\n",
      "Iteration 35550 => Loss: 6.69989597165505212928\n",
      "Iteration 35551 => Loss: 6.69989574014208510988\n",
      "Iteration 35552 => Loss: 6.69989550866031535747\n",
      "Iteration 35553 => Loss: 6.69989527720973132574\n",
      "Iteration 35554 => Loss: 6.69989504579034633736\n",
      "Iteration 35555 => Loss: 6.69989481440213285879\n",
      "Iteration 35556 => Loss: 6.69989458304511042996\n",
      "Iteration 35557 => Loss: 6.69989435171925507007\n",
      "Iteration 35558 => Loss: 6.69989412042457299634\n",
      "Iteration 35559 => Loss: 6.69989388916106154426\n",
      "Iteration 35560 => Loss: 6.69989365792871183203\n",
      "Iteration 35561 => Loss: 6.69989342672752297148\n",
      "Iteration 35562 => Loss: 6.69989319555748252810\n",
      "Iteration 35563 => Loss: 6.69989296441859405462\n",
      "Iteration 35564 => Loss: 6.69989273331085044560\n",
      "Iteration 35565 => Loss: 6.69989250223425347741\n",
      "Iteration 35566 => Loss: 6.69989227118878982736\n",
      "Iteration 35567 => Loss: 6.69989204017446127182\n",
      "Iteration 35568 => Loss: 6.69989180919125626446\n",
      "Iteration 35569 => Loss: 6.69989157823918812795\n",
      "Iteration 35570 => Loss: 6.69989134731823288149\n",
      "Iteration 35571 => Loss: 6.69989111642839763050\n",
      "Iteration 35572 => Loss: 6.69989088556967171684\n",
      "Iteration 35573 => Loss: 6.69989065474205425232\n",
      "Iteration 35574 => Loss: 6.69989042394553813153\n",
      "Iteration 35575 => Loss: 6.69989019318013312443\n",
      "Iteration 35576 => Loss: 6.69988996244581347383\n",
      "Iteration 35577 => Loss: 6.69988973174258628518\n",
      "Iteration 35578 => Loss: 6.69988950107044534121\n",
      "Iteration 35579 => Loss: 6.69988927042939241829\n",
      "Iteration 35580 => Loss: 6.69988903981941597010\n",
      "Iteration 35581 => Loss: 6.69988880924051333210\n",
      "Iteration 35582 => Loss: 6.69988857869268095158\n",
      "Iteration 35583 => Loss: 6.69988834817591616400\n",
      "Iteration 35584 => Loss: 6.69988811769021097575\n",
      "Iteration 35585 => Loss: 6.69988788723556716320\n",
      "Iteration 35586 => Loss: 6.69988765681198028545\n",
      "Iteration 35587 => Loss: 6.69988742641943435530\n",
      "Iteration 35588 => Loss: 6.69988719605794624812\n",
      "Iteration 35589 => Loss: 6.69988696572748221314\n",
      "Iteration 35590 => Loss: 6.69988673542807067207\n",
      "Iteration 35591 => Loss: 6.69988650515968497956\n",
      "Iteration 35592 => Loss: 6.69988627492232868832\n",
      "Iteration 35593 => Loss: 6.69988604471599291657\n",
      "Iteration 35594 => Loss: 6.69988581454068032883\n",
      "Iteration 35595 => Loss: 6.69988558439639003694\n",
      "Iteration 35596 => Loss: 6.69988535428310960640\n",
      "Iteration 35597 => Loss: 6.69988512420083193177\n",
      "Iteration 35598 => Loss: 6.69988489414955878942\n",
      "Iteration 35599 => Loss: 6.69988466412928662663\n",
      "Iteration 35600 => Loss: 6.69988443414000833798\n",
      "Iteration 35601 => Loss: 6.69988420418172214710\n",
      "Iteration 35602 => Loss: 6.69988397425442450128\n",
      "Iteration 35603 => Loss: 6.69988374435810740692\n",
      "Iteration 35604 => Loss: 6.69988351449277086402\n",
      "Iteration 35605 => Loss: 6.69988328465840776715\n",
      "Iteration 35606 => Loss: 6.69988305485501545178\n",
      "Iteration 35607 => Loss: 6.69988282508258770065\n",
      "Iteration 35608 => Loss: 6.69988259534112184923\n",
      "Iteration 35609 => Loss: 6.69988236563061612117\n",
      "Iteration 35610 => Loss: 6.69988213595105630560\n",
      "Iteration 35611 => Loss: 6.69988190630245750157\n",
      "Iteration 35612 => Loss: 6.69988167668479484007\n",
      "Iteration 35613 => Loss: 6.69988144709807453836\n",
      "Iteration 35614 => Loss: 6.69988121754229837279\n",
      "Iteration 35615 => Loss: 6.69988098801744946797\n",
      "Iteration 35616 => Loss: 6.69988075852352427120\n",
      "Iteration 35617 => Loss: 6.69988052906052544699\n",
      "Iteration 35618 => Loss: 6.69988029962844766629\n",
      "Iteration 35619 => Loss: 6.69988007022729270545\n",
      "Iteration 35620 => Loss: 6.69987984085704013637\n",
      "Iteration 35621 => Loss: 6.69987961151770061718\n",
      "Iteration 35622 => Loss: 6.69987938220926526611\n",
      "Iteration 35623 => Loss: 6.69987915293172697773\n",
      "Iteration 35624 => Loss: 6.69987892368508308749\n",
      "Iteration 35625 => Loss: 6.69987869446932826634\n",
      "Iteration 35626 => Loss: 6.69987846528446873151\n",
      "Iteration 35627 => Loss: 6.69987823613047961402\n",
      "Iteration 35628 => Loss: 6.69987800700738134196\n",
      "Iteration 35629 => Loss: 6.69987777791515082271\n",
      "Iteration 35630 => Loss: 6.69987754885379072078\n",
      "Iteration 35631 => Loss: 6.69987731982329304259\n",
      "Iteration 35632 => Loss: 6.69987709082366489355\n",
      "Iteration 35633 => Loss: 6.69987686185489206281\n",
      "Iteration 35634 => Loss: 6.69987663291697188583\n",
      "Iteration 35635 => Loss: 6.69987640400989814538\n",
      "Iteration 35636 => Loss: 6.69987617513367350597\n",
      "Iteration 35637 => Loss: 6.69987594628829263854\n",
      "Iteration 35638 => Loss: 6.69987571747374310860\n",
      "Iteration 35639 => Loss: 6.69987548869003113339\n",
      "Iteration 35640 => Loss: 6.69987525993714427841\n",
      "Iteration 35641 => Loss: 6.69987503121508254367\n",
      "Iteration 35642 => Loss: 6.69987480252384326462\n",
      "Iteration 35643 => Loss: 6.69987457386341223042\n",
      "Iteration 35644 => Loss: 6.69987434523380098739\n",
      "Iteration 35645 => Loss: 6.69987411663499710102\n",
      "Iteration 35646 => Loss: 6.69987388806699524224\n",
      "Iteration 35647 => Loss: 6.69987365952979452288\n",
      "Iteration 35648 => Loss: 6.69987343102338517298\n",
      "Iteration 35649 => Loss: 6.69987320254776985706\n",
      "Iteration 35650 => Loss: 6.69987297410294413424\n",
      "Iteration 35651 => Loss: 6.69987274568889734638\n",
      "Iteration 35652 => Loss: 6.69987251730563304619\n",
      "Iteration 35653 => Loss: 6.69987228895313968735\n",
      "Iteration 35654 => Loss: 6.69987206063142082257\n",
      "Iteration 35655 => Loss: 6.69987183234046579372\n",
      "Iteration 35656 => Loss: 6.69987160408027282443\n",
      "Iteration 35657 => Loss: 6.69987137585083658564\n",
      "Iteration 35658 => Loss: 6.69987114765215796552\n",
      "Iteration 35659 => Loss: 6.69987091948422452958\n",
      "Iteration 35660 => Loss: 6.69987069134703894235\n",
      "Iteration 35661 => Loss: 6.69987046324059765112\n",
      "Iteration 35662 => Loss: 6.69987023516489266228\n",
      "Iteration 35663 => Loss: 6.69987000711991154134\n",
      "Iteration 35664 => Loss: 6.69986977910567382821\n",
      "Iteration 35665 => Loss: 6.69986955112215021302\n",
      "Iteration 35666 => Loss: 6.69986932316935579479\n",
      "Iteration 35667 => Loss: 6.69986909524727192178\n",
      "Iteration 35668 => Loss: 6.69986886735590658759\n",
      "Iteration 35669 => Loss: 6.69986863949523847594\n",
      "Iteration 35670 => Loss: 6.69986841166529067948\n",
      "Iteration 35671 => Loss: 6.69986818386603388831\n",
      "Iteration 35672 => Loss: 6.69986795609747254332\n",
      "Iteration 35673 => Loss: 6.69986772835960397998\n",
      "Iteration 35674 => Loss: 6.69986750065242464558\n",
      "Iteration 35675 => Loss: 6.69986727297592299379\n",
      "Iteration 35676 => Loss: 6.69986704533011057094\n",
      "Iteration 35677 => Loss: 6.69986681771496872528\n",
      "Iteration 35678 => Loss: 6.69986659013049834499\n",
      "Iteration 35679 => Loss: 6.69986636257668966010\n",
      "Iteration 35680 => Loss: 6.69986613505354622333\n",
      "Iteration 35681 => Loss: 6.69986590756105915290\n",
      "Iteration 35682 => Loss: 6.69986568009923644240\n",
      "Iteration 35683 => Loss: 6.69986545266806032828\n",
      "Iteration 35684 => Loss: 6.69986522526752636963\n",
      "Iteration 35685 => Loss: 6.69986499789763900736\n",
      "Iteration 35686 => Loss: 6.69986477055838847150\n",
      "Iteration 35687 => Loss: 6.69986454324977387387\n",
      "Iteration 35688 => Loss: 6.69986431597178100361\n",
      "Iteration 35689 => Loss: 6.69986408872442140705\n",
      "Iteration 35690 => Loss: 6.69986386150768087333\n",
      "Iteration 35691 => Loss: 6.69986363432156029063\n",
      "Iteration 35692 => Loss: 6.69986340716604900081\n",
      "Iteration 35693 => Loss: 6.69986318004114966840\n",
      "Iteration 35694 => Loss: 6.69986295294685962887\n",
      "Iteration 35695 => Loss: 6.69986272588316378318\n",
      "Iteration 35696 => Loss: 6.69986249885006657223\n",
      "Iteration 35697 => Loss: 6.69986227184756000241\n",
      "Iteration 35698 => Loss: 6.69986204487564585008\n",
      "Iteration 35699 => Loss: 6.69986181793431878617\n",
      "Iteration 35700 => Loss: 6.69986159102356815254\n",
      "Iteration 35701 => Loss: 6.69986136414338950829\n",
      "Iteration 35702 => Loss: 6.69986113729378818249\n",
      "Iteration 35703 => Loss: 6.69986091047475529336\n",
      "Iteration 35704 => Loss: 6.69986068368628640002\n",
      "Iteration 35705 => Loss: 6.69986045692837706156\n",
      "Iteration 35706 => Loss: 6.69986023020102550163\n",
      "Iteration 35707 => Loss: 6.69986000350421218030\n",
      "Iteration 35708 => Loss: 6.69985977683796019022\n",
      "Iteration 35709 => Loss: 6.69985955020224555057\n",
      "Iteration 35710 => Loss: 6.69985932359707625494\n",
      "Iteration 35711 => Loss: 6.69985909702243631614\n",
      "Iteration 35712 => Loss: 6.69985887047832839869\n",
      "Iteration 35713 => Loss: 6.69985864396474894988\n",
      "Iteration 35714 => Loss: 6.69985841748168819976\n",
      "Iteration 35715 => Loss: 6.69985819102914792467\n",
      "Iteration 35716 => Loss: 6.69985796460712368372\n",
      "Iteration 35717 => Loss: 6.69985773821561370056\n",
      "Iteration 35718 => Loss: 6.69985751185460287616\n",
      "Iteration 35719 => Loss: 6.69985728552409209868\n",
      "Iteration 35720 => Loss: 6.69985705922408847357\n",
      "Iteration 35721 => Loss: 6.69985683295457867814\n",
      "Iteration 35722 => Loss: 6.69985660671555116608\n",
      "Iteration 35723 => Loss: 6.69985638050700682555\n",
      "Iteration 35724 => Loss: 6.69985615432895809107\n",
      "Iteration 35725 => Loss: 6.69985592818137387638\n",
      "Iteration 35726 => Loss: 6.69985570206426661599\n",
      "Iteration 35727 => Loss: 6.69985547597763186900\n",
      "Iteration 35728 => Loss: 6.69985524992146430634\n",
      "Iteration 35729 => Loss: 6.69985502389574794080\n",
      "Iteration 35730 => Loss: 6.69985479790049875959\n",
      "Iteration 35731 => Loss: 6.69985457193569899914\n",
      "Iteration 35732 => Loss: 6.69985434600134421856\n",
      "Iteration 35733 => Loss: 6.69985412009743708239\n",
      "Iteration 35734 => Loss: 6.69985389422397403791\n",
      "Iteration 35735 => Loss: 6.69985366838094087427\n",
      "Iteration 35736 => Loss: 6.69985344256834469689\n",
      "Iteration 35737 => Loss: 6.69985321678617573582\n",
      "Iteration 35738 => Loss: 6.69985299103443043833\n",
      "Iteration 35739 => Loss: 6.69985276531310258719\n",
      "Iteration 35740 => Loss: 6.69985253962219129420\n",
      "Iteration 35741 => Loss: 6.69985231396169478302\n",
      "Iteration 35742 => Loss: 6.69985208833161038910\n",
      "Iteration 35743 => Loss: 6.69985186273192301343\n",
      "Iteration 35744 => Loss: 6.69985163716263976141\n",
      "Iteration 35745 => Loss: 6.69985141162374375767\n",
      "Iteration 35746 => Loss: 6.69985118611524388399\n",
      "Iteration 35747 => Loss: 6.69985096063713569947\n",
      "Iteration 35748 => Loss: 6.69985073518940232873\n",
      "Iteration 35749 => Loss: 6.69985050977205620626\n",
      "Iteration 35750 => Loss: 6.69985028438507956849\n",
      "Iteration 35751 => Loss: 6.69985005902847596815\n",
      "Iteration 35752 => Loss: 6.69984983370224007615\n",
      "Iteration 35753 => Loss: 6.69984960840636478707\n",
      "Iteration 35754 => Loss: 6.69984938314085187727\n",
      "Iteration 35755 => Loss: 6.69984915790568980043\n",
      "Iteration 35756 => Loss: 6.69984893270088033290\n",
      "Iteration 35757 => Loss: 6.69984870752641104019\n",
      "Iteration 35758 => Loss: 6.69984848238228902773\n",
      "Iteration 35759 => Loss: 6.69984825726850719008\n",
      "Iteration 35760 => Loss: 6.69984803218505309275\n",
      "Iteration 35761 => Loss: 6.69984780713194183477\n",
      "Iteration 35762 => Loss: 6.69984758210914410625\n",
      "Iteration 35763 => Loss: 6.69984735711666967717\n",
      "Iteration 35764 => Loss: 6.69984713215451677115\n",
      "Iteration 35765 => Loss: 6.69984690722267739460\n",
      "Iteration 35766 => Loss: 6.69984668232114355391\n",
      "Iteration 35767 => Loss: 6.69984645744992146632\n",
      "Iteration 35768 => Loss: 6.69984623260899692099\n",
      "Iteration 35769 => Loss: 6.69984600779836547702\n",
      "Iteration 35770 => Loss: 6.69984578301803512801\n",
      "Iteration 35771 => Loss: 6.69984555826799077494\n",
      "Iteration 35772 => Loss: 6.69984533354823152962\n",
      "Iteration 35773 => Loss: 6.69984510885874851027\n",
      "Iteration 35774 => Loss: 6.69984488419955237504\n",
      "Iteration 35775 => Loss: 6.69984465957061914310\n",
      "Iteration 35776 => Loss: 6.69984443497196302530\n",
      "Iteration 35777 => Loss: 6.69984421040356981081\n",
      "Iteration 35778 => Loss: 6.69984398586543328236\n",
      "Iteration 35779 => Loss: 6.69984376135755876902\n",
      "Iteration 35780 => Loss: 6.69984353687992939541\n",
      "Iteration 35781 => Loss: 6.69984331243254871424\n",
      "Iteration 35782 => Loss: 6.69984308801541761369\n",
      "Iteration 35783 => Loss: 6.69984286362852277108\n",
      "Iteration 35784 => Loss: 6.69984263927186596277\n",
      "Iteration 35785 => Loss: 6.69984241494544630058\n",
      "Iteration 35786 => Loss: 6.69984219064925312637\n",
      "Iteration 35787 => Loss: 6.69984196638327755835\n",
      "Iteration 35788 => Loss: 6.69984174214752492560\n",
      "Iteration 35789 => Loss: 6.69984151794199433994\n",
      "Iteration 35790 => Loss: 6.69984129376666182054\n",
      "Iteration 35791 => Loss: 6.69984106962154335463\n",
      "Iteration 35792 => Loss: 6.69984084550663183677\n",
      "Iteration 35793 => Loss: 6.69984062142191927336\n",
      "Iteration 35794 => Loss: 6.69984039736739767079\n",
      "Iteration 35795 => Loss: 6.69984017334307413449\n",
      "Iteration 35796 => Loss: 6.69983994934893090090\n",
      "Iteration 35797 => Loss: 6.69983972538497596361\n",
      "Iteration 35798 => Loss: 6.69983950145119866448\n",
      "Iteration 35799 => Loss: 6.69983927754759367446\n",
      "Iteration 35800 => Loss: 6.69983905367415921717\n",
      "Iteration 35801 => Loss: 6.69983882983089618079\n",
      "Iteration 35802 => Loss: 6.69983860601779745991\n",
      "Iteration 35803 => Loss: 6.69983838223485239638\n",
      "Iteration 35804 => Loss: 6.69983815848206276655\n",
      "Iteration 35805 => Loss: 6.69983793475943034679\n",
      "Iteration 35806 => Loss: 6.69983771106693648534\n",
      "Iteration 35807 => Loss: 6.69983748740459006399\n",
      "Iteration 35808 => Loss: 6.69983726377236887828\n",
      "Iteration 35809 => Loss: 6.69983704017030401445\n",
      "Iteration 35810 => Loss: 6.69983681659836083355\n",
      "Iteration 35811 => Loss: 6.69983659305654022376\n",
      "Iteration 35812 => Loss: 6.69983636954484573778\n",
      "Iteration 35813 => Loss: 6.69983614606326405294\n",
      "Iteration 35814 => Loss: 6.69983592261179961014\n",
      "Iteration 35815 => Loss: 6.69983569919044796848\n",
      "Iteration 35816 => Loss: 6.69983547579919491710\n",
      "Iteration 35817 => Loss: 6.69983525243805466687\n",
      "Iteration 35818 => Loss: 6.69983502910700412514\n",
      "Iteration 35819 => Loss: 6.69983480580604862098\n",
      "Iteration 35820 => Loss: 6.69983458253518904257\n",
      "Iteration 35821 => Loss: 6.69983435929440140910\n",
      "Iteration 35822 => Loss: 6.69983413608370792502\n",
      "Iteration 35823 => Loss: 6.69983391290309349131\n",
      "Iteration 35824 => Loss: 6.69983368975254389710\n",
      "Iteration 35825 => Loss: 6.69983346663206980054\n",
      "Iteration 35826 => Loss: 6.69983324354165699077\n",
      "Iteration 35827 => Loss: 6.69983302048131257322\n",
      "Iteration 35828 => Loss: 6.69983279745101700797\n",
      "Iteration 35829 => Loss: 6.69983257445078184134\n",
      "Iteration 35830 => Loss: 6.69983235148059108610\n",
      "Iteration 35831 => Loss: 6.69983212854044918316\n",
      "Iteration 35832 => Loss: 6.69983190563034725074\n",
      "Iteration 35833 => Loss: 6.69983168275028173611\n",
      "Iteration 35834 => Loss: 6.69983145990025175109\n",
      "Iteration 35835 => Loss: 6.69983123708024486120\n",
      "Iteration 35836 => Loss: 6.69983101429026817186\n",
      "Iteration 35837 => Loss: 6.69983079153030924857\n",
      "Iteration 35838 => Loss: 6.69983056880036986769\n",
      "Iteration 35839 => Loss: 6.69983034610044558832\n",
      "Iteration 35840 => Loss: 6.69983012343052219961\n",
      "Iteration 35841 => Loss: 6.69982990079061213606\n",
      "Iteration 35842 => Loss: 6.69982967818069585775\n",
      "Iteration 35843 => Loss: 6.69982945560078047009\n",
      "Iteration 35844 => Loss: 6.69982923305086242038\n",
      "Iteration 35845 => Loss: 6.69982901053092216870\n",
      "Iteration 35846 => Loss: 6.69982878804097214953\n",
      "Iteration 35847 => Loss: 6.69982856558099992839\n",
      "Iteration 35848 => Loss: 6.69982834315100905798\n",
      "Iteration 35849 => Loss: 6.69982812075098888016\n",
      "Iteration 35850 => Loss: 6.69982789838093584223\n",
      "Iteration 35851 => Loss: 6.69982767604084639146\n",
      "Iteration 35852 => Loss: 6.69982745373072052786\n",
      "Iteration 35853 => Loss: 6.69982723145055114600\n",
      "Iteration 35854 => Loss: 6.69982700920033025227\n",
      "Iteration 35855 => Loss: 6.69982678698005784668\n",
      "Iteration 35856 => Loss: 6.69982656478972948833\n",
      "Iteration 35857 => Loss: 6.69982634262934606539\n",
      "Iteration 35858 => Loss: 6.69982612049889425521\n",
      "Iteration 35859 => Loss: 6.69982589839837316958\n",
      "Iteration 35860 => Loss: 6.69982567632778280853\n",
      "Iteration 35861 => Loss: 6.69982545428711606661\n",
      "Iteration 35862 => Loss: 6.69982523227636939112\n",
      "Iteration 35863 => Loss: 6.69982501029554100569\n",
      "Iteration 35864 => Loss: 6.69982478834462291672\n",
      "Iteration 35865 => Loss: 6.69982456642361068333\n",
      "Iteration 35866 => Loss: 6.69982434453250341733\n",
      "Iteration 35867 => Loss: 6.69982412267129934236\n",
      "Iteration 35868 => Loss: 6.69982390083998957664\n",
      "Iteration 35869 => Loss: 6.69982367903856790292\n",
      "Iteration 35870 => Loss: 6.69982345726703520938\n",
      "Iteration 35871 => Loss: 6.69982323552538794331\n",
      "Iteration 35872 => Loss: 6.69982301381362344017\n",
      "Iteration 35873 => Loss: 6.69982279213172748911\n",
      "Iteration 35874 => Loss: 6.69982257047970630737\n",
      "Iteration 35875 => Loss: 6.69982234885755811860\n",
      "Iteration 35876 => Loss: 6.69982212726526871194\n",
      "Iteration 35877 => Loss: 6.69982190570283719921\n",
      "Iteration 35878 => Loss: 6.69982168417026358043\n",
      "Iteration 35879 => Loss: 6.69982146266754163833\n",
      "Iteration 35880 => Loss: 6.69982124119466693202\n",
      "Iteration 35881 => Loss: 6.69982101975163413243\n",
      "Iteration 35882 => Loss: 6.69982079833843968686\n",
      "Iteration 35883 => Loss: 6.69982057695508537165\n",
      "Iteration 35884 => Loss: 6.69982035560156052867\n",
      "Iteration 35885 => Loss: 6.69982013427786249338\n",
      "Iteration 35886 => Loss: 6.69981991298398593671\n",
      "Iteration 35887 => Loss: 6.69981969171992730594\n",
      "Iteration 35888 => Loss: 6.69981947048568748926\n",
      "Iteration 35889 => Loss: 6.69981924928126115759\n",
      "Iteration 35890 => Loss: 6.69981902810664298187\n",
      "Iteration 35891 => Loss: 6.69981880696182408030\n",
      "Iteration 35892 => Loss: 6.69981858584680356472\n",
      "Iteration 35893 => Loss: 6.69981836476158143512\n",
      "Iteration 35894 => Loss: 6.69981814370614792153\n",
      "Iteration 35895 => Loss: 6.69981792268050480033\n",
      "Iteration 35896 => Loss: 6.69981770168463786064\n",
      "Iteration 35897 => Loss: 6.69981748071856042515\n",
      "Iteration 35898 => Loss: 6.69981725978225206575\n",
      "Iteration 35899 => Loss: 6.69981703887571811151\n",
      "Iteration 35900 => Loss: 6.69981681799894968066\n",
      "Iteration 35901 => Loss: 6.69981659715194233229\n",
      "Iteration 35902 => Loss: 6.69981637633469340187\n",
      "Iteration 35903 => Loss: 6.69981615554720555394\n",
      "Iteration 35904 => Loss: 6.69981593478946368947\n",
      "Iteration 35905 => Loss: 6.69981571406146780845\n",
      "Iteration 35906 => Loss: 6.69981549336321791088\n",
      "Iteration 35907 => Loss: 6.69981527269470777952\n",
      "Iteration 35908 => Loss: 6.69981505205593297347\n",
      "Iteration 35909 => Loss: 6.69981483144688727549\n",
      "Iteration 35910 => Loss: 6.69981461086757335011\n",
      "Iteration 35911 => Loss: 6.69981439031797254557\n",
      "Iteration 35912 => Loss: 6.69981416979809640821\n",
      "Iteration 35913 => Loss: 6.69981394930793872078\n",
      "Iteration 35914 => Loss: 6.69981372884749326602\n",
      "Iteration 35915 => Loss: 6.69981350841674494490\n",
      "Iteration 35916 => Loss: 6.69981328801570352738\n",
      "Iteration 35917 => Loss: 6.69981306764436812529\n",
      "Iteration 35918 => Loss: 6.69981284730272275141\n",
      "Iteration 35919 => Loss: 6.69981262699077362299\n",
      "Iteration 35920 => Loss: 6.69981240670850564101\n",
      "Iteration 35921 => Loss: 6.69981218645591347638\n",
      "Iteration 35922 => Loss: 6.69981196623301489268\n",
      "Iteration 35923 => Loss: 6.69981174603978235638\n",
      "Iteration 35924 => Loss: 6.69981152587622208472\n",
      "Iteration 35925 => Loss: 6.69981130574232963681\n",
      "Iteration 35926 => Loss: 6.69981108563809879541\n",
      "Iteration 35927 => Loss: 6.69981086556353311323\n",
      "Iteration 35928 => Loss: 6.69981064551862193213\n",
      "Iteration 35929 => Loss: 6.69981042550335637031\n",
      "Iteration 35930 => Loss: 6.69981020551773642779\n",
      "Iteration 35931 => Loss: 6.69980998556176032821\n",
      "Iteration 35932 => Loss: 6.69980976563542807156\n",
      "Iteration 35933 => Loss: 6.69980954573873166424\n",
      "Iteration 35934 => Loss: 6.69980932587165955994\n",
      "Iteration 35935 => Loss: 6.69980910603421442318\n",
      "Iteration 35936 => Loss: 6.69980888622640424757\n",
      "Iteration 35937 => Loss: 6.69980866644820327593\n",
      "Iteration 35938 => Loss: 6.69980844669962127824\n",
      "Iteration 35939 => Loss: 6.69980822698064937271\n",
      "Iteration 35940 => Loss: 6.69980800729128489479\n",
      "Iteration 35941 => Loss: 6.69980778763151629818\n",
      "Iteration 35942 => Loss: 6.69980756800136045825\n",
      "Iteration 35943 => Loss: 6.69980734840079339421\n",
      "Iteration 35944 => Loss: 6.69980712882981421785\n",
      "Iteration 35945 => Loss: 6.69980690928842648191\n",
      "Iteration 35946 => Loss: 6.69980668977661952823\n",
      "Iteration 35947 => Loss: 6.69980647029438980411\n",
      "Iteration 35948 => Loss: 6.69980625084173997408\n",
      "Iteration 35949 => Loss: 6.69980603141865760364\n",
      "Iteration 35950 => Loss: 6.69980581202514358097\n",
      "Iteration 35951 => Loss: 6.69980559266119612971\n",
      "Iteration 35952 => Loss: 6.69980537332680192719\n",
      "Iteration 35953 => Loss: 6.69980515402196807884\n",
      "Iteration 35954 => Loss: 6.69980493474668303833\n",
      "Iteration 35955 => Loss: 6.69980471550095035838\n",
      "Iteration 35956 => Loss: 6.69980449628475671631\n",
      "Iteration 35957 => Loss: 6.69980427709809944758\n",
      "Iteration 35958 => Loss: 6.69980405794098032857\n",
      "Iteration 35959 => Loss: 6.69980383881339314200\n",
      "Iteration 35960 => Loss: 6.69980361971533522336\n",
      "Iteration 35961 => Loss: 6.69980340064679946721\n",
      "Iteration 35962 => Loss: 6.69980318160778498537\n",
      "Iteration 35963 => Loss: 6.69980296259828467242\n",
      "Iteration 35964 => Loss: 6.69980274361829231111\n",
      "Iteration 35965 => Loss: 6.69980252466780612508\n",
      "Iteration 35966 => Loss: 6.69980230574684032518\n",
      "Iteration 35967 => Loss: 6.69980208685535139068\n",
      "Iteration 35968 => Loss: 6.69980186799337218417\n",
      "Iteration 35969 => Loss: 6.69980164916088405391\n",
      "Iteration 35970 => Loss: 6.69980143035787634176\n",
      "Iteration 35971 => Loss: 6.69980121158435171225\n",
      "Iteration 35972 => Loss: 6.69980099284031371809\n",
      "Iteration 35973 => Loss: 6.69980077412575081297\n",
      "Iteration 35974 => Loss: 6.69980055544065500328\n",
      "Iteration 35975 => Loss: 6.69980033678503605898\n",
      "Iteration 35976 => Loss: 6.69980011815887177562\n",
      "Iteration 35977 => Loss: 6.69979989956217636404\n",
      "Iteration 35978 => Loss: 6.69979968099492406708\n",
      "Iteration 35979 => Loss: 6.69979946245713442465\n",
      "Iteration 35980 => Loss: 6.69979924394878256777\n",
      "Iteration 35981 => Loss: 6.69979902546988626000\n",
      "Iteration 35982 => Loss: 6.69979880702042418505\n",
      "Iteration 35983 => Loss: 6.69979858860040167201\n",
      "Iteration 35984 => Loss: 6.69979837020980895090\n",
      "Iteration 35985 => Loss: 6.69979815184864069266\n",
      "Iteration 35986 => Loss: 6.69979793351690400272\n",
      "Iteration 35987 => Loss: 6.69979771521458378203\n",
      "Iteration 35988 => Loss: 6.69979749694168091878\n",
      "Iteration 35989 => Loss: 6.69979727869818386665\n",
      "Iteration 35990 => Loss: 6.69979706048410328378\n",
      "Iteration 35991 => Loss: 6.69979684229942495932\n",
      "Iteration 35992 => Loss: 6.69979662414414534055\n",
      "Iteration 35993 => Loss: 6.69979640601826442747\n",
      "Iteration 35994 => Loss: 6.69979618792177333830\n",
      "Iteration 35995 => Loss: 6.69979596985466852033\n",
      "Iteration 35996 => Loss: 6.69979575181695263808\n",
      "Iteration 35997 => Loss: 6.69979553380861947431\n",
      "Iteration 35998 => Loss: 6.69979531582965837089\n",
      "Iteration 35999 => Loss: 6.69979509788007376869\n",
      "Iteration 36000 => Loss: 6.69979487995985856230\n",
      "Iteration 36001 => Loss: 6.69979466206899765268\n",
      "Iteration 36002 => Loss: 6.69979444420751413247\n",
      "Iteration 36003 => Loss: 6.69979422637537425089\n",
      "Iteration 36004 => Loss: 6.69979400857259044244\n",
      "Iteration 36005 => Loss: 6.69979379079915648987\n",
      "Iteration 36006 => Loss: 6.69979357305506617593\n",
      "Iteration 36007 => Loss: 6.69979335534031594790\n",
      "Iteration 36008 => Loss: 6.69979313765491379939\n",
      "Iteration 36009 => Loss: 6.69979291999883130870\n",
      "Iteration 36010 => Loss: 6.69979270237208268668\n",
      "Iteration 36011 => Loss: 6.69979248477465905154\n",
      "Iteration 36012 => Loss: 6.69979226720655862692\n",
      "Iteration 36013 => Loss: 6.69979204966777430741\n",
      "Iteration 36014 => Loss: 6.69979183215830520481\n",
      "Iteration 36015 => Loss: 6.69979161467813710829\n",
      "Iteration 36016 => Loss: 6.69979139722728511686\n",
      "Iteration 36017 => Loss: 6.69979117980572791424\n",
      "Iteration 36018 => Loss: 6.69979096241347260587\n",
      "Iteration 36019 => Loss: 6.69979074505050853361\n",
      "Iteration 36020 => Loss: 6.69979052771683125655\n",
      "Iteration 36021 => Loss: 6.69979031041244521560\n",
      "Iteration 36022 => Loss: 6.69979009313733797626\n",
      "Iteration 36023 => Loss: 6.69978987589151220305\n",
      "Iteration 36024 => Loss: 6.69978965867495901421\n",
      "Iteration 36025 => Loss: 6.69978944148767663336\n",
      "Iteration 36026 => Loss: 6.69978922432964907330\n",
      "Iteration 36027 => Loss: 6.69978900720090120302\n",
      "Iteration 36028 => Loss: 6.69978879010139927175\n",
      "Iteration 36029 => Loss: 6.69978857303115216126\n",
      "Iteration 36030 => Loss: 6.69978835599016164792\n",
      "Iteration 36031 => Loss: 6.69978813897840819180\n",
      "Iteration 36032 => Loss: 6.69978792199590156287\n",
      "Iteration 36033 => Loss: 6.69978770504264087293\n",
      "Iteration 36034 => Loss: 6.69978748811860214118\n",
      "Iteration 36035 => Loss: 6.69978727122380313119\n",
      "Iteration 36036 => Loss: 6.69978705435822607939\n",
      "Iteration 36037 => Loss: 6.69978683752187542666\n",
      "Iteration 36038 => Loss: 6.69978662071473785034\n",
      "Iteration 36039 => Loss: 6.69978640393682134402\n",
      "Iteration 36040 => Loss: 6.69978618718811258503\n",
      "Iteration 36041 => Loss: 6.69978597046861334974\n",
      "Iteration 36042 => Loss: 6.69978575377831031545\n",
      "Iteration 36043 => Loss: 6.69978553711720969943\n",
      "Iteration 36044 => Loss: 6.69978532048530706078\n",
      "Iteration 36045 => Loss: 6.69978510388259795860\n",
      "Iteration 36046 => Loss: 6.69978488730907351112\n",
      "Iteration 36047 => Loss: 6.69978467076472572472\n",
      "Iteration 36048 => Loss: 6.69978445424956436938\n",
      "Iteration 36049 => Loss: 6.69978423776357612240\n",
      "Iteration 36050 => Loss: 6.69978402130676364834\n",
      "Iteration 36051 => Loss: 6.69978380487910829544\n",
      "Iteration 36052 => Loss: 6.69978358848062516273\n",
      "Iteration 36053 => Loss: 6.69978337211129826301\n",
      "Iteration 36054 => Loss: 6.69978315577112493173\n",
      "Iteration 36055 => Loss: 6.69978293946010605708\n",
      "Iteration 36056 => Loss: 6.69978272317823630999\n",
      "Iteration 36057 => Loss: 6.69978250692551480228\n",
      "Iteration 36058 => Loss: 6.69978229070192909944\n",
      "Iteration 36059 => Loss: 6.69978207450747831331\n",
      "Iteration 36060 => Loss: 6.69978185834215977934\n",
      "Iteration 36061 => Loss: 6.69978164220596816847\n",
      "Iteration 36062 => Loss: 6.69978142609890525705\n",
      "Iteration 36063 => Loss: 6.69978121002095949876\n",
      "Iteration 36064 => Loss: 6.69978099397213711086\n",
      "Iteration 36065 => Loss: 6.69978077795241322434\n",
      "Iteration 36066 => Loss: 6.69978056196181359638\n",
      "Iteration 36067 => Loss: 6.69978034600031246981\n",
      "Iteration 36068 => Loss: 6.69978013006790806827\n",
      "Iteration 36069 => Loss: 6.69977991416460216811\n",
      "Iteration 36070 => Loss: 6.69977969829039388117\n",
      "Iteration 36071 => Loss: 6.69977948244527787836\n",
      "Iteration 36072 => Loss: 6.69977926662923461976\n",
      "Iteration 36073 => Loss: 6.69977905084227920440\n",
      "Iteration 36074 => Loss: 6.69977883508439919780\n",
      "Iteration 36075 => Loss: 6.69977861935559459994\n",
      "Iteration 36076 => Loss: 6.69977840365585919358\n",
      "Iteration 36077 => Loss: 6.69977818798518676147\n",
      "Iteration 36078 => Loss: 6.69977797234358174450\n",
      "Iteration 36079 => Loss: 6.69977775673102904364\n",
      "Iteration 36080 => Loss: 6.69977754114753221160\n",
      "Iteration 36081 => Loss: 6.69977732559308858384\n",
      "Iteration 36082 => Loss: 6.69977711006768483770\n",
      "Iteration 36083 => Loss: 6.69977689457132097317\n",
      "Iteration 36084 => Loss: 6.69977667910400320750\n",
      "Iteration 36085 => Loss: 6.69977646366571821801\n",
      "Iteration 36086 => Loss: 6.69977624825646334017\n",
      "Iteration 36087 => Loss: 6.69977603287622969219\n",
      "Iteration 36088 => Loss: 6.69977581752502793222\n",
      "Iteration 36089 => Loss: 6.69977560220283852033\n",
      "Iteration 36090 => Loss: 6.69977538690966145651\n",
      "Iteration 36091 => Loss: 6.69977517164549762896\n",
      "Iteration 36092 => Loss: 6.69977495641034259677\n",
      "Iteration 36093 => Loss: 6.69977474120419014270\n",
      "Iteration 36094 => Loss: 6.69977452602703849038\n",
      "Iteration 36095 => Loss: 6.69977431087887786987\n",
      "Iteration 36096 => Loss: 6.69977409575970561662\n",
      "Iteration 36097 => Loss: 6.69977388066952794787\n",
      "Iteration 36098 => Loss: 6.69977366560832710007\n",
      "Iteration 36099 => Loss: 6.69977345057611461954\n",
      "Iteration 36100 => Loss: 6.69977323557286918998\n",
      "Iteration 36101 => Loss: 6.69977302059859880501\n",
      "Iteration 36102 => Loss: 6.69977280565329813555\n",
      "Iteration 36103 => Loss: 6.69977259073695652347\n",
      "Iteration 36104 => Loss: 6.69977237584957574512\n",
      "Iteration 36105 => Loss: 6.69977216099115580050\n",
      "Iteration 36106 => Loss: 6.69977194616168691965\n",
      "Iteration 36107 => Loss: 6.69977173136116288532\n",
      "Iteration 36108 => Loss: 6.69977151658958458569\n",
      "Iteration 36109 => Loss: 6.69977130184694580350\n",
      "Iteration 36110 => Loss: 6.69977108713324565059\n",
      "Iteration 36111 => Loss: 6.69977087244848057423\n",
      "Iteration 36112 => Loss: 6.69977065779263813994\n",
      "Iteration 36113 => Loss: 6.69977044316572278859\n",
      "Iteration 36114 => Loss: 6.69977022856773274384\n",
      "Iteration 36115 => Loss: 6.69977001399865823572\n",
      "Iteration 36116 => Loss: 6.69976979945849215881\n",
      "Iteration 36117 => Loss: 6.69976958494723984217\n",
      "Iteration 36118 => Loss: 6.69976937046488885130\n",
      "Iteration 36119 => Loss: 6.69976915601144007439\n",
      "Iteration 36120 => Loss: 6.69976894158689795233\n",
      "Iteration 36121 => Loss: 6.69976872719123850430\n",
      "Iteration 36122 => Loss: 6.69976851282447238844\n",
      "Iteration 36123 => Loss: 6.69976829848658894662\n",
      "Iteration 36124 => Loss: 6.69976808417759084335\n",
      "Iteration 36125 => Loss: 6.69976786989747186141\n",
      "Iteration 36126 => Loss: 6.69976765564622844806\n",
      "Iteration 36127 => Loss: 6.69976744142385882697\n",
      "Iteration 36128 => Loss: 6.69976722723034789908\n",
      "Iteration 36129 => Loss: 6.69976701306570188166\n",
      "Iteration 36130 => Loss: 6.69976679892991011656\n",
      "Iteration 36131 => Loss: 6.69976658482298237374\n",
      "Iteration 36132 => Loss: 6.69976637074489733692\n",
      "Iteration 36133 => Loss: 6.69976615669566211153\n",
      "Iteration 36134 => Loss: 6.69976594267527314486\n",
      "Iteration 36135 => Loss: 6.69976572868372066694\n",
      "Iteration 36136 => Loss: 6.69976551472100911866\n",
      "Iteration 36137 => Loss: 6.69976530078711807192\n",
      "Iteration 36138 => Loss: 6.69976508688206440212\n",
      "Iteration 36139 => Loss: 6.69976487300582945750\n",
      "Iteration 36140 => Loss: 6.69976465915841323806\n",
      "Iteration 36141 => Loss: 6.69976444533981840834\n",
      "Iteration 36142 => Loss: 6.69976423155003164567\n",
      "Iteration 36143 => Loss: 6.69976401778905295004\n",
      "Iteration 36144 => Loss: 6.69976380405688232145\n",
      "Iteration 36145 => Loss: 6.69976359035350643722\n",
      "Iteration 36146 => Loss: 6.69976337667893062644\n",
      "Iteration 36147 => Loss: 6.69976316303314778366\n",
      "Iteration 36148 => Loss: 6.69976294941615257983\n",
      "Iteration 36149 => Loss: 6.69976273582794235040\n",
      "Iteration 36150 => Loss: 6.69976252226851354266\n",
      "Iteration 36151 => Loss: 6.69976230873786082753\n",
      "Iteration 36152 => Loss: 6.69976209523598242868\n",
      "Iteration 36153 => Loss: 6.69976188176287301701\n",
      "Iteration 36154 => Loss: 6.69976166831852815164\n",
      "Iteration 36155 => Loss: 6.69976145490294516804\n",
      "Iteration 36156 => Loss: 6.69976124151612228985\n",
      "Iteration 36157 => Loss: 6.69976102815805507618\n",
      "Iteration 36158 => Loss: 6.69976081482873375705\n",
      "Iteration 36159 => Loss: 6.69976060152815833248\n",
      "Iteration 36160 => Loss: 6.69976038825633146701\n",
      "Iteration 36161 => Loss: 6.69976017501322740344\n",
      "Iteration 36162 => Loss: 6.69975996179887545168\n",
      "Iteration 36163 => Loss: 6.69975974861324541365\n",
      "Iteration 36164 => Loss: 6.69975953545634173025\n",
      "Iteration 36165 => Loss: 6.69975932232816440148\n",
      "Iteration 36166 => Loss: 6.69975910922870721009\n",
      "Iteration 36167 => Loss: 6.69975889615796038612\n",
      "Iteration 36168 => Loss: 6.69975868311592570592\n",
      "Iteration 36169 => Loss: 6.69975847010260672221\n",
      "Iteration 36170 => Loss: 6.69975825711798211870\n",
      "Iteration 36171 => Loss: 6.69975804416205722447\n",
      "Iteration 36172 => Loss: 6.69975783123482937498\n",
      "Iteration 36173 => Loss: 6.69975761833629501751\n",
      "Iteration 36174 => Loss: 6.69975740546645326390\n",
      "Iteration 36175 => Loss: 6.69975719262528901510\n",
      "Iteration 36176 => Loss: 6.69975697981280227111\n",
      "Iteration 36177 => Loss: 6.69975676702900280191\n",
      "Iteration 36178 => Loss: 6.69975655427386485030\n",
      "Iteration 36179 => Loss: 6.69975634154740085080\n",
      "Iteration 36180 => Loss: 6.69975612884960280979\n",
      "Iteration 36181 => Loss: 6.69975591618046539821\n",
      "Iteration 36182 => Loss: 6.69975570353998151063\n",
      "Iteration 36183 => Loss: 6.69975549092815469976\n",
      "Iteration 36184 => Loss: 6.69975527834497963653\n",
      "Iteration 36185 => Loss: 6.69975506579044566280\n",
      "Iteration 36186 => Loss: 6.69975485326454922586\n",
      "Iteration 36187 => Loss: 6.69975464076729920748\n",
      "Iteration 36188 => Loss: 6.69975442829868406136\n",
      "Iteration 36189 => Loss: 6.69975421585869490571\n",
      "Iteration 36190 => Loss: 6.69975400344732907598\n",
      "Iteration 36191 => Loss: 6.69975379106459367762\n",
      "Iteration 36192 => Loss: 6.69975357871046739433\n",
      "Iteration 36193 => Loss: 6.69975336638496887787\n",
      "Iteration 36194 => Loss: 6.69975315408806526563\n",
      "Iteration 36195 => Loss: 6.69975294181977787389\n",
      "Iteration 36196 => Loss: 6.69975272958009426816\n",
      "Iteration 36197 => Loss: 6.69975251736900467847\n",
      "Iteration 36198 => Loss: 6.69975230518651532208\n",
      "Iteration 36199 => Loss: 6.69975209303261287630\n",
      "Iteration 36200 => Loss: 6.69975188090731066382\n",
      "Iteration 36201 => Loss: 6.69975166881057582202\n",
      "Iteration 36202 => Loss: 6.69975145674243233174\n",
      "Iteration 36203 => Loss: 6.69975124470285621214\n",
      "Iteration 36204 => Loss: 6.69975103269185989774\n",
      "Iteration 36205 => Loss: 6.69975082070942917767\n",
      "Iteration 36206 => Loss: 6.69975060875556582829\n",
      "Iteration 36207 => Loss: 6.69975039683025652693\n",
      "Iteration 36208 => Loss: 6.69975018493350660265\n",
      "Iteration 36209 => Loss: 6.69974997306531516728\n",
      "Iteration 36210 => Loss: 6.69974976122566712178\n",
      "Iteration 36211 => Loss: 6.69974954941456779522\n",
      "Iteration 36212 => Loss: 6.69974933763201363490\n",
      "Iteration 36213 => Loss: 6.69974912587798510089\n",
      "Iteration 36214 => Loss: 6.69974891415250173310\n",
      "Iteration 36215 => Loss: 6.69974870245554310344\n",
      "Iteration 36216 => Loss: 6.69974849078711098826\n",
      "Iteration 36217 => Loss: 6.69974827914720449940\n",
      "Iteration 36218 => Loss: 6.69974806753581209051\n",
      "Iteration 36219 => Loss: 6.69974785595293731433\n",
      "Iteration 36220 => Loss: 6.69974764439858017084\n",
      "Iteration 36221 => Loss: 6.69974743287271401471\n",
      "Iteration 36222 => Loss: 6.69974722137535927402\n",
      "Iteration 36223 => Loss: 6.69974700990650440247\n",
      "Iteration 36224 => Loss: 6.69974679846614940004\n",
      "Iteration 36225 => Loss: 6.69974658705427650318\n",
      "Iteration 36226 => Loss: 6.69974637567089725820\n",
      "Iteration 36227 => Loss: 6.69974616431599834243\n",
      "Iteration 36228 => Loss: 6.69974595298958330858\n",
      "Iteration 36229 => Loss: 6.69974574169164327486\n",
      "Iteration 36230 => Loss: 6.69974553042217735310\n",
      "Iteration 36231 => Loss: 6.69974531918117932605\n",
      "Iteration 36232 => Loss: 6.69974510796864652917\n",
      "Iteration 36233 => Loss: 6.69974489678457008068\n",
      "Iteration 36234 => Loss: 6.69974468562895353330\n",
      "Iteration 36235 => Loss: 6.69974447450179066976\n",
      "Iteration 36236 => Loss: 6.69974426340307793737\n",
      "Iteration 36237 => Loss: 6.69974405233280378980\n",
      "Iteration 36238 => Loss: 6.69974384129098066154\n",
      "Iteration 36239 => Loss: 6.69974363027759167721\n",
      "Iteration 36240 => Loss: 6.69974341929263594864\n",
      "Iteration 36241 => Loss: 6.69974320833611169945\n",
      "Iteration 36242 => Loss: 6.69974299740800915970\n",
      "Iteration 36243 => Loss: 6.69974278650834165205\n",
      "Iteration 36244 => Loss: 6.69974257563707986662\n",
      "Iteration 36245 => Loss: 6.69974236479424067880\n",
      "Iteration 36246 => Loss: 6.69974215397981698317\n",
      "Iteration 36247 => Loss: 6.69974194319378835161\n",
      "Iteration 36248 => Loss: 6.69974173243616899498\n",
      "Iteration 36249 => Loss: 6.69974152170694292607\n",
      "Iteration 36250 => Loss: 6.69974131100612435574\n",
      "Iteration 36251 => Loss: 6.69974110033368841499\n",
      "Iteration 36252 => Loss: 6.69974088968964576196\n",
      "Iteration 36253 => Loss: 6.69974067907398129762\n",
      "Iteration 36254 => Loss: 6.69974046848670479193\n",
      "Iteration 36255 => Loss: 6.69974025792780203403\n",
      "Iteration 36256 => Loss: 6.69974004739726947122\n",
      "Iteration 36257 => Loss: 6.69973983689510887984\n",
      "Iteration 36258 => Loss: 6.69973962642131315448\n",
      "Iteration 36259 => Loss: 6.69973941597587430152\n",
      "Iteration 36260 => Loss: 6.69973920555880209093\n",
      "Iteration 36261 => Loss: 6.69973899517007964732\n",
      "Iteration 36262 => Loss: 6.69973878480970697069\n",
      "Iteration 36263 => Loss: 6.69973857447767606743\n",
      "Iteration 36264 => Loss: 6.69973836417399315479\n",
      "Iteration 36265 => Loss: 6.69973815389864757464\n",
      "Iteration 36266 => Loss: 6.69973794365163222153\n",
      "Iteration 36267 => Loss: 6.69973773343295153637\n",
      "Iteration 36268 => Loss: 6.69973752324259841373\n",
      "Iteration 36269 => Loss: 6.69973731308056930089\n",
      "Iteration 36270 => Loss: 6.69973710294685265154\n",
      "Iteration 36271 => Loss: 6.69973689284146090017\n",
      "Iteration 36272 => Loss: 6.69973668276436917779\n",
      "Iteration 36273 => Loss: 6.69973647271559968885\n",
      "Iteration 36274 => Loss: 6.69973626269512934073\n",
      "Iteration 36275 => Loss: 6.69973605270294836345\n",
      "Iteration 36276 => Loss: 6.69973584273907363240\n",
      "Iteration 36277 => Loss: 6.69973563280349360127\n",
      "Iteration 36278 => Loss: 6.69973542289620027645\n",
      "Iteration 36279 => Loss: 6.69973521301718388798\n",
      "Iteration 36280 => Loss: 6.69973500316646397579\n",
      "Iteration 36281 => Loss: 6.69973479334401211815\n",
      "Iteration 36282 => Loss: 6.69973458354983186780\n",
      "Iteration 36283 => Loss: 6.69973437378392944197\n",
      "Iteration 36284 => Loss: 6.69973416404628530074\n",
      "Iteration 36285 => Loss: 6.69973395433690654954\n",
      "Iteration 36286 => Loss: 6.69973374465578519477\n",
      "Iteration 36287 => Loss: 6.69973353500291679552\n",
      "Iteration 36288 => Loss: 6.69973332537830312816\n",
      "Iteration 36289 => Loss: 6.69973311578193531091\n",
      "Iteration 36290 => Loss: 6.69973290621380801468\n",
      "Iteration 36291 => Loss: 6.69973269667392035132\n",
      "Iteration 36292 => Loss: 6.69973248716227320898\n",
      "Iteration 36293 => Loss: 6.69973227767885859407\n",
      "Iteration 36294 => Loss: 6.69973206822366762481\n",
      "Iteration 36295 => Loss: 6.69973185879669408394\n",
      "Iteration 36296 => Loss: 6.69973164939795218231\n",
      "Iteration 36297 => Loss: 6.69973144002742060366\n",
      "Iteration 36298 => Loss: 6.69973123068510023614\n",
      "Iteration 36299 => Loss: 6.69973102137099552067\n",
      "Iteration 36300 => Loss: 6.69973081208508958184\n",
      "Iteration 36301 => Loss: 6.69973060282738597238\n",
      "Iteration 36302 => Loss: 6.69973039359788291591\n",
      "Iteration 36303 => Loss: 6.69973018439657508338\n",
      "Iteration 36304 => Loss: 6.69972997522345625754\n",
      "Iteration 36305 => Loss: 6.69972976607851578024\n",
      "Iteration 36306 => Loss: 6.69972955696176075691\n",
      "Iteration 36307 => Loss: 6.69972934787318408212\n",
      "Iteration 36308 => Loss: 6.69972913881278842041\n",
      "Iteration 36309 => Loss: 6.69972892978056311364\n",
      "Iteration 36310 => Loss: 6.69972872077649039824\n",
      "Iteration 36311 => Loss: 6.69972851180059958409\n",
      "Iteration 36312 => Loss: 6.69972830285286047314\n",
      "Iteration 36313 => Loss: 6.69972809393327661809\n",
      "Iteration 36314 => Loss: 6.69972788504184979530\n",
      "Iteration 36315 => Loss: 6.69972767617856490574\n",
      "Iteration 36316 => Loss: 6.69972746734342372577\n",
      "Iteration 36317 => Loss: 6.69972725853642980809\n",
      "Iteration 36318 => Loss: 6.69972704975756716550\n",
      "Iteration 36319 => Loss: 6.69972684100683668618\n",
      "Iteration 36320 => Loss: 6.69972663228423837012\n",
      "Iteration 36321 => Loss: 6.69972642358976777643\n",
      "Iteration 36322 => Loss: 6.69972621492340980609\n",
      "Iteration 36323 => Loss: 6.69972600628517955812\n",
      "Iteration 36324 => Loss: 6.69972579767506015713\n",
      "Iteration 36325 => Loss: 6.69972558909304538588\n",
      "Iteration 36326 => Loss: 6.69972538053914590250\n",
      "Iteration 36327 => Loss: 6.69972517201334039072\n",
      "Iteration 36328 => Loss: 6.69972496351563950867\n",
      "Iteration 36329 => Loss: 6.69972475504603437457\n",
      "Iteration 36330 => Loss: 6.69972454660451699482\n",
      "Iteration 36331 => Loss: 6.69972433819109092212\n",
      "Iteration 36332 => Loss: 6.69972412980574905106\n",
      "Iteration 36333 => Loss: 6.69972392144847983531\n",
      "Iteration 36334 => Loss: 6.69972371311929126847\n",
      "Iteration 36335 => Loss: 6.69972350481817535695\n",
      "Iteration 36336 => Loss: 6.69972329654512943620\n",
      "Iteration 36337 => Loss: 6.69972308830014995351\n",
      "Iteration 36338 => Loss: 6.69972288008322802710\n",
      "Iteration 36339 => Loss: 6.69972267189436010426\n",
      "Iteration 36340 => Loss: 6.69972246373355151405\n",
      "Iteration 36341 => Loss: 6.69972225560078893380\n",
      "Iteration 36342 => Loss: 6.69972204749607591623\n",
      "Iteration 36343 => Loss: 6.69972183941939913865\n",
      "Iteration 36344 => Loss: 6.69972163137076393014\n",
      "Iteration 36345 => Loss: 6.69972142335016584980\n",
      "Iteration 36346 => Loss: 6.69972121535759868038\n",
      "Iteration 36347 => Loss: 6.69972100739305354011\n",
      "Iteration 36348 => Loss: 6.69972079945653309352\n",
      "Iteration 36349 => Loss: 6.69972059154804000514\n",
      "Iteration 36350 => Loss: 6.69972038366754851779\n",
      "Iteration 36351 => Loss: 6.69972017581507284234\n",
      "Iteration 36352 => Loss: 6.69971996799060942607\n",
      "Iteration 36353 => Loss: 6.69971976019415471626\n",
      "Iteration 36354 => Loss: 6.69971955242569716660\n",
      "Iteration 36355 => Loss: 6.69971934468522967165\n",
      "Iteration 36356 => Loss: 6.69971913697276555411\n",
      "Iteration 36357 => Loss: 6.69971892928828616220\n",
      "Iteration 36358 => Loss: 6.69971872163178883142\n",
      "Iteration 36359 => Loss: 6.69971851400328333170\n",
      "Iteration 36360 => Loss: 6.69971830640274568225\n",
      "Iteration 36361 => Loss: 6.69971809883018387666\n",
      "Iteration 36362 => Loss: 6.69971789128559169768\n",
      "Iteration 36363 => Loss: 6.69971768376896825714\n",
      "Iteration 36364 => Loss: 6.69971747628030644961\n",
      "Iteration 36365 => Loss: 6.69971726881961160416\n",
      "Iteration 36366 => Loss: 6.69971706138686329268\n",
      "Iteration 36367 => Loss: 6.69971685398206506790\n",
      "Iteration 36368 => Loss: 6.69971664660521959433\n",
      "Iteration 36369 => Loss: 6.69971643925631710204\n",
      "Iteration 36370 => Loss: 6.69971623193535670282\n",
      "Iteration 36371 => Loss: 6.69971602464233040308\n",
      "Iteration 36372 => Loss: 6.69971581737723287375\n",
      "Iteration 36373 => Loss: 6.69971561014007654933\n",
      "Iteration 36374 => Loss: 6.69971540293083656081\n",
      "Iteration 36375 => Loss: 6.69971519574951557274\n",
      "Iteration 36376 => Loss: 6.69971498859611713783\n",
      "Iteration 36377 => Loss: 6.69971478147063326247\n",
      "Iteration 36378 => Loss: 6.69971457437305151217\n",
      "Iteration 36379 => Loss: 6.69971436730338165688\n",
      "Iteration 36380 => Loss: 6.69971416026161570301\n",
      "Iteration 36381 => Loss: 6.69971395324775098601\n",
      "Iteration 36382 => Loss: 6.69971374626177951228\n",
      "Iteration 36383 => Loss: 6.69971353930369684093\n",
      "Iteration 36384 => Loss: 6.69971333237350297196\n",
      "Iteration 36385 => Loss: 6.69971312547119701719\n",
      "Iteration 36386 => Loss: 6.69971291859676210123\n",
      "Iteration 36387 => Loss: 6.69971271175021065858\n",
      "Iteration 36388 => Loss: 6.69971250493152936656\n",
      "Iteration 36389 => Loss: 6.69971229814071911335\n",
      "Iteration 36390 => Loss: 6.69971209137777190534\n",
      "Iteration 36391 => Loss: 6.69971188464268596618\n",
      "Iteration 36392 => Loss: 6.69971167793545774316\n",
      "Iteration 36393 => Loss: 6.69971147125608634809\n",
      "Iteration 36394 => Loss: 6.69971126460456023466\n",
      "Iteration 36395 => Loss: 6.69971105798088206740\n",
      "Iteration 36396 => Loss: 6.69971085138504562906\n",
      "Iteration 36397 => Loss: 6.69971064481705180782\n",
      "Iteration 36398 => Loss: 6.69971043827689172190\n",
      "Iteration 36399 => Loss: 6.69971023176455915404\n",
      "Iteration 36400 => Loss: 6.69971002528005588061\n",
      "Iteration 36401 => Loss: 6.69970981882338012525\n",
      "Iteration 36402 => Loss: 6.69970961239452034164\n",
      "Iteration 36403 => Loss: 6.69970940599347919431\n",
      "Iteration 36404 => Loss: 6.69970919962024957783\n",
      "Iteration 36405 => Loss: 6.69970899327482882768\n",
      "Iteration 36406 => Loss: 6.69970878695721250295\n",
      "Iteration 36407 => Loss: 6.69970858066739616277\n",
      "Iteration 36408 => Loss: 6.69970837440538424801\n",
      "Iteration 36409 => Loss: 6.69970816817115633057\n",
      "Iteration 36410 => Loss: 6.69970796196472928585\n",
      "Iteration 36411 => Loss: 6.69970775578607824485\n",
      "Iteration 36412 => Loss: 6.69970754963521297753\n",
      "Iteration 36413 => Loss: 6.69970734351212637847\n",
      "Iteration 36414 => Loss: 6.69970713741681489495\n",
      "Iteration 36415 => Loss: 6.69970693134928030332\n",
      "Iteration 36416 => Loss: 6.69970672530951283363\n",
      "Iteration 36417 => Loss: 6.69970651929750182774\n",
      "Iteration 36418 => Loss: 6.69970631331325439106\n",
      "Iteration 36419 => Loss: 6.69970610735676608272\n",
      "Iteration 36420 => Loss: 6.69970590142802269185\n",
      "Iteration 36421 => Loss: 6.69970569552703754113\n",
      "Iteration 36422 => Loss: 6.69970548965379020245\n",
      "Iteration 36423 => Loss: 6.69970528380828422854\n",
      "Iteration 36424 => Loss: 6.69970507799052406028\n",
      "Iteration 36425 => Loss: 6.69970487220049637500\n",
      "Iteration 36426 => Loss: 6.69970466643819406727\n",
      "Iteration 36427 => Loss: 6.69970446070362246616\n",
      "Iteration 36428 => Loss: 6.69970425499676913716\n",
      "Iteration 36429 => Loss: 6.69970404931763585665\n",
      "Iteration 36430 => Loss: 6.69970384366621996008\n",
      "Iteration 36431 => Loss: 6.69970363804251256568\n",
      "Iteration 36432 => Loss: 6.69970343244651722614\n",
      "Iteration 36433 => Loss: 6.69970322687822150698\n",
      "Iteration 36434 => Loss: 6.69970302133762807273\n",
      "Iteration 36435 => Loss: 6.69970281582473159432\n",
      "Iteration 36436 => Loss: 6.69970261033952763086\n",
      "Iteration 36437 => Loss: 6.69970240488201174145\n",
      "Iteration 36438 => Loss: 6.69970219945218037338\n",
      "Iteration 36439 => Loss: 6.69970199405003086213\n",
      "Iteration 36440 => Loss: 6.69970178867556320768\n",
      "Iteration 36441 => Loss: 6.69970158332877030460\n",
      "Iteration 36442 => Loss: 6.69970137800964593566\n",
      "Iteration 36443 => Loss: 6.69970117271818654814\n",
      "Iteration 36444 => Loss: 6.69970096745439303021\n",
      "Iteration 36445 => Loss: 6.69970076221825472373\n",
      "Iteration 36446 => Loss: 6.69970055700977429325\n",
      "Iteration 36447 => Loss: 6.69970035182894729786\n",
      "Iteration 36448 => Loss: 6.69970014667576485579\n",
      "Iteration 36449 => Loss: 6.69969994155023318427\n",
      "Iteration 36450 => Loss: 6.69969973645233451975\n",
      "Iteration 36451 => Loss: 6.69969953138207774401\n",
      "Iteration 36452 => Loss: 6.69969932633945397527\n",
      "Iteration 36453 => Loss: 6.69969912132445788444\n",
      "Iteration 36454 => Loss: 6.69969891633708414247\n",
      "Iteration 36455 => Loss: 6.69969871137733807842\n",
      "Iteration 36456 => Loss: 6.69969850644521081051\n",
      "Iteration 36457 => Loss: 6.69969830154069523331\n",
      "Iteration 36458 => Loss: 6.69969809666379134683\n",
      "Iteration 36459 => Loss: 6.69969789181448938109\n",
      "Iteration 36460 => Loss: 6.69969768699279999424\n",
      "Iteration 36461 => Loss: 6.69969748219870719907\n",
      "Iteration 36462 => Loss: 6.69969727743220833105\n",
      "Iteration 36463 => Loss: 6.69969707269330161381\n",
      "Iteration 36464 => Loss: 6.69969686798198615918\n",
      "Iteration 36465 => Loss: 6.69969666329825574991\n",
      "Iteration 36466 => Loss: 6.69969645864210683328\n",
      "Iteration 36467 => Loss: 6.69969625401352963934\n",
      "Iteration 36468 => Loss: 6.69969604941253749075\n",
      "Iteration 36469 => Loss: 6.69969584483910995942\n",
      "Iteration 36470 => Loss: 6.69969564029323816357\n",
      "Iteration 36471 => Loss: 6.69969543577494164310\n",
      "Iteration 36472 => Loss: 6.69969523128420174629\n",
      "Iteration 36473 => Loss: 6.69969502682101047952\n",
      "Iteration 36474 => Loss: 6.69969482238537850094\n",
      "Iteration 36475 => Loss: 6.69969461797729159969\n",
      "Iteration 36476 => Loss: 6.69969441359674711123\n",
      "Iteration 36477 => Loss: 6.69969420924374237103\n",
      "Iteration 36478 => Loss: 6.69969400491827649091\n",
      "Iteration 36479 => Loss: 6.69969380062033881273\n",
      "Iteration 36480 => Loss: 6.69969359634994265917\n",
      "Iteration 36481 => Loss: 6.69969339210706227306\n",
      "Iteration 36482 => Loss: 6.69969318789170209527\n",
      "Iteration 36483 => Loss: 6.69969298370386123764\n",
      "Iteration 36484 => Loss: 6.69969277954353703564\n",
      "Iteration 36485 => Loss: 6.69969257541072771289\n",
      "Iteration 36486 => Loss: 6.69969237130541905856\n",
      "Iteration 36487 => Loss: 6.69969216722761284899\n",
      "Iteration 36488 => Loss: 6.69969196317730730783\n",
      "Iteration 36489 => Loss: 6.69969175915450332326\n",
      "Iteration 36490 => Loss: 6.69969155515918490806\n",
      "Iteration 36491 => Loss: 6.69969135119135206224\n",
      "Iteration 36492 => Loss: 6.69969114725101011487\n",
      "Iteration 36493 => Loss: 6.69969094333814840780\n",
      "Iteration 36494 => Loss: 6.69969073945275983561\n",
      "Iteration 36495 => Loss: 6.69969053559485328009\n",
      "Iteration 36496 => Loss: 6.69969033176440920130\n",
      "Iteration 36497 => Loss: 6.69969012796143204014\n",
      "Iteration 36498 => Loss: 6.69968992418591735571\n",
      "Iteration 36499 => Loss: 6.69968972043786425985\n",
      "Iteration 36500 => Loss: 6.69968951671726742347\n",
      "Iteration 36501 => Loss: 6.69968931302411707662\n",
      "Iteration 36502 => Loss: 6.69968910935841499565\n",
      "Iteration 36503 => Loss: 6.69968890572016118057\n",
      "Iteration 36504 => Loss: 6.69968870210934408504\n",
      "Iteration 36505 => Loss: 6.69968849852596637362\n",
      "Iteration 36506 => Loss: 6.69968829497001738815\n",
      "Iteration 36507 => Loss: 6.69968809144150068136\n",
      "Iteration 36508 => Loss: 6.69968788794040737145\n",
      "Iteration 36509 => Loss: 6.69968768446673301753\n",
      "Iteration 36510 => Loss: 6.69968748102048650139\n",
      "Iteration 36511 => Loss: 6.69968727760164561857\n",
      "Iteration 36512 => Loss: 6.69968707421022013904\n",
      "Iteration 36513 => Loss: 6.69968687084619851646\n",
      "Iteration 36514 => Loss: 6.69968666750958341538\n",
      "Iteration 36515 => Loss: 6.69968646420036595401\n",
      "Iteration 36516 => Loss: 6.69968626091854346782\n",
      "Iteration 36517 => Loss: 6.69968605766411151592\n",
      "Iteration 36518 => Loss: 6.69968585443707365101\n",
      "Iteration 36519 => Loss: 6.69968565123741743861\n",
      "Iteration 36520 => Loss: 6.69968544806514731960\n",
      "Iteration 36521 => Loss: 6.69968524492024553041\n",
      "Iteration 36522 => Loss: 6.69968504180272450554\n",
      "Iteration 36523 => Loss: 6.69968483871257536322\n",
      "Iteration 36524 => Loss: 6.69968463564978655711\n",
      "Iteration 36525 => Loss: 6.69968443261436341629\n",
      "Iteration 36526 => Loss: 6.69968422960629705898\n",
      "Iteration 36527 => Loss: 6.69968402662558926153\n",
      "Iteration 36528 => Loss: 6.69968382367223291851\n",
      "Iteration 36529 => Loss: 6.69968362074621914815\n",
      "Iteration 36530 => Loss: 6.69968341784755683221\n",
      "Iteration 36531 => Loss: 6.69968321497623087168\n",
      "Iteration 36532 => Loss: 6.69968301213224037838\n",
      "Iteration 36533 => Loss: 6.69968280931558446412\n",
      "Iteration 36534 => Loss: 6.69968260652626312890\n",
      "Iteration 36535 => Loss: 6.69968240376426038551\n",
      "Iteration 36536 => Loss: 6.69968220102958689210\n",
      "Iteration 36537 => Loss: 6.69968199832222577328\n",
      "Iteration 36538 => Loss: 6.69968179564218058175\n",
      "Iteration 36539 => Loss: 6.69968159298945042934\n",
      "Iteration 36540 => Loss: 6.69968139036402288156\n",
      "Iteration 36541 => Loss: 6.69968118776589882657\n",
      "Iteration 36542 => Loss: 6.69968098519507737620\n",
      "Iteration 36543 => Loss: 6.69968078265155675410\n",
      "Iteration 36544 => Loss: 6.69968058013532452577\n",
      "Iteration 36545 => Loss: 6.69968037764638157938\n",
      "Iteration 36546 => Loss: 6.69968017518472613858\n",
      "Iteration 36547 => Loss: 6.69967997275034932159\n",
      "Iteration 36548 => Loss: 6.69967977034324668750\n",
      "Iteration 36549 => Loss: 6.69967956796342800629\n",
      "Iteration 36550 => Loss: 6.69967936561087373804\n",
      "Iteration 36551 => Loss: 6.69967916328558832362\n",
      "Iteration 36552 => Loss: 6.69967896098756554579\n",
      "Iteration 36553 => Loss: 6.69967875871680274003\n",
      "Iteration 36554 => Loss: 6.69967855647329368907\n",
      "Iteration 36555 => Loss: 6.69967835425704194563\n",
      "Iteration 36556 => Loss: 6.69967815206803152250\n",
      "Iteration 36557 => Loss: 6.69967794990627485419\n",
      "Iteration 36558 => Loss: 6.69967774777175417711\n",
      "Iteration 36559 => Loss: 6.69967754566447570852\n",
      "Iteration 36560 => Loss: 6.69967734358442612574\n",
      "Iteration 36561 => Loss: 6.69967714153160809332\n",
      "Iteration 36562 => Loss: 6.69967693950601539399\n",
      "Iteration 36563 => Loss: 6.69967673750764625140\n",
      "Iteration 36564 => Loss: 6.69967653553650066556\n",
      "Iteration 36565 => Loss: 6.69967633359257153103\n",
      "Iteration 36566 => Loss: 6.69967613167584197242\n",
      "Iteration 36567 => Loss: 6.69967592978633152967\n",
      "Iteration 36568 => Loss: 6.69967572792402776827\n",
      "Iteration 36569 => Loss: 6.69967552608892003008\n",
      "Iteration 36570 => Loss: 6.69967532428100920328\n",
      "Iteration 36571 => Loss: 6.69967512250029706422\n",
      "Iteration 36572 => Loss: 6.69967492074676851388\n",
      "Iteration 36573 => Loss: 6.69967471902042799314\n",
      "Iteration 36574 => Loss: 6.69967451732126928476\n",
      "Iteration 36575 => Loss: 6.69967431564928972421\n",
      "Iteration 36576 => Loss: 6.69967411400448575876\n",
      "Iteration 36577 => Loss: 6.69967391238685738841\n",
      "Iteration 36578 => Loss: 6.69967371079638862597\n",
      "Iteration 36579 => Loss: 6.69967350923309101773\n",
      "Iteration 36580 => Loss: 6.69967330769695390558\n",
      "Iteration 36581 => Loss: 6.69967310618797640132\n",
      "Iteration 36582 => Loss: 6.69967290470615051134\n",
      "Iteration 36583 => Loss: 6.69967270325146824206\n",
      "Iteration 36584 => Loss: 6.69967250182393669888\n",
      "Iteration 36585 => Loss: 6.69967230042354522368\n",
      "Iteration 36586 => Loss: 6.69967209905029115191\n",
      "Iteration 36587 => Loss: 6.69967189770417625994\n",
      "Iteration 36588 => Loss: 6.69967169638519344232\n",
      "Iteration 36589 => Loss: 6.69967149509333292912\n",
      "Iteration 36590 => Loss: 6.69967129382860360209\n",
      "Iteration 36591 => Loss: 6.69967109259099125040\n",
      "Iteration 36592 => Loss: 6.69967089138049409769\n",
      "Iteration 36593 => Loss: 6.69967069019711569666\n",
      "Iteration 36594 => Loss: 6.69967048904083650740\n",
      "Iteration 36595 => Loss: 6.69967028791167162893\n",
      "Iteration 36596 => Loss: 6.69967008680960685041\n",
      "Iteration 36597 => Loss: 6.69966988573464217183\n",
      "Iteration 36598 => Loss: 6.69966968468676782322\n",
      "Iteration 36599 => Loss: 6.69966948366599535092\n",
      "Iteration 36600 => Loss: 6.69966928267229366867\n",
      "Iteration 36601 => Loss: 6.69966908170568231640\n",
      "Iteration 36602 => Loss: 6.69966888076615507686\n",
      "Iteration 36603 => Loss: 6.69966867985371017369\n",
      "Iteration 36604 => Loss: 6.69966847896832806697\n",
      "Iteration 36605 => Loss: 6.69966827811002030302\n",
      "Iteration 36606 => Loss: 6.69966807727877267098\n",
      "Iteration 36607 => Loss: 6.69966787647458961175\n",
      "Iteration 36608 => Loss: 6.69966767569746757260\n",
      "Iteration 36609 => Loss: 6.69966747494739855995\n",
      "Iteration 36610 => Loss: 6.69966727422437990924\n",
      "Iteration 36611 => Loss: 6.69966707352840717959\n",
      "Iteration 36612 => Loss: 6.69966687285948214736\n",
      "Iteration 36613 => Loss: 6.69966667221759770712\n",
      "Iteration 36614 => Loss: 6.69966647160274408890\n",
      "Iteration 36615 => Loss: 6.69966627101492306906\n",
      "Iteration 36616 => Loss: 6.69966607045413731214\n",
      "Iteration 36617 => Loss: 6.69966586992037882453\n",
      "Iteration 36618 => Loss: 6.69966566941363872445\n",
      "Iteration 36619 => Loss: 6.69966546893391345918\n",
      "Iteration 36620 => Loss: 6.69966526848120302873\n",
      "Iteration 36621 => Loss: 6.69966506805550654491\n",
      "Iteration 36622 => Loss: 6.69966486765682045501\n",
      "Iteration 36623 => Loss: 6.69966466728513942996\n",
      "Iteration 36624 => Loss: 6.69966446694045192345\n",
      "Iteration 36625 => Loss: 6.69966426662276415271\n",
      "Iteration 36626 => Loss: 6.69966406633206812415\n",
      "Iteration 36627 => Loss: 6.69966386606835939688\n",
      "Iteration 36628 => Loss: 6.69966366583163708270\n",
      "Iteration 36629 => Loss: 6.69966346562189851710\n",
      "Iteration 36630 => Loss: 6.69966326543913837099\n",
      "Iteration 36631 => Loss: 6.69966306528335664439\n",
      "Iteration 36632 => Loss: 6.69966286515453823824\n",
      "Iteration 36633 => Loss: 6.69966266505268848164\n",
      "Iteration 36634 => Loss: 6.69966246497780293367\n",
      "Iteration 36635 => Loss: 6.69966226492988248253\n",
      "Iteration 36636 => Loss: 6.69966206490891025283\n",
      "Iteration 36637 => Loss: 6.69966186491490223176\n",
      "Iteration 36638 => Loss: 6.69966166494783355034\n",
      "Iteration 36639 => Loss: 6.69966146500770864947\n",
      "Iteration 36640 => Loss: 6.69966126509453463456\n",
      "Iteration 36641 => Loss: 6.69966106520829196569\n",
      "Iteration 36642 => Loss: 6.69966086534898863647\n",
      "Iteration 36643 => Loss: 6.69966066551661665329\n",
      "Iteration 36644 => Loss: 6.69966046571116802255\n",
      "Iteration 36645 => Loss: 6.69966026593264807332\n",
      "Iteration 36646 => Loss: 6.69966006618104525927\n",
      "Iteration 36647 => Loss: 6.69965986645635691588\n",
      "Iteration 36648 => Loss: 6.69965966675857949042\n",
      "Iteration 36649 => Loss: 6.69965946708771920015\n",
      "Iteration 36650 => Loss: 6.69965926744376094604\n",
      "Iteration 36651 => Loss: 6.69965906782670206354\n",
      "Iteration 36652 => Loss: 6.69965886823654788174\n",
      "Iteration 36653 => Loss: 6.69965866867328596612\n",
      "Iteration 36654 => Loss: 6.69965846913691454034\n",
      "Iteration 36655 => Loss: 6.69965826962742827533\n",
      "Iteration 36656 => Loss: 6.69965807014482717108\n",
      "Iteration 36657 => Loss: 6.69965787068911033941\n",
      "Iteration 36658 => Loss: 6.69965767126026090494\n",
      "Iteration 36659 => Loss: 6.69965747185829396670\n",
      "Iteration 36660 => Loss: 6.69965727248319087295\n",
      "Iteration 36661 => Loss: 6.69965707313495872910\n",
      "Iteration 36662 => Loss: 6.69965687381358421248\n",
      "Iteration 36663 => Loss: 6.69965667451907354035\n",
      "Iteration 36664 => Loss: 6.69965647525141250185\n",
      "Iteration 36665 => Loss: 6.69965627601059932061\n",
      "Iteration 36666 => Loss: 6.69965607679664199026\n",
      "Iteration 36667 => Loss: 6.69965587760952718810\n",
      "Iteration 36668 => Loss: 6.69965567844925491414\n",
      "Iteration 36669 => Loss: 6.69965547931581184571\n",
      "Iteration 36670 => Loss: 6.69965528020920242369\n",
      "Iteration 36671 => Loss: 6.69965508112943286534\n",
      "Iteration 36672 => Loss: 6.69965488207648096619\n",
      "Iteration 36673 => Loss: 6.69965468305035471985\n",
      "Iteration 36674 => Loss: 6.69965448405104258001\n",
      "Iteration 36675 => Loss: 6.69965428507854809936\n",
      "Iteration 36676 => Loss: 6.69965408613286861339\n",
      "Iteration 36677 => Loss: 6.69965388721398813487\n",
      "Iteration 36678 => Loss: 6.69965368832192531556\n",
      "Iteration 36679 => Loss: 6.69965348945665706282\n",
      "Iteration 36680 => Loss: 6.69965329061818337664\n",
      "Iteration 36681 => Loss: 6.69965309180650603338\n",
      "Iteration 36682 => Loss: 6.69965289302161703944\n",
      "Iteration 36683 => Loss: 6.69965269426351195392\n",
      "Iteration 36684 => Loss: 6.69965249553219255318\n",
      "Iteration 36685 => Loss: 6.69965229682765261998\n",
      "Iteration 36686 => Loss: 6.69965209814988682524\n",
      "Iteration 36687 => Loss: 6.69965189949889339260\n",
      "Iteration 36688 => Loss: 6.69965170087467232207\n",
      "Iteration 36689 => Loss: 6.69965150227721117915\n",
      "Iteration 36690 => Loss: 6.69965130370650729930\n",
      "Iteration 36691 => Loss: 6.69965110516257134066\n",
      "Iteration 36692 => Loss: 6.69965090664538198695\n",
      "Iteration 36693 => Loss: 6.69965070815494190271\n",
      "Iteration 36694 => Loss: 6.69965050969124931157\n",
      "Iteration 36695 => Loss: 6.69965031125430154901\n",
      "Iteration 36696 => Loss: 6.69965011284409150960\n",
      "Iteration 36697 => Loss: 6.69964991446062363423\n",
      "Iteration 36698 => Loss: 6.69964971610387749479\n",
      "Iteration 36699 => Loss: 6.69964951777386907850\n",
      "Iteration 36700 => Loss: 6.69964931947057795725\n",
      "Iteration 36701 => Loss: 6.69964912119401478918\n",
      "Iteration 36702 => Loss: 6.69964892294416625163\n",
      "Iteration 36703 => Loss: 6.69964872472103056822\n",
      "Iteration 36704 => Loss: 6.69964852652460773896\n",
      "Iteration 36705 => Loss: 6.69964832835489332297\n",
      "Iteration 36706 => Loss: 6.69964813021188199116\n",
      "Iteration 36707 => Loss: 6.69964793209556752629\n",
      "Iteration 36708 => Loss: 6.69964773400594371111\n",
      "Iteration 36709 => Loss: 6.69964753594302120376\n",
      "Iteration 36710 => Loss: 6.69964733790679378700\n",
      "Iteration 36711 => Loss: 6.69964713989724014453\n",
      "Iteration 36712 => Loss: 6.69964694191437093451\n",
      "Iteration 36713 => Loss: 6.69964674395818349240\n",
      "Iteration 36714 => Loss: 6.69964654602866538369\n",
      "Iteration 36715 => Loss: 6.69964634812581660839\n",
      "Iteration 36716 => Loss: 6.69964615024964516010\n",
      "Iteration 36717 => Loss: 6.69964595240012350530\n",
      "Iteration 36718 => Loss: 6.69964575457727473662\n",
      "Iteration 36719 => Loss: 6.69964555678107753778\n",
      "Iteration 36720 => Loss: 6.69964535901153812603\n",
      "Iteration 36721 => Loss: 6.69964516126864317869\n",
      "Iteration 36722 => Loss: 6.69964496355238914305\n",
      "Iteration 36723 => Loss: 6.69964476586278934178\n",
      "Iteration 36724 => Loss: 6.69964456819982068225\n",
      "Iteration 36725 => Loss: 6.69964437056349293442\n",
      "Iteration 36726 => Loss: 6.69964417295378478201\n",
      "Iteration 36727 => Loss: 6.69964397537071754130\n",
      "Iteration 36728 => Loss: 6.69964377781426545511\n",
      "Iteration 36729 => Loss: 6.69964358028443562887\n",
      "Iteration 36730 => Loss: 6.69964338278123427983\n",
      "Iteration 36731 => Loss: 6.69964318530463387447\n",
      "Iteration 36732 => Loss: 6.69964298785464507091\n",
      "Iteration 36733 => Loss: 6.69964279043126431645\n",
      "Iteration 36734 => Loss: 6.69964259303449427563\n",
      "Iteration 36735 => Loss: 6.69964239566430741490\n",
      "Iteration 36736 => Loss: 6.69964219832073037963\n",
      "Iteration 36737 => Loss: 6.69964200100373918900\n",
      "Iteration 36738 => Loss: 6.69964180371333473119\n",
      "Iteration 36739 => Loss: 6.69964160644952322343\n",
      "Iteration 36740 => Loss: 6.69964140921228690218\n",
      "Iteration 36741 => Loss: 6.69964121200162487924\n",
      "Iteration 36742 => Loss: 6.69964101481753893097\n",
      "Iteration 36743 => Loss: 6.69964081766002639284\n",
      "Iteration 36744 => Loss: 6.69964062052908104761\n",
      "Iteration 36745 => Loss: 6.69964042342469756619\n",
      "Iteration 36746 => Loss: 6.69964022634687506041\n",
      "Iteration 36747 => Loss: 6.69964002929559931943\n",
      "Iteration 36748 => Loss: 6.69963983227089343586\n",
      "Iteration 36749 => Loss: 6.69963963527272987619\n",
      "Iteration 36750 => Loss: 6.69963943830110952860\n",
      "Iteration 36751 => Loss: 6.69963924135603328125\n",
      "Iteration 36752 => Loss: 6.69963904443749846962\n",
      "Iteration 36753 => Loss: 6.69963884754548466560\n",
      "Iteration 36754 => Loss: 6.69963865068001407366\n",
      "Iteration 36755 => Loss: 6.69963845384106715386\n",
      "Iteration 36756 => Loss: 6.69963825702865012346\n",
      "Iteration 36757 => Loss: 6.69963806024274699524\n",
      "Iteration 36758 => Loss: 6.69963786348336132193\n",
      "Iteration 36759 => Loss: 6.69963766675049043897\n",
      "Iteration 36760 => Loss: 6.69963747004413257002\n",
      "Iteration 36761 => Loss: 6.69963727336427350423\n",
      "Iteration 36762 => Loss: 6.69963707671092034701\n",
      "Iteration 36763 => Loss: 6.69963688008407043384\n",
      "Iteration 36764 => Loss: 6.69963668348370688932\n",
      "Iteration 36765 => Loss: 6.69963648690984747702\n",
      "Iteration 36766 => Loss: 6.69963629036246910431\n",
      "Iteration 36767 => Loss: 6.69963609384157798843\n",
      "Iteration 36768 => Loss: 6.69963589734716258306\n",
      "Iteration 36769 => Loss: 6.69963570087923088181\n",
      "Iteration 36770 => Loss: 6.69963550443777311472\n",
      "Iteration 36771 => Loss: 6.69963530802278039999\n",
      "Iteration 36772 => Loss: 6.69963511163425984307\n",
      "Iteration 36773 => Loss: 6.69963491527219989763\n",
      "Iteration 36774 => Loss: 6.69963471893660234002\n",
      "Iteration 36775 => Loss: 6.69963452262745651211\n",
      "Iteration 36776 => Loss: 6.69963432634476774297\n",
      "Iteration 36777 => Loss: 6.69963413008852803898\n",
      "Iteration 36778 => Loss: 6.69963393385873828834\n",
      "Iteration 36779 => Loss: 6.69963373765537806293\n",
      "Iteration 36780 => Loss: 6.69963354147845802089\n",
      "Iteration 36781 => Loss: 6.69963334532798437948\n",
      "Iteration 36782 => Loss: 6.69963314920393049334\n",
      "Iteration 36783 => Loss: 6.69963295310630435608\n",
      "Iteration 36784 => Loss: 6.69963275703511662584\n",
      "Iteration 36785 => Loss: 6.69963256099033888091\n",
      "Iteration 36786 => Loss: 6.69963236497197911490\n",
      "Iteration 36787 => Loss: 6.69963216898003199873\n",
      "Iteration 36788 => Loss: 6.69963197301450019694\n",
      "Iteration 36789 => Loss: 6.69963177707536505778\n",
      "Iteration 36790 => Loss: 6.69963158116263901576\n",
      "Iteration 36791 => Loss: 6.69963138527631141272\n",
      "Iteration 36792 => Loss: 6.69963118941638224868\n",
      "Iteration 36793 => Loss: 6.69963099358284264184\n",
      "Iteration 36794 => Loss: 6.69963079777568726314\n",
      "Iteration 36795 => Loss: 6.69963060199492499436\n",
      "Iteration 36796 => Loss: 6.69963040624054162464\n",
      "Iteration 36797 => Loss: 6.69963021051252827220\n",
      "Iteration 36798 => Loss: 6.69963001481089914790\n",
      "Iteration 36799 => Loss: 6.69962981913563915271\n",
      "Iteration 36800 => Loss: 6.69962962348674029300\n",
      "Iteration 36801 => Loss: 6.69962942786421233876\n",
      "Iteration 36802 => Loss: 6.69962923226803663823\n",
      "Iteration 36803 => Loss: 6.69962903669822651409\n",
      "Iteration 36804 => Loss: 6.69962884115476509095\n",
      "Iteration 36805 => Loss: 6.69962864563765947423\n",
      "Iteration 36806 => Loss: 6.69962845014688923584\n",
      "Iteration 36807 => Loss: 6.69962825468246858662\n",
      "Iteration 36808 => Loss: 6.69962805924438686844\n",
      "Iteration 36809 => Loss: 6.69962786383263519951\n",
      "Iteration 36810 => Loss: 6.69962766844721624437\n",
      "Iteration 36811 => Loss: 6.69962747308812645031\n",
      "Iteration 36812 => Loss: 6.69962727775536670549\n",
      "Iteration 36813 => Loss: 6.69962708244891835818\n",
      "Iteration 36814 => Loss: 6.69962688716879650741\n",
      "Iteration 36815 => Loss: 6.69962669191498072507\n",
      "Iteration 36816 => Loss: 6.69962649668748166931\n",
      "Iteration 36817 => Loss: 6.69962630148628779381\n",
      "Iteration 36818 => Loss: 6.69962610631139554584\n",
      "Iteration 36819 => Loss: 6.69962591116280226089\n",
      "Iteration 36820 => Loss: 6.69962571604050705076\n",
      "Iteration 36821 => Loss: 6.69962552094450991547\n",
      "Iteration 36822 => Loss: 6.69962532587479309143\n",
      "Iteration 36823 => Loss: 6.69962513083136546044\n",
      "Iteration 36824 => Loss: 6.69962493581422613431\n",
      "Iteration 36825 => Loss: 6.69962474082336179038\n",
      "Iteration 36826 => Loss: 6.69962454585876532320\n",
      "Iteration 36827 => Loss: 6.69962435092044827911\n",
      "Iteration 36828 => Loss: 6.69962415600839644725\n",
      "Iteration 36829 => Loss: 6.69962396112260982761\n",
      "Iteration 36830 => Loss: 6.69962376626308131478\n",
      "Iteration 36831 => Loss: 6.69962357142981446145\n",
      "Iteration 36832 => Loss: 6.69962337662279860950\n",
      "Iteration 36833 => Loss: 6.69962318184203375893\n",
      "Iteration 36834 => Loss: 6.69962298708751191612\n",
      "Iteration 36835 => Loss: 6.69962279235923663379\n",
      "Iteration 36836 => Loss: 6.69962259765720169469\n",
      "Iteration 36837 => Loss: 6.69962240298140265793\n",
      "Iteration 36838 => Loss: 6.69962220833183419444\n",
      "Iteration 36839 => Loss: 6.69962201370849719240\n",
      "Iteration 36840 => Loss: 6.69962181911138454637\n",
      "Iteration 36841 => Loss: 6.69962162454049003912\n",
      "Iteration 36842 => Loss: 6.69962142999581899971\n",
      "Iteration 36843 => Loss: 6.69962123547736343454\n",
      "Iteration 36844 => Loss: 6.69962104098511801453\n",
      "Iteration 36845 => Loss: 6.69962084651907918698\n",
      "Iteration 36846 => Loss: 6.69962065207925050458\n",
      "Iteration 36847 => Loss: 6.69962045766561420379\n",
      "Iteration 36848 => Loss: 6.69962026327817916638\n",
      "Iteration 36849 => Loss: 6.69962006891693917510\n",
      "Iteration 36850 => Loss: 6.69961987458188801270\n",
      "Iteration 36851 => Loss: 6.69961968027302212647\n",
      "Iteration 36852 => Loss: 6.69961948599033707552\n",
      "Iteration 36853 => Loss: 6.69961929173383552438\n",
      "Iteration 36854 => Loss: 6.69961909750350947945\n",
      "Iteration 36855 => Loss: 6.69961890329935094712\n",
      "Iteration 36856 => Loss: 6.69961870912136614464\n",
      "Iteration 36857 => Loss: 6.69961851496955240748\n",
      "Iteration 36858 => Loss: 6.69961832084389818931\n",
      "Iteration 36859 => Loss: 6.69961812674439816107\n",
      "Iteration 36860 => Loss: 6.69961793267105054639\n",
      "Iteration 36861 => Loss: 6.69961773862385889799\n",
      "Iteration 36862 => Loss: 6.69961754460281788681\n",
      "Iteration 36863 => Loss: 6.69961735060791419016\n",
      "Iteration 36864 => Loss: 6.69961715663916113073\n",
      "Iteration 36865 => Loss: 6.69961696269653472768\n",
      "Iteration 36866 => Loss: 6.69961676878004386282\n",
      "Iteration 36867 => Loss: 6.69961657488968853613\n",
      "Iteration 36868 => Loss: 6.69961638102545542495\n",
      "Iteration 36869 => Loss: 6.69961618718734808198\n",
      "Iteration 36870 => Loss: 6.69961599337535851362\n",
      "Iteration 36871 => Loss: 6.69961579958948849622\n",
      "Iteration 36872 => Loss: 6.69961560582973270073\n",
      "Iteration 36873 => Loss: 6.69961541209607691627\n",
      "Iteration 36874 => Loss: 6.69961521838853890642\n",
      "Iteration 36875 => Loss: 6.69961502470709735491\n",
      "Iteration 36876 => Loss: 6.69961483105174693264\n",
      "Iteration 36877 => Loss: 6.69961463742250096232\n",
      "Iteration 36878 => Loss: 6.69961444381934079217\n",
      "Iteration 36879 => Loss: 6.69961425024227441583\n",
      "Iteration 36880 => Loss: 6.69961405669128584606\n",
      "Iteration 36881 => Loss: 6.69961386316638307648\n",
      "Iteration 36882 => Loss: 6.69961366966755811347\n",
      "Iteration 36883 => Loss: 6.69961347619480385163\n",
      "Iteration 36884 => Loss: 6.69961328274811940275\n",
      "Iteration 36885 => Loss: 6.69961308932750476686\n",
      "Iteration 36886 => Loss: 6.69961289593295905576\n",
      "Iteration 36887 => Loss: 6.69961270256446628224\n",
      "Iteration 36888 => Loss: 6.69961250922202644631\n",
      "Iteration 36889 => Loss: 6.69961231590564754157\n",
      "Iteration 36890 => Loss: 6.69961212261531713352\n",
      "Iteration 36891 => Loss: 6.69961192935102811674\n",
      "Iteration 36892 => Loss: 6.69961173611278493212\n",
      "Iteration 36893 => Loss: 6.69961154290057869787\n",
      "Iteration 36894 => Loss: 6.69961134971441119035\n",
      "Iteration 36895 => Loss: 6.69961115655426908688\n",
      "Iteration 36896 => Loss: 6.69961096342015949290\n",
      "Iteration 36897 => Loss: 6.69961077031207885568\n",
      "Iteration 36898 => Loss: 6.69961057723000674713\n",
      "Iteration 36899 => Loss: 6.69961038417396625988\n",
      "Iteration 36900 => Loss: 6.69961019114393252494\n",
      "Iteration 36901 => Loss: 6.69960999813991353591\n",
      "Iteration 36902 => Loss: 6.69960980516189685829\n",
      "Iteration 36903 => Loss: 6.69960961220988515663\n",
      "Iteration 36904 => Loss: 6.69960941928388198363\n",
      "Iteration 36905 => Loss: 6.69960922638386424666\n",
      "Iteration 36906 => Loss: 6.69960903350984526838\n",
      "Iteration 36907 => Loss: 6.69960884066181172614\n",
      "Iteration 36908 => Loss: 6.69960864783977338988\n",
      "Iteration 36909 => Loss: 6.69960845504370805514\n",
      "Iteration 36910 => Loss: 6.69960826227363792640\n",
      "Iteration 36911 => Loss: 6.69960806952952747650\n",
      "Iteration 36912 => Loss: 6.69960787681139091632\n",
      "Iteration 36913 => Loss: 6.69960768411922824583\n",
      "Iteration 36914 => Loss: 6.69960749145303058327\n",
      "Iteration 36915 => Loss: 6.69960729881279171138\n",
      "Iteration 36916 => Loss: 6.69960710619851429470\n",
      "Iteration 36917 => Loss: 6.69960691361018678691\n",
      "Iteration 36918 => Loss: 6.69960672104781362890\n",
      "Iteration 36919 => Loss: 6.69960652851139126795\n",
      "Iteration 36920 => Loss: 6.69960633600090993411\n",
      "Iteration 36921 => Loss: 6.69960614351637051556\n",
      "Iteration 36922 => Loss: 6.69960595105777123592\n",
      "Iteration 36923 => Loss: 6.69960575862509610801\n",
      "Iteration 36924 => Loss: 6.69960556621835845448\n",
      "Iteration 36925 => Loss: 6.69960537383755294627\n",
      "Iteration 36926 => Loss: 6.69960518148266093164\n",
      "Iteration 36927 => Loss: 6.69960498915368596329\n",
      "Iteration 36928 => Loss: 6.69960479685064225208\n",
      "Iteration 36929 => Loss: 6.69960460457349871177\n",
      "Iteration 36930 => Loss: 6.69960441232226688868\n",
      "Iteration 36931 => Loss: 6.69960422009694323009\n",
      "Iteration 36932 => Loss: 6.69960402789752329511\n",
      "Iteration 36933 => Loss: 6.69960383572399464924\n",
      "Iteration 36934 => Loss: 6.69960364357637594424\n",
      "Iteration 36935 => Loss: 6.69960345145463520566\n",
      "Iteration 36936 => Loss: 6.69960325935878664438\n",
      "Iteration 36937 => Loss: 6.69960306728882226679\n",
      "Iteration 36938 => Loss: 6.69960287524474829013\n",
      "Iteration 36939 => Loss: 6.69960268322654428630\n",
      "Iteration 36940 => Loss: 6.69960249123421469619\n",
      "Iteration 36941 => Loss: 6.69960229926775863163\n",
      "Iteration 36942 => Loss: 6.69960210732716809900\n",
      "Iteration 36943 => Loss: 6.69960191541244309832\n",
      "Iteration 36944 => Loss: 6.69960172352357563597\n",
      "Iteration 36945 => Loss: 6.69960153166056304741\n",
      "Iteration 36946 => Loss: 6.69960133982340533265\n",
      "Iteration 36947 => Loss: 6.69960114801210515623\n",
      "Iteration 36948 => Loss: 6.69960095622664475457\n",
      "Iteration 36949 => Loss: 6.69960076446703300945\n",
      "Iteration 36950 => Loss: 6.69960057273325837457\n",
      "Iteration 36951 => Loss: 6.69960038102531640902\n",
      "Iteration 36952 => Loss: 6.69960018934320888917\n",
      "Iteration 36953 => Loss: 6.69959999768693315048\n",
      "Iteration 36954 => Loss: 6.69959980605648031116\n",
      "Iteration 36955 => Loss: 6.69959961445185303575\n",
      "Iteration 36956 => Loss: 6.69959942287303888975\n",
      "Iteration 36957 => Loss: 6.69959923132004497859\n",
      "Iteration 36958 => Loss: 6.69959903979286153231\n",
      "Iteration 36959 => Loss: 6.69959884829148588636\n",
      "Iteration 36960 => Loss: 6.69959865681591359987\n",
      "Iteration 36961 => Loss: 6.69959846536614467283\n",
      "Iteration 36962 => Loss: 6.69959827394218088159\n",
      "Iteration 36963 => Loss: 6.69959808254400002170\n",
      "Iteration 36964 => Loss: 6.69959789117160919858\n",
      "Iteration 36965 => Loss: 6.69959769982501551766\n",
      "Iteration 36966 => Loss: 6.69959750850420032720\n",
      "Iteration 36967 => Loss: 6.69959731720916540354\n",
      "Iteration 36968 => Loss: 6.69959712593990452945\n",
      "Iteration 36969 => Loss: 6.69959693469642214581\n",
      "Iteration 36970 => Loss: 6.69959674347871292355\n",
      "Iteration 36971 => Loss: 6.69959655228676176364\n",
      "Iteration 36972 => Loss: 6.69959636112057754787\n",
      "Iteration 36973 => Loss: 6.69959616998015672351\n",
      "Iteration 36974 => Loss: 6.69959597886548419154\n",
      "Iteration 36975 => Loss: 6.69959578777657238646\n",
      "Iteration 36976 => Loss: 6.69959559671340620923\n",
      "Iteration 36977 => Loss: 6.69959540567598210714\n",
      "Iteration 36978 => Loss: 6.69959521466430540926\n",
      "Iteration 36979 => Loss: 6.69959502367836901016\n",
      "Iteration 36980 => Loss: 6.69959483271816225169\n",
      "Iteration 36981 => Loss: 6.69959464178369135112\n",
      "Iteration 36982 => Loss: 6.69959445087494298576\n",
      "Iteration 36983 => Loss: 6.69959425999193047829\n",
      "Iteration 36984 => Loss: 6.69959406913463695332\n",
      "Iteration 36985 => Loss: 6.69959387830305086453\n",
      "Iteration 36986 => Loss: 6.69959368749719264002\n",
      "Iteration 36987 => Loss: 6.69959349671703918716\n",
      "Iteration 36988 => Loss: 6.69959330596258872959\n",
      "Iteration 36989 => Loss: 6.69959311523385103726\n",
      "Iteration 36990 => Loss: 6.69959292453080745844\n",
      "Iteration 36991 => Loss: 6.69959273385346509855\n",
      "Iteration 36992 => Loss: 6.69959254320181596398\n",
      "Iteration 36993 => Loss: 6.69959235257585739021\n",
      "Iteration 36994 => Loss: 6.69959216197558315997\n",
      "Iteration 36995 => Loss: 6.69959197140098972056\n",
      "Iteration 36996 => Loss: 6.69959178085208506559\n",
      "Iteration 36997 => Loss: 6.69959159032884876694\n",
      "Iteration 36998 => Loss: 6.69959139983128970641\n",
      "Iteration 36999 => Loss: 6.69959120935940521946\n",
      "Iteration 37000 => Loss: 6.69959101891318109523\n",
      "Iteration 37001 => Loss: 6.69959082849261733372\n",
      "Iteration 37002 => Loss: 6.69959063809771393494\n",
      "Iteration 37003 => Loss: 6.69959044772846912252\n",
      "Iteration 37004 => Loss: 6.69959025738487135015\n",
      "Iteration 37005 => Loss: 6.69959006706693038780\n",
      "Iteration 37006 => Loss: 6.69958987677463202459\n",
      "Iteration 37007 => Loss: 6.69958968650796649058\n",
      "Iteration 37008 => Loss: 6.69958949626695599022\n",
      "Iteration 37009 => Loss: 6.69958930605156410820\n",
      "Iteration 37010 => Loss: 6.69958911586181216080\n",
      "Iteration 37011 => Loss: 6.69958892569768593717\n",
      "Iteration 37012 => Loss: 6.69958873555918543730\n",
      "Iteration 37013 => Loss: 6.69958854544630533212\n",
      "Iteration 37014 => Loss: 6.69958835535905006253\n",
      "Iteration 37015 => Loss: 6.69958816529739831225\n",
      "Iteration 37016 => Loss: 6.69958797526136251577\n",
      "Iteration 37017 => Loss: 6.69958778525093290312\n",
      "Iteration 37018 => Loss: 6.69958759526610592161\n",
      "Iteration 37019 => Loss: 6.69958740530688512393\n",
      "Iteration 37020 => Loss: 6.69958721537326251649\n",
      "Iteration 37021 => Loss: 6.69958702546522033572\n",
      "Iteration 37022 => Loss: 6.69958683558277634518\n",
      "Iteration 37023 => Loss: 6.69958664572591899855\n",
      "Iteration 37024 => Loss: 6.69958645589465096037\n",
      "Iteration 37025 => Loss: 6.69958626608895535526\n",
      "Iteration 37026 => Loss: 6.69958607630883662409\n",
      "Iteration 37027 => Loss: 6.69958588655429121417\n",
      "Iteration 37028 => Loss: 6.69958569682531202005\n",
      "Iteration 37029 => Loss: 6.69958550712190081811\n",
      "Iteration 37030 => Loss: 6.69958531744405849651\n",
      "Iteration 37031 => Loss: 6.69958512779177350893\n",
      "Iteration 37032 => Loss: 6.69958493816503963814\n",
      "Iteration 37033 => Loss: 6.69958474856385866048\n",
      "Iteration 37034 => Loss: 6.69958455898822435870\n",
      "Iteration 37035 => Loss: 6.69958436943814117370\n",
      "Iteration 37036 => Loss: 6.69958417991359667099\n",
      "Iteration 37037 => Loss: 6.69958399041458996237\n",
      "Iteration 37038 => Loss: 6.69958380094112460057\n",
      "Iteration 37039 => Loss: 6.69958361149318282202\n",
      "Iteration 37040 => Loss: 6.69958342207076462671\n",
      "Iteration 37041 => Loss: 6.69958323267388156097\n",
      "Iteration 37042 => Loss: 6.69958304330252030212\n",
      "Iteration 37043 => Loss: 6.69958285395667019202\n",
      "Iteration 37044 => Loss: 6.69958266463633655974\n",
      "Iteration 37045 => Loss: 6.69958247534152029345\n",
      "Iteration 37046 => Loss: 6.69958228607220274142\n",
      "Iteration 37047 => Loss: 6.69958209682839367360\n",
      "Iteration 37048 => Loss: 6.69958190761008154368\n",
      "Iteration 37049 => Loss: 6.69958171841726990436\n",
      "Iteration 37050 => Loss: 6.69958152924994809752\n",
      "Iteration 37051 => Loss: 6.69958134010812589310\n",
      "Iteration 37052 => Loss: 6.69958115099178019847\n",
      "Iteration 37053 => Loss: 6.69958096190092078359\n",
      "Iteration 37054 => Loss: 6.69958077283554764847\n",
      "Iteration 37055 => Loss: 6.69958058379563947682\n",
      "Iteration 37056 => Loss: 6.69958039478121403221\n",
      "Iteration 37057 => Loss: 6.69958020579225532742\n",
      "Iteration 37058 => Loss: 6.69958001682876336247\n",
      "Iteration 37059 => Loss: 6.69957982789072836738\n",
      "Iteration 37060 => Loss: 6.69957963897816011212\n",
      "Iteration 37061 => Loss: 6.69957945009104971490\n",
      "Iteration 37062 => Loss: 6.69957926122938651758\n",
      "Iteration 37063 => Loss: 6.69957907239317407289\n",
      "Iteration 37064 => Loss: 6.69957888358240793991\n",
      "Iteration 37065 => Loss: 6.69957869479707923688\n",
      "Iteration 37066 => Loss: 6.69957850603719862193\n",
      "Iteration 37067 => Loss: 6.69957831730274833149\n",
      "Iteration 37068 => Loss: 6.69957812859373103009\n",
      "Iteration 37069 => Loss: 6.69957793991014050050\n",
      "Iteration 37070 => Loss: 6.69957775125197141364\n",
      "Iteration 37071 => Loss: 6.69957756261922998675\n",
      "Iteration 37072 => Loss: 6.69957737401190289717\n",
      "Iteration 37073 => Loss: 6.69957718542999725031\n",
      "Iteration 37074 => Loss: 6.69957699687349972351\n",
      "Iteration 37075 => Loss: 6.69957680834240321133\n",
      "Iteration 37076 => Loss: 6.69957661983672281281\n",
      "Iteration 37077 => Loss: 6.69957643135643809984\n",
      "Iteration 37078 => Loss: 6.69957624290154907243\n",
      "Iteration 37079 => Loss: 6.69957605447205928328\n",
      "Iteration 37080 => Loss: 6.69957586606795807427\n",
      "Iteration 37081 => Loss: 6.69957567768924278084\n",
      "Iteration 37082 => Loss: 6.69957548933591429119\n",
      "Iteration 37083 => Loss: 6.69957530100796372352\n",
      "Iteration 37084 => Loss: 6.69957511270539463055\n",
      "Iteration 37085 => Loss: 6.69957492442819191325\n",
      "Iteration 37086 => Loss: 6.69957473617636267704\n",
      "Iteration 37087 => Loss: 6.69957454794989715197\n",
      "Iteration 37088 => Loss: 6.69957435974880599616\n",
      "Iteration 37089 => Loss: 6.69957417157306434063\n",
      "Iteration 37090 => Loss: 6.69957398342268550806\n",
      "Iteration 37091 => Loss: 6.69957379529765528758\n",
      "Iteration 37092 => Loss: 6.69957360719797900828\n",
      "Iteration 37093 => Loss: 6.69957341912365045289\n",
      "Iteration 37094 => Loss: 6.69957323107465985146\n",
      "Iteration 37095 => Loss: 6.69957304305101519759\n",
      "Iteration 37096 => Loss: 6.69957285505269428683\n",
      "Iteration 37097 => Loss: 6.69957266707971754727\n",
      "Iteration 37098 => Loss: 6.69957247913206721535\n",
      "Iteration 37099 => Loss: 6.69957229120974595560\n",
      "Iteration 37100 => Loss: 6.69957210331273866899\n",
      "Iteration 37101 => Loss: 6.69957191544105779002\n",
      "Iteration 37102 => Loss: 6.69957172759469177237\n",
      "Iteration 37103 => Loss: 6.69957153977363262243\n",
      "Iteration 37104 => Loss: 6.69957135197789099834\n",
      "Iteration 37105 => Loss: 6.69957116420745002472\n",
      "Iteration 37106 => Loss: 6.69957097646231503063\n",
      "Iteration 37107 => Loss: 6.69957078874247269340\n",
      "Iteration 37108 => Loss: 6.69957060104793189481\n",
      "Iteration 37109 => Loss: 6.69957041337868108855\n",
      "Iteration 37110 => Loss: 6.69957022573471405735\n",
      "Iteration 37111 => Loss: 6.69957003811603435395\n",
      "Iteration 37112 => Loss: 6.69956985052263842562\n",
      "Iteration 37113 => Loss: 6.69956966295451650240\n",
      "Iteration 37114 => Loss: 6.69956947541167657789\n",
      "Iteration 37115 => Loss: 6.69956928789409555947\n",
      "Iteration 37116 => Loss: 6.69956910040179831611\n",
      "Iteration 37117 => Loss: 6.69956891293475464977\n",
      "Iteration 37118 => Loss: 6.69956872549297610675\n",
      "Iteration 37119 => Loss: 6.69956853807645646981\n",
      "Iteration 37120 => Loss: 6.69956835068519218623\n",
      "Iteration 37121 => Loss: 6.69956816331917437424\n",
      "Iteration 37122 => Loss: 6.69956797597840658653\n",
      "Iteration 37123 => Loss: 6.69956778866287994134\n",
      "Iteration 37124 => Loss: 6.69956760137259799137\n",
      "Iteration 37125 => Loss: 6.69956741410754563759\n",
      "Iteration 37126 => Loss: 6.69956722686773709086\n",
      "Iteration 37127 => Loss: 6.69956703965315636395\n",
      "Iteration 37128 => Loss: 6.69956685246380168053\n",
      "Iteration 37129 => Loss: 6.69956666529967570511\n",
      "Iteration 37130 => Loss: 6.69956647816076245050\n",
      "Iteration 37131 => Loss: 6.69956629104706813393\n",
      "Iteration 37132 => Loss: 6.69956610395858920270\n",
      "Iteration 37133 => Loss: 6.69956591689532388045\n",
      "Iteration 37134 => Loss: 6.69956572985725440361\n",
      "Iteration 37135 => Loss: 6.69956554284439764757\n",
      "Iteration 37136 => Loss: 6.69956535585674206601\n",
      "Iteration 37137 => Loss: 6.69956516889427611261\n",
      "Iteration 37138 => Loss: 6.69956498195700600462\n",
      "Iteration 37139 => Loss: 6.69956479504492907751\n",
      "Iteration 37140 => Loss: 6.69956460815803112041\n",
      "Iteration 37141 => Loss: 6.69956442129632723237\n",
      "Iteration 37142 => Loss: 6.69956423445979432074\n",
      "Iteration 37143 => Loss: 6.69956404764843682642\n",
      "Iteration 37144 => Loss: 6.69956386086226363119\n",
      "Iteration 37145 => Loss: 6.69956367410124542516\n",
      "Iteration 37146 => Loss: 6.69956348736540441280\n",
      "Iteration 37147 => Loss: 6.69956330065472016599\n",
      "Iteration 37148 => Loss: 6.69956311396919357293\n",
      "Iteration 37149 => Loss: 6.69956292730882463360\n",
      "Iteration 37150 => Loss: 6.69956274067361068347\n",
      "Iteration 37151 => Loss: 6.69956255406354461712\n",
      "Iteration 37152 => Loss: 6.69956236747862288183\n",
      "Iteration 37153 => Loss: 6.69956218091884814214\n",
      "Iteration 37154 => Loss: 6.69956199438420707537\n",
      "Iteration 37155 => Loss: 6.69956180787470412241\n",
      "Iteration 37156 => Loss: 6.69956162139033306602\n",
      "Iteration 37157 => Loss: 6.69956143493108235987\n",
      "Iteration 37158 => Loss: 6.69956124849696887935\n",
      "Iteration 37159 => Loss: 6.69956106208797752544\n",
      "Iteration 37160 => Loss: 6.69956087570409497545\n",
      "Iteration 37161 => Loss: 6.69956068934533721659\n",
      "Iteration 37162 => Loss: 6.69956050301168737349\n",
      "Iteration 37163 => Loss: 6.69956031670314366977\n",
      "Iteration 37164 => Loss: 6.69956013041971321087\n",
      "Iteration 37165 => Loss: 6.69955994416137379233\n",
      "Iteration 37166 => Loss: 6.69955975792814051317\n",
      "Iteration 37167 => Loss: 6.69955957171999649802\n",
      "Iteration 37168 => Loss: 6.69955938553694618776\n",
      "Iteration 37169 => Loss: 6.69955919937898425331\n",
      "Iteration 37170 => Loss: 6.69955901324610803016\n",
      "Iteration 37171 => Loss: 6.69955882713831218922\n",
      "Iteration 37172 => Loss: 6.69955864105559495414\n",
      "Iteration 37173 => Loss: 6.69955845499795810127\n",
      "Iteration 37174 => Loss: 6.69955826896538564341\n",
      "Iteration 37175 => Loss: 6.69955808295787846873\n",
      "Iteration 37176 => Loss: 6.69955789697543835359\n",
      "Iteration 37177 => Loss: 6.69955771101806263346\n",
      "Iteration 37178 => Loss: 6.69955752508574153836\n",
      "Iteration 37179 => Loss: 6.69955733917847684467\n",
      "Iteration 37180 => Loss: 6.69955715329626588783\n",
      "Iteration 37181 => Loss: 6.69955696743910156243\n",
      "Iteration 37182 => Loss: 6.69955678160697498669\n",
      "Iteration 37183 => Loss: 6.69955659579989237784\n",
      "Iteration 37184 => Loss: 6.69955641001784929500\n",
      "Iteration 37185 => Loss: 6.69955622426083952092\n",
      "Iteration 37186 => Loss: 6.69955603852886216742\n",
      "Iteration 37187 => Loss: 6.69955585282190568819\n",
      "Iteration 37188 => Loss: 6.69955566713997807682\n",
      "Iteration 37189 => Loss: 6.69955548148307222789\n",
      "Iteration 37190 => Loss: 6.69955529585118281233\n",
      "Iteration 37191 => Loss: 6.69955511024430805378\n",
      "Iteration 37192 => Loss: 6.69955492466244173499\n",
      "Iteration 37193 => Loss: 6.69955473910558296780\n",
      "Iteration 37194 => Loss: 6.69955455357373264036\n",
      "Iteration 37195 => Loss: 6.69955436806688098272\n",
      "Iteration 37196 => Loss: 6.69955418258502000128\n",
      "Iteration 37197 => Loss: 6.69955399712815502511\n",
      "Iteration 37198 => Loss: 6.69955381169628694238\n",
      "Iteration 37199 => Loss: 6.69955362628939532499\n",
      "Iteration 37200 => Loss: 6.69955344090749260744\n",
      "Iteration 37201 => Loss: 6.69955325555056635523\n",
      "Iteration 37202 => Loss: 6.69955307021862900285\n",
      "Iteration 37203 => Loss: 6.69955288491165479314\n",
      "Iteration 37204 => Loss: 6.69955269962964994335\n",
      "Iteration 37205 => Loss: 6.69955251437261800618\n",
      "Iteration 37206 => Loss: 6.69955232914054743532\n",
      "Iteration 37207 => Loss: 6.69955214393343645440\n",
      "Iteration 37208 => Loss: 6.69955195875128417526\n",
      "Iteration 37209 => Loss: 6.69955177359407638704\n",
      "Iteration 37210 => Loss: 6.69955158846182463606\n",
      "Iteration 37211 => Loss: 6.69955140335452181688\n",
      "Iteration 37212 => Loss: 6.69955121827216082409\n",
      "Iteration 37213 => Loss: 6.69955103321473455225\n",
      "Iteration 37214 => Loss: 6.69955084818225454768\n",
      "Iteration 37215 => Loss: 6.69955066317470038229\n",
      "Iteration 37216 => Loss: 6.69955047819208004967\n",
      "Iteration 37217 => Loss: 6.69955029323438022715\n",
      "Iteration 37218 => Loss: 6.69955010830160979651\n",
      "Iteration 37219 => Loss: 6.69954992339375721144\n",
      "Iteration 37220 => Loss: 6.69954973851082069558\n",
      "Iteration 37221 => Loss: 6.69954955365279136714\n",
      "Iteration 37222 => Loss: 6.69954936881968077245\n",
      "Iteration 37223 => Loss: 6.69954918401147114793\n",
      "Iteration 37224 => Loss: 6.69954899922816871083\n",
      "Iteration 37225 => Loss: 6.69954881446976280301\n",
      "Iteration 37226 => Loss: 6.69954862973625431266\n",
      "Iteration 37227 => Loss: 6.69954844502763968705\n",
      "Iteration 37228 => Loss: 6.69954826034391270895\n",
      "Iteration 37229 => Loss: 6.69954807568507160198\n",
      "Iteration 37230 => Loss: 6.69954789105111281344\n",
      "Iteration 37231 => Loss: 6.69954770644203279062\n",
      "Iteration 37232 => Loss: 6.69954752185783508622\n",
      "Iteration 37233 => Loss: 6.69954733729850371304\n",
      "Iteration 37234 => Loss: 6.69954715276404133562\n",
      "Iteration 37235 => Loss: 6.69954696825444351305\n",
      "Iteration 37236 => Loss: 6.69954678376971024534\n",
      "Iteration 37237 => Loss: 6.69954659930983709160\n",
      "Iteration 37238 => Loss: 6.69954641487482227546\n",
      "Iteration 37239 => Loss: 6.69954623046465602698\n",
      "Iteration 37240 => Loss: 6.69954604607934722793\n",
      "Iteration 37241 => Loss: 6.69954586171887811474\n",
      "Iteration 37242 => Loss: 6.69954567738325046378\n",
      "Iteration 37243 => Loss: 6.69954549307246693957\n",
      "Iteration 37244 => Loss: 6.69954530878651155490\n",
      "Iteration 37245 => Loss: 6.69954512452539141520\n",
      "Iteration 37246 => Loss: 6.69954494028910563230\n",
      "Iteration 37247 => Loss: 6.69954475607764621259\n",
      "Iteration 37248 => Loss: 6.69954457189100072156\n",
      "Iteration 37249 => Loss: 6.69954438772918425826\n",
      "Iteration 37250 => Loss: 6.69954420359217461822\n",
      "Iteration 37251 => Loss: 6.69954401947998512412\n",
      "Iteration 37252 => Loss: 6.69954383539259978875\n",
      "Iteration 37253 => Loss: 6.69954365133002127664\n",
      "Iteration 37254 => Loss: 6.69954346729224781143\n",
      "Iteration 37255 => Loss: 6.69954328327927051134\n",
      "Iteration 37256 => Loss: 6.69954309929108493549\n",
      "Iteration 37257 => Loss: 6.69954291532769818929\n",
      "Iteration 37258 => Loss: 6.69954273138909872642\n",
      "Iteration 37259 => Loss: 6.69954254747528565872\n",
      "Iteration 37260 => Loss: 6.69954236358625276893\n",
      "Iteration 37261 => Loss: 6.69954217972200183340\n",
      "Iteration 37262 => Loss: 6.69954199588252574671\n",
      "Iteration 37263 => Loss: 6.69954181206782184432\n",
      "Iteration 37264 => Loss: 6.69954162827788923806\n",
      "Iteration 37265 => Loss: 6.69954144451272171068\n",
      "Iteration 37266 => Loss: 6.69954126077230771585\n",
      "Iteration 37267 => Loss: 6.69954107705665702355\n",
      "Iteration 37268 => Loss: 6.69954089336576874558\n",
      "Iteration 37269 => Loss: 6.69954070969962778292\n",
      "Iteration 37270 => Loss: 6.69954052605823857647\n",
      "Iteration 37271 => Loss: 6.69954034244159224443\n",
      "Iteration 37272 => Loss: 6.69954015884968789862\n",
      "Iteration 37273 => Loss: 6.69953997528252553906\n",
      "Iteration 37274 => Loss: 6.69953979174009806030\n",
      "Iteration 37275 => Loss: 6.69953960822240279782\n",
      "Iteration 37276 => Loss: 6.69953942472943708708\n",
      "Iteration 37277 => Loss: 6.69953924126119737537\n",
      "Iteration 37278 => Loss: 6.69953905781767566907\n",
      "Iteration 37279 => Loss: 6.69953887439887729727\n",
      "Iteration 37280 => Loss: 6.69953869100479426635\n",
      "Iteration 37281 => Loss: 6.69953850763541502999\n",
      "Iteration 37282 => Loss: 6.69953832429075823995\n",
      "Iteration 37283 => Loss: 6.69953814097080169176\n",
      "Iteration 37284 => Loss: 6.69953795767554627361\n",
      "Iteration 37285 => Loss: 6.69953777440498932094\n",
      "Iteration 37286 => Loss: 6.69953759115913083377\n",
      "Iteration 37287 => Loss: 6.69953740793796281849\n",
      "Iteration 37288 => Loss: 6.69953722474148616328\n",
      "Iteration 37289 => Loss: 6.69953704156969198635\n",
      "Iteration 37290 => Loss: 6.69953685842258117589\n",
      "Iteration 37291 => Loss: 6.69953667530015017917\n",
      "Iteration 37292 => Loss: 6.69953649220239988438\n",
      "Iteration 37293 => Loss: 6.69953630912931519248\n",
      "Iteration 37294 => Loss: 6.69953612608090143254\n",
      "Iteration 37295 => Loss: 6.69953594305715061097\n",
      "Iteration 37296 => Loss: 6.69953576005806539229\n",
      "Iteration 37297 => Loss: 6.69953557708363955925\n",
      "Iteration 37298 => Loss: 6.69953539413386867096\n",
      "Iteration 37299 => Loss: 6.69953521120875361561\n",
      "Iteration 37300 => Loss: 6.69953502830828018233\n",
      "Iteration 37301 => Loss: 6.69953484543246347016\n",
      "Iteration 37302 => Loss: 6.69953466258127594557\n",
      "Iteration 37303 => Loss: 6.69953447975473981302\n",
      "Iteration 37304 => Loss: 6.69953429695283020351\n",
      "Iteration 37305 => Loss: 6.69953411417555688701\n",
      "Iteration 37306 => Loss: 6.69953393142291897533\n",
      "Iteration 37307 => Loss: 6.69953374869490136945\n",
      "Iteration 37308 => Loss: 6.69953356599150406936\n",
      "Iteration 37309 => Loss: 6.69953338331273240414\n",
      "Iteration 37310 => Loss: 6.69953320065857305110\n",
      "Iteration 37311 => Loss: 6.69953301802902601025\n",
      "Iteration 37312 => Loss: 6.69953283542408772888\n",
      "Iteration 37313 => Loss: 6.69953265284375198974\n",
      "Iteration 37314 => Loss: 6.69953247028802589824\n",
      "Iteration 37315 => Loss: 6.69953228775688991448\n",
      "Iteration 37316 => Loss: 6.69953210525035913747\n",
      "Iteration 37317 => Loss: 6.69953192276842024455\n",
      "Iteration 37318 => Loss: 6.69953174031107234754\n",
      "Iteration 37319 => Loss: 6.69953155787830301193\n",
      "Iteration 37320 => Loss: 6.69953137547012200770\n",
      "Iteration 37321 => Loss: 6.69953119308652045305\n",
      "Iteration 37322 => Loss: 6.69953101072749390710\n",
      "Iteration 37323 => Loss: 6.69953082839303881713\n",
      "Iteration 37324 => Loss: 6.69953064608315873585\n",
      "Iteration 37325 => Loss: 6.69953046379783412334\n",
      "Iteration 37326 => Loss: 6.69953028153708274317\n",
      "Iteration 37327 => Loss: 6.69953009930088771995\n",
      "Iteration 37328 => Loss: 6.69952991708924638914\n",
      "Iteration 37329 => Loss: 6.69952973490216141528\n",
      "Iteration 37330 => Loss: 6.69952955273962480476\n",
      "Iteration 37331 => Loss: 6.69952937060163655758\n",
      "Iteration 37332 => Loss: 6.69952918848818956832\n",
      "Iteration 37333 => Loss: 6.69952900639928028426\n",
      "Iteration 37334 => Loss: 6.69952882433490781722\n",
      "Iteration 37335 => Loss: 6.69952864229507571991\n",
      "Iteration 37336 => Loss: 6.69952846027976622878\n",
      "Iteration 37337 => Loss: 6.69952827828898822560\n",
      "Iteration 37338 => Loss: 6.69952809632272749951\n",
      "Iteration 37339 => Loss: 6.69952791438099115595\n",
      "Iteration 37340 => Loss: 6.69952773246376853677\n",
      "Iteration 37341 => Loss: 6.69952755057106319470\n",
      "Iteration 37342 => Loss: 6.69952736870286891246\n",
      "Iteration 37343 => Loss: 6.69952718685917769648\n",
      "Iteration 37344 => Loss: 6.69952700503998421766\n",
      "Iteration 37345 => Loss: 6.69952682324530091051\n",
      "Iteration 37346 => Loss: 6.69952664147511534054\n",
      "Iteration 37347 => Loss: 6.69952645972941596142\n",
      "Iteration 37348 => Loss: 6.69952627800821254311\n",
      "Iteration 37349 => Loss: 6.69952609631148909841\n",
      "Iteration 37350 => Loss: 6.69952591463925894999\n",
      "Iteration 37351 => Loss: 6.69952573299149722885\n",
      "Iteration 37352 => Loss: 6.69952555136822081039\n",
      "Iteration 37353 => Loss: 6.69952536976941193103\n",
      "Iteration 37354 => Loss: 6.69952518819508036074\n",
      "Iteration 37355 => Loss: 6.69952500664521188867\n",
      "Iteration 37356 => Loss: 6.69952482511981095570\n",
      "Iteration 37357 => Loss: 6.69952464361886868005\n",
      "Iteration 37358 => Loss: 6.69952446214238594990\n",
      "Iteration 37359 => Loss: 6.69952428069035210711\n",
      "Iteration 37360 => Loss: 6.69952409926277336893\n",
      "Iteration 37361 => Loss: 6.69952391785964085358\n",
      "Iteration 37362 => Loss: 6.69952373648095189651\n",
      "Iteration 37363 => Loss: 6.69952355512669850413\n",
      "Iteration 37364 => Loss: 6.69952337379688867003\n",
      "Iteration 37365 => Loss: 6.69952319249151351244\n",
      "Iteration 37366 => Loss: 6.69952301121057214317\n",
      "Iteration 37367 => Loss: 6.69952282995405390409\n",
      "Iteration 37368 => Loss: 6.69952264872195701884\n",
      "Iteration 37369 => Loss: 6.69952246751428770466\n",
      "Iteration 37370 => Loss: 6.69952228633102908617\n",
      "Iteration 37371 => Loss: 6.69952210517218915697\n",
      "Iteration 37372 => Loss: 6.69952192403775992346\n",
      "Iteration 37373 => Loss: 6.69952174292773516839\n",
      "Iteration 37374 => Loss: 6.69952156184212199719\n",
      "Iteration 37375 => Loss: 6.69952138078090797535\n",
      "Iteration 37376 => Loss: 6.69952119974408688563\n",
      "Iteration 37377 => Loss: 6.69952101873166494528\n",
      "Iteration 37378 => Loss: 6.69952083774363593704\n",
      "Iteration 37379 => Loss: 6.69952065677998476190\n",
      "Iteration 37380 => Loss: 6.69952047584073095976\n",
      "Iteration 37381 => Loss: 6.69952029492585676707\n",
      "Iteration 37382 => Loss: 6.69952011403535951928\n",
      "Iteration 37383 => Loss: 6.69951993316923388733\n",
      "Iteration 37384 => Loss: 6.69951975232747898303\n",
      "Iteration 37385 => Loss: 6.69951957151009747093\n",
      "Iteration 37386 => Loss: 6.69951939071707691653\n",
      "Iteration 37387 => Loss: 6.69951920994842442525\n",
      "Iteration 37388 => Loss: 6.69951902920412223352\n",
      "Iteration 37389 => Loss: 6.69951884848417922314\n",
      "Iteration 37390 => Loss: 6.69951866778859539409\n",
      "Iteration 37391 => Loss: 6.69951848711734943009\n",
      "Iteration 37392 => Loss: 6.69951830647045110112\n",
      "Iteration 37393 => Loss: 6.69951812584789951899\n",
      "Iteration 37394 => Loss: 6.69951794524968757827\n",
      "Iteration 37395 => Loss: 6.69951776467580284447\n",
      "Iteration 37396 => Loss: 6.69951758412625686390\n",
      "Iteration 37397 => Loss: 6.69951740360103986660\n",
      "Iteration 37398 => Loss: 6.69951722310014741169\n",
      "Iteration 37399 => Loss: 6.69951704262357417008\n",
      "Iteration 37400 => Loss: 6.69951686217132724721\n",
      "Iteration 37401 => Loss: 6.69951668174339065587\n",
      "Iteration 37402 => Loss: 6.69951650133977238966\n",
      "Iteration 37403 => Loss: 6.69951632096045734954\n",
      "Iteration 37404 => Loss: 6.69951614060545175278\n",
      "Iteration 37405 => Loss: 6.69951596027474405304\n",
      "Iteration 37406 => Loss: 6.69951577996834579665\n",
      "Iteration 37407 => Loss: 6.69951559968623211461\n",
      "Iteration 37408 => Loss: 6.69951541942841988231\n",
      "Iteration 37409 => Loss: 6.69951523919489844161\n",
      "Iteration 37410 => Loss: 6.69951505898565979891\n",
      "Iteration 37411 => Loss: 6.69951487880070484238\n",
      "Iteration 37412 => Loss: 6.69951469864002380206\n",
      "Iteration 37413 => Loss: 6.69951451850363532969\n",
      "Iteration 37414 => Loss: 6.69951433839150567451\n",
      "Iteration 37415 => Loss: 6.69951415830365171189\n",
      "Iteration 37416 => Loss: 6.69951397824005834281\n",
      "Iteration 37417 => Loss: 6.69951379820073178450\n",
      "Iteration 37418 => Loss: 6.69951361818567470152\n",
      "Iteration 37419 => Loss: 6.69951343819486488940\n",
      "Iteration 37420 => Loss: 6.69951325822831211809\n",
      "Iteration 37421 => Loss: 6.69951307828600839400\n",
      "Iteration 37422 => Loss: 6.69951289836795371713\n",
      "Iteration 37423 => Loss: 6.69951271847414187022\n",
      "Iteration 37424 => Loss: 6.69951253860456397149\n",
      "Iteration 37425 => Loss: 6.69951235875923245544\n",
      "Iteration 37426 => Loss: 6.69951217893813755211\n",
      "Iteration 37427 => Loss: 6.69951199914126593882\n",
      "Iteration 37428 => Loss: 6.69951181936862472099\n",
      "Iteration 37429 => Loss: 6.69951163962020412868\n",
      "Iteration 37430 => Loss: 6.69951145989601215547\n",
      "Iteration 37431 => Loss: 6.69951128019603725505\n",
      "Iteration 37432 => Loss: 6.69951110052027321018\n",
      "Iteration 37433 => Loss: 6.69951092086872357356\n",
      "Iteration 37434 => Loss: 6.69951074124138212795\n",
      "Iteration 37435 => Loss: 6.69951056163823999157\n",
      "Iteration 37436 => Loss: 6.69951038205930515801\n",
      "Iteration 37437 => Loss: 6.69951020250455986371\n",
      "Iteration 37438 => Loss: 6.69951002297402009589\n",
      "Iteration 37439 => Loss: 6.69950984346766720279\n",
      "Iteration 37440 => Loss: 6.69950966398550473713\n",
      "Iteration 37441 => Loss: 6.69950948452752470530\n",
      "Iteration 37442 => Loss: 6.69950930509372888366\n",
      "Iteration 37443 => Loss: 6.69950912568410839043\n",
      "Iteration 37444 => Loss: 6.69950894629866500196\n",
      "Iteration 37445 => Loss: 6.69950876693739871826\n",
      "Iteration 37446 => Loss: 6.69950858760029710481\n",
      "Iteration 37447 => Loss: 6.69950840828736104982\n",
      "Iteration 37448 => Loss: 6.69950822899858611237\n",
      "Iteration 37449 => Loss: 6.69950804973397229247\n",
      "Iteration 37450 => Loss: 6.69950787049351692559\n",
      "Iteration 37451 => Loss: 6.69950769127720668905\n",
      "Iteration 37452 => Loss: 6.69950751208505224099\n",
      "Iteration 37453 => Loss: 6.69950733291704647598\n",
      "Iteration 37454 => Loss: 6.69950715377317873589\n",
      "Iteration 37455 => Loss: 6.69950697465345346160\n",
      "Iteration 37456 => Loss: 6.69950679555786177133\n",
      "Iteration 37457 => Loss: 6.69950661648640366508\n",
      "Iteration 37458 => Loss: 6.69950643743907381378\n",
      "Iteration 37459 => Loss: 6.69950625841586955289\n",
      "Iteration 37460 => Loss: 6.69950607941679709967\n",
      "Iteration 37461 => Loss: 6.69950590044183691418\n",
      "Iteration 37462 => Loss: 6.69950572149100231911\n",
      "Iteration 37463 => Loss: 6.69950554256427199817\n",
      "Iteration 37464 => Loss: 6.69950536366165394497\n",
      "Iteration 37465 => Loss: 6.69950518478314105408\n",
      "Iteration 37466 => Loss: 6.69950500592874487182\n",
      "Iteration 37467 => Loss: 6.69950482709843075924\n",
      "Iteration 37468 => Loss: 6.69950464829222536167\n",
      "Iteration 37469 => Loss: 6.69950446951011180374\n",
      "Iteration 37470 => Loss: 6.69950429075209008545\n",
      "Iteration 37471 => Loss: 6.69950411201815132500\n",
      "Iteration 37472 => Loss: 6.69950393330830440419\n",
      "Iteration 37473 => Loss: 6.69950375462253600034\n",
      "Iteration 37474 => Loss: 6.69950357596083811984\n",
      "Iteration 37475 => Loss: 6.69950339732322941444\n",
      "Iteration 37476 => Loss: 6.69950321870968412696\n",
      "Iteration 37477 => Loss: 6.69950304012020936284\n",
      "Iteration 37478 => Loss: 6.69950286155479179939\n",
      "Iteration 37479 => Loss: 6.69950268301344387112\n",
      "Iteration 37480 => Loss: 6.69950250449615047899\n",
      "Iteration 37481 => Loss: 6.69950232600291606389\n",
      "Iteration 37482 => Loss: 6.69950214753372375043\n",
      "Iteration 37483 => Loss: 6.69950196908859574307\n",
      "Iteration 37484 => Loss: 6.69950179066750539647\n",
      "Iteration 37485 => Loss: 6.69950161227045448697\n",
      "Iteration 37486 => Loss: 6.69950143389744656730\n",
      "Iteration 37487 => Loss: 6.69950125554847009113\n",
      "Iteration 37488 => Loss: 6.69950107722353482842\n",
      "Iteration 37489 => Loss: 6.69950089892262212743\n",
      "Iteration 37490 => Loss: 6.69950072064573021180\n",
      "Iteration 37491 => Loss: 6.69950054239287062785\n",
      "Iteration 37492 => Loss: 6.69950036416402738837\n",
      "Iteration 37493 => Loss: 6.69950018595920049336\n",
      "Iteration 37494 => Loss: 6.69950000777838639010\n",
      "Iteration 37495 => Loss: 6.69949982962158419042\n",
      "Iteration 37496 => Loss: 6.69949965148878767707\n",
      "Iteration 37497 => Loss: 6.69949947337999152097\n",
      "Iteration 37498 => Loss: 6.69949929529519927485\n",
      "Iteration 37499 => Loss: 6.69949911723440116873\n",
      "Iteration 37500 => Loss: 6.69949893919760430805\n",
      "Iteration 37501 => Loss: 6.69949876118478560016\n",
      "Iteration 37502 => Loss: 6.69949858319596192047\n",
      "Iteration 37503 => Loss: 6.69949840523112349899\n",
      "Iteration 37504 => Loss: 6.69949822729025790125\n",
      "Iteration 37505 => Loss: 6.69949804937337667354\n",
      "Iteration 37506 => Loss: 6.69949787148046560503\n",
      "Iteration 37507 => Loss: 6.69949769361153002478\n",
      "Iteration 37508 => Loss: 6.69949751576655838647\n",
      "Iteration 37509 => Loss: 6.69949733794555513100\n",
      "Iteration 37510 => Loss: 6.69949716014850960022\n",
      "Iteration 37511 => Loss: 6.69949698237542534685\n",
      "Iteration 37512 => Loss: 6.69949680462629615363\n",
      "Iteration 37513 => Loss: 6.69949662690111757968\n",
      "Iteration 37514 => Loss: 6.69949644919988518410\n",
      "Iteration 37515 => Loss: 6.69949627152260074325\n",
      "Iteration 37516 => Loss: 6.69949609386925981624\n",
      "Iteration 37517 => Loss: 6.69949591623985352129\n",
      "Iteration 37518 => Loss: 6.69949573863438629928\n",
      "Iteration 37519 => Loss: 6.69949556105284838026\n",
      "Iteration 37520 => Loss: 6.69949538349524242875\n",
      "Iteration 37521 => Loss: 6.69949520596155867480\n",
      "Iteration 37522 => Loss: 6.69949502845179800659\n",
      "Iteration 37523 => Loss: 6.69949485096596042411\n",
      "Iteration 37524 => Loss: 6.69949467350404326282\n",
      "Iteration 37525 => Loss: 6.69949449606603231189\n",
      "Iteration 37526 => Loss: 6.69949431865192668312\n",
      "Iteration 37527 => Loss: 6.69949414126173525830\n",
      "Iteration 37528 => Loss: 6.69949396389544560293\n",
      "Iteration 37529 => Loss: 6.69949378655305771701\n",
      "Iteration 37530 => Loss: 6.69949360923456449513\n",
      "Iteration 37531 => Loss: 6.69949343193995972001\n",
      "Iteration 37532 => Loss: 6.69949325466925582617\n",
      "Iteration 37533 => Loss: 6.69949307742243238550\n",
      "Iteration 37534 => Loss: 6.69949290019949028618\n",
      "Iteration 37535 => Loss: 6.69949272300044018635\n",
      "Iteration 37536 => Loss: 6.69949254582526521062\n",
      "Iteration 37537 => Loss: 6.69949236867395203632\n",
      "Iteration 37538 => Loss: 6.69949219154651487429\n",
      "Iteration 37539 => Loss: 6.69949201444295106000\n",
      "Iteration 37540 => Loss: 6.69949183736324282989\n",
      "Iteration 37541 => Loss: 6.69949166030740972388\n",
      "Iteration 37542 => Loss: 6.69949148327542864934\n",
      "Iteration 37543 => Loss: 6.69949130626730493532\n",
      "Iteration 37544 => Loss: 6.69949112928302970005\n",
      "Iteration 37545 => Loss: 6.69949095232260116717\n",
      "Iteration 37546 => Loss: 6.69949077538602466575\n",
      "Iteration 37547 => Loss: 6.69949059847328154405\n",
      "Iteration 37548 => Loss: 6.69949042158438690109\n",
      "Iteration 37549 => Loss: 6.69949024471931942060\n",
      "Iteration 37550 => Loss: 6.69949006787809242525\n",
      "Iteration 37551 => Loss: 6.69948989106068726329\n",
      "Iteration 37552 => Loss: 6.69948971426711281651\n",
      "Iteration 37553 => Loss: 6.69948953749736197949\n",
      "Iteration 37554 => Loss: 6.69948936075143119950\n",
      "Iteration 37555 => Loss: 6.69948918402931337113\n",
      "Iteration 37556 => Loss: 6.69948900733100849436\n",
      "Iteration 37557 => Loss: 6.69948883065651834556\n",
      "Iteration 37558 => Loss: 6.69948865400583315477\n",
      "Iteration 37559 => Loss: 6.69948847737895114562\n",
      "Iteration 37560 => Loss: 6.69948830077586876541\n",
      "Iteration 37561 => Loss: 6.69948812419658334960\n",
      "Iteration 37562 => Loss: 6.69948794764109845090\n",
      "Iteration 37563 => Loss: 6.69948777110939275303\n",
      "Iteration 37564 => Loss: 6.69948759460147780231\n",
      "Iteration 37565 => Loss: 6.69948741811735626328\n",
      "Iteration 37566 => Loss: 6.69948724165700948419\n",
      "Iteration 37567 => Loss: 6.69948706522044279410\n",
      "Iteration 37568 => Loss: 6.69948688880764375853\n",
      "Iteration 37569 => Loss: 6.69948671241862392378\n",
      "Iteration 37570 => Loss: 6.69948653605337440808\n",
      "Iteration 37571 => Loss: 6.69948635971188366511\n",
      "Iteration 37572 => Loss: 6.69948618339415613576\n",
      "Iteration 37573 => Loss: 6.69948600710019448456\n",
      "Iteration 37574 => Loss: 6.69948583082997917160\n",
      "Iteration 37575 => Loss: 6.69948565458351819046\n",
      "Iteration 37576 => Loss: 6.69948547836081331752\n",
      "Iteration 37577 => Loss: 6.69948530216184767738\n",
      "Iteration 37578 => Loss: 6.69948512598662837547\n",
      "Iteration 37579 => Loss: 6.69948494983514208911\n",
      "Iteration 37580 => Loss: 6.69948477370740480552\n",
      "Iteration 37581 => Loss: 6.69948459760338810298\n",
      "Iteration 37582 => Loss: 6.69948442152310796871\n",
      "Iteration 37583 => Loss: 6.69948424546655374456\n",
      "Iteration 37584 => Loss: 6.69948406943371832512\n",
      "Iteration 37585 => Loss: 6.69948389342461592122\n",
      "Iteration 37586 => Loss: 6.69948371743921988752\n",
      "Iteration 37587 => Loss: 6.69948354147753732946\n",
      "Iteration 37588 => Loss: 6.69948336553956913519\n",
      "Iteration 37589 => Loss: 6.69948318962530819931\n",
      "Iteration 37590 => Loss: 6.69948301373475452181\n",
      "Iteration 37591 => Loss: 6.69948283786789922090\n",
      "Iteration 37592 => Loss: 6.69948266202473696751\n",
      "Iteration 37593 => Loss: 6.69948248620527930797\n",
      "Iteration 37594 => Loss: 6.69948231040950492599\n",
      "Iteration 37595 => Loss: 6.69948213463742625606\n",
      "Iteration 37596 => Loss: 6.69948195888902997552\n",
      "Iteration 37597 => Loss: 6.69948178316431786072\n",
      "Iteration 37598 => Loss: 6.69948160746327925352\n",
      "Iteration 37599 => Loss: 6.69948143178592570024\n",
      "Iteration 37600 => Loss: 6.69948125613222877917\n",
      "Iteration 37601 => Loss: 6.69948108050220980658\n",
      "Iteration 37602 => Loss: 6.69948090489586611795\n",
      "Iteration 37603 => Loss: 6.69948072931317462064\n",
      "Iteration 37604 => Loss: 6.69948055375414419643\n",
      "Iteration 37605 => Loss: 6.69948037821877129261\n",
      "Iteration 37606 => Loss: 6.69948020270704969192\n",
      "Iteration 37607 => Loss: 6.69948002721898028256\n",
      "Iteration 37608 => Loss: 6.69947985175455773543\n",
      "Iteration 37609 => Loss: 6.69947967631378116238\n",
      "Iteration 37610 => Loss: 6.69947950089664701068\n",
      "Iteration 37611 => Loss: 6.69947932550314995126\n",
      "Iteration 37612 => Loss: 6.69947915013328376688\n",
      "Iteration 37613 => Loss: 6.69947897478704756935\n",
      "Iteration 37614 => Loss: 6.69947879946443602961\n",
      "Iteration 37615 => Loss: 6.69947862416546069397\n",
      "Iteration 37616 => Loss: 6.69947844889009580527\n",
      "Iteration 37617 => Loss: 6.69947827363835646253\n",
      "Iteration 37618 => Loss: 6.69947809841022490218\n",
      "Iteration 37619 => Loss: 6.69947792320571178237\n",
      "Iteration 37620 => Loss: 6.69947774802480822132\n",
      "Iteration 37621 => Loss: 6.69947757286750622541\n",
      "Iteration 37622 => Loss: 6.69947739773381201189\n",
      "Iteration 37623 => Loss: 6.69947722262370781721\n",
      "Iteration 37624 => Loss: 6.69947704753721140492\n",
      "Iteration 37625 => Loss: 6.69947687247429701785\n",
      "Iteration 37626 => Loss: 6.69947669743497886685\n",
      "Iteration 37627 => Loss: 6.69947652241924629379\n",
      "Iteration 37628 => Loss: 6.69947634742709308142\n",
      "Iteration 37629 => Loss: 6.69947617245852367063\n",
      "Iteration 37630 => Loss: 6.69947599751353539688\n",
      "Iteration 37631 => Loss: 6.69947582259211671385\n",
      "Iteration 37632 => Loss: 6.69947564769426229248\n",
      "Iteration 37633 => Loss: 6.69947547281998101454\n",
      "Iteration 37634 => Loss: 6.69947529796926666279\n",
      "Iteration 37635 => Loss: 6.69947512314211657269\n",
      "Iteration 37636 => Loss: 6.69947494833851919793\n",
      "Iteration 37637 => Loss: 6.69947477355847631486\n",
      "Iteration 37638 => Loss: 6.69947459880198348259\n",
      "Iteration 37639 => Loss: 6.69947442406904158929\n",
      "Iteration 37640 => Loss: 6.69947424935964352954\n",
      "Iteration 37641 => Loss: 6.69947407467378930335\n",
      "Iteration 37642 => Loss: 6.69947390001147358163\n",
      "Iteration 37643 => Loss: 6.69947372537269458803\n",
      "Iteration 37644 => Loss: 6.69947355075744788167\n",
      "Iteration 37645 => Loss: 6.69947337616572546892\n",
      "Iteration 37646 => Loss: 6.69947320159753978430\n",
      "Iteration 37647 => Loss: 6.69947302705286240609\n",
      "Iteration 37648 => Loss: 6.69947285253171820329\n",
      "Iteration 37649 => Loss: 6.69947267803408763598\n",
      "Iteration 37650 => Loss: 6.69947250355996182236\n",
      "Iteration 37651 => Loss: 6.69947232910935852601\n",
      "Iteration 37652 => Loss: 6.69947215468225465429\n",
      "Iteration 37653 => Loss: 6.69947198027865997716\n",
      "Iteration 37654 => Loss: 6.69947180589856561284\n",
      "Iteration 37655 => Loss: 6.69947163154196712043\n",
      "Iteration 37656 => Loss: 6.69947145720886005904\n",
      "Iteration 37657 => Loss: 6.69947128289924798139\n",
      "Iteration 37658 => Loss: 6.69947110861312378205\n",
      "Iteration 37659 => Loss: 6.69947093435048657284\n",
      "Iteration 37660 => Loss: 6.69947076011132924833\n",
      "Iteration 37661 => Loss: 6.69947058589565092035\n",
      "Iteration 37662 => Loss: 6.69947041170345070071\n",
      "Iteration 37663 => Loss: 6.69947023753471793128\n",
      "Iteration 37664 => Loss: 6.69947006338945705295\n",
      "Iteration 37665 => Loss: 6.69946988926765829575\n",
      "Iteration 37666 => Loss: 6.69946971516933054147\n",
      "Iteration 37667 => Loss: 6.69946954109445780290\n",
      "Iteration 37668 => Loss: 6.69946936704304452093\n",
      "Iteration 37669 => Loss: 6.69946919301507737288\n",
      "Iteration 37670 => Loss: 6.69946901901056612871\n",
      "Iteration 37671 => Loss: 6.69946884502950013029\n",
      "Iteration 37672 => Loss: 6.69946867107188026580\n",
      "Iteration 37673 => Loss: 6.69946849713770031798\n",
      "Iteration 37674 => Loss: 6.69946832322695673412\n",
      "Iteration 37675 => Loss: 6.69946814933965217875\n",
      "Iteration 37676 => Loss: 6.69946797547577244103\n",
      "Iteration 37677 => Loss: 6.69946780163532196184\n",
      "Iteration 37678 => Loss: 6.69946762781829985300\n",
      "Iteration 37679 => Loss: 6.69946745402469900910\n",
      "Iteration 37680 => Loss: 6.69946728025451854194\n",
      "Iteration 37681 => Loss: 6.69946710650775401064\n",
      "Iteration 37682 => Loss: 6.69946693278439475705\n",
      "Iteration 37683 => Loss: 6.69946675908445055114\n",
      "Iteration 37684 => Loss: 6.69946658540790895842\n",
      "Iteration 37685 => Loss: 6.69946641175477441976\n",
      "Iteration 37686 => Loss: 6.69946623812503538886\n",
      "Iteration 37687 => Loss: 6.69946606451869364207\n",
      "Iteration 37688 => Loss: 6.69946589093574917939\n",
      "Iteration 37689 => Loss: 6.69946571737618778997\n",
      "Iteration 37690 => Loss: 6.69946554384002279647\n",
      "Iteration 37691 => Loss: 6.69946537032723465899\n",
      "Iteration 37692 => Loss: 6.69946519683783403565\n",
      "Iteration 37693 => Loss: 6.69946502337180582742\n",
      "Iteration 37694 => Loss: 6.69946484992915269885\n",
      "Iteration 37695 => Loss: 6.69946467650987287357\n",
      "Iteration 37696 => Loss: 6.69946450311396191069\n",
      "Iteration 37697 => Loss: 6.69946432974141536931\n",
      "Iteration 37698 => Loss: 6.69946415639222436766\n",
      "Iteration 37699 => Loss: 6.69946398306639867570\n",
      "Iteration 37700 => Loss: 6.69946380976392763529\n",
      "Iteration 37701 => Loss: 6.69946363648480414099\n",
      "Iteration 37702 => Loss: 6.69946346322903618642\n",
      "Iteration 37703 => Loss: 6.69946328999661488979\n",
      "Iteration 37704 => Loss: 6.69946311678753581020\n",
      "Iteration 37705 => Loss: 6.69946294360179539495\n",
      "Iteration 37706 => Loss: 6.69946277043939453222\n",
      "Iteration 37707 => Loss: 6.69946259730032789292\n",
      "Iteration 37708 => Loss: 6.69946242418458481893\n",
      "Iteration 37709 => Loss: 6.69946225109217419202\n",
      "Iteration 37710 => Loss: 6.69946207802308446588\n",
      "Iteration 37711 => Loss: 6.69946190497732096958\n",
      "Iteration 37712 => Loss: 6.69946173195487304497\n",
      "Iteration 37713 => Loss: 6.69946155895573536299\n",
      "Iteration 37714 => Loss: 6.69946138597991591723\n",
      "Iteration 37715 => Loss: 6.69946121302740049686\n",
      "Iteration 37716 => Loss: 6.69946104009819087821\n",
      "Iteration 37717 => Loss: 6.69946086719228794948\n",
      "Iteration 37718 => Loss: 6.69946069430968105252\n",
      "Iteration 37719 => Loss: 6.69946052145036485825\n",
      "Iteration 37720 => Loss: 6.69946034861435535390\n",
      "Iteration 37721 => Loss: 6.69946017580162500593\n",
      "Iteration 37722 => Loss: 6.69946000301218180795\n",
      "Iteration 37723 => Loss: 6.69945983024602487177\n",
      "Iteration 37724 => Loss: 6.69945965750314620379\n",
      "Iteration 37725 => Loss: 6.69945948478354580402\n",
      "Iteration 37726 => Loss: 6.69945931208721212613\n",
      "Iteration 37727 => Loss: 6.69945913941416115733\n",
      "Iteration 37728 => Loss: 6.69945896676437246953\n",
      "Iteration 37729 => Loss: 6.69945879413784428635\n",
      "Iteration 37730 => Loss: 6.69945862153458460142\n",
      "Iteration 37731 => Loss: 6.69945844895458364476\n",
      "Iteration 37732 => Loss: 6.69945827639783342278\n",
      "Iteration 37733 => Loss: 6.69945810386433393546\n",
      "Iteration 37734 => Loss: 6.69945793135408695917\n",
      "Iteration 37735 => Loss: 6.69945775886707473035\n",
      "Iteration 37736 => Loss: 6.69945758640332211797\n",
      "Iteration 37737 => Loss: 6.69945741396280780577\n",
      "Iteration 37738 => Loss: 6.69945724154552291196\n",
      "Iteration 37739 => Loss: 6.69945706915147454197\n",
      "Iteration 37740 => Loss: 6.69945689678064582040\n",
      "Iteration 37741 => Loss: 6.69945672443305895172\n",
      "Iteration 37742 => Loss: 6.69945655210868551421\n",
      "Iteration 37743 => Loss: 6.69945637980753971874\n",
      "Iteration 37744 => Loss: 6.69945620752960735445\n",
      "Iteration 37745 => Loss: 6.69945603527488842133\n",
      "Iteration 37746 => Loss: 6.69945586304338558392\n",
      "Iteration 37747 => Loss: 6.69945569083508907227\n",
      "Iteration 37748 => Loss: 6.69945551864999178093\n",
      "Iteration 37749 => Loss: 6.69945534648810703260\n",
      "Iteration 37750 => Loss: 6.69945517434940729373\n",
      "Iteration 37751 => Loss: 6.69945500223391743333\n",
      "Iteration 37752 => Loss: 6.69945483014160991786\n",
      "Iteration 37753 => Loss: 6.69945465807249629364\n",
      "Iteration 37754 => Loss: 6.69945448602657211978\n",
      "Iteration 37755 => Loss: 6.69945431400382762632\n",
      "Iteration 37756 => Loss: 6.69945414200426103690\n",
      "Iteration 37757 => Loss: 6.69945397002787146334\n",
      "Iteration 37758 => Loss: 6.69945379807466068200\n",
      "Iteration 37759 => Loss: 6.69945362614461714656\n",
      "Iteration 37760 => Loss: 6.69945345423774529792\n",
      "Iteration 37761 => Loss: 6.69945328235403625428\n",
      "Iteration 37762 => Loss: 6.69945311049348202204\n",
      "Iteration 37763 => Loss: 6.69945293865609325934\n",
      "Iteration 37764 => Loss: 6.69945276684185753169\n",
      "Iteration 37765 => Loss: 6.69945259505077395090\n",
      "Iteration 37766 => Loss: 6.69945242328283896427\n",
      "Iteration 37767 => Loss: 6.69945225153804990725\n",
      "Iteration 37768 => Loss: 6.69945207981640056261\n",
      "Iteration 37769 => Loss: 6.69945190811789359486\n",
      "Iteration 37770 => Loss: 6.69945173644252367495\n",
      "Iteration 37771 => Loss: 6.69945156479028458563\n",
      "Iteration 37772 => Loss: 6.69945139316118076778\n",
      "Iteration 37773 => Loss: 6.69945122155520156326\n",
      "Iteration 37774 => Loss: 6.69945104997233897848\n",
      "Iteration 37775 => Loss: 6.69945087841260811246\n",
      "Iteration 37776 => Loss: 6.69945070687599120163\n",
      "Iteration 37777 => Loss: 6.69945053536248291692\n",
      "Iteration 37778 => Loss: 6.69945036387209480466\n",
      "Iteration 37779 => Loss: 6.69945019240480732492\n",
      "Iteration 37780 => Loss: 6.69945002096063468855\n",
      "Iteration 37781 => Loss: 6.69944984953955557927\n",
      "Iteration 37782 => Loss: 6.69944967814157799069\n",
      "Iteration 37783 => Loss: 6.69944950676669925826\n",
      "Iteration 37784 => Loss: 6.69944933541490872386\n",
      "Iteration 37785 => Loss: 6.69944916408620727566\n",
      "Iteration 37786 => Loss: 6.69944899278059491365\n",
      "Iteration 37787 => Loss: 6.69944882149806986149\n",
      "Iteration 37788 => Loss: 6.69944865023862057285\n",
      "Iteration 37789 => Loss: 6.69944847900224527137\n",
      "Iteration 37790 => Loss: 6.69944830778894928613\n",
      "Iteration 37791 => Loss: 6.69944813659872639988\n",
      "Iteration 37792 => Loss: 6.69944796543156773083\n",
      "Iteration 37793 => Loss: 6.69944779428747416716\n",
      "Iteration 37794 => Loss: 6.69944762316643949163\n",
      "Iteration 37795 => Loss: 6.69944745206847169783\n",
      "Iteration 37796 => Loss: 6.69944728099355035766\n",
      "Iteration 37797 => Loss: 6.69944710994168612928\n",
      "Iteration 37798 => Loss: 6.69944693891286835452\n",
      "Iteration 37799 => Loss: 6.69944676790709880976\n",
      "Iteration 37800 => Loss: 6.69944659692437394227\n",
      "Iteration 37801 => Loss: 6.69944642596468042939\n",
      "Iteration 37802 => Loss: 6.69944625502803425832\n",
      "Iteration 37803 => Loss: 6.69944608411441766549\n",
      "Iteration 37804 => Loss: 6.69944591322383153909\n",
      "Iteration 37805 => Loss: 6.69944574235627499093\n",
      "Iteration 37806 => Loss: 6.69944557151173913923\n",
      "Iteration 37807 => Loss: 6.69944540069022576034\n",
      "Iteration 37808 => Loss: 6.69944522989172774885\n",
      "Iteration 37809 => Loss: 6.69944505911624688110\n",
      "Iteration 37810 => Loss: 6.69944488836378582164\n",
      "Iteration 37811 => Loss: 6.69944471763432236600\n",
      "Iteration 37812 => Loss: 6.69944454692787516592\n",
      "Iteration 37813 => Loss: 6.69944437624442379331\n",
      "Iteration 37814 => Loss: 6.69944420558397357723\n",
      "Iteration 37815 => Loss: 6.69944403494651830044\n",
      "Iteration 37816 => Loss: 6.69944386433206329201\n",
      "Iteration 37817 => Loss: 6.69944369374059167654\n",
      "Iteration 37818 => Loss: 6.69944352317210789494\n",
      "Iteration 37819 => Loss: 6.69944335262660661812\n",
      "Iteration 37820 => Loss: 6.69944318210408784608\n",
      "Iteration 37821 => Loss: 6.69944301160455246702\n",
      "Iteration 37822 => Loss: 6.69944284112798538189\n",
      "Iteration 37823 => Loss: 6.69944267067439280794\n",
      "Iteration 37824 => Loss: 6.69944250024377119246\n",
      "Iteration 37825 => Loss: 6.69944232983611343002\n",
      "Iteration 37826 => Loss: 6.69944215945141241519\n",
      "Iteration 37827 => Loss: 6.69944198908967436523\n",
      "Iteration 37828 => Loss: 6.69944181875088862199\n",
      "Iteration 37829 => Loss: 6.69944164843506584361\n",
      "Iteration 37830 => Loss: 6.69944147814218826653\n",
      "Iteration 37831 => Loss: 6.69944130787226121981\n",
      "Iteration 37832 => Loss: 6.69944113762527582168\n",
      "Iteration 37833 => Loss: 6.69944096740122141398\n",
      "Iteration 37834 => Loss: 6.69944079720011664847\n",
      "Iteration 37835 => Loss: 6.69944062702194109704\n",
      "Iteration 37836 => Loss: 6.69944045686670008877\n",
      "Iteration 37837 => Loss: 6.69944028673438474186\n",
      "Iteration 37838 => Loss: 6.69944011662499505633\n",
      "Iteration 37839 => Loss: 6.69943994653852836763\n",
      "Iteration 37840 => Loss: 6.69943977647498378758\n",
      "Iteration 37841 => Loss: 6.69943960643435332258\n",
      "Iteration 37842 => Loss: 6.69943943641663430810\n",
      "Iteration 37843 => Loss: 6.69943926642182763231\n",
      "Iteration 37844 => Loss: 6.69943909644992263708\n",
      "Iteration 37845 => Loss: 6.69943892650092642782\n",
      "Iteration 37846 => Loss: 6.69943875657483012276\n",
      "Iteration 37847 => Loss: 6.69943858667163105736\n",
      "Iteration 37848 => Loss: 6.69943841679132745526\n",
      "Iteration 37849 => Loss: 6.69943824693391132286\n",
      "Iteration 37850 => Loss: 6.69943807709938443651\n",
      "Iteration 37851 => Loss: 6.69943790728774679621\n",
      "Iteration 37852 => Loss: 6.69943773749899129655\n",
      "Iteration 37853 => Loss: 6.69943756773311260844\n",
      "Iteration 37854 => Loss: 6.69943739799010806735\n",
      "Iteration 37855 => Loss: 6.69943722826998211417\n",
      "Iteration 37856 => Loss: 6.69943705857272142623\n",
      "Iteration 37857 => Loss: 6.69943688889832067446\n",
      "Iteration 37858 => Loss: 6.69943671924679584606\n",
      "Iteration 37859 => Loss: 6.69943654961812384840\n",
      "Iteration 37860 => Loss: 6.69943638001231445145\n",
      "Iteration 37861 => Loss: 6.69943621042935699705\n",
      "Iteration 37862 => Loss: 6.69943604086924882068\n",
      "Iteration 37863 => Loss: 6.69943587133199347505\n",
      "Iteration 37864 => Loss: 6.69943570181758119020\n",
      "Iteration 37865 => Loss: 6.69943553232601196612\n",
      "Iteration 37866 => Loss: 6.69943536285727869739\n",
      "Iteration 37867 => Loss: 6.69943519341138671308\n",
      "Iteration 37868 => Loss: 6.69943502398832091416\n",
      "Iteration 37869 => Loss: 6.69943485458808574151\n",
      "Iteration 37870 => Loss: 6.69943468521067853061\n",
      "Iteration 37871 => Loss: 6.69943451585609217602\n",
      "Iteration 37872 => Loss: 6.69943434652433289500\n",
      "Iteration 37873 => Loss: 6.69943417721539002940\n",
      "Iteration 37874 => Loss: 6.69943400792926180287\n",
      "Iteration 37875 => Loss: 6.69943383866593844544\n",
      "Iteration 37876 => Loss: 6.69943366942543150344\n",
      "Iteration 37877 => Loss: 6.69943350020772321329\n",
      "Iteration 37878 => Loss: 6.69943333101282156861\n",
      "Iteration 37879 => Loss: 6.69943316184071324670\n",
      "Iteration 37880 => Loss: 6.69943299269140979391\n",
      "Iteration 37881 => Loss: 6.69943282356489344664\n",
      "Iteration 37882 => Loss: 6.69943265446116775763\n",
      "Iteration 37883 => Loss: 6.69943248538022206873\n",
      "Iteration 37884 => Loss: 6.69943231632207236714\n",
      "Iteration 37885 => Loss: 6.69943214728669733660\n",
      "Iteration 37886 => Loss: 6.69943197827409786527\n",
      "Iteration 37887 => Loss: 6.69943180928427839405\n",
      "Iteration 37888 => Loss: 6.69943164031722382390\n",
      "Iteration 37889 => Loss: 6.69943147137294570115\n",
      "Iteration 37890 => Loss: 6.69943130245142803858\n",
      "Iteration 37891 => Loss: 6.69943113355266994802\n",
      "Iteration 37892 => Loss: 6.69943096467667142946\n",
      "Iteration 37893 => Loss: 6.69943079582343248290\n",
      "Iteration 37894 => Loss: 6.69943062699294511475\n",
      "Iteration 37895 => Loss: 6.69943045818521021317\n",
      "Iteration 37896 => Loss: 6.69943028940021445550\n",
      "Iteration 37897 => Loss: 6.69943012063797294076\n",
      "Iteration 37898 => Loss: 6.69942995189846435267\n",
      "Iteration 37899 => Loss: 6.69942978318169579666\n",
      "Iteration 37900 => Loss: 6.69942961448765572641\n",
      "Iteration 37901 => Loss: 6.69942944581635835277\n",
      "Iteration 37902 => Loss: 6.69942927716778413583\n",
      "Iteration 37903 => Loss: 6.69942910854192685832\n",
      "Iteration 37904 => Loss: 6.69942893993880872472\n",
      "Iteration 37905 => Loss: 6.69942877135839864877\n",
      "Iteration 37906 => Loss: 6.69942860280070906498\n",
      "Iteration 37907 => Loss: 6.69942843426573286791\n",
      "Iteration 37908 => Loss: 6.69942826575346384033\n",
      "Iteration 37909 => Loss: 6.69942809726390553493\n",
      "Iteration 37910 => Loss: 6.69942792879704906994\n",
      "Iteration 37911 => Loss: 6.69942776035289533354\n",
      "Iteration 37912 => Loss: 6.69942759193143810847\n",
      "Iteration 37913 => Loss: 6.69942742353267650657\n",
      "Iteration 37914 => Loss: 6.69942725515660519875\n",
      "Iteration 37915 => Loss: 6.69942708680322063231\n",
      "Iteration 37916 => Loss: 6.69942691847252547177\n",
      "Iteration 37917 => Loss: 6.69942675016450728265\n",
      "Iteration 37918 => Loss: 6.69942658187917938761\n",
      "Iteration 37919 => Loss: 6.69942641361652047038\n",
      "Iteration 37920 => Loss: 6.69942624537653319550\n",
      "Iteration 37921 => Loss: 6.69942607715921756295\n",
      "Iteration 37922 => Loss: 6.69942590896457357275\n",
      "Iteration 37923 => Loss: 6.69942574079258612585\n",
      "Iteration 37924 => Loss: 6.69942557264326676858\n",
      "Iteration 37925 => Loss: 6.69942540451660040191\n",
      "Iteration 37926 => Loss: 6.69942523641259324307\n",
      "Iteration 37927 => Loss: 6.69942506833123818666\n",
      "Iteration 37928 => Loss: 6.69942490027252990359\n",
      "Iteration 37929 => Loss: 6.69942473223646839386\n",
      "Iteration 37930 => Loss: 6.69942456422305365749\n",
      "Iteration 37931 => Loss: 6.69942439623227325995\n",
      "Iteration 37932 => Loss: 6.69942422826412897763\n",
      "Iteration 37933 => Loss: 6.69942406031862169868\n",
      "Iteration 37934 => Loss: 6.69942389239574342952\n",
      "Iteration 37935 => Loss: 6.69942372449549061741\n",
      "Iteration 37936 => Loss: 6.69942355661786770327\n",
      "Iteration 37937 => Loss: 6.69942338876286047622\n",
      "Iteration 37938 => Loss: 6.69942322093048048259\n",
      "Iteration 37939 => Loss: 6.69942305312070462975\n",
      "Iteration 37940 => Loss: 6.69942288533354712854\n",
      "Iteration 37941 => Loss: 6.69942271756899732083\n",
      "Iteration 37942 => Loss: 6.69942254982705964750\n",
      "Iteration 37943 => Loss: 6.69942238210772256224\n",
      "Iteration 37944 => Loss: 6.69942221441097895962\n",
      "Iteration 37945 => Loss: 6.69942204673683949778\n",
      "Iteration 37946 => Loss: 6.69942187908528996587\n",
      "Iteration 37947 => Loss: 6.69942171145633658114\n",
      "Iteration 37948 => Loss: 6.69942154384996513272\n",
      "Iteration 37949 => Loss: 6.69942137626618450241\n",
      "Iteration 37950 => Loss: 6.69942120870497959118\n",
      "Iteration 37951 => Loss: 6.69942104116636016897\n",
      "Iteration 37952 => Loss: 6.69942087365031291313\n",
      "Iteration 37953 => Loss: 6.69942070615684226453\n",
      "Iteration 37954 => Loss: 6.69942053868593578869\n",
      "Iteration 37955 => Loss: 6.69942037123759703832\n",
      "Iteration 37956 => Loss: 6.69942020381182334887\n",
      "Iteration 37957 => Loss: 6.69942003640861116764\n",
      "Iteration 37958 => Loss: 6.69941986902795871828\n",
      "Iteration 37959 => Loss: 6.69941970166985445445\n",
      "Iteration 37960 => Loss: 6.69941953433430548159\n",
      "Iteration 37961 => Loss: 6.69941936702130913517\n",
      "Iteration 37962 => Loss: 6.69941919973084676343\n",
      "Iteration 37963 => Loss: 6.69941903246293524177\n",
      "Iteration 37964 => Loss: 6.69941886521756035933\n",
      "Iteration 37965 => Loss: 6.69941869799472389246\n",
      "Iteration 37966 => Loss: 6.69941853079442317664\n",
      "Iteration 37967 => Loss: 6.69941836361664755373\n",
      "Iteration 37968 => Loss: 6.69941819646139880007\n",
      "Iteration 37969 => Loss: 6.69941802932867691567\n",
      "Iteration 37970 => Loss: 6.69941786221847390692\n",
      "Iteration 37971 => Loss: 6.69941769513079243836\n",
      "Iteration 37972 => Loss: 6.69941752806562274003\n",
      "Iteration 37973 => Loss: 6.69941736102297014099\n",
      "Iteration 37974 => Loss: 6.69941719400281687768\n",
      "Iteration 37975 => Loss: 6.69941702700518160185\n",
      "Iteration 37976 => Loss: 6.69941686003003944450\n",
      "Iteration 37977 => Loss: 6.69941669307740550465\n",
      "Iteration 37978 => Loss: 6.69941652614726024240\n",
      "Iteration 37979 => Loss: 6.69941635923961431587\n",
      "Iteration 37980 => Loss: 6.69941619235446061964\n",
      "Iteration 37981 => Loss: 6.69941602549178849557\n",
      "Iteration 37982 => Loss: 6.69941585865160504909\n",
      "Iteration 37983 => Loss: 6.69941569183390228659\n",
      "Iteration 37984 => Loss: 6.69941552503867665536\n",
      "Iteration 37985 => Loss: 6.69941535826592460268\n",
      "Iteration 37986 => Loss: 6.69941519151565412216\n",
      "Iteration 37987 => Loss: 6.69941502478785100294\n",
      "Iteration 37988 => Loss: 6.69941485808250902778\n",
      "Iteration 37989 => Loss: 6.69941469139963352575\n",
      "Iteration 37990 => Loss: 6.69941452473921739141\n",
      "Iteration 37991 => Loss: 6.69941435810125707206\n",
      "Iteration 37992 => Loss: 6.69941419148576144948\n",
      "Iteration 37993 => Loss: 6.69941402489270831921\n",
      "Iteration 37994 => Loss: 6.69941385832210478668\n",
      "Iteration 37995 => Loss: 6.69941369177394729917\n",
      "Iteration 37996 => Loss: 6.69941352524823052761\n",
      "Iteration 37997 => Loss: 6.69941335874495624836\n",
      "Iteration 37998 => Loss: 6.69941319226411202692\n",
      "Iteration 37999 => Loss: 6.69941302580570674507\n",
      "Iteration 38000 => Loss: 6.69941285936973152104\n",
      "Iteration 38001 => Loss: 6.69941269295618191393\n",
      "Iteration 38002 => Loss: 6.69941252656505881191\n",
      "Iteration 38003 => Loss: 6.69941236019635244503\n",
      "Iteration 38004 => Loss: 6.69941219385006636600\n",
      "Iteration 38005 => Loss: 6.69941202752619613392\n",
      "Iteration 38006 => Loss: 6.69941186122473819609\n",
      "Iteration 38007 => Loss: 6.69941169494569166432\n",
      "Iteration 38008 => Loss: 6.69941152868905298590\n",
      "Iteration 38009 => Loss: 6.69941136245480617362\n",
      "Iteration 38010 => Loss: 6.69941119624297076740\n",
      "Iteration 38011 => Loss: 6.69941103005352633915\n",
      "Iteration 38012 => Loss: 6.69941086388647555339\n",
      "Iteration 38013 => Loss: 6.69941069774182640373\n",
      "Iteration 38014 => Loss: 6.69941053161955224482\n",
      "Iteration 38015 => Loss: 6.69941036551966728751\n",
      "Iteration 38016 => Loss: 6.69941019944216531457\n",
      "Iteration 38017 => Loss: 6.69941003338704188508\n",
      "Iteration 38018 => Loss: 6.69940986735429433452\n",
      "Iteration 38019 => Loss: 6.69940970134391911017\n",
      "Iteration 38020 => Loss: 6.69940953535591710022\n",
      "Iteration 38021 => Loss: 6.69940936939027320562\n",
      "Iteration 38022 => Loss: 6.69940920344700518996\n",
      "Iteration 38023 => Loss: 6.69940903752608996058\n",
      "Iteration 38024 => Loss: 6.69940887162753639927\n",
      "Iteration 38025 => Loss: 6.69940870575133562426\n",
      "Iteration 38026 => Loss: 6.69940853989748763553\n",
      "Iteration 38027 => Loss: 6.69940837406598621584\n",
      "Iteration 38028 => Loss: 6.69940820825682692430\n",
      "Iteration 38029 => Loss: 6.69940804247002397176\n",
      "Iteration 38030 => Loss: 6.69940787670554982469\n",
      "Iteration 38031 => Loss: 6.69940771096341158852\n",
      "Iteration 38032 => Loss: 6.69940754524361548050\n",
      "Iteration 38033 => Loss: 6.69940737954614107252\n",
      "Iteration 38034 => Loss: 6.69940721387100168727\n",
      "Iteration 38035 => Loss: 6.69940704821817778480\n",
      "Iteration 38036 => Loss: 6.69940688258768890506\n",
      "Iteration 38037 => Loss: 6.69940671697951106722\n",
      "Iteration 38038 => Loss: 6.69940655139364515946\n",
      "Iteration 38039 => Loss: 6.69940638583010006357\n",
      "Iteration 38040 => Loss: 6.69940622028885623962\n",
      "Iteration 38041 => Loss: 6.69940605476992789846\n",
      "Iteration 38042 => Loss: 6.69940588927330260560\n",
      "Iteration 38043 => Loss: 6.69940572379897059108\n",
      "Iteration 38044 => Loss: 6.69940555834694251303\n",
      "Iteration 38045 => Loss: 6.69940539291721304238\n",
      "Iteration 38046 => Loss: 6.69940522750976263922\n",
      "Iteration 38047 => Loss: 6.69940506212460817892\n",
      "Iteration 38048 => Loss: 6.69940489676173367428\n",
      "Iteration 38049 => Loss: 6.69940473142114889527\n",
      "Iteration 38050 => Loss: 6.69940456610284140737\n",
      "Iteration 38051 => Loss: 6.69940440080681209878\n",
      "Iteration 38052 => Loss: 6.69940423553305297588\n",
      "Iteration 38053 => Loss: 6.69940407028157380864\n",
      "Iteration 38054 => Loss: 6.69940390505234972807\n",
      "Iteration 38055 => Loss: 6.69940373984539760954\n",
      "Iteration 38056 => Loss: 6.69940357466070590675\n",
      "Iteration 38057 => Loss: 6.69940340949827373151\n",
      "Iteration 38058 => Loss: 6.69940324435809664294\n",
      "Iteration 38059 => Loss: 6.69940307924016664742\n",
      "Iteration 38060 => Loss: 6.69940291414448996221\n",
      "Iteration 38061 => Loss: 6.69940274907106303459\n",
      "Iteration 38062 => Loss: 6.69940258401987254189\n",
      "Iteration 38063 => Loss: 6.69940241899093091860\n",
      "Iteration 38064 => Loss: 6.69940225398422040115\n",
      "Iteration 38065 => Loss: 6.69940208899975342405\n",
      "Iteration 38066 => Loss: 6.69940192403751222372\n",
      "Iteration 38067 => Loss: 6.69940175909750035288\n",
      "Iteration 38068 => Loss: 6.69940159417970892974\n",
      "Iteration 38069 => Loss: 6.69940142928414683610\n",
      "Iteration 38070 => Loss: 6.69940126441080874287\n",
      "Iteration 38071 => Loss: 6.69940109955967155742\n",
      "Iteration 38072 => Loss: 6.69940093473076192510\n",
      "Iteration 38073 => Loss: 6.69940076992405941780\n",
      "Iteration 38074 => Loss: 6.69940060513956758825\n",
      "Iteration 38075 => Loss: 6.69940044037727666648\n",
      "Iteration 38076 => Loss: 6.69940027563718754067\n",
      "Iteration 38077 => Loss: 6.69940011091929754627\n",
      "Iteration 38078 => Loss: 6.69939994622359868970\n",
      "Iteration 38079 => Loss: 6.69939978155010251726\n",
      "Iteration 38080 => Loss: 6.69939961689878948903\n",
      "Iteration 38081 => Loss: 6.69939945226966582226\n",
      "Iteration 38082 => Loss: 6.69939928766272441152\n",
      "Iteration 38083 => Loss: 6.69939912307796259228\n",
      "Iteration 38084 => Loss: 6.69939895851538391724\n",
      "Iteration 38085 => Loss: 6.69939879397497239921\n",
      "Iteration 38086 => Loss: 6.69939862945673780814\n",
      "Iteration 38087 => Loss: 6.69939846496067215043\n",
      "Iteration 38088 => Loss: 6.69939830048677453789\n",
      "Iteration 38089 => Loss: 6.69939813603502898331\n",
      "Iteration 38090 => Loss: 6.69939797160544969756\n",
      "Iteration 38091 => Loss: 6.69939780719803135156\n",
      "Iteration 38092 => Loss: 6.69939764281276151081\n",
      "Iteration 38093 => Loss: 6.69939747844964461621\n",
      "Iteration 38094 => Loss: 6.69939731410867711503\n",
      "Iteration 38095 => Loss: 6.69939714978985545457\n",
      "Iteration 38096 => Loss: 6.69939698549316986487\n",
      "Iteration 38097 => Loss: 6.69939682121862833952\n",
      "Iteration 38098 => Loss: 6.69939665696622022040\n",
      "Iteration 38099 => Loss: 6.69939649273594728385\n",
      "Iteration 38100 => Loss: 6.69939632852780775352\n",
      "Iteration 38101 => Loss: 6.69939616434178564219\n",
      "Iteration 38102 => Loss: 6.69939600017789693709\n",
      "Iteration 38103 => Loss: 6.69939583603612298646\n",
      "Iteration 38104 => Loss: 6.69939567191647178390\n",
      "Iteration 38105 => Loss: 6.69939550781892645404\n",
      "Iteration 38106 => Loss: 6.69939534374350298407\n",
      "Iteration 38107 => Loss: 6.69939517969018716315\n",
      "Iteration 38108 => Loss: 6.69939501565897543855\n",
      "Iteration 38109 => Loss: 6.69939485164987313937\n",
      "Iteration 38110 => Loss: 6.69939468766286339019\n",
      "Iteration 38111 => Loss: 6.69939452369795596098\n",
      "Iteration 38112 => Loss: 6.69939435975513575272\n",
      "Iteration 38113 => Loss: 6.69939419583441519990\n",
      "Iteration 38114 => Loss: 6.69939403193577120987\n",
      "Iteration 38115 => Loss: 6.69939386805922865165\n",
      "Iteration 38116 => Loss: 6.69939370420475288626\n",
      "Iteration 38117 => Loss: 6.69939354037237144723\n",
      "Iteration 38118 => Loss: 6.69939337656205058380\n",
      "Iteration 38119 => Loss: 6.69939321277381782949\n",
      "Iteration 38120 => Loss: 6.69939304900764742712\n",
      "Iteration 38121 => Loss: 6.69939288526354381759\n",
      "Iteration 38122 => Loss: 6.69939272154150877725\n",
      "Iteration 38123 => Loss: 6.69939255784153697704\n",
      "Iteration 38124 => Loss: 6.69939239416361953516\n",
      "Iteration 38125 => Loss: 6.69939223050775467527\n",
      "Iteration 38126 => Loss: 6.69939206687394239736\n",
      "Iteration 38127 => Loss: 6.69939190326219158322\n",
      "Iteration 38128 => Loss: 6.69939173967247825203\n",
      "Iteration 38129 => Loss: 6.69939157610481128557\n",
      "Iteration 38130 => Loss: 6.69939141255918446660\n",
      "Iteration 38131 => Loss: 6.69939124903559335422\n",
      "Iteration 38132 => Loss: 6.69939108553404150115\n",
      "Iteration 38133 => Loss: 6.69939092205451647288\n",
      "Iteration 38134 => Loss: 6.69939075859702004578\n",
      "Iteration 38135 => Loss: 6.69939059516155221985\n",
      "Iteration 38136 => Loss: 6.69939043174810588965\n",
      "Iteration 38137 => Loss: 6.69939026835668283155\n",
      "Iteration 38138 => Loss: 6.69939010498727416376\n",
      "Iteration 38139 => Loss: 6.69938994163987633357\n",
      "Iteration 38140 => Loss: 6.69938977831449999911\n",
      "Iteration 38141 => Loss: 6.69938961501112029140\n",
      "Iteration 38142 => Loss: 6.69938945172975408582\n",
      "Iteration 38143 => Loss: 6.69938928847038450698\n",
      "Iteration 38144 => Loss: 6.69938912523301688395\n",
      "Iteration 38145 => Loss: 6.69938896201764144678\n",
      "Iteration 38146 => Loss: 6.69938879882427240631\n",
      "Iteration 38147 => Loss: 6.69938863565287689994\n",
      "Iteration 38148 => Loss: 6.69938847250347446760\n",
      "Iteration 38149 => Loss: 6.69938830937605178661\n",
      "Iteration 38150 => Loss: 6.69938814627062395601\n",
      "Iteration 38151 => Loss: 6.69938798318716610680\n",
      "Iteration 38152 => Loss: 6.69938782012568534441\n",
      "Iteration 38153 => Loss: 6.69938765708617811612\n",
      "Iteration 38154 => Loss: 6.69938749406863198743\n",
      "Iteration 38155 => Loss: 6.69938733107306649828\n",
      "Iteration 38156 => Loss: 6.69938716809945233877\n",
      "Iteration 38157 => Loss: 6.69938700514780638429\n",
      "Iteration 38158 => Loss: 6.69938684221811620034\n",
      "Iteration 38159 => Loss: 6.69938667931038089876\n",
      "Iteration 38160 => Loss: 6.69938651642459959135\n",
      "Iteration 38161 => Loss: 6.69938635356076250815\n",
      "Iteration 38162 => Loss: 6.69938619071887409007\n",
      "Iteration 38163 => Loss: 6.69938602789892456713\n",
      "Iteration 38164 => Loss: 6.69938586510092104476\n",
      "Iteration 38165 => Loss: 6.69938570232485108846\n",
      "Iteration 38166 => Loss: 6.69938553957071558642\n",
      "Iteration 38167 => Loss: 6.69938537683850832138\n",
      "Iteration 38168 => Loss: 6.69938521412823639878\n",
      "Iteration 38169 => Loss: 6.69938505143989093682\n",
      "Iteration 38170 => Loss: 6.69938488877345772465\n",
      "Iteration 38171 => Loss: 6.69938472612894919678\n",
      "Iteration 38172 => Loss: 6.69938456350635380687\n",
      "Iteration 38173 => Loss: 6.69938440090567777219\n",
      "Iteration 38174 => Loss: 6.69938423832690865822\n",
      "Iteration 38175 => Loss: 6.69938407577004468862\n",
      "Iteration 38176 => Loss: 6.69938391323509030428\n",
      "Iteration 38177 => Loss: 6.69938375072203573524\n",
      "Iteration 38178 => Loss: 6.69938358823088186966\n",
      "Iteration 38179 => Loss: 6.69938342576161627306\n",
      "Iteration 38180 => Loss: 6.69938326331424516269\n",
      "Iteration 38181 => Loss: 6.69938310088877120307\n",
      "Iteration 38182 => Loss: 6.69938293848518107154\n",
      "Iteration 38183 => Loss: 6.69938277610347387991\n",
      "Iteration 38184 => Loss: 6.69938261374364874001\n",
      "Iteration 38185 => Loss: 6.69938245140569854641\n",
      "Iteration 38186 => Loss: 6.69938228908962418728\n",
      "Iteration 38187 => Loss: 6.69938212679542566264\n",
      "Iteration 38188 => Loss: 6.69938196452308876161\n",
      "Iteration 38189 => Loss: 6.69938180227262414235\n",
      "Iteration 38190 => Loss: 6.69938164004402381124\n",
      "Iteration 38191 => Loss: 6.69938147783727888651\n",
      "Iteration 38192 => Loss: 6.69938131565239025633\n",
      "Iteration 38193 => Loss: 6.69938115348936502613\n",
      "Iteration 38194 => Loss: 6.69938099134818632052\n",
      "Iteration 38195 => Loss: 6.69938082922885325132\n",
      "Iteration 38196 => Loss: 6.69938066713136581853\n",
      "Iteration 38197 => Loss: 6.69938050505572224580\n",
      "Iteration 38198 => Loss: 6.69938034300192519765\n",
      "Iteration 38199 => Loss: 6.69938018096995868689\n",
      "Iteration 38200 => Loss: 6.69938001895982981893\n",
      "Iteration 38201 => Loss: 6.69937985697152615927\n",
      "Iteration 38202 => Loss: 6.69937969500505303699\n",
      "Iteration 38203 => Loss: 6.69937953306040245849\n",
      "Iteration 38204 => Loss: 6.69937937113757620011\n",
      "Iteration 38205 => Loss: 6.69937920923657159733\n",
      "Iteration 38206 => Loss: 6.69937904735737799200\n",
      "Iteration 38207 => Loss: 6.69937888550000426591\n",
      "Iteration 38208 => Loss: 6.69937872366443265548\n",
      "Iteration 38209 => Loss: 6.69937856185067470705\n",
      "Iteration 38210 => Loss: 6.69937840005871976246\n",
      "Iteration 38211 => Loss: 6.69937823828856959807\n",
      "Iteration 38212 => Loss: 6.69937807654020733850\n",
      "Iteration 38213 => Loss: 6.69937791481365074731\n",
      "Iteration 38214 => Loss: 6.69937775310888294911\n",
      "Iteration 38215 => Loss: 6.69937759142590305572\n",
      "Iteration 38216 => Loss: 6.69937742976471106715\n",
      "Iteration 38217 => Loss: 6.69937726812530431886\n",
      "Iteration 38218 => Loss: 6.69937710650768103449\n",
      "Iteration 38219 => Loss: 6.69937694491183055590\n",
      "Iteration 38220 => Loss: 6.69937678333775732398\n",
      "Iteration 38221 => Loss: 6.69937662178545956237\n",
      "Iteration 38222 => Loss: 6.69937646025492838930\n",
      "Iteration 38223 => Loss: 6.69937629874616291659\n",
      "Iteration 38224 => Loss: 6.69937613725915870333\n",
      "Iteration 38225 => Loss: 6.69937597579391663771\n",
      "Iteration 38226 => Loss: 6.69937581435043316702\n",
      "Iteration 38227 => Loss: 6.69937565292870207401\n",
      "Iteration 38228 => Loss: 6.69937549152872691138\n",
      "Iteration 38229 => Loss: 6.69937533015050057372\n",
      "Iteration 38230 => Loss: 6.69937516879401862013\n",
      "Iteration 38231 => Loss: 6.69937500745927039247\n",
      "Iteration 38232 => Loss: 6.69937484614627365431\n",
      "Iteration 38233 => Loss: 6.69937468485500531301\n",
      "Iteration 38234 => Loss: 6.69937452358547691489\n",
      "Iteration 38235 => Loss: 6.69937436233768135452\n",
      "Iteration 38236 => Loss: 6.69937420111160975011\n",
      "Iteration 38237 => Loss: 6.69937403990725854896\n",
      "Iteration 38238 => Loss: 6.69937387872464018557\n",
      "Iteration 38239 => Loss: 6.69937371756372979092\n",
      "Iteration 38240 => Loss: 6.69937355642454868132\n",
      "Iteration 38241 => Loss: 6.69937339530706843505\n",
      "Iteration 38242 => Loss: 6.69937323421130948020\n",
      "Iteration 38243 => Loss: 6.69937307313725227687\n",
      "Iteration 38244 => Loss: 6.69937291208489593686\n",
      "Iteration 38245 => Loss: 6.69937275105425111832\n",
      "Iteration 38246 => Loss: 6.69937259004530183404\n",
      "Iteration 38247 => Loss: 6.69937242905804630766\n",
      "Iteration 38248 => Loss: 6.69937226809248720372\n",
      "Iteration 38249 => Loss: 6.69937210714861475225\n",
      "Iteration 38250 => Loss: 6.69937194622643161779\n",
      "Iteration 38251 => Loss: 6.69937178532592891855\n",
      "Iteration 38252 => Loss: 6.69937162444711287179\n",
      "Iteration 38253 => Loss: 6.69937146358997726026\n",
      "Iteration 38254 => Loss: 6.69937130275451142580\n",
      "Iteration 38255 => Loss: 6.69937114194071714479\n",
      "Iteration 38256 => Loss: 6.69937098114859264086\n",
      "Iteration 38257 => Loss: 6.69937082037814057855\n",
      "Iteration 38258 => Loss: 6.69937065962934674701\n",
      "Iteration 38259 => Loss: 6.69937049890222091619\n",
      "Iteration 38260 => Loss: 6.69937033819673999346\n",
      "Iteration 38261 => Loss: 6.69937017751292973600\n",
      "Iteration 38262 => Loss: 6.69937001685076083390\n",
      "Iteration 38263 => Loss: 6.69936985621024305715\n",
      "Iteration 38264 => Loss: 6.69936969559137018848\n",
      "Iteration 38265 => Loss: 6.69936953499414933333\n",
      "Iteration 38266 => Loss: 6.69936937441856272812\n",
      "Iteration 38267 => Loss: 6.69936921386461214922\n",
      "Iteration 38268 => Loss: 6.69936905333230026116\n",
      "Iteration 38269 => Loss: 6.69936889282161640580\n",
      "Iteration 38270 => Loss: 6.69936873233257035309\n",
      "Iteration 38271 => Loss: 6.69936857186513456952\n",
      "Iteration 38272 => Loss: 6.69936841141933125954\n",
      "Iteration 38273 => Loss: 6.69936825099515065318\n",
      "Iteration 38274 => Loss: 6.69936809059258120413\n",
      "Iteration 38275 => Loss: 6.69936793021163001782\n",
      "Iteration 38276 => Loss: 6.69936776985228821246\n",
      "Iteration 38277 => Loss: 6.69936760951455401170\n",
      "Iteration 38278 => Loss: 6.69936744919843008006\n",
      "Iteration 38279 => Loss: 6.69936728890390309488\n",
      "Iteration 38280 => Loss: 6.69936712863097838522\n",
      "Iteration 38281 => Loss: 6.69936696837964618112\n",
      "Iteration 38282 => Loss: 6.69936680814991181165\n",
      "Iteration 38283 => Loss: 6.69936664794177350046\n",
      "Iteration 38284 => Loss: 6.69936648775521437216\n",
      "Iteration 38285 => Loss: 6.69936632759024419670\n",
      "Iteration 38286 => Loss: 6.69936616744685409230\n",
      "Iteration 38287 => Loss: 6.69936600732504583533\n",
      "Iteration 38288 => Loss: 6.69936584722481676124\n",
      "Iteration 38289 => Loss: 6.69936568714615710007\n",
      "Iteration 38290 => Loss: 6.69936552708907040454\n",
      "Iteration 38291 => Loss: 6.69936536705354956922\n",
      "Iteration 38292 => Loss: 6.69936520703959192957\n",
      "Iteration 38293 => Loss: 6.69936504704720103831\n",
      "Iteration 38294 => Loss: 6.69936488707636446094\n",
      "Iteration 38295 => Loss: 6.69936472712708575017\n",
      "Iteration 38296 => Loss: 6.69936456719935780058\n",
      "Iteration 38297 => Loss: 6.69936440729318594123\n",
      "Iteration 38298 => Loss: 6.69936424740855773763\n",
      "Iteration 38299 => Loss: 6.69936408754547585431\n",
      "Iteration 38300 => Loss: 6.69936392770393052132\n",
      "Iteration 38301 => Loss: 6.69936376788393150861\n",
      "Iteration 38302 => Loss: 6.69936360808546016443\n",
      "Iteration 38303 => Loss: 6.69936344830852359422\n",
      "Iteration 38304 => Loss: 6.69936328855312535069\n",
      "Iteration 38305 => Loss: 6.69936312881923878848\n",
      "Iteration 38306 => Loss: 6.69936296910689144113\n",
      "Iteration 38307 => Loss: 6.69936280941605843964\n",
      "Iteration 38308 => Loss: 6.69936264974674600126\n",
      "Iteration 38309 => Loss: 6.69936249009894702056\n",
      "Iteration 38310 => Loss: 6.69936233047266060936\n",
      "Iteration 38311 => Loss: 6.69936217086788676767\n",
      "Iteration 38312 => Loss: 6.69936201128461394916\n",
      "Iteration 38313 => Loss: 6.69936185172285014744\n",
      "Iteration 38314 => Loss: 6.69936169218258292801\n",
      "Iteration 38315 => Loss: 6.69936153266381406723\n",
      "Iteration 38316 => Loss: 6.69936137316654445328\n",
      "Iteration 38317 => Loss: 6.69936121369076609255\n",
      "Iteration 38318 => Loss: 6.69936105423647898505\n",
      "Iteration 38319 => Loss: 6.69936089480367602533\n",
      "Iteration 38320 => Loss: 6.69936073539235810159\n",
      "Iteration 38321 => Loss: 6.69936057600251722022\n",
      "Iteration 38322 => Loss: 6.69936041663415426939\n",
      "Iteration 38323 => Loss: 6.69936025728726836093\n",
      "Iteration 38324 => Loss: 6.69936009796185594212\n",
      "Iteration 38325 => Loss: 6.69935993865791612478\n",
      "Iteration 38326 => Loss: 6.69935977937543025718\n",
      "Iteration 38327 => Loss: 6.69935962011441610287\n",
      "Iteration 38328 => Loss: 6.69935946087485767464\n",
      "Iteration 38329 => Loss: 6.69935930165676474246\n",
      "Iteration 38330 => Loss: 6.69935914246012043094\n",
      "Iteration 38331 => Loss: 6.69935898328492385190\n",
      "Iteration 38332 => Loss: 6.69935882413119010437\n",
      "Iteration 38333 => Loss: 6.69935866499889254300\n",
      "Iteration 38334 => Loss: 6.69935850588803294414\n",
      "Iteration 38335 => Loss: 6.69935834679862285412\n",
      "Iteration 38336 => Loss: 6.69935818773064362119\n",
      "Iteration 38337 => Loss: 6.69935802868409879807\n",
      "Iteration 38338 => Loss: 6.69935786965899549017\n",
      "Iteration 38339 => Loss: 6.69935771065531504576\n",
      "Iteration 38340 => Loss: 6.69935755167305924118\n",
      "Iteration 38341 => Loss: 6.69935739271222630009\n",
      "Iteration 38342 => Loss: 6.69935723377281711066\n",
      "Iteration 38343 => Loss: 6.69935707485481657386\n",
      "Iteration 38344 => Loss: 6.69935691595824334144\n",
      "Iteration 38345 => Loss: 6.69935675708306987985\n",
      "Iteration 38346 => Loss: 6.69935659822930862362\n",
      "Iteration 38347 => Loss: 6.69935643939695157911\n",
      "Iteration 38348 => Loss: 6.69935628058600052270\n",
      "Iteration 38349 => Loss: 6.69935612179644568442\n",
      "Iteration 38350 => Loss: 6.69935596302829061699\n",
      "Iteration 38351 => Loss: 6.69935580428152377408\n",
      "Iteration 38352 => Loss: 6.69935564555615403748\n",
      "Iteration 38353 => Loss: 6.69935548685216897269\n",
      "Iteration 38354 => Loss: 6.69935532816957746149\n",
      "Iteration 38355 => Loss: 6.69935516950835552308\n",
      "Iteration 38356 => Loss: 6.69935501086851914465\n",
      "Iteration 38357 => Loss: 6.69935485225006122079\n",
      "Iteration 38358 => Loss: 6.69935469365297731059\n",
      "Iteration 38359 => Loss: 6.69935453507726652589\n",
      "Iteration 38360 => Loss: 6.69935437652291820854\n",
      "Iteration 38361 => Loss: 6.69935421798993591125\n",
      "Iteration 38362 => Loss: 6.69935405947831963402\n",
      "Iteration 38363 => Loss: 6.69935390098806049508\n",
      "Iteration 38364 => Loss: 6.69935374251915582988\n",
      "Iteration 38365 => Loss: 6.69935358407160741478\n",
      "Iteration 38366 => Loss: 6.69935342564540903254\n",
      "Iteration 38367 => Loss: 6.69935326724056245951\n",
      "Iteration 38368 => Loss: 6.69935310885705082029\n",
      "Iteration 38369 => Loss: 6.69935295049489365482\n",
      "Iteration 38370 => Loss: 6.69935279215406964681\n",
      "Iteration 38371 => Loss: 6.69935263383457879627\n",
      "Iteration 38372 => Loss: 6.69935247553642643226\n",
      "Iteration 38373 => Loss: 6.69935231725960811389\n",
      "Iteration 38374 => Loss: 6.69935215900411673573\n",
      "Iteration 38375 => Loss: 6.69935200076994341600\n",
      "Iteration 38376 => Loss: 6.69935184255709614831\n",
      "Iteration 38377 => Loss: 6.69935168436556693905\n",
      "Iteration 38378 => Loss: 6.69935152619535490004\n",
      "Iteration 38379 => Loss: 6.69935136804645114950\n",
      "Iteration 38380 => Loss: 6.69935120991886723374\n",
      "Iteration 38381 => Loss: 6.69935105181258538920\n",
      "Iteration 38382 => Loss: 6.69935089372760828041\n",
      "Iteration 38383 => Loss: 6.69935073566393057831\n",
      "Iteration 38384 => Loss: 6.69935057762156205285\n",
      "Iteration 38385 => Loss: 6.69935041960048316412\n",
      "Iteration 38386 => Loss: 6.69935026160070012935\n",
      "Iteration 38387 => Loss: 6.69935010362220406677\n",
      "Iteration 38388 => Loss: 6.69934994566499408819\n",
      "Iteration 38389 => Loss: 6.69934978772907285816\n",
      "Iteration 38390 => Loss: 6.69934962981443238306\n",
      "Iteration 38391 => Loss: 6.69934947192106822200\n",
      "Iteration 38392 => Loss: 6.69934931404898748042\n",
      "Iteration 38393 => Loss: 6.69934915619817150656\n",
      "Iteration 38394 => Loss: 6.69934899836863095857\n",
      "Iteration 38395 => Loss: 6.69934884056035517830\n",
      "Iteration 38396 => Loss: 6.69934868277334683029\n",
      "Iteration 38397 => Loss: 6.69934852500759880911\n",
      "Iteration 38398 => Loss: 6.69934836726310933841\n",
      "Iteration 38399 => Loss: 6.69934820953987841818\n",
      "Iteration 38400 => Loss: 6.69934805183790071936\n",
      "Iteration 38401 => Loss: 6.69934789415716736016\n",
      "Iteration 38402 => Loss: 6.69934773649768811055\n",
      "Iteration 38403 => Loss: 6.69934757885945142419\n",
      "Iteration 38404 => Loss: 6.69934742124245641293\n",
      "Iteration 38405 => Loss: 6.69934726364670307674\n",
      "Iteration 38406 => Loss: 6.69934710607218075751\n",
      "Iteration 38407 => Loss: 6.69934694851888945522\n",
      "Iteration 38408 => Loss: 6.69934679098683716347\n",
      "Iteration 38409 => Loss: 6.69934663347600789507\n",
      "Iteration 38410 => Loss: 6.69934647598640165000\n",
      "Iteration 38411 => Loss: 6.69934631851802109281\n",
      "Iteration 38412 => Loss: 6.69934616107085467718\n",
      "Iteration 38413 => Loss: 6.69934600364490329127\n",
      "Iteration 38414 => Loss: 6.69934584624016871146\n",
      "Iteration 38415 => Loss: 6.69934568885665093774\n",
      "Iteration 38416 => Loss: 6.69934553149433131836\n",
      "Iteration 38417 => Loss: 6.69934537415321518239\n",
      "Iteration 38418 => Loss: 6.69934521683330697073\n",
      "Iteration 38419 => Loss: 6.69934505953459158434\n",
      "Iteration 38420 => Loss: 6.69934490225707612865\n",
      "Iteration 38421 => Loss: 6.69934474500075882730\n",
      "Iteration 38422 => Loss: 6.69934458776562280491\n",
      "Iteration 38423 => Loss: 6.69934443055167871961\n",
      "Iteration 38424 => Loss: 6.69934427335891857780\n",
      "Iteration 38425 => Loss: 6.69934411618734326765\n",
      "Iteration 38426 => Loss: 6.69934395903693946650\n",
      "Iteration 38427 => Loss: 6.69934380190771872066\n",
      "Iteration 38428 => Loss: 6.69934364479966859562\n",
      "Iteration 38429 => Loss: 6.69934348771278553869\n",
      "Iteration 38430 => Loss: 6.69934333064707665528\n",
      "Iteration 38431 => Loss: 6.69934317360252329365\n",
      "Iteration 38432 => Loss: 6.69934301657914144101\n",
      "Iteration 38433 => Loss: 6.69934285957691244562\n",
      "Iteration 38434 => Loss: 6.69934270259583986018\n",
      "Iteration 38435 => Loss: 6.69934254563592279652\n",
      "Iteration 38436 => Loss: 6.69934238869715503739\n",
      "Iteration 38437 => Loss: 6.69934223177953480644\n",
      "Iteration 38438 => Loss: 6.69934207488306121547\n",
      "Iteration 38439 => Loss: 6.69934191800772271819\n",
      "Iteration 38440 => Loss: 6.69934176115353086089\n",
      "Iteration 38441 => Loss: 6.69934160432046788003\n",
      "Iteration 38442 => Loss: 6.69934144750854354555\n",
      "Iteration 38443 => Loss: 6.69934129071775164022\n",
      "Iteration 38444 => Loss: 6.69934113394807972952\n",
      "Iteration 38445 => Loss: 6.69934097719954113614\n",
      "Iteration 38446 => Loss: 6.69934082047211632016\n",
      "Iteration 38447 => Loss: 6.69934066376581771607\n",
      "Iteration 38448 => Loss: 6.69934050708063022483\n",
      "Iteration 38449 => Loss: 6.69934035041655828735\n",
      "Iteration 38450 => Loss: 6.69934019377359657454\n",
      "Iteration 38451 => Loss: 6.69934003715174064553\n",
      "Iteration 38452 => Loss: 6.69933988055099494119\n",
      "Iteration 38453 => Loss: 6.69933972397134436250\n",
      "Iteration 38454 => Loss: 6.69933956741279867941\n",
      "Iteration 38455 => Loss: 6.69933941087534723380\n",
      "Iteration 38456 => Loss: 6.69933925435898469658\n",
      "Iteration 38457 => Loss: 6.69933909786371550865\n",
      "Iteration 38458 => Loss: 6.69933894138953789366\n",
      "Iteration 38459 => Loss: 6.69933878493644119345\n",
      "Iteration 38460 => Loss: 6.69933862850443340164\n",
      "Iteration 38461 => Loss: 6.69933847209349320195\n",
      "Iteration 38462 => Loss: 6.69933831570363391705\n",
      "Iteration 38463 => Loss: 6.69933815933485821148\n",
      "Iteration 38464 => Loss: 6.69933800298714121624\n",
      "Iteration 38465 => Loss: 6.69933784666049714218\n",
      "Iteration 38466 => Loss: 6.69933769035491444299\n",
      "Iteration 38467 => Loss: 6.69933753407039489502\n",
      "Iteration 38468 => Loss: 6.69933737780693583375\n",
      "Iteration 38469 => Loss: 6.69933722156453548280\n",
      "Iteration 38470 => Loss: 6.69933706534318673675\n",
      "Iteration 38471 => Loss: 6.69933690914288781926\n",
      "Iteration 38472 => Loss: 6.69933675296363606577\n",
      "Iteration 38473 => Loss: 6.69933659680543236448\n",
      "Iteration 38474 => Loss: 6.69933644066826960994\n",
      "Iteration 38475 => Loss: 6.69933628455214869035\n",
      "Iteration 38476 => Loss: 6.69933612845705894756\n",
      "Iteration 38477 => Loss: 6.69933597238300659882\n",
      "Iteration 38478 => Loss: 6.69933581632998809141\n",
      "Iteration 38479 => Loss: 6.69933566029799099084\n",
      "Iteration 38480 => Loss: 6.69933550428702684343\n",
      "Iteration 38481 => Loss: 6.69933534829707877378\n",
      "Iteration 38482 => Loss: 6.69933519232815388733\n",
      "Iteration 38483 => Loss: 6.69933503638024596682\n",
      "Iteration 38484 => Loss: 6.69933488045335145955\n",
      "Iteration 38485 => Loss: 6.69933472454746592462\n",
      "Iteration 38486 => Loss: 6.69933456866259469109\n",
      "Iteration 38487 => Loss: 6.69933441279871910723\n",
      "Iteration 38488 => Loss: 6.69933425695585249571\n",
      "Iteration 38489 => Loss: 6.69933410113398597474\n",
      "Iteration 38490 => Loss: 6.69933394533311865615\n",
      "Iteration 38491 => Loss: 6.69933378955323899362\n",
      "Iteration 38492 => Loss: 6.69933363379435498075\n",
      "Iteration 38493 => Loss: 6.69933347805645951212\n",
      "Iteration 38494 => Loss: 6.69933332233954725865\n",
      "Iteration 38495 => Loss: 6.69933316664361733217\n",
      "Iteration 38496 => Loss: 6.69933301096867062085\n",
      "Iteration 38497 => Loss: 6.69933285531470179563\n",
      "Iteration 38498 => Loss: 6.69933269968170641562\n",
      "Iteration 38499 => Loss: 6.69933254406967915173\n",
      "Iteration 38500 => Loss: 6.69933238847862444487\n",
      "Iteration 38501 => Loss: 6.69933223290853341325\n",
      "Iteration 38502 => Loss: 6.69933207735940516869\n",
      "Iteration 38503 => Loss: 6.69933192183123882302\n",
      "Iteration 38504 => Loss: 6.69933176632402815898\n",
      "Iteration 38505 => Loss: 6.69933161083777939382\n",
      "Iteration 38506 => Loss: 6.69933145537247298762\n",
      "Iteration 38507 => Loss: 6.69933129992811871034\n",
      "Iteration 38508 => Loss: 6.69933114450470768020\n",
      "Iteration 38509 => Loss: 6.69933098910224700262\n",
      "Iteration 38510 => Loss: 6.69933083372071891404\n",
      "Iteration 38511 => Loss: 6.69933067836013584895\n",
      "Iteration 38512 => Loss: 6.69933052302048448468\n",
      "Iteration 38513 => Loss: 6.69933036770176393304\n",
      "Iteration 38514 => Loss: 6.69933021240397419405\n",
      "Iteration 38515 => Loss: 6.69933005712711437951\n",
      "Iteration 38516 => Loss: 6.69932990187116672587\n",
      "Iteration 38517 => Loss: 6.69932974663614722033\n",
      "Iteration 38518 => Loss: 6.69932959142205053382\n",
      "Iteration 38519 => Loss: 6.69932943622886156732\n",
      "Iteration 38520 => Loss: 6.69932928105658653806\n",
      "Iteration 38521 => Loss: 6.69932912590521922880\n",
      "Iteration 38522 => Loss: 6.69932897077476141590\n",
      "Iteration 38523 => Loss: 6.69932881566521309935\n",
      "Iteration 38524 => Loss: 6.69932866057655740377\n",
      "Iteration 38525 => Loss: 6.69932850550880232277\n",
      "Iteration 38526 => Loss: 6.69932835046194341544\n",
      "Iteration 38527 => Loss: 6.69932819543598068179\n",
      "Iteration 38528 => Loss: 6.69932804043089813462\n",
      "Iteration 38529 => Loss: 6.69932788544670554387\n",
      "Iteration 38530 => Loss: 6.69932773048340379773\n",
      "Iteration 38531 => Loss: 6.69932757554097513264\n",
      "Iteration 38532 => Loss: 6.69932742061943464762\n",
      "Iteration 38533 => Loss: 6.69932726571876191457\n",
      "Iteration 38534 => Loss: 6.69932711083896315074\n",
      "Iteration 38535 => Loss: 6.69932695598003213888\n",
      "Iteration 38536 => Loss: 6.69932680114197243171\n",
      "Iteration 38537 => Loss: 6.69932664632478314104\n",
      "Iteration 38538 => Loss: 6.69932649152844206242\n",
      "Iteration 38539 => Loss: 6.69932633675296340670\n",
      "Iteration 38540 => Loss: 6.69932618199835250294\n",
      "Iteration 38541 => Loss: 6.69932602726458004128\n",
      "Iteration 38542 => Loss: 6.69932587255166556162\n",
      "Iteration 38543 => Loss: 6.69932571785959485311\n",
      "Iteration 38544 => Loss: 6.69932556318836880394\n",
      "Iteration 38545 => Loss: 6.69932540853799185498\n",
      "Iteration 38546 => Loss: 6.69932525390845157176\n",
      "Iteration 38547 => Loss: 6.69932509929974173701\n",
      "Iteration 38548 => Loss: 6.69932494471186767981\n",
      "Iteration 38549 => Loss: 6.69932479014482495927\n",
      "Iteration 38550 => Loss: 6.69932463559861268720\n",
      "Iteration 38551 => Loss: 6.69932448107322464637\n",
      "Iteration 38552 => Loss: 6.69932432656865994858\n",
      "Iteration 38553 => Loss: 6.69932417208491060023\n",
      "Iteration 38554 => Loss: 6.69932401762198193040\n",
      "Iteration 38555 => Loss: 6.69932386317986061641\n",
      "Iteration 38556 => Loss: 6.69932370875856175729\n",
      "Iteration 38557 => Loss: 6.69932355435806226041\n",
      "Iteration 38558 => Loss: 6.69932339997837100753\n",
      "Iteration 38559 => Loss: 6.69932324561947556418\n",
      "Iteration 38560 => Loss: 6.69932309128139458210\n",
      "Iteration 38561 => Loss: 6.69932293696409697503\n",
      "Iteration 38562 => Loss: 6.69932278266759961838\n",
      "Iteration 38563 => Loss: 6.69932262839189540671\n",
      "Iteration 38564 => Loss: 6.69932247413697989913\n",
      "Iteration 38565 => Loss: 6.69932231990284599021\n",
      "Iteration 38566 => Loss: 6.69932216568949989721\n",
      "Iteration 38567 => Loss: 6.69932201149693096198\n",
      "Iteration 38568 => Loss: 6.69932185732513474363\n",
      "Iteration 38569 => Loss: 6.69932170317412545302\n",
      "Iteration 38570 => Loss: 6.69932154904387378025\n",
      "Iteration 38571 => Loss: 6.69932139493440104161\n",
      "Iteration 38572 => Loss: 6.69932124084568858535\n",
      "Iteration 38573 => Loss: 6.69932108677774706962\n",
      "Iteration 38574 => Loss: 6.69932093273055961902\n",
      "Iteration 38575 => Loss: 6.69932077870413067444\n",
      "Iteration 38576 => Loss: 6.69932062469846112407\n",
      "Iteration 38577 => Loss: 6.69932047071354652701\n",
      "Iteration 38578 => Loss: 6.69932031674937089605\n",
      "Iteration 38579 => Loss: 6.69932016280594755386\n",
      "Iteration 38580 => Loss: 6.69932000888326850685\n",
      "Iteration 38581 => Loss: 6.69931985498132931411\n",
      "Iteration 38582 => Loss: 6.69931970110013086384\n",
      "Iteration 38583 => Loss: 6.69931954723966427423\n",
      "Iteration 38584 => Loss: 6.69931939339993220983\n",
      "Iteration 38585 => Loss: 6.69931923958093289428\n",
      "Iteration 38586 => Loss: 6.69931908578265211673\n",
      "Iteration 38587 => Loss: 6.69931893200510408803\n",
      "Iteration 38588 => Loss: 6.69931877824827193280\n",
      "Iteration 38589 => Loss: 6.69931862451216098009\n",
      "Iteration 38590 => Loss: 6.69931847079676590084\n",
      "Iteration 38591 => Loss: 6.69931831710208758324\n",
      "Iteration 38592 => Loss: 6.69931816342811981002\n",
      "Iteration 38593 => Loss: 6.69931800977485103488\n",
      "Iteration 38594 => Loss: 6.69931785614229458048\n",
      "Iteration 38595 => Loss: 6.69931770253043712415\n",
      "Iteration 38596 => Loss: 6.69931754893927955408\n",
      "Iteration 38597 => Loss: 6.69931739536881831754\n",
      "Iteration 38598 => Loss: 6.69931724181905341453\n",
      "Iteration 38599 => Loss: 6.69931708828997507510\n",
      "Iteration 38600 => Loss: 6.69931693478158774013\n",
      "Iteration 38601 => Loss: 6.69931678129388519238\n",
      "Iteration 38602 => Loss: 6.69931662782686299096\n",
      "Iteration 38603 => Loss: 6.69931647438052557675\n",
      "Iteration 38604 => Loss: 6.69931632095485607437\n",
      "Iteration 38605 => Loss: 6.69931616754986958284\n",
      "Iteration 38606 => Loss: 6.69931601416554922679\n",
      "Iteration 38607 => Loss: 6.69931586080190566435\n",
      "Iteration 38608 => Loss: 6.69931570745891757923\n",
      "Iteration 38609 => Loss: 6.69931555413659385323\n",
      "Iteration 38610 => Loss: 6.69931540083493182181\n",
      "Iteration 38611 => Loss: 6.69931524755392704407\n",
      "Iteration 38612 => Loss: 6.69931509429358040819\n",
      "Iteration 38613 => Loss: 6.69931494105388036786\n",
      "Iteration 38614 => Loss: 6.69931478783483225214\n",
      "Iteration 38615 => Loss: 6.69931463463643162015\n",
      "Iteration 38616 => Loss: 6.69931448145867580735\n",
      "Iteration 38617 => Loss: 6.69931432830155237923\n",
      "Iteration 38618 => Loss: 6.69931417516507288212\n",
      "Iteration 38619 => Loss: 6.69931402204923021060\n",
      "Iteration 38620 => Loss: 6.69931386895402170012\n",
      "Iteration 38621 => Loss: 6.69931371587943313983\n",
      "Iteration 38622 => Loss: 6.69931356282547962877\n",
      "Iteration 38623 => Loss: 6.69931340979214873244\n",
      "Iteration 38624 => Loss: 6.69931325677943334540\n",
      "Iteration 38625 => Loss: 6.69931310378734945488\n",
      "Iteration 38626 => Loss: 6.69931295081586863915\n",
      "Iteration 38627 => Loss: 6.69931279786500422091\n",
      "Iteration 38628 => Loss: 6.69931264493474998289\n",
      "Iteration 38629 => Loss: 6.69931249202510592511\n",
      "Iteration 38630 => Loss: 6.69931233913606938302\n",
      "Iteration 38631 => Loss: 6.69931218626763147483\n",
      "Iteration 38632 => Loss: 6.69931203341978331878\n",
      "Iteration 38633 => Loss: 6.69931188059253557299\n",
      "Iteration 38634 => Loss: 6.69931172778588290839\n",
      "Iteration 38635 => Loss: 6.69931157499983154224\n",
      "Iteration 38636 => Loss: 6.69931142223435571736\n",
      "Iteration 38637 => Loss: 6.69931126948946875643\n",
      "Iteration 38638 => Loss: 6.69931111676517154763\n",
      "Iteration 38639 => Loss: 6.69931096406143744559\n",
      "Iteration 38640 => Loss: 6.69931081137828776662\n",
      "Iteration 38641 => Loss: 6.69931065871571806980\n",
      "Iteration 38642 => Loss: 6.69931050607371236794\n",
      "Iteration 38643 => Loss: 6.69931035345228131916\n",
      "Iteration 38644 => Loss: 6.69931020085141160081\n",
      "Iteration 38645 => Loss: 6.69931004827110587740\n",
      "Iteration 38646 => Loss: 6.69930989571136237259\n",
      "Iteration 38647 => Loss: 6.69930974317217042824\n",
      "Iteration 38648 => Loss: 6.69930959065354070248\n",
      "Iteration 38649 => Loss: 6.69930943815545365538\n",
      "Iteration 38650 => Loss: 6.69930928567792260964\n",
      "Iteration 38651 => Loss: 6.69930913322094045981\n",
      "Iteration 38652 => Loss: 6.69930898078450187683\n",
      "Iteration 38653 => Loss: 6.69930882836859886709\n",
      "Iteration 38654 => Loss: 6.69930867597323409512\n",
      "Iteration 38655 => Loss: 6.69930852359840400823\n",
      "Iteration 38656 => Loss: 6.69930837124410949457\n",
      "Iteration 38657 => Loss: 6.69930821891034522508\n",
      "Iteration 38658 => Loss: 6.69930806659710587070\n",
      "Iteration 38659 => Loss: 6.69930791430439054324\n",
      "Iteration 38660 => Loss: 6.69930776203219657816\n",
      "Iteration 38661 => Loss: 6.69930760978051953458\n",
      "Iteration 38662 => Loss: 6.69930745754936385339\n",
      "Iteration 38663 => Loss: 6.69930730533871887644\n",
      "Iteration 38664 => Loss: 6.69930715314858016285\n",
      "Iteration 38665 => Loss: 6.69930700097896192347\n",
      "Iteration 38666 => Loss: 6.69930684882983218387\n",
      "Iteration 38667 => Loss: 6.69930669670121581305\n",
      "Iteration 38668 => Loss: 6.69930654459309060655\n",
      "Iteration 38669 => Loss: 6.69930639250546633434\n",
      "Iteration 38670 => Loss: 6.69930624043833677916\n",
      "Iteration 38671 => Loss: 6.69930608839169750013\n",
      "Iteration 38672 => Loss: 6.69930593636554760906\n",
      "Iteration 38673 => Loss: 6.69930578435987467145\n",
      "Iteration 38674 => Loss: 6.69930563237469289817\n",
      "Iteration 38675 => Loss: 6.69930548040998807835\n",
      "Iteration 38676 => Loss: 6.69930532846576909378\n",
      "Iteration 38677 => Loss: 6.69930517654201196365\n",
      "Iteration 38678 => Loss: 6.69930502463873356334\n",
      "Iteration 38679 => Loss: 6.69930487275592767560\n",
      "Iteration 38680 => Loss: 6.69930472089358097776\n",
      "Iteration 38681 => Loss: 6.69930456905170057524\n",
      "Iteration 38682 => Loss: 6.69930441723028291534\n",
      "Iteration 38683 => Loss: 6.69930426542931645173\n",
      "Iteration 38684 => Loss: 6.69930411364881361891\n",
      "Iteration 38685 => Loss: 6.69930396188875665331\n",
      "Iteration 38686 => Loss: 6.69930381014915088400\n",
      "Iteration 38687 => Loss: 6.69930365842998831738\n",
      "Iteration 38688 => Loss: 6.69930350673127872341\n",
      "Iteration 38689 => Loss: 6.69930335505301055576\n",
      "Iteration 38690 => Loss: 6.69930320339517670902\n",
      "Iteration 38691 => Loss: 6.69930305175777895954\n",
      "Iteration 38692 => Loss: 6.69930290014080753735\n",
      "Iteration 38693 => Loss: 6.69930274854427576514\n",
      "Iteration 38694 => Loss: 6.69930259696817120840\n",
      "Iteration 38695 => Loss: 6.69930244541248676171\n",
      "Iteration 38696 => Loss: 6.69930229387723308321\n",
      "Iteration 38697 => Loss: 6.69930214236239240932\n",
      "Iteration 38698 => Loss: 6.69930199086796474006\n",
      "Iteration 38699 => Loss: 6.69930183939395273995\n",
      "Iteration 38700 => Loss: 6.69930168794035107993\n",
      "Iteration 38701 => Loss: 6.69930153650716775360\n",
      "Iteration 38702 => Loss: 6.69930138509438144467\n",
      "Iteration 38703 => Loss: 6.69930123370200014676\n",
      "Iteration 38704 => Loss: 6.69930108233001853080\n",
      "Iteration 38705 => Loss: 6.69930093097843215588\n",
      "Iteration 38706 => Loss: 6.69930077964724368655\n",
      "Iteration 38707 => Loss: 6.69930062833644690556\n",
      "Iteration 38708 => Loss: 6.69930047704604092473\n",
      "Iteration 38709 => Loss: 6.69930032577601952681\n",
      "Iteration 38710 => Loss: 6.69930017452637915909\n",
      "Iteration 38711 => Loss: 6.69930002329712603881\n",
      "Iteration 38712 => Loss: 6.69929987208823884970\n",
      "Iteration 38713 => Loss: 6.69929972089973535532\n",
      "Iteration 38714 => Loss: 6.69929956973159956846\n",
      "Iteration 38715 => Loss: 6.69929941858384569997\n",
      "Iteration 38716 => Loss: 6.69929926745645509811\n",
      "Iteration 38717 => Loss: 6.69929911634942243381\n",
      "Iteration 38718 => Loss: 6.69929896526275392432\n",
      "Iteration 38719 => Loss: 6.69929881419644246421\n",
      "Iteration 38720 => Loss: 6.69929866315049160619\n",
      "Iteration 38721 => Loss: 6.69929851212489246848\n",
      "Iteration 38722 => Loss: 6.69929836111964238654\n",
      "Iteration 38723 => Loss: 6.69929821013474136038\n",
      "Iteration 38724 => Loss: 6.69929805917018406092\n",
      "Iteration 38725 => Loss: 6.69929790822597759359\n",
      "Iteration 38726 => Loss: 6.69929775730209264850\n",
      "Iteration 38727 => Loss: 6.69929760639856297644\n",
      "Iteration 38728 => Loss: 6.69929745551536282022\n",
      "Iteration 38729 => Loss: 6.69929730465248862714\n",
      "Iteration 38730 => Loss: 6.69929715380994839080\n",
      "Iteration 38731 => Loss: 6.69929700298773322942\n",
      "Iteration 38732 => Loss: 6.69929685218583781392\n",
      "Iteration 38733 => Loss: 6.69929670140427013791\n",
      "Iteration 38734 => Loss: 6.69929655064301421419\n",
      "Iteration 38735 => Loss: 6.69929639990207270728\n",
      "Iteration 38736 => Loss: 6.69929624918144650536\n",
      "Iteration 38737 => Loss: 6.69929609848112939119\n",
      "Iteration 38738 => Loss: 6.69929594780112136476\n",
      "Iteration 38739 => Loss: 6.69929579714141532065\n",
      "Iteration 38740 => Loss: 6.69929564650201037068\n",
      "Iteration 38741 => Loss: 6.69929549588290740303\n",
      "Iteration 38742 => Loss: 6.69929534528409043048\n",
      "Iteration 38743 => Loss: 6.69929519470557632843\n",
      "Iteration 38744 => Loss: 6.69929504414734999784\n",
      "Iteration 38745 => Loss: 6.69929489360941321507\n",
      "Iteration 38746 => Loss: 6.69929474309176153923\n",
      "Iteration 38747 => Loss: 6.69929459259438786489\n",
      "Iteration 38748 => Loss: 6.69929444211729574477\n",
      "Iteration 38749 => Loss: 6.69929429166048073796\n",
      "Iteration 38750 => Loss: 6.69929414122393573905\n",
      "Iteration 38751 => Loss: 6.69929399080766874164\n",
      "Iteration 38752 => Loss: 6.69929384041166464669\n",
      "Iteration 38753 => Loss: 6.69929369003593500054\n",
      "Iteration 38754 => Loss: 6.69929353968046203960\n",
      "Iteration 38755 => Loss: 6.69929338934524931659\n",
      "Iteration 38756 => Loss: 6.69929323903029860787\n",
      "Iteration 38757 => Loss: 6.69929308873559747894\n",
      "Iteration 38758 => Loss: 6.69929293846115214706\n",
      "Iteration 38759 => Loss: 6.69929278820695905949\n",
      "Iteration 38760 => Loss: 6.69929263797300578176\n",
      "Iteration 38761 => Loss: 6.69929248775929675475\n",
      "Iteration 38762 => Loss: 6.69929233756583375481\n",
      "Iteration 38763 => Loss: 6.69929218739260701199\n",
      "Iteration 38764 => Loss: 6.69929203723961119721\n",
      "Iteration 38765 => Loss: 6.69929188710685874497\n",
      "Iteration 38766 => Loss: 6.69929173699433011535\n",
      "Iteration 38767 => Loss: 6.69929158690203241378\n",
      "Iteration 38768 => Loss: 6.69929143682995498210\n",
      "Iteration 38769 => Loss: 6.69929128677810048487\n",
      "Iteration 38770 => Loss: 6.69929113674647336296\n",
      "Iteration 38771 => Loss: 6.69929098673505141193\n",
      "Iteration 38772 => Loss: 6.69929083674385505986\n",
      "Iteration 38773 => Loss: 6.69929068677286387867\n",
      "Iteration 38774 => Loss: 6.69929053682207786835\n",
      "Iteration 38775 => Loss: 6.69929038689150768704\n",
      "Iteration 38776 => Loss: 6.69929023698113290664\n",
      "Iteration 38777 => Loss: 6.69929008709095885621\n",
      "Iteration 38778 => Loss: 6.69928993722098731212\n",
      "Iteration 38779 => Loss: 6.69928978737120228715\n",
      "Iteration 38780 => Loss: 6.69928963754161710398\n",
      "Iteration 38781 => Loss: 6.69928948773222110447\n",
      "Iteration 38782 => Loss: 6.69928933794301073590\n",
      "Iteration 38783 => Loss: 6.69928918817397889285\n",
      "Iteration 38784 => Loss: 6.69928903842513445710\n",
      "Iteration 38785 => Loss: 6.69928888869646854687\n",
      "Iteration 38786 => Loss: 6.69928873898797672126\n",
      "Iteration 38787 => Loss: 6.69928858929966164482\n",
      "Iteration 38788 => Loss: 6.69928843963151443575\n",
      "Iteration 38789 => Loss: 6.69928828998352798862\n",
      "Iteration 38790 => Loss: 6.69928814035571651431\n",
      "Iteration 38791 => Loss: 6.69928799074806491376\n",
      "Iteration 38792 => Loss: 6.69928784116056874609\n",
      "Iteration 38793 => Loss: 6.69928769159323778126\n",
      "Iteration 38794 => Loss: 6.69928754204606224931\n",
      "Iteration 38795 => Loss: 6.69928739251902705121\n",
      "Iteration 38796 => Loss: 6.69928724301214728598\n",
      "Iteration 38797 => Loss: 6.69928709352541051913\n",
      "Iteration 38798 => Loss: 6.69928694405882207974\n",
      "Iteration 38799 => Loss: 6.69928679461236775694\n",
      "Iteration 38800 => Loss: 6.69928664518605465616\n",
      "Iteration 38801 => Loss: 6.69928649577987833652\n",
      "Iteration 38802 => Loss: 6.69928634639382813987\n",
      "Iteration 38803 => Loss: 6.69928619702790850710\n",
      "Iteration 38804 => Loss: 6.69928604768212831999\n",
      "Iteration 38805 => Loss: 6.69928589835646270956\n",
      "Iteration 38806 => Loss: 6.69928574905092322211\n",
      "Iteration 38807 => Loss: 6.69928559976549919952\n",
      "Iteration 38808 => Loss: 6.69928545050018975360\n",
      "Iteration 38809 => Loss: 6.69928530125499399617\n",
      "Iteration 38810 => Loss: 6.69928515202991192723\n",
      "Iteration 38811 => Loss: 6.69928500282493644136\n",
      "Iteration 38812 => Loss: 6.69928485364007020308\n",
      "Iteration 38813 => Loss: 6.69928470447529900156\n",
      "Iteration 38814 => Loss: 6.69928455533063527128\n",
      "Iteration 38815 => Loss: 6.69928440620606480138\n",
      "Iteration 38816 => Loss: 6.69928425710158670370\n",
      "Iteration 38817 => Loss: 6.69928410801720808365\n",
      "Iteration 38818 => Loss: 6.69928395895291739492\n",
      "Iteration 38819 => Loss: 6.69928380990870753209\n",
      "Iteration 38820 => Loss: 6.69928366088458027150\n",
      "Iteration 38821 => Loss: 6.69928351188053738952\n",
      "Iteration 38822 => Loss: 6.69928336289657355707\n",
      "Iteration 38823 => Loss: 6.69928321393268255690\n",
      "Iteration 38824 => Loss: 6.69928306498886527720\n",
      "Iteration 38825 => Loss: 6.69928291606511994161\n",
      "Iteration 38826 => Loss: 6.69928276716144122105\n",
      "Iteration 38827 => Loss: 6.69928261827782645099\n",
      "Iteration 38828 => Loss: 6.69928246941427651961\n",
      "Iteration 38829 => Loss: 6.69928232057078698602\n",
      "Iteration 38830 => Loss: 6.69928217174734985662\n",
      "Iteration 38831 => Loss: 6.69928202294396957228\n",
      "Iteration 38832 => Loss: 6.69928187416063813941\n",
      "Iteration 38833 => Loss: 6.69928172539735466984\n",
      "Iteration 38834 => Loss: 6.69928157665412182808\n",
      "Iteration 38835 => Loss: 6.69928142793092895602\n",
      "Iteration 38836 => Loss: 6.69928127922777516545\n",
      "Iteration 38837 => Loss: 6.69928113054465956822\n",
      "Iteration 38838 => Loss: 6.69928098188158127613\n",
      "Iteration 38839 => Loss: 6.69928083323853496012\n",
      "Iteration 38840 => Loss: 6.69928068461551529111\n",
      "Iteration 38841 => Loss: 6.69928053601252759819\n",
      "Iteration 38842 => Loss: 6.69928038742956299956\n",
      "Iteration 38843 => Loss: 6.69928023886661172526\n",
      "Iteration 38844 => Loss: 6.69928009032369242703\n",
      "Iteration 38845 => Loss: 6.69927994180077934772\n",
      "Iteration 38846 => Loss: 6.69927979329787959273\n",
      "Iteration 38847 => Loss: 6.69927964481499760296\n",
      "Iteration 38848 => Loss: 6.69927949635211650303\n",
      "Iteration 38849 => Loss: 6.69927934790924961561\n",
      "Iteration 38850 => Loss: 6.69927919948637828895\n",
      "Iteration 38851 => Loss: 6.69927905108350874031\n",
      "Iteration 38852 => Loss: 6.69927890270063830513\n",
      "Iteration 38853 => Loss: 6.69927875433775543712\n",
      "Iteration 38854 => Loss: 6.69927860599487345894\n",
      "Iteration 38855 => Loss: 6.69927845767197993609\n",
      "Iteration 38856 => Loss: 6.69927830936906598680\n",
      "Iteration 38857 => Loss: 6.69927816108614138102\n",
      "Iteration 38858 => Loss: 6.69927801282319723697\n",
      "Iteration 38859 => Loss: 6.69927786458023266647\n",
      "Iteration 38860 => Loss: 6.69927771635723967592\n",
      "Iteration 38861 => Loss: 6.69927756815422892345\n",
      "Iteration 38862 => Loss: 6.69927741997117554007\n",
      "Iteration 38863 => Loss: 6.69927727180809462482\n",
      "Iteration 38864 => Loss: 6.69927712366498528951\n",
      "Iteration 38865 => Loss: 6.69927697554182888240\n",
      "Iteration 38866 => Loss: 6.69927682743863961434\n",
      "Iteration 38867 => Loss: 6.69927667935539883359\n",
      "Iteration 38868 => Loss: 6.69927653129211897465\n",
      "Iteration 38869 => Loss: 6.69927638324878849119\n",
      "Iteration 38870 => Loss: 6.69927623522541004775\n",
      "Iteration 38871 => Loss: 6.69927608722197565072\n",
      "Iteration 38872 => Loss: 6.69927593923848618829\n",
      "Iteration 38873 => Loss: 6.69927579127493100231\n",
      "Iteration 38874 => Loss: 6.69927564333132696817\n",
      "Iteration 38875 => Loss: 6.69927549540764388780\n",
      "Iteration 38876 => Loss: 6.69927534750389952478\n",
      "Iteration 38877 => Loss: 6.69927519962008144461\n",
      "Iteration 38878 => Loss: 6.69927505175619941724\n",
      "Iteration 38879 => Loss: 6.69927490391223035004\n",
      "Iteration 38880 => Loss: 6.69927475608819644748\n",
      "Iteration 38881 => Loss: 6.69927460828407728144\n",
      "Iteration 38882 => Loss: 6.69927446049986752286\n",
      "Iteration 38883 => Loss: 6.69927431273557871805\n",
      "Iteration 38884 => Loss: 6.69927416499119754434\n",
      "Iteration 38885 => Loss: 6.69927401726673199533\n",
      "Iteration 38886 => Loss: 6.69927386956216164293\n",
      "Iteration 38887 => Loss: 6.69927372187750247434\n",
      "Iteration 38888 => Loss: 6.69927357421274471960\n",
      "Iteration 38889 => Loss: 6.69927342656788038511\n",
      "Iteration 38890 => Loss: 6.69927327894291124721\n",
      "Iteration 38891 => Loss: 6.69927313133783464139\n",
      "Iteration 38892 => Loss: 6.69927298375264879127\n",
      "Iteration 38893 => Loss: 6.69927283618735014414\n",
      "Iteration 38894 => Loss: 6.69927268864193603548\n",
      "Iteration 38895 => Loss: 6.69927254111640468892\n",
      "Iteration 38896 => Loss: 6.69927239361075343993\n",
      "Iteration 38897 => Loss: 6.69927224612497784761\n",
      "Iteration 38898 => Loss: 6.69927209865906991837\n",
      "Iteration 38899 => Loss: 6.69927195121304119851\n",
      "Iteration 38900 => Loss: 6.69927180378687303630\n",
      "Iteration 38901 => Loss: 6.69927165638057875441\n",
      "Iteration 38902 => Loss: 6.69927150899414769469\n",
      "Iteration 38903 => Loss: 6.69927136162756919902\n",
      "Iteration 38904 => Loss: 6.69927121428085037280\n",
      "Iteration 38905 => Loss: 6.69927106695398677516\n",
      "Iteration 38906 => Loss: 6.69927091964697840609\n",
      "Iteration 38907 => Loss: 6.69927077235982082470\n",
      "Iteration 38908 => Loss: 6.69927062509250514921\n",
      "Iteration 38909 => Loss: 6.69927047784503404415\n",
      "Iteration 38910 => Loss: 6.69927033061740484499\n",
      "Iteration 38911 => Loss: 6.69927018340961843990\n",
      "Iteration 38912 => Loss: 6.69927003622167038799\n",
      "Iteration 38913 => Loss: 6.69926988905355003112\n",
      "Iteration 38914 => Loss: 6.69926974190526092201\n",
      "Iteration 38915 => Loss: 6.69926959477680039612\n",
      "Iteration 38916 => Loss: 6.69926944766816934163\n",
      "Iteration 38917 => Loss: 6.69926930057935621221\n",
      "Iteration 38918 => Loss: 6.69926915351036367241\n",
      "Iteration 38919 => Loss: 6.69926900646119349858\n",
      "Iteration 38920 => Loss: 6.69926885943183414440\n",
      "Iteration 38921 => Loss: 6.69926871242228649805\n",
      "Iteration 38922 => Loss: 6.69926856543254878318\n",
      "Iteration 38923 => Loss: 6.69926841846262188795\n",
      "Iteration 38924 => Loss: 6.69926827151249515424\n",
      "Iteration 38925 => Loss: 6.69926812458217124657\n",
      "Iteration 38926 => Loss: 6.69926797767164394770\n",
      "Iteration 38927 => Loss: 6.69926783078091769852\n",
      "Iteration 38928 => Loss: 6.69926768390998184088\n",
      "Iteration 38929 => Loss: 6.69926753705883992751\n",
      "Iteration 38930 => Loss: 6.69926739022748574115\n",
      "Iteration 38931 => Loss: 6.69926724341591217637\n",
      "Iteration 38932 => Loss: 6.69926709662412545043\n",
      "Iteration 38933 => Loss: 6.69926694985211845790\n",
      "Iteration 38934 => Loss: 6.69926680309988675788\n",
      "Iteration 38935 => Loss: 6.69926665636743035037\n",
      "Iteration 38936 => Loss: 6.69926650965475101174\n",
      "Iteration 38937 => Loss: 6.69926636296183541930\n",
      "Iteration 38938 => Loss: 6.69926621628868801395\n",
      "Iteration 38939 => Loss: 6.69926606963530524297\n",
      "Iteration 38940 => Loss: 6.69926592300168621819\n",
      "Iteration 38941 => Loss: 6.69926577638782472235\n",
      "Iteration 38942 => Loss: 6.69926562979371720274\n",
      "Iteration 38943 => Loss: 6.69926548321936277119\n",
      "Iteration 38944 => Loss: 6.69926533666476320406\n",
      "Iteration 38945 => Loss: 6.69926519012990873136\n",
      "Iteration 38946 => Loss: 6.69926504361480379401\n",
      "Iteration 38947 => Loss: 6.69926489711943684568\n",
      "Iteration 38948 => Loss: 6.69926475064381765634\n",
      "Iteration 38949 => Loss: 6.69926460418792757423\n",
      "Iteration 38950 => Loss: 6.69926445775177814568\n",
      "Iteration 38951 => Loss: 6.69926431133535871254\n",
      "Iteration 38952 => Loss: 6.69926416493866749846\n",
      "Iteration 38953 => Loss: 6.69926401856170716798\n",
      "Iteration 38954 => Loss: 6.69926387220446439841\n",
      "Iteration 38955 => Loss: 6.69926372586695251243\n",
      "Iteration 38956 => Loss: 6.69926357954915019377\n",
      "Iteration 38957 => Loss: 6.69926343325107076510\n",
      "Iteration 38958 => Loss: 6.69926328697270001555\n",
      "Iteration 38959 => Loss: 6.69926314071404593875\n",
      "Iteration 38960 => Loss: 6.69926299447509610019\n",
      "Iteration 38961 => Loss: 6.69926284825585405258\n",
      "Iteration 38962 => Loss: 6.69926270205631357868\n",
      "Iteration 38963 => Loss: 6.69926255587647467848\n",
      "Iteration 38964 => Loss: 6.69926240971633646382\n",
      "Iteration 38965 => Loss: 6.69926226357588561200\n",
      "Iteration 38966 => Loss: 6.69926211745513455753\n",
      "Iteration 38967 => Loss: 6.69926197135406464866\n",
      "Iteration 38968 => Loss: 6.69926182527269364897\n",
      "Iteration 38969 => Loss: 6.69926167921100024216\n",
      "Iteration 38970 => Loss: 6.69926153316898975731\n",
      "Iteration 38971 => Loss: 6.69926138714665775353\n",
      "Iteration 38972 => Loss: 6.69926124114400156628\n",
      "Iteration 38973 => Loss: 6.69926109516101941921\n",
      "Iteration 38974 => Loss: 6.69926094919771397684\n",
      "Iteration 38975 => Loss: 6.69926080325407280469\n",
      "Iteration 38976 => Loss: 6.69926065733009412639\n",
      "Iteration 38977 => Loss: 6.69926051142578327102\n",
      "Iteration 38978 => Loss: 6.69926036554112958044\n",
      "Iteration 38979 => Loss: 6.69926021967613216646\n",
      "Iteration 38980 => Loss: 6.69926007383080168722\n",
      "Iteration 38981 => Loss: 6.69925992800510883285\n",
      "Iteration 38982 => Loss: 6.69925978219907580780\n",
      "Iteration 38983 => Loss: 6.69925963641269017756\n",
      "Iteration 38984 => Loss: 6.69925949064594217219\n",
      "Iteration 38985 => Loss: 6.69925934489884244982\n",
      "Iteration 38986 => Loss: 6.69925919917137857595\n",
      "Iteration 38987 => Loss: 6.69925905346355587966\n",
      "Iteration 38988 => Loss: 6.69925890777535926190\n",
      "Iteration 38989 => Loss: 6.69925876210680293354\n",
      "Iteration 38990 => Loss: 6.69925861645787179555\n",
      "Iteration 38991 => Loss: 6.69925847082856673609\n",
      "Iteration 38992 => Loss: 6.69925832521888420246\n",
      "Iteration 38993 => Loss: 6.69925817962882774736\n",
      "Iteration 38994 => Loss: 6.69925803405838671267\n",
      "Iteration 38995 => Loss: 6.69925788850755843384\n",
      "Iteration 38996 => Loss: 6.69925774297635268084\n",
      "Iteration 38997 => Loss: 6.69925759746474458467\n",
      "Iteration 38998 => Loss: 6.69925745197274391529\n",
      "Iteration 38999 => Loss: 6.69925730650035955449\n",
      "Iteration 39000 => Loss: 6.69925716104757018599\n",
      "Iteration 39001 => Loss: 6.69925701561438735609\n",
      "Iteration 39002 => Loss: 6.69925687020079063672\n",
      "Iteration 39003 => Loss: 6.69925672480679779142\n",
      "Iteration 39004 => Loss: 6.69925657943239016845\n",
      "Iteration 39005 => Loss: 6.69925643407757753778\n",
      "Iteration 39006 => Loss: 6.69925628874234746490\n",
      "Iteration 39007 => Loss: 6.69925614342670705526\n",
      "Iteration 39008 => Loss: 6.69925599813063765708\n",
      "Iteration 39009 => Loss: 6.69925585285415081671\n",
      "Iteration 39010 => Loss: 6.69925570759724386960\n",
      "Iteration 39011 => Loss: 6.69925556235990882215\n",
      "Iteration 39012 => Loss: 6.69925541714214212163\n",
      "Iteration 39013 => Loss: 6.69925527194394909714\n",
      "Iteration 39014 => Loss: 6.69925512676531642597\n",
      "Iteration 39015 => Loss: 6.69925498160624943722\n",
      "Iteration 39016 => Loss: 6.69925483646674013727\n",
      "Iteration 39017 => Loss: 6.69925469134678852612\n",
      "Iteration 39018 => Loss: 6.69925454624639726831\n",
      "Iteration 39019 => Loss: 6.69925440116555481751\n",
      "Iteration 39020 => Loss: 6.69925425610426206191\n",
      "Iteration 39021 => Loss: 6.69925411106251278426\n",
      "Iteration 39022 => Loss: 6.69925396604031497816\n",
      "Iteration 39023 => Loss: 6.69925382103764910369\n",
      "Iteration 39024 => Loss: 6.69925367605452848352\n",
      "Iteration 39025 => Loss: 6.69925353109094778858\n",
      "Iteration 39026 => Loss: 6.69925338614689280803\n",
      "Iteration 39027 => Loss: 6.69925324122237775271\n",
      "Iteration 39028 => Loss: 6.69925309631738130633\n",
      "Iteration 39029 => Loss: 6.69925295143192034431\n",
      "Iteration 39030 => Loss: 6.69925280656597621487\n",
      "Iteration 39031 => Loss: 6.69925266171955335892\n",
      "Iteration 39032 => Loss: 6.69925251689264822375\n",
      "Iteration 39033 => Loss: 6.69925237208526436206\n",
      "Iteration 39034 => Loss: 6.69925222729738845118\n",
      "Iteration 39035 => Loss: 6.69925208252901871475\n",
      "Iteration 39036 => Loss: 6.69925193778016758728\n",
      "Iteration 39037 => Loss: 6.69925179305081730519\n",
      "Iteration 39038 => Loss: 6.69925164834096609212\n",
      "Iteration 39039 => Loss: 6.69925150365061572444\n",
      "Iteration 39040 => Loss: 6.69925135897976087307\n",
      "Iteration 39041 => Loss: 6.69925121432839976165\n",
      "Iteration 39042 => Loss: 6.69925106969653327837\n",
      "Iteration 39043 => Loss: 6.69925092508415875869\n",
      "Iteration 39044 => Loss: 6.69925078049126110358\n",
      "Iteration 39045 => Loss: 6.69925063591786074113\n",
      "Iteration 39046 => Loss: 6.69925049136393635507\n",
      "Iteration 39047 => Loss: 6.69925034682948350451\n",
      "Iteration 39048 => Loss: 6.69925020231451373576\n",
      "Iteration 39049 => Loss: 6.69925005781901372615\n",
      "Iteration 39050 => Loss: 6.69924991334298791656\n",
      "Iteration 39051 => Loss: 6.69924976888642920159\n",
      "Iteration 39052 => Loss: 6.69924962444933402850\n",
      "Iteration 39053 => Loss: 6.69924948003170772637\n",
      "Iteration 39054 => Loss: 6.69924933563353075527\n",
      "Iteration 39055 => Loss: 6.69924919125482531967\n",
      "Iteration 39056 => Loss: 6.69924904689556388604\n",
      "Iteration 39057 => Loss: 6.69924890255575800069\n",
      "Iteration 39058 => Loss: 6.69924875823540411091\n",
      "Iteration 39059 => Loss: 6.69924861393449599944\n",
      "Iteration 39060 => Loss: 6.69924846965303011359\n",
      "Iteration 39061 => Loss: 6.69924832539101267059\n",
      "Iteration 39062 => Loss: 6.69924818114842857142\n",
      "Iteration 39063 => Loss: 6.69924803692527959242\n",
      "Iteration 39064 => Loss: 6.69924789272156750997\n",
      "Iteration 39065 => Loss: 6.69924774853728610680\n",
      "Iteration 39066 => Loss: 6.69924760437244248834\n",
      "Iteration 39067 => Loss: 6.69924746022701356196\n",
      "Iteration 39068 => Loss: 6.69924731610100909762\n",
      "Iteration 39069 => Loss: 6.69924717199442731896\n",
      "Iteration 39070 => Loss: 6.69924702790726644963\n",
      "Iteration 39071 => Loss: 6.69924688383952027237\n",
      "Iteration 39072 => Loss: 6.69924673979118701084\n",
      "Iteration 39073 => Loss: 6.69924659576226488866\n",
      "Iteration 39074 => Loss: 6.69924645175275035314\n",
      "Iteration 39075 => Loss: 6.69924630776264162790\n",
      "Iteration 39076 => Loss: 6.69924616379192805482\n",
      "Iteration 39077 => Loss: 6.69924601984063006199\n",
      "Iteration 39078 => Loss: 6.69924587590871656317\n",
      "Iteration 39079 => Loss: 6.69924573199620532193\n",
      "Iteration 39080 => Loss: 6.69924558810307413381\n",
      "Iteration 39081 => Loss: 6.69924544422934697963\n",
      "Iteration 39082 => Loss: 6.69924530037500343127\n",
      "Iteration 39083 => Loss: 6.69924515654003993603\n",
      "Iteration 39084 => Loss: 6.69924501272446448752\n",
      "Iteration 39085 => Loss: 6.69924486892825754580\n",
      "Iteration 39086 => Loss: 6.69924472515143598628\n",
      "Iteration 39087 => Loss: 6.69924458139398826262\n",
      "Iteration 39088 => Loss: 6.69924443765591082212\n",
      "Iteration 39089 => Loss: 6.69924429393719833570\n",
      "Iteration 39090 => Loss: 6.69924415023785346790\n",
      "Iteration 39091 => Loss: 6.69924400655787444236\n",
      "Iteration 39092 => Loss: 6.69924386289725681820\n",
      "Iteration 39093 => Loss: 6.69924371925599704269\n",
      "Iteration 39094 => Loss: 6.69924357563408801042\n",
      "Iteration 39095 => Loss: 6.69924343203153149773\n",
      "Iteration 39096 => Loss: 6.69924328844833549823\n",
      "Iteration 39097 => Loss: 6.69924314488447958382\n",
      "Iteration 39098 => Loss: 6.69924300133996997175\n",
      "Iteration 39099 => Loss: 6.69924285781480755020\n",
      "Iteration 39100 => Loss: 6.69924271430897722013\n",
      "Iteration 39101 => Loss: 6.69924257082249141604\n",
      "Iteration 39102 => Loss: 6.69924242735533415072\n",
      "Iteration 39103 => Loss: 6.69924228390751430595\n",
      "Iteration 39104 => Loss: 6.69924214047902477631\n",
      "Iteration 39105 => Loss: 6.69924199706985756819\n",
      "Iteration 39106 => Loss: 6.69924185368001889884\n",
      "Iteration 39107 => Loss: 6.69924171030949633376\n",
      "Iteration 39108 => Loss: 6.69924156695830141928\n",
      "Iteration 39109 => Loss: 6.69924142362641727999\n",
      "Iteration 39110 => Loss: 6.69924128031384213955\n",
      "Iteration 39111 => Loss: 6.69924113702058487974\n",
      "Iteration 39112 => Loss: 6.69924099374663928330\n",
      "Iteration 39113 => Loss: 6.69924085049199380393\n",
      "Iteration 39114 => Loss: 6.69924070725665465886\n",
      "Iteration 39115 => Loss: 6.69924056404061918357\n",
      "Iteration 39116 => Loss: 6.69924042084387938445\n",
      "Iteration 39117 => Loss: 6.69924027766643526149\n",
      "Iteration 39118 => Loss: 6.69924013450828592653\n",
      "Iteration 39119 => Loss: 6.69923999136942605048\n",
      "Iteration 39120 => Loss: 6.69923984824985119246\n",
      "Iteration 39121 => Loss: 6.69923970514956312883\n",
      "Iteration 39122 => Loss: 6.69923956206855919504\n",
      "Iteration 39123 => Loss: 6.69923941900683583839\n",
      "Iteration 39124 => Loss: 6.69923927596438861798\n",
      "Iteration 39125 => Loss: 6.69923913294121486928\n",
      "Iteration 39126 => Loss: 6.69923898993732258589\n",
      "Iteration 39127 => Loss: 6.69923884695268956335\n",
      "Iteration 39128 => Loss: 6.69923870398732024256\n",
      "Iteration 39129 => Loss: 6.69923856104122528166\n",
      "Iteration 39130 => Loss: 6.69923841811439491067\n",
      "Iteration 39131 => Loss: 6.69923827520681491876\n",
      "Iteration 39132 => Loss: 6.69923813231848974681\n",
      "Iteration 39133 => Loss: 6.69923798944943360567\n",
      "Iteration 39134 => Loss: 6.69923784659961540910\n",
      "Iteration 39135 => Loss: 6.69923770376905469703\n",
      "Iteration 39136 => Loss: 6.69923756095773548225\n",
      "Iteration 39137 => Loss: 6.69923741816565954110\n",
      "Iteration 39138 => Loss: 6.69923727539282864996\n",
      "Iteration 39139 => Loss: 6.69923713263923037431\n",
      "Iteration 39140 => Loss: 6.69923698990487537230\n",
      "Iteration 39141 => Loss: 6.69923684718975032126\n",
      "Iteration 39142 => Loss: 6.69923670449386055026\n",
      "Iteration 39143 => Loss: 6.69923656181719628933\n",
      "Iteration 39144 => Loss: 6.69923641915975043304\n",
      "Iteration 39145 => Loss: 6.69923627652153186318\n",
      "Iteration 39146 => Loss: 6.69923613390253702704\n",
      "Iteration 39147 => Loss: 6.69923599130275437830\n",
      "Iteration 39148 => Loss: 6.69923584872219368691\n",
      "Iteration 39149 => Loss: 6.69923570616084607110\n",
      "Iteration 39150 => Loss: 6.69923556361870353726\n",
      "Iteration 39151 => Loss: 6.69923542109577674353\n",
      "Iteration 39152 => Loss: 6.69923527859204348545\n",
      "Iteration 39153 => Loss: 6.69923513610751708569\n",
      "Iteration 39154 => Loss: 6.69923499364219487973\n",
      "Iteration 39155 => Loss: 6.69923485119606798577\n",
      "Iteration 39156 => Loss: 6.69923470876913107475\n",
      "Iteration 39157 => Loss: 6.69923456636139214027\n",
      "Iteration 39158 => Loss: 6.69923442397283519512\n",
      "Iteration 39159 => Loss: 6.69923428160347089744\n",
      "Iteration 39160 => Loss: 6.69923413925329036545\n",
      "Iteration 39161 => Loss: 6.69923399692229271096\n",
      "Iteration 39162 => Loss: 6.69923385461047438127\n",
      "Iteration 39163 => Loss: 6.69923371231782560642\n",
      "Iteration 39164 => Loss: 6.69923357004435704454\n",
      "Iteration 39165 => Loss: 6.69923342779006070202\n",
      "Iteration 39166 => Loss: 6.69923328555492858527\n",
      "Iteration 39167 => Loss: 6.69923314333896602335\n",
      "Iteration 39168 => Loss: 6.69923300114216502266\n",
      "Iteration 39169 => Loss: 6.69923285896453002408\n",
      "Iteration 39170 => Loss: 6.69923271680604326406\n",
      "Iteration 39171 => Loss: 6.69923257466672339433\n",
      "Iteration 39172 => Loss: 6.69923243254655176315\n",
      "Iteration 39173 => Loss: 6.69923229044553103506\n",
      "Iteration 39174 => Loss: 6.69923214836365765734\n",
      "Iteration 39175 => Loss: 6.69923200630093162999\n",
      "Iteration 39176 => Loss: 6.69923186425734673577\n",
      "Iteration 39177 => Loss: 6.69923172223290563920\n",
      "Iteration 39178 => Loss: 6.69923158022760301122\n",
      "Iteration 39179 => Loss: 6.69923143824142908187\n",
      "Iteration 39180 => Loss: 6.69923129627439184475\n",
      "Iteration 39181 => Loss: 6.69923115432648419443\n",
      "Iteration 39182 => Loss: 6.69923101239770524273\n",
      "Iteration 39183 => Loss: 6.69923087048804788424\n",
      "Iteration 39184 => Loss: 6.69923072859751567165\n",
      "Iteration 39185 => Loss: 6.69923058672610061137\n",
      "Iteration 39186 => Loss: 6.69923044487380536793\n",
      "Iteration 39187 => Loss: 6.69923030304062816498\n",
      "Iteration 39188 => Loss: 6.69923016122656100890\n",
      "Iteration 39189 => Loss: 6.69923001943159590610\n",
      "Iteration 39190 => Loss: 6.69922987765574173835\n",
      "Iteration 39191 => Loss: 6.69922973589899406477\n",
      "Iteration 39192 => Loss: 6.69922959416134755628\n",
      "Iteration 39193 => Loss: 6.69922945244279688382\n",
      "Iteration 39194 => Loss: 6.69922931074334737644\n",
      "Iteration 39195 => Loss: 6.69922916906299281692\n",
      "Iteration 39196 => Loss: 6.69922902740172609981\n",
      "Iteration 39197 => Loss: 6.69922888575954900148\n",
      "Iteration 39198 => Loss: 6.69922874413645796920\n",
      "Iteration 39199 => Loss: 6.69922860253245300299\n",
      "Iteration 39200 => Loss: 6.69922846094752610924\n",
      "Iteration 39201 => Loss: 6.69922831938167995247\n",
      "Iteration 39202 => Loss: 6.69922817783490742727\n",
      "Iteration 39203 => Loss: 6.69922803630720764545\n",
      "Iteration 39204 => Loss: 6.69922789479858060702\n",
      "Iteration 39205 => Loss: 6.69922775330902542379\n",
      "Iteration 39206 => Loss: 6.69922761183853232581\n",
      "Iteration 39207 => Loss: 6.69922747038709864853\n",
      "Iteration 39208 => Loss: 6.69922732895472794468\n",
      "Iteration 39209 => Loss: 6.69922718754142376696\n",
      "Iteration 39210 => Loss: 6.69922704614716391092\n",
      "Iteration 39211 => Loss: 6.69922690477196525194\n",
      "Iteration 39212 => Loss: 6.69922676341580469739\n",
      "Iteration 39213 => Loss: 6.69922662207870622808\n",
      "Iteration 39214 => Loss: 6.69922648076064408684\n",
      "Iteration 39215 => Loss: 6.69922633946163337271\n",
      "Iteration 39216 => Loss: 6.69922619818165276939\n",
      "Iteration 39217 => Loss: 6.69922605692071115868\n",
      "Iteration 39218 => Loss: 6.69922591567880498786\n",
      "Iteration 39219 => Loss: 6.69922577445593692147\n",
      "Iteration 39220 => Loss: 6.69922563325209541318\n",
      "Iteration 39221 => Loss: 6.69922549206728046300\n",
      "Iteration 39222 => Loss: 6.69922535090148940640\n",
      "Iteration 39223 => Loss: 6.69922520975471424975\n",
      "Iteration 39224 => Loss: 6.69922506862697275665\n",
      "Iteration 39225 => Loss: 6.69922492751823561719\n",
      "Iteration 39226 => Loss: 6.69922478642852414765\n",
      "Iteration 39227 => Loss: 6.69922464535781259087\n",
      "Iteration 39228 => Loss: 6.69922450430612048677\n",
      "Iteration 39229 => Loss: 6.69922436327343362450\n",
      "Iteration 39230 => Loss: 6.69922422225974401044\n",
      "Iteration 39231 => Loss: 6.69922408126505786186\n",
      "Iteration 39232 => Loss: 6.69922394028938050781\n",
      "Iteration 39233 => Loss: 6.69922379933268974384\n",
      "Iteration 39234 => Loss: 6.69922365839499356355\n",
      "Iteration 39235 => Loss: 6.69922351747629285512\n",
      "Iteration 39236 => Loss: 6.69922337657657518406\n",
      "Iteration 39237 => Loss: 6.69922323569584943215\n",
      "Iteration 39238 => Loss: 6.69922309483410671760\n",
      "Iteration 39239 => Loss: 6.69922295399134348770\n",
      "Iteration 39240 => Loss: 6.69922281316756063063\n",
      "Iteration 39241 => Loss: 6.69922267236275637003\n",
      "Iteration 39242 => Loss: 6.69922253157691560688\n",
      "Iteration 39243 => Loss: 6.69922239081004811112\n",
      "Iteration 39244 => Loss: 6.69922225006215832366\n",
      "Iteration 39245 => Loss: 6.69922210933322404003\n",
      "Iteration 39246 => Loss: 6.69922196862326035927\n",
      "Iteration 39247 => Loss: 6.69922182793225129416\n",
      "Iteration 39248 => Loss: 6.69922168726020395013\n",
      "Iteration 39249 => Loss: 6.69922154660710944540\n",
      "Iteration 39250 => Loss: 6.69922140597297133269\n",
      "Iteration 39251 => Loss: 6.69922126535778605927\n",
      "Iteration 39252 => Loss: 6.69922112476154651972\n",
      "Iteration 39253 => Loss: 6.69922098418424827315\n",
      "Iteration 39254 => Loss: 6.69922084362589309592\n",
      "Iteration 39255 => Loss: 6.69922070308648454073\n",
      "Iteration 39256 => Loss: 6.69922056256601194946\n",
      "Iteration 39257 => Loss: 6.69922042206446732848\n",
      "Iteration 39258 => Loss: 6.69922028158186133595\n",
      "Iteration 39259 => Loss: 6.69922014111818509008\n",
      "Iteration 39260 => Loss: 6.69922000067343503815\n",
      "Iteration 39261 => Loss: 6.69921986024760940381\n",
      "Iteration 39262 => Loss: 6.69921971984071085160\n",
      "Iteration 39263 => Loss: 6.69921957945272694701\n",
      "Iteration 39264 => Loss: 6.69921943908365946641\n",
      "Iteration 39265 => Loss: 6.69921929873351018614\n",
      "Iteration 39266 => Loss: 6.69921915840227288896\n",
      "Iteration 39267 => Loss: 6.69921901808994491034\n",
      "Iteration 39268 => Loss: 6.69921887779652269757\n",
      "Iteration 39269 => Loss: 6.69921873752200625063\n",
      "Iteration 39270 => Loss: 6.69921859726639379318\n",
      "Iteration 39271 => Loss: 6.69921845702967289071\n",
      "Iteration 39272 => Loss: 6.69921831681186308316\n",
      "Iteration 39273 => Loss: 6.69921817661293150792\n",
      "Iteration 39274 => Loss: 6.69921803643290481034\n",
      "Iteration 39275 => Loss: 6.69921789627175900961\n",
      "Iteration 39276 => Loss: 6.69921775612950298751\n",
      "Iteration 39277 => Loss: 6.69921761600612875043\n",
      "Iteration 39278 => Loss: 6.69921747590163985109\n",
      "Iteration 39279 => Loss: 6.69921733581602651952\n",
      "Iteration 39280 => Loss: 6.69921719574929319663\n",
      "Iteration 39281 => Loss: 6.69921705570143100061\n",
      "Iteration 39282 => Loss: 6.69921691567244259602\n",
      "Iteration 39283 => Loss: 6.69921677566232531831\n",
      "Iteration 39284 => Loss: 6.69921663567106584480\n",
      "Iteration 39285 => Loss: 6.69921649569867749818\n",
      "Iteration 39286 => Loss: 6.69921635574514873213\n",
      "Iteration 39287 => Loss: 6.69921621581048043481\n",
      "Iteration 39288 => Loss: 6.69921607589466461263\n",
      "Iteration 39289 => Loss: 6.69921593599770393013\n",
      "Iteration 39290 => Loss: 6.69921579611959394640\n",
      "Iteration 39291 => Loss: 6.69921565626033554963\n",
      "Iteration 39292 => Loss: 6.69921551641992252257\n",
      "Iteration 39293 => Loss: 6.69921537659834775980\n",
      "Iteration 39294 => Loss: 6.69921523679562280762\n",
      "Iteration 39295 => Loss: 6.69921509701172990248\n",
      "Iteration 39296 => Loss: 6.69921495724666993254\n",
      "Iteration 39297 => Loss: 6.69921481750044822689\n",
      "Iteration 39298 => Loss: 6.69921467777305590374\n",
      "Iteration 39299 => Loss: 6.69921453806449562762\n",
      "Iteration 39300 => Loss: 6.69921439837475318768\n",
      "Iteration 39301 => Loss: 6.69921425870384368295\n",
      "Iteration 39302 => Loss: 6.69921411905174846169\n",
      "Iteration 39303 => Loss: 6.69921397941847196478\n",
      "Iteration 39304 => Loss: 6.69921383980401063951\n",
      "Iteration 39305 => Loss: 6.69921370020836182135\n",
      "Iteration 39306 => Loss: 6.69921356063152906302\n",
      "Iteration 39307 => Loss: 6.69921342107350081818\n",
      "Iteration 39308 => Loss: 6.69921328153427886321\n",
      "Iteration 39309 => Loss: 6.69921314201385786902\n",
      "Iteration 39310 => Loss: 6.69921300251223783562\n",
      "Iteration 39311 => Loss: 6.69921286302941521029\n",
      "Iteration 39312 => Loss: 6.69921272356538732851\n",
      "Iteration 39313 => Loss: 6.69921258412015596662\n",
      "Iteration 39314 => Loss: 6.69921244469371224284\n",
      "Iteration 39315 => Loss: 6.69921230528605882171\n",
      "Iteration 39316 => Loss: 6.69921216589718770962\n",
      "Iteration 39317 => Loss: 6.69921202652709890657\n",
      "Iteration 39318 => Loss: 6.69921188717578708349\n",
      "Iteration 39319 => Loss: 6.69921174784326023399\n",
      "Iteration 39320 => Loss: 6.69921160852950059450\n",
      "Iteration 39321 => Loss: 6.69921146923452148769\n",
      "Iteration 39322 => Loss: 6.69921132995830692636\n",
      "Iteration 39323 => Loss: 6.69921119070086579228\n",
      "Iteration 39324 => Loss: 6.69921105146218387461\n",
      "Iteration 39325 => Loss: 6.69921091224226739058\n",
      "Iteration 39326 => Loss: 6.69921077304111101114\n",
      "Iteration 39327 => Loss: 6.69921063385870407814\n",
      "Iteration 39328 => Loss: 6.69921049469506257878\n",
      "Iteration 39329 => Loss: 6.69921035555016786134\n",
      "Iteration 39330 => Loss: 6.69921021642402969576\n",
      "Iteration 39331 => Loss: 6.69921007731663475937\n",
      "Iteration 39332 => Loss: 6.69920993822797683492\n",
      "Iteration 39333 => Loss: 6.69920979915807279781\n",
      "Iteration 39334 => Loss: 6.69920966010690399628\n",
      "Iteration 39335 => Loss: 6.69920952107446776580\n",
      "Iteration 39336 => Loss: 6.69920938206076588273\n",
      "Iteration 39337 => Loss: 6.69920924306580722885\n",
      "Iteration 39338 => Loss: 6.69920910408957048787\n",
      "Iteration 39339 => Loss: 6.69920896513205477163\n",
      "Iteration 39340 => Loss: 6.69920882619327517915\n",
      "Iteration 39341 => Loss: 6.69920868727321305869\n",
      "Iteration 39342 => Loss: 6.69920854837186752206\n",
      "Iteration 39343 => Loss: 6.69920840948924745106\n",
      "Iteration 39344 => Loss: 6.69920827062533241758\n",
      "Iteration 39345 => Loss: 6.69920813178013041522\n",
      "Iteration 39346 => Loss: 6.69920799295364055581\n",
      "Iteration 39347 => Loss: 6.69920785414585662210\n",
      "Iteration 39348 => Loss: 6.69920771535677594954\n",
      "Iteration 39349 => Loss: 6.69920757658640120269\n",
      "Iteration 39350 => Loss: 6.69920743783472527610\n",
      "Iteration 39351 => Loss: 6.69920729910174461708\n",
      "Iteration 39352 => Loss: 6.69920716038745389653\n",
      "Iteration 39353 => Loss: 6.69920702169186288444\n",
      "Iteration 39354 => Loss: 6.69920688301495736994\n",
      "Iteration 39355 => Loss: 6.69920674435673291214\n",
      "Iteration 39356 => Loss: 6.69920660571720372189\n",
      "Iteration 39357 => Loss: 6.69920646709634759475\n",
      "Iteration 39358 => Loss: 6.69920632849417252430\n",
      "Iteration 39359 => Loss: 6.69920618991068028691\n",
      "Iteration 39360 => Loss: 6.69920605134585400720\n",
      "Iteration 39361 => Loss: 6.69920591279970256693\n",
      "Iteration 39362 => Loss: 6.69920577427221619615\n",
      "Iteration 39363 => Loss: 6.69920563576340200029\n",
      "Iteration 39364 => Loss: 6.69920549727325109757\n",
      "Iteration 39365 => Loss: 6.69920535880176082344\n",
      "Iteration 39366 => Loss: 6.69920522034892762520\n",
      "Iteration 39367 => Loss: 6.69920508191475683191\n",
      "Iteration 39368 => Loss: 6.69920494349923512090\n",
      "Iteration 39369 => Loss: 6.69920480510236426852\n",
      "Iteration 39370 => Loss: 6.69920466672414782749\n",
      "Iteration 39371 => Loss: 6.69920452836457069878\n",
      "Iteration 39372 => Loss: 6.69920439002364176417\n",
      "Iteration 39373 => Loss: 6.69920425170135569459\n",
      "Iteration 39374 => Loss: 6.69920411339769472647\n",
      "Iteration 39375 => Loss: 6.69920397511269261059\n",
      "Iteration 39376 => Loss: 6.69920383684631115528\n",
      "Iteration 39377 => Loss: 6.69920369859856368322\n",
      "Iteration 39378 => Loss: 6.69920356036944042444\n",
      "Iteration 39379 => Loss: 6.69920342215895114890\n",
      "Iteration 39380 => Loss: 6.69920328396708164576\n",
      "Iteration 39381 => Loss: 6.69920314579383990861\n",
      "Iteration 39382 => Loss: 6.69920300763920462117\n",
      "Iteration 39383 => Loss: 6.69920286950319621155\n",
      "Iteration 39384 => Loss: 6.69920273138579691619\n",
      "Iteration 39385 => Loss: 6.69920259328700851142\n",
      "Iteration 39386 => Loss: 6.69920245520683099727\n",
      "Iteration 39387 => Loss: 6.69920231714525549194\n",
      "Iteration 39388 => Loss: 6.69920217910229798264\n",
      "Iteration 39389 => Loss: 6.69920204107792383041\n",
      "Iteration 39390 => Loss: 6.69920190307216234515\n",
      "Iteration 39391 => Loss: 6.69920176508498510515\n",
      "Iteration 39392 => Loss: 6.69920162711641165032\n",
      "Iteration 39393 => Loss: 6.69920148916642510528\n",
      "Iteration 39394 => Loss: 6.69920135123502724639\n",
      "Iteration 39395 => Loss: 6.69920121332221540911\n",
      "Iteration 39396 => Loss: 6.69920107542799403433\n",
      "Iteration 39397 => Loss: 6.69920093755233825306\n",
      "Iteration 39398 => Loss: 6.69920079969527382246\n",
      "Iteration 39399 => Loss: 6.69920066185678209081\n",
      "Iteration 39400 => Loss: 6.69920052403686572262\n",
      "Iteration 39401 => Loss: 6.69920038623551850065\n",
      "Iteration 39402 => Loss: 6.69920024845274308944\n",
      "Iteration 39403 => Loss: 6.69920011068852883085\n",
      "Iteration 39404 => Loss: 6.69919997294287927758\n",
      "Iteration 39405 => Loss: 6.69919983521579798236\n",
      "Iteration 39406 => Loss: 6.69919969750727251068\n",
      "Iteration 39407 => Loss: 6.69919955981729664529\n",
      "Iteration 39408 => Loss: 6.69919942214588104434\n",
      "Iteration 39409 => Loss: 6.69919928449301149698\n",
      "Iteration 39410 => Loss: 6.69919914685869510862\n",
      "Iteration 39411 => Loss: 6.69919900924293099109\n",
      "Iteration 39412 => Loss: 6.69919887164569782811\n",
      "Iteration 39413 => Loss: 6.69919873406700716600\n",
      "Iteration 39414 => Loss: 6.69919859650686255748\n",
      "Iteration 39415 => Loss: 6.69919845896525245621\n",
      "Iteration 39416 => Loss: 6.69919832144217330949\n",
      "Iteration 39417 => Loss: 6.69919818393762334097\n",
      "Iteration 39418 => Loss: 6.69919804645161143242\n",
      "Iteration 39419 => Loss: 6.69919790898411537938\n",
      "Iteration 39420 => Loss: 6.69919777153515383361\n",
      "Iteration 39421 => Loss: 6.69919763410470459064\n",
      "Iteration 39422 => Loss: 6.69919749669277475590\n",
      "Iteration 39423 => Loss: 6.69919735929936255303\n",
      "Iteration 39424 => Loss: 6.69919722192446354114\n",
      "Iteration 39425 => Loss: 6.69919708456807949659\n",
      "Iteration 39426 => Loss: 6.69919694723019709670\n",
      "Iteration 39427 => Loss: 6.69919680991083055233\n",
      "Iteration 39428 => Loss: 6.69919667260996920533\n",
      "Iteration 39429 => Loss: 6.69919653532760150938\n",
      "Iteration 39430 => Loss: 6.69919639806372835267\n",
      "Iteration 39431 => Loss: 6.69919626081835772879\n",
      "Iteration 39432 => Loss: 6.69919612359148519687\n",
      "Iteration 39433 => Loss: 6.69919598638309299332\n",
      "Iteration 39434 => Loss: 6.69919584919319976990\n",
      "Iteration 39435 => Loss: 6.69919571202178598668\n",
      "Iteration 39436 => Loss: 6.69919557486886407816\n",
      "Iteration 39437 => Loss: 6.69919543773441716894\n",
      "Iteration 39438 => Loss: 6.69919530061844703539\n",
      "Iteration 39439 => Loss: 6.69919516352095723022\n",
      "Iteration 39440 => Loss: 6.69919502644193975982\n",
      "Iteration 39441 => Loss: 6.69919488938139906509\n",
      "Iteration 39442 => Loss: 6.69919475233932093516\n",
      "Iteration 39443 => Loss: 6.69919461531571069912\n",
      "Iteration 39444 => Loss: 6.69919447831056302789\n",
      "Iteration 39445 => Loss: 6.69919434132388413872\n",
      "Iteration 39446 => Loss: 6.69919420435565804439\n",
      "Iteration 39447 => Loss: 6.69919406740588385674\n",
      "Iteration 39448 => Loss: 6.69919393047457312207\n",
      "Iteration 39449 => Loss: 6.69919379356170718864\n",
      "Iteration 39450 => Loss: 6.69919365666729849096\n",
      "Iteration 39451 => Loss: 6.69919351979132748909\n",
      "Iteration 39452 => Loss: 6.69919338293380572935\n",
      "Iteration 39453 => Loss: 6.69919324609472255361\n",
      "Iteration 39454 => Loss: 6.69919310927407618550\n",
      "Iteration 39455 => Loss: 6.69919297247187373046\n",
      "Iteration 39456 => Loss: 6.69919283568810541851\n",
      "Iteration 39457 => Loss: 6.69919269892276059153\n",
      "Iteration 39458 => Loss: 6.69919256217584635493\n",
      "Iteration 39459 => Loss: 6.69919242544736004419\n",
      "Iteration 39460 => Loss: 6.69919228873730165930\n",
      "Iteration 39461 => Loss: 6.69919215204566409483\n",
      "Iteration 39462 => Loss: 6.69919201537244646261\n",
      "Iteration 39463 => Loss: 6.69919187871763632813\n",
      "Iteration 39464 => Loss: 6.69919174208125145498\n",
      "Iteration 39465 => Loss: 6.69919160546327319139\n",
      "Iteration 39466 => Loss: 6.69919146886371041916\n",
      "Iteration 39467 => Loss: 6.69919133228254448653\n",
      "Iteration 39468 => Loss: 6.69919119571978871619\n",
      "Iteration 39469 => Loss: 6.69919105917543511453\n",
      "Iteration 39470 => Loss: 6.69919092264947657611\n",
      "Iteration 39471 => Loss: 6.69919078614191665366\n",
      "Iteration 39472 => Loss: 6.69919064965275179446\n",
      "Iteration 39473 => Loss: 6.69919051318198111034\n",
      "Iteration 39474 => Loss: 6.69919037672960016039\n",
      "Iteration 39475 => Loss: 6.69919024029560805644\n",
      "Iteration 39476 => Loss: 6.69919010387999769307\n",
      "Iteration 39477 => Loss: 6.69918996748276374120\n",
      "Iteration 39478 => Loss: 6.69918983110391330626\n",
      "Iteration 39479 => Loss: 6.69918969474344283555\n",
      "Iteration 39480 => Loss: 6.69918955840134167090\n",
      "Iteration 39481 => Loss: 6.69918942207761247687\n",
      "Iteration 39482 => Loss: 6.69918928577226147070\n",
      "Iteration 39483 => Loss: 6.69918914948527000064\n",
      "Iteration 39484 => Loss: 6.69918901321664250759\n",
      "Iteration 39485 => Loss: 6.69918887696638343243\n",
      "Iteration 39486 => Loss: 6.69918874073448833428\n",
      "Iteration 39487 => Loss: 6.69918860452093767321\n",
      "Iteration 39488 => Loss: 6.69918846832574832462\n",
      "Iteration 39489 => Loss: 6.69918833214890785399\n",
      "Iteration 39490 => Loss: 6.69918819599041448498\n",
      "Iteration 39491 => Loss: 6.69918805985027798755\n",
      "Iteration 39492 => Loss: 6.69918792372848592720\n",
      "Iteration 39493 => Loss: 6.69918778762502142854\n",
      "Iteration 39494 => Loss: 6.69918765153991735417\n",
      "Iteration 39495 => Loss: 6.69918751547313995331\n",
      "Iteration 39496 => Loss: 6.69918737942469899593\n",
      "Iteration 39497 => Loss: 6.69918724339458560024\n",
      "Iteration 39498 => Loss: 6.69918710738280953620\n",
      "Iteration 39499 => Loss: 6.69918697138935659297\n",
      "Iteration 39500 => Loss: 6.69918683541423298777\n",
      "Iteration 39501 => Loss: 6.69918669945742983884\n",
      "Iteration 39502 => Loss: 6.69918656351894181711\n",
      "Iteration 39503 => Loss: 6.69918642759878046888\n",
      "Iteration 39504 => Loss: 6.69918629169693069514\n",
      "Iteration 39505 => Loss: 6.69918615581339427223\n",
      "Iteration 39506 => Loss: 6.69918601994816231837\n",
      "Iteration 39507 => Loss: 6.69918588410124637988\n",
      "Iteration 39508 => Loss: 6.69918574827262780502\n",
      "Iteration 39509 => Loss: 6.69918561246231991646\n",
      "Iteration 39510 => Loss: 6.69918547667030761517\n",
      "Iteration 39511 => Loss: 6.69918534089659445385\n",
      "Iteration 39512 => Loss: 6.69918520514117776798\n",
      "Iteration 39513 => Loss: 6.69918506940406022210\n",
      "Iteration 39514 => Loss: 6.69918493368522582898\n",
      "Iteration 39515 => Loss: 6.69918479798467547681\n",
      "Iteration 39516 => Loss: 6.69918466230242248827\n",
      "Iteration 39517 => Loss: 6.69918452663844110617\n",
      "Iteration 39518 => Loss: 6.69918439099274731774\n",
      "Iteration 39519 => Loss: 6.69918425536532957665\n",
      "Iteration 39520 => Loss: 6.69918411975619143561\n",
      "Iteration 39521 => Loss: 6.69918398416531868378\n",
      "Iteration 39522 => Loss: 6.69918384859272375564\n",
      "Iteration 39523 => Loss: 6.69918371303839776942\n",
      "Iteration 39524 => Loss: 6.69918357750232562609\n",
      "Iteration 39525 => Loss: 6.69918344198453397098\n",
      "Iteration 39526 => Loss: 6.69918330648499615876\n",
      "Iteration 39527 => Loss: 6.69918317100371307760\n",
      "Iteration 39528 => Loss: 6.69918303554068916839\n",
      "Iteration 39529 => Loss: 6.69918290009591732570\n",
      "Iteration 39530 => Loss: 6.69918276466940465497\n",
      "Iteration 39531 => Loss: 6.69918262926113605715\n",
      "Iteration 39532 => Loss: 6.69918249387110353865\n",
      "Iteration 39533 => Loss: 6.69918235849932841575\n",
      "Iteration 39534 => Loss: 6.69918222314578848398\n",
      "Iteration 39535 => Loss: 6.69918208781048818423\n",
      "Iteration 39536 => Loss: 6.69918195249342662834\n",
      "Iteration 39537 => Loss: 6.69918181719459493451\n",
      "Iteration 39538 => Loss: 6.69918168191399931999\n",
      "Iteration 39539 => Loss: 6.69918154665163090300\n",
      "Iteration 39540 => Loss: 6.69918141140748790718\n",
      "Iteration 39541 => Loss: 6.69918127618156500347\n",
      "Iteration 39542 => Loss: 6.69918114097387196182\n",
      "Iteration 39543 => Loss: 6.69918100578439812409\n",
      "Iteration 39544 => Loss: 6.69918087061313283215\n",
      "Iteration 39545 => Loss: 6.69918073546008763230\n",
      "Iteration 39546 => Loss: 6.69918060032525808367\n",
      "Iteration 39547 => Loss: 6.69918046520863086357\n",
      "Iteration 39548 => Loss: 6.69918033011021307743\n",
      "Iteration 39549 => Loss: 6.69918019503000206072\n",
      "Iteration 39550 => Loss: 6.69918005996798715529\n",
      "Iteration 39551 => Loss: 6.69917992492417901929\n",
      "Iteration 39552 => Loss: 6.69917978989856255367\n",
      "Iteration 39553 => Loss: 6.69917965489114219935\n",
      "Iteration 39554 => Loss: 6.69917951990191706813\n",
      "Iteration 39555 => Loss: 6.69917938493088183094\n",
      "Iteration 39556 => Loss: 6.69917924997803027054\n",
      "Iteration 39557 => Loss: 6.69917911504336416328\n",
      "Iteration 39558 => Loss: 6.69917898012688173282\n",
      "Iteration 39559 => Loss: 6.69917884522858120278\n",
      "Iteration 39560 => Loss: 6.69917871034845280320\n",
      "Iteration 39561 => Loss: 6.69917857548650186317\n",
      "Iteration 39562 => Loss: 6.69917844064272305360\n",
      "Iteration 39563 => Loss: 6.69917830581711992721\n",
      "Iteration 39564 => Loss: 6.69917817100967472044\n",
      "Iteration 39565 => Loss: 6.69917803622040253231\n",
      "Iteration 39566 => Loss: 6.69917790144929181650\n",
      "Iteration 39567 => Loss: 6.69917776669634079667\n",
      "Iteration 39568 => Loss: 6.69917763196155036098\n",
      "Iteration 39569 => Loss: 6.69917749724491606855\n",
      "Iteration 39570 => Loss: 6.69917736254643170213\n",
      "Iteration 39571 => Loss: 6.69917722786609370900\n",
      "Iteration 39572 => Loss: 6.69917709320390653005\n",
      "Iteration 39573 => Loss: 6.69917695855986927711\n",
      "Iteration 39574 => Loss: 6.69917682393396862750\n",
      "Iteration 39575 => Loss: 6.69917668932621968025\n",
      "Iteration 39576 => Loss: 6.69917655473660023091\n",
      "Iteration 39577 => Loss: 6.69917642016511827308\n",
      "Iteration 39578 => Loss: 6.69917628561177380675\n",
      "Iteration 39579 => Loss: 6.69917615107655528561\n",
      "Iteration 39580 => Loss: 6.69917601655946537420\n",
      "Iteration 39581 => Loss: 6.69917588206050496069\n",
      "Iteration 39582 => Loss: 6.69917574757966427512\n",
      "Iteration 39583 => Loss: 6.69917561311694509385\n",
      "Iteration 39584 => Loss: 6.69917547867235096959\n",
      "Iteration 39585 => Loss: 6.69917534424587302055\n",
      "Iteration 39586 => Loss: 6.69917520983750769403\n",
      "Iteration 39587 => Loss: 6.69917507544724699642\n",
      "Iteration 39588 => Loss: 6.69917494107510069767\n",
      "Iteration 39589 => Loss: 6.69917480672105902784\n",
      "Iteration 39590 => Loss: 6.69917467238512287508\n",
      "Iteration 39591 => Loss: 6.69917453806729312760\n",
      "Iteration 39592 => Loss: 6.69917440376755468634\n",
      "Iteration 39593 => Loss: 6.69917426948591909763\n",
      "Iteration 39594 => Loss: 6.69917413522237215062\n",
      "Iteration 39595 => Loss: 6.69917400097692272709\n",
      "Iteration 39596 => Loss: 6.69917386674956372161\n",
      "Iteration 39597 => Loss: 6.69917373254028447604\n",
      "Iteration 39598 => Loss: 6.69917359834909209582\n",
      "Iteration 39599 => Loss: 6.69917346417598835728\n",
      "Iteration 39600 => Loss: 6.69917333002095638506\n",
      "Iteration 39601 => Loss: 6.69917319588400950181\n",
      "Iteration 39602 => Loss: 6.69917306176513793758\n",
      "Iteration 39603 => Loss: 6.69917292766433192241\n",
      "Iteration 39604 => Loss: 6.69917279358160122626\n",
      "Iteration 39605 => Loss: 6.69917265951693163828\n",
      "Iteration 39606 => Loss: 6.69917252547033648113\n",
      "Iteration 39607 => Loss: 6.69917239144179088584\n",
      "Iteration 39608 => Loss: 6.69917225743131261595\n",
      "Iteration 39609 => Loss: 6.69917212343890167148\n",
      "Iteration 39610 => Loss: 6.69917198946453407160\n",
      "Iteration 39611 => Loss: 6.69917185550822491535\n",
      "Iteration 39612 => Loss: 6.69917172156996354460\n",
      "Iteration 39613 => Loss: 6.69917158764975262386\n",
      "Iteration 39614 => Loss: 6.69917145374758682408\n",
      "Iteration 39615 => Loss: 6.69917131986346436889\n",
      "Iteration 39616 => Loss: 6.69917118599738348195\n",
      "Iteration 39617 => Loss: 6.69917105214933883417\n",
      "Iteration 39618 => Loss: 6.69917091831933486645\n",
      "Iteration 39619 => Loss: 6.69917078450735825612\n",
      "Iteration 39620 => Loss: 6.69917065071341699678\n",
      "Iteration 39621 => Loss: 6.69917051693750842389\n",
      "Iteration 39622 => Loss: 6.69917038317961566207\n",
      "Iteration 39623 => Loss: 6.69917024943975381035\n",
      "Iteration 39624 => Loss: 6.69917011571791576330\n",
      "Iteration 39625 => Loss: 6.69916998201408819824\n",
      "Iteration 39626 => Loss: 6.69916984832827910878\n",
      "Iteration 39627 => Loss: 6.69916971466048849493\n",
      "Iteration 39628 => Loss: 6.69916958101071013942\n",
      "Iteration 39629 => Loss: 6.69916944737893960138\n",
      "Iteration 39630 => Loss: 6.69916931376517243990\n",
      "Iteration 39631 => Loss: 6.69916918016940776681\n",
      "Iteration 39632 => Loss: 6.69916904659165091118\n",
      "Iteration 39633 => Loss: 6.69916891303189832030\n",
      "Iteration 39634 => Loss: 6.69916877949013933602\n",
      "Iteration 39635 => Loss: 6.69916864596636774110\n",
      "Iteration 39636 => Loss: 6.69916851246059774638\n",
      "Iteration 39637 => Loss: 6.69916837897281158831\n",
      "Iteration 39638 => Loss: 6.69916824550301726049\n",
      "Iteration 39639 => Loss: 6.69916811205120232842\n",
      "Iteration 39640 => Loss: 6.69916797861737656206\n",
      "Iteration 39641 => Loss: 6.69916784520153107962\n",
      "Iteration 39642 => Loss: 6.69916771180365966387\n",
      "Iteration 39643 => Loss: 6.69916757842376320298\n",
      "Iteration 39644 => Loss: 6.69916744506183636787\n",
      "Iteration 39645 => Loss: 6.69916731171788804033\n",
      "Iteration 39646 => Loss: 6.69916717839190489769\n",
      "Iteration 39647 => Loss: 6.69916704508388693995\n",
      "Iteration 39648 => Loss: 6.69916691179383683163\n",
      "Iteration 39649 => Loss: 6.69916677852174213825\n",
      "Iteration 39650 => Loss: 6.69916664526760463616\n",
      "Iteration 39651 => Loss: 6.69916651203142166082\n",
      "Iteration 39652 => Loss: 6.69916637881320120584\n",
      "Iteration 39653 => Loss: 6.69916624561291751405\n",
      "Iteration 39654 => Loss: 6.69916611243059545444\n",
      "Iteration 39655 => Loss: 6.69916597926621637527\n",
      "Iteration 39656 => Loss: 6.69916584611978205288\n",
      "Iteration 39657 => Loss: 6.69916571299128804640\n",
      "Iteration 39658 => Loss: 6.69916557988073169128\n",
      "Iteration 39659 => Loss: 6.69916544678811387570\n",
      "Iteration 39660 => Loss: 6.69916531371342660606\n",
      "Iteration 39661 => Loss: 6.69916518065667609960\n",
      "Iteration 39662 => Loss: 6.69916504761785169819\n",
      "Iteration 39663 => Loss: 6.69916491459695162547\n",
      "Iteration 39664 => Loss: 6.69916478159398920411\n",
      "Iteration 39665 => Loss: 6.69916464860893778877\n",
      "Iteration 39666 => Loss: 6.69916451564180803757\n",
      "Iteration 39667 => Loss: 6.69916438269259106875\n",
      "Iteration 39668 => Loss: 6.69916424976129309954\n",
      "Iteration 39669 => Loss: 6.69916411684791412995\n",
      "Iteration 39670 => Loss: 6.69916398395243550823\n",
      "Iteration 39671 => Loss: 6.69916385107486700434\n",
      "Iteration 39672 => Loss: 6.69916371821520861829\n",
      "Iteration 39673 => Loss: 6.69916358537345146829\n",
      "Iteration 39674 => Loss: 6.69916345254958933708\n",
      "Iteration 39675 => Loss: 6.69916331974363021828\n",
      "Iteration 39676 => Loss: 6.69916318695556345375\n",
      "Iteration 39677 => Loss: 6.69916305418539348437\n",
      "Iteration 39678 => Loss: 6.69916292143311231655\n",
      "Iteration 39679 => Loss: 6.69916278869872261481\n",
      "Iteration 39680 => Loss: 6.69916265598221905009\n",
      "Iteration 39681 => Loss: 6.69916252328359629331\n",
      "Iteration 39682 => Loss: 6.69916239060284990359\n",
      "Iteration 39683 => Loss: 6.69916225793998609817\n",
      "Iteration 39684 => Loss: 6.69916212529500043615\n",
      "Iteration 39685 => Loss: 6.69916199266788314759\n",
      "Iteration 39686 => Loss: 6.69916186005865110786\n",
      "Iteration 39687 => Loss: 6.69916172746727323073\n",
      "Iteration 39688 => Loss: 6.69916159489376727976\n",
      "Iteration 39689 => Loss: 6.69916146233812614952\n",
      "Iteration 39690 => Loss: 6.69916132980034628730\n",
      "Iteration 39691 => Loss: 6.69916119728042769310\n",
      "Iteration 39692 => Loss: 6.69916106477836592603\n",
      "Iteration 39693 => Loss: 6.69916093229415299248\n",
      "Iteration 39694 => Loss: 6.69916079982779510971\n",
      "Iteration 39695 => Loss: 6.69916066737929227770\n",
      "Iteration 39696 => Loss: 6.69916053494863383833\n",
      "Iteration 39697 => Loss: 6.69916040253581801522\n",
      "Iteration 39698 => Loss: 6.69916027014085191382\n",
      "Iteration 39699 => Loss: 6.69916013776371688238\n",
      "Iteration 39700 => Loss: 6.69916000540442890809\n",
      "Iteration 39701 => Loss: 6.69915987306297022741\n",
      "Iteration 39702 => Loss: 6.69915974073934883393\n",
      "Iteration 39703 => Loss: 6.69915960843354696408\n",
      "Iteration 39704 => Loss: 6.69915947614558415779\n",
      "Iteration 39705 => Loss: 6.69915934387544531603\n",
      "Iteration 39706 => Loss: 6.69915921162312866244\n",
      "Iteration 39707 => Loss: 6.69915907938863330884\n",
      "Iteration 39708 => Loss: 6.69915894717196103159\n",
      "Iteration 39709 => Loss: 6.69915881497309761983\n",
      "Iteration 39710 => Loss: 6.69915868279205106717\n",
      "Iteration 39711 => Loss: 6.69915855062881515636\n",
      "Iteration 39712 => Loss: 6.69915841848338988740\n",
      "Iteration 39713 => Loss: 6.69915828635576637851\n",
      "Iteration 39714 => Loss: 6.69915815424595351146\n",
      "Iteration 39715 => Loss: 6.69915802215393973995\n",
      "Iteration 39716 => Loss: 6.69915789007972861668\n",
      "Iteration 39717 => Loss: 6.69915775802331037170\n",
      "Iteration 39718 => Loss: 6.69915762598468322864\n",
      "Iteration 39719 => Loss: 6.69915749396385518111\n",
      "Iteration 39720 => Loss: 6.69915736196081912368\n",
      "Iteration 39721 => Loss: 6.69915722997555729279\n",
      "Iteration 39722 => Loss: 6.69915709800809633379\n",
      "Iteration 39723 => Loss: 6.69915696605840960132\n",
      "Iteration 39724 => Loss: 6.69915683412649975992\n",
      "Iteration 39725 => Loss: 6.69915670221237213866\n",
      "Iteration 39726 => Loss: 6.69915657031602140847\n",
      "Iteration 39727 => Loss: 6.69915643843744046393\n",
      "Iteration 39728 => Loss: 6.69915630657663907499\n",
      "Iteration 39729 => Loss: 6.69915617473359947809\n",
      "Iteration 39730 => Loss: 6.69915604290832167322\n",
      "Iteration 39731 => Loss: 6.69915591110080743675\n",
      "Iteration 39732 => Loss: 6.69915577931105943321\n",
      "Iteration 39733 => Loss: 6.69915564753906966899\n",
      "Iteration 39734 => Loss: 6.69915551578483814410\n",
      "Iteration 39735 => Loss: 6.69915538404835597674\n",
      "Iteration 39736 => Loss: 6.69915525232962849600\n",
      "Iteration 39737 => Loss: 6.69915512062864504372\n",
      "Iteration 39738 => Loss: 6.69915498894541361352\n",
      "Iteration 39739 => Loss: 6.69915485727992177090\n",
      "Iteration 39740 => Loss: 6.69915472563218106217\n",
      "Iteration 39741 => Loss: 6.69915459400216928287\n",
      "Iteration 39742 => Loss: 6.69915446238989709116\n",
      "Iteration 39743 => Loss: 6.69915433079536004612\n",
      "Iteration 39744 => Loss: 6.69915419921855992413\n",
      "Iteration 39745 => Loss: 6.69915406765948695522\n",
      "Iteration 39746 => Loss: 6.69915393611813847485\n",
      "Iteration 39747 => Loss: 6.69915380459451981210\n",
      "Iteration 39748 => Loss: 6.69915367308862474971\n",
      "Iteration 39749 => Loss: 6.69915354160044618226\n",
      "Iteration 39750 => Loss: 6.69915341012998499792\n",
      "Iteration 39751 => Loss: 6.69915327867724386124\n",
      "Iteration 39752 => Loss: 6.69915314724220944953\n",
      "Iteration 39753 => Loss: 6.69915301582488975640\n",
      "Iteration 39754 => Loss: 6.69915288442528034096\n",
      "Iteration 39755 => Loss: 6.69915275304336965689\n",
      "Iteration 39756 => Loss: 6.69915262167917635594\n",
      "Iteration 39757 => Loss: 6.69915249033267290457\n",
      "Iteration 39758 => Loss: 6.69915235900387262546\n",
      "Iteration 39759 => Loss: 6.69915222769276574866\n",
      "Iteration 39760 => Loss: 6.69915209639935582686\n",
      "Iteration 39761 => Loss: 6.69915196512364108372\n",
      "Iteration 39762 => Loss: 6.69915183386560908474\n",
      "Iteration 39763 => Loss: 6.69915170262527048806\n",
      "Iteration 39764 => Loss: 6.69915157140261285917\n",
      "Iteration 39765 => Loss: 6.69915144019763442174\n",
      "Iteration 39766 => Loss: 6.69915130901033695210\n",
      "Iteration 39767 => Loss: 6.69915117784071956208\n",
      "Iteration 39768 => Loss: 6.69915104668877869898\n",
      "Iteration 39769 => Loss: 6.69915091555451169825\n",
      "Iteration 39770 => Loss: 6.69915078443791500717\n",
      "Iteration 39771 => Loss: 6.69915065333898418487\n",
      "Iteration 39772 => Loss: 6.69915052225771034955\n",
      "Iteration 39773 => Loss: 6.69915039119411037660\n",
      "Iteration 39774 => Loss: 6.69915026014817449607\n",
      "Iteration 39775 => Loss: 6.69915012911988405619\n",
      "Iteration 39776 => Loss: 6.69914999810926037327\n",
      "Iteration 39777 => Loss: 6.69914986711628390736\n",
      "Iteration 39778 => Loss: 6.69914973614096531662\n",
      "Iteration 39779 => Loss: 6.69914960518329216654\n",
      "Iteration 39780 => Loss: 6.69914947424325823988\n",
      "Iteration 39781 => Loss: 6.69914934332088574109\n",
      "Iteration 39782 => Loss: 6.69914921241613914304\n",
      "Iteration 39783 => Loss: 6.69914908152903887384\n",
      "Iteration 39784 => Loss: 6.69914895065957072262\n",
      "Iteration 39785 => Loss: 6.69914881980774712389\n",
      "Iteration 39786 => Loss: 6.69914868897354587318\n",
      "Iteration 39787 => Loss: 6.69914855815698118136\n",
      "Iteration 39788 => Loss: 6.69914842735803439666\n",
      "Iteration 39789 => Loss: 6.69914829657672594720\n",
      "Iteration 39790 => Loss: 6.69914816581303274035\n",
      "Iteration 39791 => Loss: 6.69914803506696099333\n",
      "Iteration 39792 => Loss: 6.69914790433850448892\n",
      "Iteration 39793 => Loss: 6.69914777362766589164\n",
      "Iteration 39794 => Loss: 6.69914764293444431331\n",
      "Iteration 39795 => Loss: 6.69914751225883442487\n",
      "Iteration 39796 => Loss: 6.69914738160083000906\n",
      "Iteration 39797 => Loss: 6.69914725096042840136\n",
      "Iteration 39798 => Loss: 6.69914712033763404264\n",
      "Iteration 39799 => Loss: 6.69914698973243716296\n",
      "Iteration 39800 => Loss: 6.69914685914484220319\n",
      "Iteration 39801 => Loss: 6.69914672857484738699\n",
      "Iteration 39802 => Loss: 6.69914659802244116804\n",
      "Iteration 39803 => Loss: 6.69914646748763331630\n",
      "Iteration 39804 => Loss: 6.69914633697040873272\n",
      "Iteration 39805 => Loss: 6.69914620647077452276\n",
      "Iteration 39806 => Loss: 6.69914607598872358096\n",
      "Iteration 39807 => Loss: 6.69914594552425590734\n",
      "Iteration 39808 => Loss: 6.69914581507737061372\n",
      "Iteration 39809 => Loss: 6.69914568464805615378\n",
      "Iteration 39810 => Loss: 6.69914555423633029108\n",
      "Iteration 39811 => Loss: 6.69914542384216460391\n",
      "Iteration 39812 => Loss: 6.69914529346557507949\n",
      "Iteration 39813 => Loss: 6.69914516310655194786\n",
      "Iteration 39814 => Loss: 6.69914503276509343266\n",
      "Iteration 39815 => Loss: 6.69914490244120219842\n",
      "Iteration 39816 => Loss: 6.69914477213486758700\n",
      "Iteration 39817 => Loss: 6.69914464184609581565\n",
      "Iteration 39818 => Loss: 6.69914451157488421984\n",
      "Iteration 39819 => Loss: 6.69914438132121592417\n",
      "Iteration 39820 => Loss: 6.69914425108510602769\n",
      "Iteration 39821 => Loss: 6.69914412086654120770\n",
      "Iteration 39822 => Loss: 6.69914399066553034601\n",
      "Iteration 39823 => Loss: 6.69914386048205923174\n",
      "Iteration 39824 => Loss: 6.69914373031612697673\n",
      "Iteration 39825 => Loss: 6.69914360016774423912\n",
      "Iteration 39826 => Loss: 6.69914347003689591986\n",
      "Iteration 39827 => Loss: 6.69914333992357846626\n",
      "Iteration 39828 => Loss: 6.69914320982779010194\n",
      "Iteration 39829 => Loss: 6.69914307974954503777\n",
      "Iteration 39830 => Loss: 6.69914294968882018111\n",
      "Iteration 39831 => Loss: 6.69914281964562086102\n",
      "Iteration 39832 => Loss: 6.69914268961994530116\n",
      "Iteration 39833 => Loss: 6.69914255961178639609\n",
      "Iteration 39834 => Loss: 6.69914242962115658031\n",
      "Iteration 39835 => Loss: 6.69914229964803542572\n",
      "Iteration 39836 => Loss: 6.69914216969243003774\n",
      "Iteration 39837 => Loss: 6.69914203975433686367\n",
      "Iteration 39838 => Loss: 6.69914190983375323896\n",
      "Iteration 39839 => Loss: 6.69914177993067205819\n",
      "Iteration 39840 => Loss: 6.69914165004510042678\n",
      "Iteration 39841 => Loss: 6.69914152017702679842\n",
      "Iteration 39842 => Loss: 6.69914139032645739036\n",
      "Iteration 39843 => Loss: 6.69914126049338509716\n",
      "Iteration 39844 => Loss: 6.69914113067780370159\n",
      "Iteration 39845 => Loss: 6.69914100087971764452\n",
      "Iteration 39846 => Loss: 6.69914087109912248508\n",
      "Iteration 39847 => Loss: 6.69914074133601200600\n",
      "Iteration 39848 => Loss: 6.69914061159038976001\n",
      "Iteration 39849 => Loss: 6.69914048186225041803\n",
      "Iteration 39850 => Loss: 6.69914035215158865100\n",
      "Iteration 39851 => Loss: 6.69914022245841955794\n",
      "Iteration 39852 => Loss: 6.69914009278270938808\n",
      "Iteration 39853 => Loss: 6.69913996312448212223\n",
      "Iteration 39854 => Loss: 6.69913983348371733229\n",
      "Iteration 39855 => Loss: 6.69913970386043367000\n",
      "Iteration 39856 => Loss: 6.69913957425460626638\n",
      "Iteration 39857 => Loss: 6.69913944466625288499\n",
      "Iteration 39858 => Loss: 6.69913931509535043318\n",
      "Iteration 39859 => Loss: 6.69913918554191933907\n",
      "Iteration 39860 => Loss: 6.69913905600593739820\n",
      "Iteration 39861 => Loss: 6.69913892648741260416\n",
      "Iteration 39862 => Loss: 6.69913879698634318061\n",
      "Iteration 39863 => Loss: 6.69913866750272113393\n",
      "Iteration 39864 => Loss: 6.69913853803654468777\n",
      "Iteration 39865 => Loss: 6.69913840858782005938\n",
      "Iteration 39866 => Loss: 6.69913827915653303791\n",
      "Iteration 39867 => Loss: 6.69913814974268628788\n",
      "Iteration 39868 => Loss: 6.69913802034628602655\n",
      "Iteration 39869 => Loss: 6.69913789096731626671\n",
      "Iteration 39870 => Loss: 6.69913776160578322560\n",
      "Iteration 39871 => Loss: 6.69913763226167890963\n",
      "Iteration 39872 => Loss: 6.69913750293499887789\n",
      "Iteration 39873 => Loss: 6.69913737362575378853\n",
      "Iteration 39874 => Loss: 6.69913724433392854252\n",
      "Iteration 39875 => Loss: 6.69913711505952935710\n",
      "Iteration 39876 => Loss: 6.69913698580254823867\n",
      "Iteration 39877 => Loss: 6.69913685656298074633\n",
      "Iteration 39878 => Loss: 6.69913672734083398552\n",
      "Iteration 39879 => Loss: 6.69913659813609374538\n",
      "Iteration 39880 => Loss: 6.69913646894877157223\n",
      "Iteration 39881 => Loss: 6.69913633977885503157\n",
      "Iteration 39882 => Loss: 6.69913621062634057068\n",
      "Iteration 39883 => Loss: 6.69913608149123529500\n",
      "Iteration 39884 => Loss: 6.69913595237352677003\n",
      "Iteration 39885 => Loss: 6.69913582327321410759\n",
      "Iteration 39886 => Loss: 6.69913569419030352492\n",
      "Iteration 39887 => Loss: 6.69913556512477903482\n",
      "Iteration 39888 => Loss: 6.69913543607665395996\n",
      "Iteration 39889 => Loss: 6.69913530704591586584\n",
      "Iteration 39890 => Loss: 6.69913517803256208794\n",
      "Iteration 39891 => Loss: 6.69913504903659440259\n",
      "Iteration 39892 => Loss: 6.69913492005800659257\n",
      "Iteration 39893 => Loss: 6.69913479109680576329\n",
      "Iteration 39894 => Loss: 6.69913466215297415118\n",
      "Iteration 39895 => Loss: 6.69913453322652507893\n",
      "Iteration 39896 => Loss: 6.69913440431744611203\n",
      "Iteration 39897 => Loss: 6.69913427542573636231\n",
      "Iteration 39898 => Loss: 6.69913414655139316523\n",
      "Iteration 39899 => Loss: 6.69913401769442007350\n",
      "Iteration 39900 => Loss: 6.69913388885480909352\n",
      "Iteration 39901 => Loss: 6.69913376003256200164\n",
      "Iteration 39902 => Loss: 6.69913363122767080426\n",
      "Iteration 39903 => Loss: 6.69913350244013816592\n",
      "Iteration 39904 => Loss: 6.69913337366994809940\n",
      "Iteration 39905 => Loss: 6.69913324491712369735\n",
      "Iteration 39906 => Loss: 6.69913311618164719619\n",
      "Iteration 39907 => Loss: 6.69913298746350971413\n",
      "Iteration 39908 => Loss: 6.69913285876272102115\n",
      "Iteration 39909 => Loss: 6.69913273007928022906\n",
      "Iteration 39910 => Loss: 6.69913260141317490337\n",
      "Iteration 39911 => Loss: 6.69913247276440237954\n",
      "Iteration 39912 => Loss: 6.69913234413296709846\n",
      "Iteration 39913 => Loss: 6.69913221551887527738\n",
      "Iteration 39914 => Loss: 6.69913208692210560002\n",
      "Iteration 39915 => Loss: 6.69913195834267050088\n",
      "Iteration 39916 => Loss: 6.69913182978055399275\n",
      "Iteration 39917 => Loss: 6.69913170123576229287\n",
      "Iteration 39918 => Loss: 6.69913157270829717760\n",
      "Iteration 39919 => Loss: 6.69913144419814265973\n",
      "Iteration 39920 => Loss: 6.69913131570531206194\n",
      "Iteration 39921 => Loss: 6.69913118722979383790\n",
      "Iteration 39922 => Loss: 6.69913105877158887580\n",
      "Iteration 39923 => Loss: 6.69913093033068474114\n",
      "Iteration 39924 => Loss: 6.69913080190710630291\n",
      "Iteration 39925 => Loss: 6.69913067350082425122\n",
      "Iteration 39926 => Loss: 6.69913054511184125062\n",
      "Iteration 39927 => Loss: 6.69913041674015463656\n",
      "Iteration 39928 => Loss: 6.69913028838577329083\n",
      "Iteration 39929 => Loss: 6.69913016004868477893\n",
      "Iteration 39930 => Loss: 6.69913003172889265358\n",
      "Iteration 39931 => Loss: 6.69912990342639158570\n",
      "Iteration 39932 => Loss: 6.69912977514117891076\n",
      "Iteration 39933 => Loss: 6.69912964687324929969\n",
      "Iteration 39934 => Loss: 6.69912951862260808156\n",
      "Iteration 39935 => Loss: 6.69912939038924637458\n",
      "Iteration 39936 => Loss: 6.69912926217316861965\n",
      "Iteration 39937 => Loss: 6.69912913397436060592\n",
      "Iteration 39938 => Loss: 6.69912900579283476787\n",
      "Iteration 39939 => Loss: 6.69912887762857511831\n",
      "Iteration 39940 => Loss: 6.69912874948158520994\n",
      "Iteration 39941 => Loss: 6.69912862135186681911\n",
      "Iteration 39942 => Loss: 6.69912849323941639312\n",
      "Iteration 39943 => Loss: 6.69912836514422327383\n",
      "Iteration 39944 => Loss: 6.69912823706628834941\n",
      "Iteration 39945 => Loss: 6.69912810900561872529\n",
      "Iteration 39946 => Loss: 6.69912798096220640787\n",
      "Iteration 39947 => Loss: 6.69912785293604517989\n",
      "Iteration 39948 => Loss: 6.69912772492713504136\n",
      "Iteration 39949 => Loss: 6.69912759693547510409\n",
      "Iteration 39950 => Loss: 6.69912746896105737449\n",
      "Iteration 39951 => Loss: 6.69912734100389162251\n",
      "Iteration 39952 => Loss: 6.69912721306396274912\n",
      "Iteration 39953 => Loss: 6.69912708514127874793\n",
      "Iteration 39954 => Loss: 6.69912695723582807261\n",
      "Iteration 39955 => Loss: 6.69912682934761516407\n",
      "Iteration 39956 => Loss: 6.69912670147662936415\n",
      "Iteration 39957 => Loss: 6.69912657362288133100\n",
      "Iteration 39958 => Loss: 6.69912644578635685377\n",
      "Iteration 39959 => Loss: 6.69912631796706481424\n",
      "Iteration 39960 => Loss: 6.69912619016499277791\n",
      "Iteration 39961 => Loss: 6.69912606238013985660\n",
      "Iteration 39962 => Loss: 6.69912593461251049121\n",
      "Iteration 39963 => Loss: 6.69912580686209135905\n",
      "Iteration 39964 => Loss: 6.69912567912889400645\n",
      "Iteration 39965 => Loss: 6.69912555141290244620\n",
      "Iteration 39966 => Loss: 6.69912542371412111919\n",
      "Iteration 39967 => Loss: 6.69912529603255002542\n",
      "Iteration 39968 => Loss: 6.69912516836818294763\n",
      "Iteration 39969 => Loss: 6.69912504072101899766\n",
      "Iteration 39970 => Loss: 6.69912491309105728732\n",
      "Iteration 39971 => Loss: 6.69912478547829159936\n",
      "Iteration 39972 => Loss: 6.69912465788271660472\n",
      "Iteration 39973 => Loss: 6.69912453030434296153\n",
      "Iteration 39974 => Loss: 6.69912440274315557076\n",
      "Iteration 39975 => Loss: 6.69912427519916597873\n",
      "Iteration 39976 => Loss: 6.69912414767235286917\n",
      "Iteration 39977 => Loss: 6.69912402016272601202\n",
      "Iteration 39978 => Loss: 6.69912389267028540729\n",
      "Iteration 39979 => Loss: 6.69912376519502217320\n",
      "Iteration 39980 => Loss: 6.69912363773693986246\n",
      "Iteration 39981 => Loss: 6.69912351029602959329\n",
      "Iteration 39982 => Loss: 6.69912338287228426026\n",
      "Iteration 39983 => Loss: 6.69912325546571985058\n",
      "Iteration 39984 => Loss: 6.69912312807631682432\n",
      "Iteration 39985 => Loss: 6.69912300070408495145\n",
      "Iteration 39986 => Loss: 6.69912287334901179747\n",
      "Iteration 39987 => Loss: 6.69912274601110446781\n",
      "Iteration 39988 => Loss: 6.69912261869035141615\n",
      "Iteration 39989 => Loss: 6.69912249138675619520\n",
      "Iteration 39990 => Loss: 6.69912236410031969314\n",
      "Iteration 39991 => Loss: 6.69912223683102592275\n",
      "Iteration 39992 => Loss: 6.69912210957888909491\n",
      "Iteration 39993 => Loss: 6.69912198234389766327\n",
      "Iteration 39994 => Loss: 6.69912185512604363424\n",
      "Iteration 39995 => Loss: 6.69912172792534121868\n",
      "Iteration 39996 => Loss: 6.69912160074177975844\n",
      "Iteration 39997 => Loss: 6.69912147357534859538\n",
      "Iteration 39998 => Loss: 6.69912134642605838764\n",
      "Iteration 39999 => Loss: 6.69912121929390824704\n",
      "Iteration 40000 => Loss: 6.69912109217888307455\n",
      "Iteration 40001 => Loss: 6.69912096508098375836\n",
      "Iteration 40002 => Loss: 6.69912083800021562752\n",
      "Iteration 40003 => Loss: 6.69912071093656624754\n",
      "Iteration 40004 => Loss: 6.69912058389004361203\n",
      "Iteration 40005 => Loss: 6.69912045686063084560\n",
      "Iteration 40006 => Loss: 6.69912032984834393545\n",
      "Iteration 40007 => Loss: 6.69912020285317133528\n",
      "Iteration 40008 => Loss: 6.69912007587511215689\n",
      "Iteration 40009 => Loss: 6.69911994891416195941\n",
      "Iteration 40010 => Loss: 6.69911982197032163100\n",
      "Iteration 40011 => Loss: 6.69911969504358406624\n",
      "Iteration 40012 => Loss: 6.69911956813395015331\n",
      "Iteration 40013 => Loss: 6.69911944124142078039\n",
      "Iteration 40014 => Loss: 6.69911931436598351297\n",
      "Iteration 40015 => Loss: 6.69911918750764723285\n",
      "Iteration 40016 => Loss: 6.69911906066640661095\n",
      "Iteration 40017 => Loss: 6.69911893384225898274\n",
      "Iteration 40018 => Loss: 6.69911880703519369007\n",
      "Iteration 40019 => Loss: 6.69911868024522139109\n",
      "Iteration 40020 => Loss: 6.69911855347233320401\n",
      "Iteration 40021 => Loss: 6.69911842671652557613\n",
      "Iteration 40022 => Loss: 6.69911829997780028378\n",
      "Iteration 40023 => Loss: 6.69911817325615288610\n",
      "Iteration 40024 => Loss: 6.69911804655158338306\n",
      "Iteration 40025 => Loss: 6.69911791986407934019\n",
      "Iteration 40026 => Loss: 6.69911779319365319196\n",
      "Iteration 40027 => Loss: 6.69911766654029161572\n",
      "Iteration 40028 => Loss: 6.69911753990399994052\n",
      "Iteration 40029 => Loss: 6.69911741328476928459\n",
      "Iteration 40030 => Loss: 6.69911728668260852970\n",
      "Iteration 40031 => Loss: 6.69911716009750257683\n",
      "Iteration 40032 => Loss: 6.69911703352945853140\n",
      "Iteration 40033 => Loss: 6.69911690697845862985\n",
      "Iteration 40034 => Loss: 6.69911678044452063574\n",
      "Iteration 40035 => Loss: 6.69911665392763389093\n",
      "Iteration 40036 => Loss: 6.69911652742777974368\n",
      "Iteration 40037 => Loss: 6.69911640094498750386\n",
      "Iteration 40038 => Loss: 6.69911627447923674339\n",
      "Iteration 40039 => Loss: 6.69911614803052568590\n",
      "Iteration 40040 => Loss: 6.69911602159884900232\n",
      "Iteration 40041 => Loss: 6.69911589518421113354\n",
      "Iteration 40042 => Loss: 6.69911576878661474410\n",
      "Iteration 40043 => Loss: 6.69911564240604473497\n",
      "Iteration 40044 => Loss: 6.69911551604250288250\n",
      "Iteration 40045 => Loss: 6.69911538969598829851\n",
      "Iteration 40046 => Loss: 6.69911526336650187119\n",
      "Iteration 40047 => Loss: 6.69911513705403915964\n",
      "Iteration 40048 => Loss: 6.69911501075859483478\n",
      "Iteration 40049 => Loss: 6.69911488448017511388\n",
      "Iteration 40050 => Loss: 6.69911475821876045700\n",
      "Iteration 40051 => Loss: 6.69911463197436418682\n",
      "Iteration 40052 => Loss: 6.69911450574698186244\n",
      "Iteration 40053 => Loss: 6.69911437953660549027\n",
      "Iteration 40054 => Loss: 6.69911425334323595848\n",
      "Iteration 40055 => Loss: 6.69911412716687326707\n",
      "Iteration 40056 => Loss: 6.69911400100751475151\n",
      "Iteration 40057 => Loss: 6.69911387486514886547\n",
      "Iteration 40058 => Loss: 6.69911374873978893163\n",
      "Iteration 40059 => Loss: 6.69911362263141985096\n",
      "Iteration 40060 => Loss: 6.69911349654004428800\n",
      "Iteration 40061 => Loss: 6.69911337046565691367\n",
      "Iteration 40062 => Loss: 6.69911324440826216886\n",
      "Iteration 40063 => Loss: 6.69911311836785916540\n",
      "Iteration 40064 => Loss: 6.69911299234442658701\n",
      "Iteration 40065 => Loss: 6.69911286633798752632\n",
      "Iteration 40066 => Loss: 6.69911274034852333159\n",
      "Iteration 40067 => Loss: 6.69911261437603666735\n",
      "Iteration 40068 => Loss: 6.69911248842051954000\n",
      "Iteration 40069 => Loss: 6.69911236248197727861\n",
      "Iteration 40070 => Loss: 6.69911223656041165953\n",
      "Iteration 40071 => Loss: 6.69911211065580669555\n",
      "Iteration 40072 => Loss: 6.69911198476817215663\n",
      "Iteration 40073 => Loss: 6.69911185889749916100\n",
      "Iteration 40074 => Loss: 6.69911173304378237958\n",
      "Iteration 40075 => Loss: 6.69911160720703247051\n",
      "Iteration 40076 => Loss: 6.69911148138723611112\n",
      "Iteration 40077 => Loss: 6.69911135558439418958\n",
      "Iteration 40078 => Loss: 6.69911122979850937043\n",
      "Iteration 40079 => Loss: 6.69911110402956122556\n",
      "Iteration 40080 => Loss: 6.69911097827757107126\n",
      "Iteration 40081 => Loss: 6.69911085254252380849\n",
      "Iteration 40082 => Loss: 6.69911072682441588455\n",
      "Iteration 40083 => Loss: 6.69911060112325174032\n",
      "Iteration 40084 => Loss: 6.69911047543902160584\n",
      "Iteration 40085 => Loss: 6.69911034977173436289\n",
      "Iteration 40086 => Loss: 6.69911022412137846516\n",
      "Iteration 40087 => Loss: 6.69911009848795035992\n",
      "Iteration 40088 => Loss: 6.69910997287145448809\n",
      "Iteration 40089 => Loss: 6.69910984727188463239\n",
      "Iteration 40090 => Loss: 6.69910972168924079284\n",
      "Iteration 40091 => Loss: 6.69910959612351764036\n",
      "Iteration 40092 => Loss: 6.69910947057471606314\n",
      "Iteration 40093 => Loss: 6.69910934504282806756\n",
      "Iteration 40094 => Loss: 6.69910921952785898270\n",
      "Iteration 40095 => Loss: 6.69910909402979992677\n",
      "Iteration 40096 => Loss: 6.69910896854865978156\n",
      "Iteration 40097 => Loss: 6.69910884308441900714\n",
      "Iteration 40098 => Loss: 6.69910871763708914983\n",
      "Iteration 40099 => Loss: 6.69910859220666221603\n",
      "Iteration 40100 => Loss: 6.69910846679313731755\n",
      "Iteration 40101 => Loss: 6.69910834139651178987\n",
      "Iteration 40102 => Loss: 6.69910821601678385662\n",
      "Iteration 40103 => Loss: 6.69910809065394818873\n",
      "Iteration 40104 => Loss: 6.69910796530801100346\n",
      "Iteration 40105 => Loss: 6.69910783997895897812\n",
      "Iteration 40106 => Loss: 6.69910771466679744179\n",
      "Iteration 40107 => Loss: 6.69910758937151928905\n",
      "Iteration 40108 => Loss: 6.69910746409312984895\n",
      "Iteration 40109 => Loss: 6.69910733883161224611\n",
      "Iteration 40110 => Loss: 6.69910721358698957317\n",
      "Iteration 40111 => Loss: 6.69910708835922719118\n",
      "Iteration 40112 => Loss: 6.69910696314834286369\n",
      "Iteration 40113 => Loss: 6.69910683795433836707\n",
      "Iteration 40114 => Loss: 6.69910671277719682593\n",
      "Iteration 40115 => Loss: 6.69910658761692623386\n",
      "Iteration 40116 => Loss: 6.69910646247351770910\n",
      "Iteration 40117 => Loss: 6.69910633734697658070\n",
      "Iteration 40118 => Loss: 6.69910621223729130236\n",
      "Iteration 40119 => Loss: 6.69910608714447342038\n",
      "Iteration 40120 => Loss: 6.69910596206849806578\n",
      "Iteration 40121 => Loss: 6.69910583700938921936\n",
      "Iteration 40122 => Loss: 6.69910571196712734121\n",
      "Iteration 40123 => Loss: 6.69910558694171509586\n",
      "Iteration 40124 => Loss: 6.69910546193314981878\n",
      "Iteration 40125 => Loss: 6.69910533694142618089\n",
      "Iteration 40126 => Loss: 6.69910521196655306397\n",
      "Iteration 40127 => Loss: 6.69910508700851270447\n",
      "Iteration 40128 => Loss: 6.69910496206731220781\n",
      "Iteration 40129 => Loss: 6.69910483714295246216\n",
      "Iteration 40130 => Loss: 6.69910471223542014485\n",
      "Iteration 40131 => Loss: 6.69910458734471969677\n",
      "Iteration 40132 => Loss: 6.69910446247085378246\n",
      "Iteration 40133 => Loss: 6.69910433761380730289\n",
      "Iteration 40134 => Loss: 6.69910421277359269254\n",
      "Iteration 40135 => Loss: 6.69910408795019485240\n",
      "Iteration 40136 => Loss: 6.69910396314361822334\n",
      "Iteration 40137 => Loss: 6.69910383835386546991\n",
      "Iteration 40138 => Loss: 6.69910371358091527583\n",
      "Iteration 40139 => Loss: 6.69910358882478451648\n",
      "Iteration 40140 => Loss: 6.69910346408546963914\n",
      "Iteration 40141 => Loss: 6.69910333936296087387\n",
      "Iteration 40142 => Loss: 6.69910321465725644430\n",
      "Iteration 40143 => Loss: 6.69910308996835190953\n",
      "Iteration 40144 => Loss: 6.69910296529626236861\n",
      "Iteration 40145 => Loss: 6.69910284064095495893\n",
      "Iteration 40146 => Loss: 6.69910271600245632584\n",
      "Iteration 40147 => Loss: 6.69910259138075492302\n",
      "Iteration 40148 => Loss: 6.69910246677584453323\n",
      "Iteration 40149 => Loss: 6.69910234218771716286\n",
      "Iteration 40150 => Loss: 6.69910221761638258187\n",
      "Iteration 40151 => Loss: 6.69910209306183102029\n",
      "Iteration 40152 => Loss: 6.69910196852406958357\n",
      "Iteration 40153 => Loss: 6.69910184400308494901\n",
      "Iteration 40154 => Loss: 6.69910171949887622844\n",
      "Iteration 40155 => Loss: 6.69910159501145141547\n",
      "Iteration 40156 => Loss: 6.69910147054080784557\n",
      "Iteration 40157 => Loss: 6.69910134608691976155\n",
      "Iteration 40158 => Loss: 6.69910122164980936788\n",
      "Iteration 40159 => Loss: 6.69910109722947044730\n",
      "Iteration 40160 => Loss: 6.69910097282589322987\n",
      "Iteration 40161 => Loss: 6.69910084843908304464\n",
      "Iteration 40162 => Loss: 6.69910072406903278619\n",
      "Iteration 40163 => Loss: 6.69910059971573712545\n",
      "Iteration 40164 => Loss: 6.69910047537919783878\n",
      "Iteration 40165 => Loss: 6.69910035105942203160\n",
      "Iteration 40166 => Loss: 6.69910022675638749945\n",
      "Iteration 40167 => Loss: 6.69910010247010401230\n",
      "Iteration 40168 => Loss: 6.69909997820057689921\n",
      "Iteration 40169 => Loss: 6.69909985394778839662\n",
      "Iteration 40170 => Loss: 6.69909972971174205725\n",
      "Iteration 40171 => Loss: 6.69909960549243965744\n",
      "Iteration 40172 => Loss: 6.69909948128988119720\n",
      "Iteration 40173 => Loss: 6.69909935710405246567\n",
      "Iteration 40174 => Loss: 6.69909923293495701557\n",
      "Iteration 40175 => Loss: 6.69909910878259040601\n",
      "Iteration 40176 => Loss: 6.69909898464695974241\n",
      "Iteration 40177 => Loss: 6.69909886052805525480\n",
      "Iteration 40178 => Loss: 6.69909873642587339049\n",
      "Iteration 40179 => Loss: 6.69909861234041503764\n",
      "Iteration 40180 => Loss: 6.69909848827167575536\n",
      "Iteration 40181 => Loss: 6.69909836421965554365\n",
      "Iteration 40182 => Loss: 6.69909824018435884341\n",
      "Iteration 40183 => Loss: 6.69909811616576700288\n",
      "Iteration 40184 => Loss: 6.69909799216388979204\n",
      "Iteration 40185 => Loss: 6.69909786817872188180\n",
      "Iteration 40186 => Loss: 6.69909774421025883129\n",
      "Iteration 40187 => Loss: 6.69909762025850508138\n",
      "Iteration 40188 => Loss: 6.69909749632344908576\n",
      "Iteration 40189 => Loss: 6.69909737240509972622\n",
      "Iteration 40190 => Loss: 6.69909724850343657465\n",
      "Iteration 40191 => Loss: 6.69909712461847828280\n",
      "Iteration 40192 => Loss: 6.69909700075021241616\n",
      "Iteration 40193 => Loss: 6.69909687689863542204\n",
      "Iteration 40194 => Loss: 6.69909675306375440584\n",
      "Iteration 40195 => Loss: 6.69909662924555071584\n",
      "Iteration 40196 => Loss: 6.69909650544404033923\n",
      "Iteration 40197 => Loss: 6.69909638165920462427\n",
      "Iteration 40198 => Loss: 6.69909625789104534732\n",
      "Iteration 40199 => Loss: 6.69909613413957849559\n",
      "Iteration 40200 => Loss: 6.69909601040477564737\n",
      "Iteration 40201 => Loss: 6.69909588668664657263\n",
      "Iteration 40202 => Loss: 6.69909576298519393589\n",
      "Iteration 40203 => Loss: 6.69909563930040530266\n",
      "Iteration 40204 => Loss: 6.69909551563228955473\n",
      "Iteration 40205 => Loss: 6.69909539198083514577\n",
      "Iteration 40206 => Loss: 6.69909526834604029943\n",
      "Iteration 40207 => Loss: 6.69909514472790501571\n",
      "Iteration 40208 => Loss: 6.69909502112643107097\n",
      "Iteration 40209 => Loss: 6.69909489754161047159\n",
      "Iteration 40210 => Loss: 6.69909477397344144123\n",
      "Iteration 40211 => Loss: 6.69909465042192309170\n",
      "Iteration 40212 => Loss: 6.69909452688705542300\n",
      "Iteration 40213 => Loss: 6.69909440336883310607\n",
      "Iteration 40214 => Loss: 6.69909427986725525273\n",
      "Iteration 40215 => Loss: 6.69909415638232008661\n",
      "Iteration 40216 => Loss: 6.69909403291402405500\n",
      "Iteration 40217 => Loss: 6.69909390946236360520\n",
      "Iteration 40218 => Loss: 6.69909378602733873720\n",
      "Iteration 40219 => Loss: 6.69909366260894945100\n",
      "Iteration 40220 => Loss: 6.69909353920718508846\n",
      "Iteration 40221 => Loss: 6.69909341582205808407\n",
      "Iteration 40222 => Loss: 6.69909329245354800975\n",
      "Iteration 40223 => Loss: 6.69909316910166907633\n",
      "Iteration 40224 => Loss: 6.69909304576640529660\n",
      "Iteration 40225 => Loss: 6.69909292244776466418\n",
      "Iteration 40226 => Loss: 6.69909279914574007364\n",
      "Iteration 40227 => Loss: 6.69909267586033152497\n",
      "Iteration 40228 => Loss: 6.69909255259153635365\n",
      "Iteration 40229 => Loss: 6.69909242933935367148\n",
      "Iteration 40230 => Loss: 6.69909230610377104398\n",
      "Iteration 40231 => Loss: 6.69909218288480090564\n",
      "Iteration 40232 => Loss: 6.69909205968243615104\n",
      "Iteration 40233 => Loss: 6.69909193649666523385\n",
      "Iteration 40234 => Loss: 6.69909181332750147675\n",
      "Iteration 40235 => Loss: 6.69909169017493422160\n",
      "Iteration 40236 => Loss: 6.69909156703895458662\n",
      "Iteration 40237 => Loss: 6.69909144391957322995\n",
      "Iteration 40238 => Loss: 6.69909132081678482251\n",
      "Iteration 40239 => Loss: 6.69909119773057870617\n",
      "Iteration 40240 => Loss: 6.69909107466096109817\n",
      "Iteration 40241 => Loss: 6.69909095160792578127\n",
      "Iteration 40242 => Loss: 6.69909082857147275547\n",
      "Iteration 40243 => Loss: 6.69909070555160646165\n",
      "Iteration 40244 => Loss: 6.69909058254830025447\n",
      "Iteration 40245 => Loss: 6.69909045956158255564\n",
      "Iteration 40246 => Loss: 6.69909033659143471340\n",
      "Iteration 40247 => Loss: 6.69909021363785495140\n",
      "Iteration 40248 => Loss: 6.69909009070084415782\n",
      "Iteration 40249 => Loss: 6.69908996778039256270\n",
      "Iteration 40250 => Loss: 6.69908984487651526507\n",
      "Iteration 40251 => Loss: 6.69908972198919361318\n",
      "Iteration 40252 => Loss: 6.69908959911843560064\n",
      "Iteration 40253 => Loss: 6.69908947626422524024\n",
      "Iteration 40254 => Loss: 6.69908935342657496648\n",
      "Iteration 40255 => Loss: 6.69908923060548211481\n",
      "Iteration 40256 => Loss: 6.69908910780093069803\n",
      "Iteration 40257 => Loss: 6.69908898501293315064\n",
      "Iteration 40258 => Loss: 6.69908886224147881450\n",
      "Iteration 40259 => Loss: 6.69908873948656680142\n",
      "Iteration 40260 => Loss: 6.69908861674819799958\n",
      "Iteration 40261 => Loss: 6.69908849402636707993\n",
      "Iteration 40262 => Loss: 6.69908837132107137791\n",
      "Iteration 40263 => Loss: 6.69908824863230911717\n",
      "Iteration 40264 => Loss: 6.69908812596008829132\n",
      "Iteration 40265 => Loss: 6.69908800330438669590\n",
      "Iteration 40266 => Loss: 6.69908788066522209448\n",
      "Iteration 40267 => Loss: 6.69908775804257583530\n",
      "Iteration 40268 => Loss: 6.69908763543645591199\n",
      "Iteration 40269 => Loss: 6.69908751284685610727\n",
      "Iteration 40270 => Loss: 6.69908739027377819752\n",
      "Iteration 40271 => Loss: 6.69908726771721951820\n",
      "Iteration 40272 => Loss: 6.69908714517716585846\n",
      "Iteration 40273 => Loss: 6.69908702265363231731\n",
      "Iteration 40274 => Loss: 6.69908690014660734846\n",
      "Iteration 40275 => Loss: 6.69908677765608739918\n",
      "Iteration 40276 => Loss: 6.69908665518207335765\n",
      "Iteration 40277 => Loss: 6.69908653272456877659\n",
      "Iteration 40278 => Loss: 6.69908641028355678060\n",
      "Iteration 40279 => Loss: 6.69908628785904891600\n",
      "Iteration 40280 => Loss: 6.69908616545103363649\n",
      "Iteration 40281 => Loss: 6.69908604305951538294\n",
      "Iteration 40282 => Loss: 6.69908592068449237900\n",
      "Iteration 40283 => Loss: 6.69908579832595929560\n",
      "Iteration 40284 => Loss: 6.69908567598390902731\n",
      "Iteration 40285 => Loss: 6.69908555365834601503\n",
      "Iteration 40286 => Loss: 6.69908543134926937057\n",
      "Iteration 40287 => Loss: 6.69908530905667465305\n",
      "Iteration 40288 => Loss: 6.69908518678055031614\n",
      "Iteration 40289 => Loss: 6.69908506452090879435\n",
      "Iteration 40290 => Loss: 6.69908494227774475860\n",
      "Iteration 40291 => Loss: 6.69908482005104932711\n",
      "Iteration 40292 => Loss: 6.69908469784082782894\n",
      "Iteration 40293 => Loss: 6.69908457564706782961\n",
      "Iteration 40294 => Loss: 6.69908445346977110546\n",
      "Iteration 40295 => Loss: 6.69908433130894387375\n",
      "Iteration 40296 => Loss: 6.69908420916457636451\n",
      "Iteration 40297 => Loss: 6.69908408703667301864\n",
      "Iteration 40298 => Loss: 6.69908396492521607257\n",
      "Iteration 40299 => Loss: 6.69908384283021884897\n",
      "Iteration 40300 => Loss: 6.69908372075167424242\n",
      "Iteration 40301 => Loss: 6.69908359868958225292\n",
      "Iteration 40302 => Loss: 6.69908347664393755139\n",
      "Iteration 40303 => Loss: 6.69908335461473303241\n",
      "Iteration 40304 => Loss: 6.69908323260197491322\n",
      "Iteration 40305 => Loss: 6.69908311060565875295\n",
      "Iteration 40306 => Loss: 6.69908298862578632793\n",
      "Iteration 40307 => Loss: 6.69908286666234253914\n",
      "Iteration 40308 => Loss: 6.69908274471533982108\n",
      "Iteration 40309 => Loss: 6.69908262278476396290\n",
      "Iteration 40310 => Loss: 6.69908250087061674094\n",
      "Iteration 40311 => Loss: 6.69908237897290614882\n",
      "Iteration 40312 => Loss: 6.69908225709161353478\n",
      "Iteration 40313 => Loss: 6.69908213522674866880\n",
      "Iteration 40314 => Loss: 6.69908201337830266908\n",
      "Iteration 40315 => Loss: 6.69908189154627997652\n",
      "Iteration 40316 => Loss: 6.69908176973066904480\n",
      "Iteration 40317 => Loss: 6.69908164793147253846\n",
      "Iteration 40318 => Loss: 6.69908152614869401020\n",
      "Iteration 40319 => Loss: 6.69908140438232280189\n",
      "Iteration 40320 => Loss: 6.69908128263236157807\n",
      "Iteration 40321 => Loss: 6.69908116089880056876\n",
      "Iteration 40322 => Loss: 6.69908103918164865576\n",
      "Iteration 40323 => Loss: 6.69908091748089695727\n",
      "Iteration 40324 => Loss: 6.69908079579654192059\n",
      "Iteration 40325 => Loss: 6.69908067412858443390\n",
      "Iteration 40326 => Loss: 6.69908055247702005630\n",
      "Iteration 40327 => Loss: 6.69908043084185678140\n",
      "Iteration 40328 => Loss: 6.69908030922307595745\n",
      "Iteration 40329 => Loss: 6.69908018762068113716\n",
      "Iteration 40330 => Loss: 6.69908006603467853779\n",
      "Iteration 40331 => Loss: 6.69907994446505306030\n",
      "Iteration 40332 => Loss: 6.69907982291181625101\n",
      "Iteration 40333 => Loss: 6.69907970137495833995\n",
      "Iteration 40334 => Loss: 6.69907957985447310989\n",
      "Iteration 40335 => Loss: 6.69907945835036233717\n",
      "Iteration 40336 => Loss: 6.69907933686263046269\n",
      "Iteration 40337 => Loss: 6.69907921539126238741\n",
      "Iteration 40338 => Loss: 6.69907909393626432859\n",
      "Iteration 40339 => Loss: 6.69907897249763095715\n",
      "Iteration 40340 => Loss: 6.69907885107536671399\n",
      "Iteration 40341 => Loss: 6.69907872966946627002\n",
      "Iteration 40342 => Loss: 6.69907860827992163166\n",
      "Iteration 40343 => Loss: 6.69907848690673102254\n",
      "Iteration 40344 => Loss: 6.69907836554989355449\n",
      "Iteration 40345 => Loss: 6.69907824420941366839\n",
      "Iteration 40346 => Loss: 6.69907812288528692335\n",
      "Iteration 40347 => Loss: 6.69907800157750354941\n",
      "Iteration 40348 => Loss: 6.69907788028607331654\n",
      "Iteration 40349 => Loss: 6.69907775901097668481\n",
      "Iteration 40350 => Loss: 6.69907763775222608871\n",
      "Iteration 40351 => Loss: 6.69907751650981797553\n",
      "Iteration 40352 => Loss: 6.69907739528374523985\n",
      "Iteration 40353 => Loss: 6.69907727407400965802\n",
      "Iteration 40354 => Loss: 6.69907715288060412462\n",
      "Iteration 40355 => Loss: 6.69907703170352863964\n",
      "Iteration 40356 => Loss: 6.69907691054278675580\n",
      "Iteration 40357 => Loss: 6.69907678939836959131\n",
      "Iteration 40358 => Loss: 6.69907666827027004075\n",
      "Iteration 40359 => Loss: 6.69907654715850231497\n",
      "Iteration 40360 => Loss: 6.69907642606305486765\n",
      "Iteration 40361 => Loss: 6.69907630498391437612\n",
      "Iteration 40362 => Loss: 6.69907618392109416305\n",
      "Iteration 40363 => Loss: 6.69907606287459600480\n",
      "Iteration 40364 => Loss: 6.69907594184439947327\n",
      "Iteration 40365 => Loss: 6.69907582083050900934\n",
      "Iteration 40366 => Loss: 6.69907569983292550120\n",
      "Iteration 40367 => Loss: 6.69907557885165338973\n",
      "Iteration 40368 => Loss: 6.69907545788667935227\n",
      "Iteration 40369 => Loss: 6.69907533693800871788\n",
      "Iteration 40370 => Loss: 6.69907521600563349295\n",
      "Iteration 40371 => Loss: 6.69907509508955900657\n",
      "Iteration 40372 => Loss: 6.69907497418977015968\n",
      "Iteration 40373 => Loss: 6.69907485330627761044\n",
      "Iteration 40374 => Loss: 6.69907473243906448346\n",
      "Iteration 40375 => Loss: 6.69907461158815298319\n",
      "Iteration 40376 => Loss: 6.69907449075351557610\n",
      "Iteration 40377 => Loss: 6.69907436993516203216\n",
      "Iteration 40378 => Loss: 6.69907424913309412773\n",
      "Iteration 40379 => Loss: 6.69907412834730120466\n",
      "Iteration 40380 => Loss: 6.69907400757778415112\n",
      "Iteration 40381 => Loss: 6.69907388682453763806\n",
      "Iteration 40382 => Loss: 6.69907376608756877090\n",
      "Iteration 40383 => Loss: 6.69907364536686422696\n",
      "Iteration 40384 => Loss: 6.69907352466242844713\n",
      "Iteration 40385 => Loss: 6.69907340397425432599\n",
      "Iteration 40386 => Loss: 6.69907328330234896896\n",
      "Iteration 40387 => Loss: 6.69907316264669994155\n",
      "Iteration 40388 => Loss: 6.69907304200731612553\n",
      "Iteration 40389 => Loss: 6.69907292138417798100\n",
      "Iteration 40390 => Loss: 6.69907280077729971879\n",
      "Iteration 40391 => Loss: 6.69907268018667423348\n",
      "Iteration 40392 => Loss: 6.69907255961229530783\n",
      "Iteration 40393 => Loss: 6.69907243905416471819\n",
      "Iteration 40394 => Loss: 6.69907231851228246455\n",
      "Iteration 40395 => Loss: 6.69907219798663700061\n",
      "Iteration 40396 => Loss: 6.69907207747723898450\n",
      "Iteration 40397 => Loss: 6.69907195698407420537\n",
      "Iteration 40398 => Loss: 6.69907183650714888046\n",
      "Iteration 40399 => Loss: 6.69907171604645679253\n",
      "Iteration 40400 => Loss: 6.69907159560200149429\n",
      "Iteration 40401 => Loss: 6.69907147517377321577\n",
      "Iteration 40402 => Loss: 6.69907135476176840427\n",
      "Iteration 40403 => Loss: 6.69907123436599594157\n",
      "Iteration 40404 => Loss: 6.69907111398643628775\n",
      "Iteration 40405 => Loss: 6.69907099362310631818\n",
      "Iteration 40406 => Loss: 6.69907087327599448656\n",
      "Iteration 40407 => Loss: 6.69907075294509635199\n",
      "Iteration 40408 => Loss: 6.69907063263041546719\n",
      "Iteration 40409 => Loss: 6.69907051233194739126\n",
      "Iteration 40410 => Loss: 6.69907039204968857149\n",
      "Iteration 40411 => Loss: 6.69907027178363634334\n",
      "Iteration 40412 => Loss: 6.69907015153379070682\n",
      "Iteration 40413 => Loss: 6.69907003130014988557\n",
      "Iteration 40414 => Loss: 6.69906991108271565594\n",
      "Iteration 40415 => Loss: 6.69906979088146847801\n",
      "Iteration 40416 => Loss: 6.69906967069643055623\n",
      "Iteration 40417 => Loss: 6.69906955052757790980\n",
      "Iteration 40418 => Loss: 6.69906943037492741411\n",
      "Iteration 40419 => Loss: 6.69906931023846041739\n",
      "Iteration 40420 => Loss: 6.69906919011818047238\n",
      "Iteration 40421 => Loss: 6.69906907001409201996\n",
      "Iteration 40422 => Loss: 6.69906894992617996110\n",
      "Iteration 40423 => Loss: 6.69906882985445939482\n",
      "Iteration 40424 => Loss: 6.69906870979891255757\n",
      "Iteration 40425 => Loss: 6.69906858975955099567\n",
      "Iteration 40426 => Loss: 6.69906846973635161646\n",
      "Iteration 40427 => Loss: 6.69906834972934195349\n",
      "Iteration 40428 => Loss: 6.69906822973848825598\n",
      "Iteration 40429 => Loss: 6.69906810976381006384\n",
      "Iteration 40430 => Loss: 6.69906798980529671894\n",
      "Iteration 40431 => Loss: 6.69906786986294822128\n",
      "Iteration 40432 => Loss: 6.69906774993676190633\n",
      "Iteration 40433 => Loss: 6.69906763002673510954\n",
      "Iteration 40434 => Loss: 6.69906751013287227181\n",
      "Iteration 40435 => Loss: 6.69906739025515296504\n",
      "Iteration 40436 => Loss: 6.69906727039359939369\n",
      "Iteration 40437 => Loss: 6.69906715054818668875\n",
      "Iteration 40438 => Loss: 6.69906703071893083745\n",
      "Iteration 40439 => Loss: 6.69906691090581674075\n",
      "Iteration 40440 => Loss: 6.69906679110885949768\n",
      "Iteration 40441 => Loss: 6.69906667132802979836\n",
      "Iteration 40442 => Loss: 6.69906655156334629453\n",
      "Iteration 40443 => Loss: 6.69906643181480188076\n",
      "Iteration 40444 => Loss: 6.69906631208239300435\n",
      "Iteration 40445 => Loss: 6.69906619236611788892\n",
      "Iteration 40446 => Loss: 6.69906607266597475814\n",
      "Iteration 40447 => Loss: 6.69906595298195917110\n",
      "Iteration 40448 => Loss: 6.69906583331407112780\n",
      "Iteration 40449 => Loss: 6.69906571366231506914\n",
      "Iteration 40450 => Loss: 6.69906559402667411973\n",
      "Iteration 40451 => Loss: 6.69906547440716071407\n",
      "Iteration 40452 => Loss: 6.69906535480375797675\n",
      "Iteration 40453 => Loss: 6.69906523521647390140\n",
      "Iteration 40454 => Loss: 6.69906511564531470526\n",
      "Iteration 40455 => Loss: 6.69906499609025640751\n",
      "Iteration 40456 => Loss: 6.69906487655131410719\n",
      "Iteration 40457 => Loss: 6.69906475702847004072\n",
      "Iteration 40458 => Loss: 6.69906463752174818893\n",
      "Iteration 40459 => Loss: 6.69906451803111568921\n",
      "Iteration 40460 => Loss: 6.69906439855658764060\n",
      "Iteration 40461 => Loss: 6.69906427909816315491\n",
      "Iteration 40462 => Loss: 6.69906415965582979766\n",
      "Iteration 40463 => Loss: 6.69906404022959822697\n",
      "Iteration 40464 => Loss: 6.69906392081945600836\n",
      "Iteration 40465 => Loss: 6.69906380142540491818\n",
      "Iteration 40466 => Loss: 6.69906368204743785100\n",
      "Iteration 40467 => Loss: 6.69906356268555747135\n",
      "Iteration 40468 => Loss: 6.69906344333976555561\n",
      "Iteration 40469 => Loss: 6.69906332401005677468\n",
      "Iteration 40470 => Loss: 6.69906320469642491133\n",
      "Iteration 40471 => Loss: 6.69906308539886730102\n",
      "Iteration 40472 => Loss: 6.69906296611739282554\n",
      "Iteration 40473 => Loss: 6.69906284685198549766\n",
      "Iteration 40474 => Loss: 6.69906272760264887012\n",
      "Iteration 40475 => Loss: 6.69906260836938383108\n",
      "Iteration 40476 => Loss: 6.69906248915218416329\n",
      "Iteration 40477 => Loss: 6.69906236995105164311\n",
      "Iteration 40478 => Loss: 6.69906225076597650059\n",
      "Iteration 40479 => Loss: 6.69906213159696140025\n",
      "Iteration 40480 => Loss: 6.69906201244400811845\n",
      "Iteration 40481 => Loss: 6.69906189330710954977\n",
      "Iteration 40482 => Loss: 6.69906177418627013509\n",
      "Iteration 40483 => Loss: 6.69906165508147299903\n",
      "Iteration 40484 => Loss: 6.69906153599272968791\n",
      "Iteration 40485 => Loss: 6.69906141692003309629\n",
      "Iteration 40486 => Loss: 6.69906129786338233600\n",
      "Iteration 40487 => Loss: 6.69906117882277296616\n",
      "Iteration 40488 => Loss: 6.69906105979820765128\n",
      "Iteration 40489 => Loss: 6.69906094078967306871\n",
      "Iteration 40490 => Loss: 6.69906082179718076475\n",
      "Iteration 40491 => Loss: 6.69906070282071919308\n",
      "Iteration 40492 => Loss: 6.69906058386029990004\n",
      "Iteration 40493 => Loss: 6.69906046491590156933\n",
      "Iteration 40494 => Loss: 6.69906034598753308273\n",
      "Iteration 40495 => Loss: 6.69906022707518911119\n",
      "Iteration 40496 => Loss: 6.69906010817887231923\n",
      "Iteration 40497 => Loss: 6.69905998929857382507\n",
      "Iteration 40498 => Loss: 6.69905987043429096417\n",
      "Iteration 40499 => Loss: 6.69905975158602817743\n",
      "Iteration 40500 => Loss: 6.69905963275378368849\n",
      "Iteration 40501 => Loss: 6.69905951393754328649\n",
      "Iteration 40502 => Loss: 6.69905939513732295865\n",
      "Iteration 40503 => Loss: 6.69905927635310582957\n",
      "Iteration 40504 => Loss: 6.69905915758489989287\n",
      "Iteration 40505 => Loss: 6.69905903883269093768\n",
      "Iteration 40506 => Loss: 6.69905892009648251673\n",
      "Iteration 40507 => Loss: 6.69905880137628262361\n",
      "Iteration 40508 => Loss: 6.69905868267207971201\n",
      "Iteration 40509 => Loss: 6.69905856398386756467\n",
      "Iteration 40510 => Loss: 6.69905844531164973432\n",
      "Iteration 40511 => Loss: 6.69905832665542622095\n",
      "Iteration 40512 => Loss: 6.69905820801518281371\n",
      "Iteration 40513 => Loss: 6.69905808939092839438\n",
      "Iteration 40514 => Loss: 6.69905797078266296296\n",
      "Iteration 40515 => Loss: 6.69905785219038030220\n",
      "Iteration 40516 => Loss: 6.69905773361407685940\n",
      "Iteration 40517 => Loss: 6.69905761505375085818\n",
      "Iteration 40518 => Loss: 6.69905749650939874584\n",
      "Iteration 40519 => Loss: 6.69905737798102585145\n",
      "Iteration 40520 => Loss: 6.69905725946861885234\n",
      "Iteration 40521 => Loss: 6.69905714097218751846\n",
      "Iteration 40522 => Loss: 6.69905702249171763896\n",
      "Iteration 40523 => Loss: 6.69905690402721631926\n",
      "Iteration 40524 => Loss: 6.69905678557867734213\n",
      "Iteration 40525 => Loss: 6.69905666714609981938\n",
      "Iteration 40526 => Loss: 6.69905654872948197465\n",
      "Iteration 40527 => Loss: 6.69905643032882025523\n",
      "Iteration 40528 => Loss: 6.69905631194411022022\n",
      "Iteration 40529 => Loss: 6.69905619357535719871\n",
      "Iteration 40530 => Loss: 6.69905607522254697983\n",
      "Iteration 40531 => Loss: 6.69905595688569022172\n",
      "Iteration 40532 => Loss: 6.69905583856477893079\n",
      "Iteration 40533 => Loss: 6.69905572025981843609\n",
      "Iteration 40534 => Loss: 6.69905560197078919771\n",
      "Iteration 40535 => Loss: 6.69905548369770365014\n",
      "Iteration 40536 => Loss: 6.69905536544055379977\n",
      "Iteration 40537 => Loss: 6.69905524719933787026\n",
      "Iteration 40538 => Loss: 6.69905512897405586159\n",
      "Iteration 40539 => Loss: 6.69905501076470599742\n",
      "Iteration 40540 => Loss: 6.69905489257128561320\n",
      "Iteration 40541 => Loss: 6.69905477439378849169\n",
      "Iteration 40542 => Loss: 6.69905465623221818561\n",
      "Iteration 40543 => Loss: 6.69905453808657114223\n",
      "Iteration 40544 => Loss: 6.69905441995684203249\n",
      "Iteration 40545 => Loss: 6.69905430184303174457\n",
      "Iteration 40546 => Loss: 6.69905418374513761393\n",
      "Iteration 40547 => Loss: 6.69905406566316230510\n",
      "Iteration 40548 => Loss: 6.69905394759709160724\n",
      "Iteration 40549 => Loss: 6.69905382954692996123\n",
      "Iteration 40550 => Loss: 6.69905371151267647889\n",
      "Iteration 40551 => Loss: 6.69905359349432849569\n",
      "Iteration 40552 => Loss: 6.69905347549188334710\n",
      "Iteration 40553 => Loss: 6.69905335750534369765\n",
      "Iteration 40554 => Loss: 6.69905323953469444831\n",
      "Iteration 40555 => Loss: 6.69905312157994448086\n",
      "Iteration 40556 => Loss: 6.69905300364109557165\n",
      "Iteration 40557 => Loss: 6.69905288571813528620\n",
      "Iteration 40558 => Loss: 6.69905276781106895356\n",
      "Iteration 40559 => Loss: 6.69905264991987792200\n",
      "Iteration 40560 => Loss: 6.69905253204458706051\n",
      "Iteration 40561 => Loss: 6.69905241418516705920\n",
      "Iteration 40562 => Loss: 6.69905229634163656982\n",
      "Iteration 40563 => Loss: 6.69905217851398582241\n",
      "Iteration 40564 => Loss: 6.69905206070220948789\n",
      "Iteration 40565 => Loss: 6.69905194290630578990\n",
      "Iteration 40566 => Loss: 6.69905182512627739300\n",
      "Iteration 40567 => Loss: 6.69905170736212696170\n",
      "Iteration 40568 => Loss: 6.69905158961383495608\n",
      "Iteration 40569 => Loss: 6.69905147188141647518\n",
      "Iteration 40570 => Loss: 6.69905135416485997268\n",
      "Iteration 40571 => Loss: 6.69905123646416722494\n",
      "Iteration 40572 => Loss: 6.69905111877933290288\n",
      "Iteration 40573 => Loss: 6.69905100111035434196\n",
      "Iteration 40574 => Loss: 6.69905088345723154220\n",
      "Iteration 40575 => Loss: 6.69905076581996539176\n",
      "Iteration 40576 => Loss: 6.69905064819855233793\n",
      "Iteration 40577 => Loss: 6.69905053059298705165\n",
      "Iteration 40578 => Loss: 6.69905041300326598019\n",
      "Iteration 40579 => Loss: 6.69905029542939622900\n",
      "Iteration 40580 => Loss: 6.69905017787136625174\n",
      "Iteration 40581 => Loss: 6.69905006032917427206\n",
      "Iteration 40582 => Loss: 6.69904994280283272445\n",
      "Iteration 40583 => Loss: 6.69904982529231141086\n",
      "Iteration 40584 => Loss: 6.69904970779763342392\n",
      "Iteration 40585 => Loss: 6.69904959031879077003\n",
      "Iteration 40586 => Loss: 6.69904947285577190286\n",
      "Iteration 40587 => Loss: 6.69904935540858037513\n",
      "Iteration 40588 => Loss: 6.69904923797722329226\n",
      "Iteration 40589 => Loss: 6.69904912056168555523\n",
      "Iteration 40590 => Loss: 6.69904900316197160492\n",
      "Iteration 40591 => Loss: 6.69904888577807078320\n",
      "Iteration 40592 => Loss: 6.69904876840999108367\n",
      "Iteration 40593 => Loss: 6.69904865105772540090\n",
      "Iteration 40594 => Loss: 6.69904853372127551125\n",
      "Iteration 40595 => Loss: 6.69904841640064141473\n",
      "Iteration 40596 => Loss: 6.69904829909580978864\n",
      "Iteration 40597 => Loss: 6.69904818180678773842\n",
      "Iteration 40598 => Loss: 6.69904806453356904683\n",
      "Iteration 40599 => Loss: 6.69904794727614749661\n",
      "Iteration 40600 => Loss: 6.69904783003453463408\n",
      "Iteration 40601 => Loss: 6.69904771280871802475\n",
      "Iteration 40602 => Loss: 6.69904759559869500407\n",
      "Iteration 40603 => Loss: 6.69904747840446734841\n",
      "Iteration 40604 => Loss: 6.69904736122603239323\n",
      "Iteration 40605 => Loss: 6.69904724406338747400\n",
      "Iteration 40606 => Loss: 6.69904712691653436707\n",
      "Iteration 40607 => Loss: 6.69904700978545797341\n",
      "Iteration 40608 => Loss: 6.69904689267016717480\n",
      "Iteration 40609 => Loss: 6.69904677557065841853\n",
      "Iteration 40610 => Loss: 6.69904665848693436914\n",
      "Iteration 40611 => Loss: 6.69904654141898259212\n",
      "Iteration 40612 => Loss: 6.69904642436680930473\n",
      "Iteration 40613 => Loss: 6.69904630733039496704\n",
      "Iteration 40614 => Loss: 6.69904619030976711258\n",
      "Iteration 40615 => Loss: 6.69904607330489909600\n",
      "Iteration 40616 => Loss: 6.69904595631581134541\n",
      "Iteration 40617 => Loss: 6.69904583934247366273\n",
      "Iteration 40618 => Loss: 6.69904572238490292335\n",
      "Iteration 40619 => Loss: 6.69904560544309113368\n",
      "Iteration 40620 => Loss: 6.69904548851703740553\n",
      "Iteration 40621 => Loss: 6.69904537160673552165\n",
      "Iteration 40622 => Loss: 6.69904525471219347565\n",
      "Iteration 40623 => Loss: 6.69904513783340238575\n",
      "Iteration 40624 => Loss: 6.69904502097035958741\n",
      "Iteration 40625 => Loss: 6.69904490412306152791\n",
      "Iteration 40626 => Loss: 6.69904478729150998362\n",
      "Iteration 40627 => Loss: 6.69904467047570140181\n",
      "Iteration 40628 => Loss: 6.69904455367563578250\n",
      "Iteration 40629 => Loss: 6.69904443689130779660\n",
      "Iteration 40630 => Loss: 6.69904432012271389141\n",
      "Iteration 40631 => Loss: 6.69904420336985495510\n",
      "Iteration 40632 => Loss: 6.69904408663273276403\n",
      "Iteration 40633 => Loss: 6.69904396991134376549\n",
      "Iteration 40634 => Loss: 6.69904385320567996587\n",
      "Iteration 40635 => Loss: 6.69904373651573514792\n",
      "Iteration 40636 => Loss: 6.69904361984151730525\n",
      "Iteration 40637 => Loss: 6.69904350318302821421\n",
      "Iteration 40638 => Loss: 6.69904338654025099942\n",
      "Iteration 40639 => Loss: 6.69904326991319543083\n",
      "Iteration 40640 => Loss: 6.69904315330185529120\n",
      "Iteration 40641 => Loss: 6.69904303670622613964\n",
      "Iteration 40642 => Loss: 6.69904292012631330522\n",
      "Iteration 40643 => Loss: 6.69904280356210612979\n",
      "Iteration 40644 => Loss: 6.69904268701360461336\n",
      "Iteration 40645 => Loss: 6.69904257048081142045\n",
      "Iteration 40646 => Loss: 6.69904245396371322840\n",
      "Iteration 40647 => Loss: 6.69904233746232780078\n",
      "Iteration 40648 => Loss: 6.69904222097663382129\n",
      "Iteration 40649 => Loss: 6.69904210450663217813\n",
      "Iteration 40650 => Loss: 6.69904198805233441760\n",
      "Iteration 40651 => Loss: 6.69904187161372455250\n",
      "Iteration 40652 => Loss: 6.69904175519080524737\n",
      "Iteration 40653 => Loss: 6.69904163878357028494\n",
      "Iteration 40654 => Loss: 6.69904152239202677066\n",
      "Iteration 40655 => Loss: 6.69904140601616315820\n",
      "Iteration 40656 => Loss: 6.69904128965598388845\n",
      "Iteration 40657 => Loss: 6.69904117331148274417\n",
      "Iteration 40658 => Loss: 6.69904105698265706081\n",
      "Iteration 40659 => Loss: 6.69904094066951127928\n",
      "Iteration 40660 => Loss: 6.69904082437203651779\n",
      "Iteration 40661 => Loss: 6.69904070809023455269\n",
      "Iteration 40662 => Loss: 6.69904059182409650219\n",
      "Iteration 40663 => Loss: 6.69904047557362858356\n",
      "Iteration 40664 => Loss: 6.69904035933882546772\n",
      "Iteration 40665 => Loss: 6.69904024311968004923\n",
      "Iteration 40666 => Loss: 6.69904012691620387443\n",
      "Iteration 40667 => Loss: 6.69904001072838184427\n",
      "Iteration 40668 => Loss: 6.69903989455621839966\n",
      "Iteration 40669 => Loss: 6.69903977839971087604\n",
      "Iteration 40670 => Loss: 6.69903966225884861529\n",
      "Iteration 40671 => Loss: 6.69903954613363605830\n",
      "Iteration 40672 => Loss: 6.69903943002408031049\n",
      "Iteration 40673 => Loss: 6.69903931393016272011\n",
      "Iteration 40674 => Loss: 6.69903919785188683989\n",
      "Iteration 40675 => Loss: 6.69903908178925711070\n",
      "Iteration 40676 => Loss: 6.69903896574226642713\n",
      "Iteration 40677 => Loss: 6.69903884971091212464\n",
      "Iteration 40678 => Loss: 6.69903873369519953229\n",
      "Iteration 40679 => Loss: 6.69903861769511177471\n",
      "Iteration 40680 => Loss: 6.69903850171065506913\n",
      "Iteration 40681 => Loss: 6.69903838574182763921\n",
      "Iteration 40682 => Loss: 6.69903826978863037311\n",
      "Iteration 40683 => Loss: 6.69903815385105705360\n",
      "Iteration 40684 => Loss: 6.69903803792910323978\n",
      "Iteration 40685 => Loss: 6.69903792202277603707\n",
      "Iteration 40686 => Loss: 6.69903780613206212280\n",
      "Iteration 40687 => Loss: 6.69903769025696682604\n",
      "Iteration 40688 => Loss: 6.69903757439749103497\n",
      "Iteration 40689 => Loss: 6.69903745855361876238\n",
      "Iteration 40690 => Loss: 6.69903734272535622551\n",
      "Iteration 40691 => Loss: 6.69903722691270608891\n",
      "Iteration 40692 => Loss: 6.69903711111565858261\n",
      "Iteration 40693 => Loss: 6.69903699533422170020\n",
      "Iteration 40694 => Loss: 6.69903687956837767814\n",
      "Iteration 40695 => Loss: 6.69903676381813628637\n",
      "Iteration 40696 => Loss: 6.69903664808349574855\n",
      "Iteration 40697 => Loss: 6.69903653236444185382\n",
      "Iteration 40698 => Loss: 6.69903641666098526031\n",
      "Iteration 40699 => Loss: 6.69903630097312152714\n",
      "Iteration 40700 => Loss: 6.69903618530084621341\n",
      "Iteration 40701 => Loss: 6.69903606964415931913\n",
      "Iteration 40702 => Loss: 6.69903595400305640339\n",
      "Iteration 40703 => Loss: 6.69903583837752947261\n",
      "Iteration 40704 => Loss: 6.69903572276759184945\n",
      "Iteration 40705 => Loss: 6.69903560717323198759\n",
      "Iteration 40706 => Loss: 6.69903549159444544614\n",
      "Iteration 40707 => Loss: 6.69903537603122956057\n",
      "Iteration 40708 => Loss: 6.69903526048359321265\n",
      "Iteration 40709 => Loss: 6.69903514495151952701\n",
      "Iteration 40710 => Loss: 6.69903502943501916178\n",
      "Iteration 40711 => Loss: 6.69903491393408767607\n",
      "Iteration 40712 => Loss: 6.69903479844870997084\n",
      "Iteration 40713 => Loss: 6.69903468297890114513\n",
      "Iteration 40714 => Loss: 6.69903456752465142898\n",
      "Iteration 40715 => Loss: 6.69903445208595904603\n",
      "Iteration 40716 => Loss: 6.69903433666282399628\n",
      "Iteration 40717 => Loss: 6.69903422125523739794\n",
      "Iteration 40718 => Loss: 6.69903410586320013920\n",
      "Iteration 40719 => Loss: 6.69903399048672110183\n",
      "Iteration 40720 => Loss: 6.69903387512578074592\n",
      "Iteration 40721 => Loss: 6.69903375978038706506\n",
      "Iteration 40722 => Loss: 6.69903364445053295384\n",
      "Iteration 40723 => Loss: 6.69903352913622995857\n",
      "Iteration 40724 => Loss: 6.69903341383745676296\n",
      "Iteration 40725 => Loss: 6.69903329855422491335\n",
      "Iteration 40726 => Loss: 6.69903318328652463975\n",
      "Iteration 40727 => Loss: 6.69903306803435771855\n",
      "Iteration 40728 => Loss: 6.69903295279772414972\n",
      "Iteration 40729 => Loss: 6.69903283757661593967\n",
      "Iteration 40730 => Loss: 6.69903272237103220021\n",
      "Iteration 40731 => Loss: 6.69903260718097026682\n",
      "Iteration 40732 => Loss: 6.69903249200643280403\n",
      "Iteration 40733 => Loss: 6.69903237684741892366\n",
      "Iteration 40734 => Loss: 6.69903226170391441485\n",
      "Iteration 40735 => Loss: 6.69903214657593437664\n",
      "Iteration 40736 => Loss: 6.69903203146346459818\n",
      "Iteration 40737 => Loss: 6.69903191636650507945\n",
      "Iteration 40738 => Loss: 6.69903180128505582047\n",
      "Iteration 40739 => Loss: 6.69903168621911504488\n",
      "Iteration 40740 => Loss: 6.69903157116867031817\n",
      "Iteration 40741 => Loss: 6.69903145613374650935\n",
      "Iteration 40742 => Loss: 6.69903134111430986763\n",
      "Iteration 40743 => Loss: 6.69903122611037371570\n",
      "Iteration 40744 => Loss: 6.69903111112193450083\n",
      "Iteration 40745 => Loss: 6.69903099614899311121\n",
      "Iteration 40746 => Loss: 6.69903088119154155322\n",
      "Iteration 40747 => Loss: 6.69903076624958071505\n",
      "Iteration 40748 => Loss: 6.69903065132310970853\n",
      "Iteration 40749 => Loss: 6.69903053641212498093\n",
      "Iteration 40750 => Loss: 6.69903042151663097314\n",
      "Iteration 40751 => Loss: 6.69903030663661080979\n",
      "Iteration 40752 => Loss: 6.69903019177207337265\n",
      "Iteration 40753 => Loss: 6.69903007692301155629\n",
      "Iteration 40754 => Loss: 6.69902996208942713707\n",
      "Iteration 40755 => Loss: 6.69902984727131478593\n",
      "Iteration 40756 => Loss: 6.69902973246867894375\n",
      "Iteration 40757 => Loss: 6.69902961768150628785\n",
      "Iteration 40758 => Loss: 6.69902950290980747639\n",
      "Iteration 40759 => Loss: 6.69902938815356652213\n",
      "Iteration 40760 => Loss: 6.69902927341280030049\n",
      "Iteration 40761 => Loss: 6.69902915868748571881\n",
      "Iteration 40762 => Loss: 6.69902904397762988253\n",
      "Iteration 40763 => Loss: 6.69902892928323545618\n",
      "Iteration 40764 => Loss: 6.69902881460430066340\n",
      "Iteration 40765 => Loss: 6.69902869994081129335\n",
      "Iteration 40766 => Loss: 6.69902858529277711597\n",
      "Iteration 40767 => Loss: 6.69902847066018747313\n",
      "Iteration 40768 => Loss: 6.69902835604305302297\n",
      "Iteration 40769 => Loss: 6.69902824144135244921\n",
      "Iteration 40770 => Loss: 6.69902812685509907453\n",
      "Iteration 40771 => Loss: 6.69902801228428579350\n",
      "Iteration 40772 => Loss: 6.69902789772891349429\n",
      "Iteration 40773 => Loss: 6.69902778318897773602\n",
      "Iteration 40774 => Loss: 6.69902766866446963689\n",
      "Iteration 40775 => Loss: 6.69902755415539807871\n",
      "Iteration 40776 => Loss: 6.69902743966175862056\n",
      "Iteration 40777 => Loss: 6.69902732518354238067\n",
      "Iteration 40778 => Loss: 6.69902721072076090536\n",
      "Iteration 40779 => Loss: 6.69902709627339643106\n",
      "Iteration 40780 => Loss: 6.69902698184145428684\n",
      "Iteration 40781 => Loss: 6.69902686742493358452\n",
      "Iteration 40782 => Loss: 6.69902675302382810685\n",
      "Iteration 40783 => Loss: 6.69902663863813252476\n",
      "Iteration 40784 => Loss: 6.69902652426786371365\n",
      "Iteration 40785 => Loss: 6.69902640991299946904\n",
      "Iteration 40786 => Loss: 6.69902629557354334366\n",
      "Iteration 40787 => Loss: 6.69902618124949711387\n",
      "Iteration 40788 => Loss: 6.69902606694085722694\n",
      "Iteration 40789 => Loss: 6.69902595264761746563\n",
      "Iteration 40790 => Loss: 6.69902583836977960630\n",
      "Iteration 40791 => Loss: 6.69902572410733299080\n",
      "Iteration 40792 => Loss: 6.69902560986029449452\n",
      "Iteration 40793 => Loss: 6.69902549562864635391\n",
      "Iteration 40794 => Loss: 6.69902538141239745073\n",
      "Iteration 40795 => Loss: 6.69902526721153268596\n",
      "Iteration 40796 => Loss: 6.69902515302605383596\n",
      "Iteration 40797 => Loss: 6.69902503885596622979\n",
      "Iteration 40798 => Loss: 6.69902492470125832114\n",
      "Iteration 40799 => Loss: 6.69902481056194343267\n",
      "Iteration 40800 => Loss: 6.69902469643799758359\n",
      "Iteration 40801 => Loss: 6.69902458232943409655\n",
      "Iteration 40802 => Loss: 6.69902446823624764249\n",
      "Iteration 40803 => Loss: 6.69902435415843111599\n",
      "Iteration 40804 => Loss: 6.69902424009598895793\n",
      "Iteration 40805 => Loss: 6.69902412604891761561\n",
      "Iteration 40806 => Loss: 6.69902401201721708901\n",
      "Iteration 40807 => Loss: 6.69902389800087849636\n",
      "Iteration 40808 => Loss: 6.69902378399990805491\n",
      "Iteration 40809 => Loss: 6.69902367001428888926\n",
      "Iteration 40810 => Loss: 6.69902355604403787481\n",
      "Iteration 40811 => Loss: 6.69902344208914435342\n",
      "Iteration 40812 => Loss: 6.69902332814960388419\n",
      "Iteration 40813 => Loss: 6.69902321422541557894\n",
      "Iteration 40814 => Loss: 6.69902310031658121403\n",
      "Iteration 40815 => Loss: 6.69902298642310256582\n",
      "Iteration 40816 => Loss: 6.69902287254495920621\n",
      "Iteration 40817 => Loss: 6.69902275868216090515\n",
      "Iteration 40818 => Loss: 6.69902264483471476808\n",
      "Iteration 40819 => Loss: 6.69902253100260658414\n",
      "Iteration 40820 => Loss: 6.69902241718583368879\n",
      "Iteration 40821 => Loss: 6.69902230338439785839\n",
      "Iteration 40822 => Loss: 6.69902218959829465206\n",
      "Iteration 40823 => Loss: 6.69902207582753295156\n",
      "Iteration 40824 => Loss: 6.69902196207209499335\n",
      "Iteration 40825 => Loss: 6.69902184833198877101\n",
      "Iteration 40826 => Loss: 6.69902173460720273823\n",
      "Iteration 40827 => Loss: 6.69902162089775021769\n",
      "Iteration 40828 => Loss: 6.69902150720361344582\n",
      "Iteration 40829 => Loss: 6.69902139352480219259\n",
      "Iteration 40830 => Loss: 6.69902127986130313531\n",
      "Iteration 40831 => Loss: 6.69902116621312426759\n",
      "Iteration 40832 => Loss: 6.69902105258025937218\n",
      "Iteration 40833 => Loss: 6.69902093896270667273\n",
      "Iteration 40834 => Loss: 6.69902082536046172834\n",
      "Iteration 40835 => Loss: 6.69902071177352897990\n",
      "Iteration 40836 => Loss: 6.69902059820189688111\n",
      "Iteration 40837 => Loss: 6.69902048464557520191\n",
      "Iteration 40838 => Loss: 6.69902037110455061963\n",
      "Iteration 40839 => Loss: 6.69902025757882135792\n",
      "Iteration 40840 => Loss: 6.69902014406839363403\n",
      "Iteration 40841 => Loss: 6.69902003057326744795\n",
      "Iteration 40842 => Loss: 6.69901991709342414794\n",
      "Iteration 40843 => Loss: 6.69901980362887528031\n",
      "Iteration 40844 => Loss: 6.69901969017961906872\n",
      "Iteration 40845 => Loss: 6.69901957674565018408\n",
      "Iteration 40846 => Loss: 6.69901946332696862640\n",
      "Iteration 40847 => Loss: 6.69901934992356462573\n",
      "Iteration 40848 => Loss: 6.69901923653544884019\n",
      "Iteration 40849 => Loss: 6.69901912316260172986\n",
      "Iteration 40850 => Loss: 6.69901900980504194649\n",
      "Iteration 40851 => Loss: 6.69901889646275083834\n",
      "Iteration 40852 => Loss: 6.69901878313573639900\n",
      "Iteration 40853 => Loss: 6.69901866982398974670\n",
      "Iteration 40854 => Loss: 6.69901855652750910508\n",
      "Iteration 40855 => Loss: 6.69901844324630157956\n",
      "Iteration 40856 => Loss: 6.69901832998035295930\n",
      "Iteration 40857 => Loss: 6.69901821672967212606\n",
      "Iteration 40858 => Loss: 6.69901810349425197444\n",
      "Iteration 40859 => Loss: 6.69901799027408717535\n",
      "Iteration 40860 => Loss: 6.69901787706918394605\n",
      "Iteration 40861 => Loss: 6.69901776387952718750\n",
      "Iteration 40862 => Loss: 6.69901765070512578149\n",
      "Iteration 40863 => Loss: 6.69901753754597528712\n",
      "Iteration 40864 => Loss: 6.69901742440207659257\n",
      "Iteration 40865 => Loss: 6.69901731127341193428\n",
      "Iteration 40866 => Loss: 6.69901719816000618124\n",
      "Iteration 40867 => Loss: 6.69901708506183624081\n",
      "Iteration 40868 => Loss: 6.69901697197890566571\n",
      "Iteration 40869 => Loss: 6.69901685891121179139\n",
      "Iteration 40870 => Loss: 6.69901674585875550605\n",
      "Iteration 40871 => Loss: 6.69901663282153325696\n",
      "Iteration 40872 => Loss: 6.69901651979954682048\n",
      "Iteration 40873 => Loss: 6.69901640679278109758\n",
      "Iteration 40874 => Loss: 6.69901629380124585822\n",
      "Iteration 40875 => Loss: 6.69901618082493932604\n",
      "Iteration 40876 => Loss: 6.69901606786386150105\n",
      "Iteration 40877 => Loss: 6.69901595491799017879\n",
      "Iteration 40878 => Loss: 6.69901584198735111642\n",
      "Iteration 40879 => Loss: 6.69901572907192743855\n",
      "Iteration 40880 => Loss: 6.69901561617171292795\n",
      "Iteration 40881 => Loss: 6.69901550328672090728\n",
      "Iteration 40882 => Loss: 6.69901539041693450116\n",
      "Iteration 40883 => Loss: 6.69901527756235548594\n",
      "Iteration 40884 => Loss: 6.69901516472298741434\n",
      "Iteration 40885 => Loss: 6.69901505189882584546\n",
      "Iteration 40886 => Loss: 6.69901493908986012116\n",
      "Iteration 40887 => Loss: 6.69901482629610089958\n",
      "Iteration 40888 => Loss: 6.69901471351753841077\n",
      "Iteration 40889 => Loss: 6.69901460075417709561\n",
      "Iteration 40890 => Loss: 6.69901448800600807232\n",
      "Iteration 40891 => Loss: 6.69901437527302778818\n",
      "Iteration 40892 => Loss: 6.69901426255524867770\n",
      "Iteration 40893 => Loss: 6.69901414985264320734\n",
      "Iteration 40894 => Loss: 6.69901403716523180520\n",
      "Iteration 40895 => Loss: 6.69901392449301180676\n",
      "Iteration 40896 => Loss: 6.69901381183596811297\n",
      "Iteration 40897 => Loss: 6.69901369919410161202\n",
      "Iteration 40898 => Loss: 6.69901358656742296205\n",
      "Iteration 40899 => Loss: 6.69901347395590907041\n",
      "Iteration 40900 => Loss: 6.69901336135957325979\n",
      "Iteration 40901 => Loss: 6.69901324877841553018\n",
      "Iteration 40902 => Loss: 6.69901313621242699980\n",
      "Iteration 40903 => Loss: 6.69901302366160589230\n",
      "Iteration 40904 => Loss: 6.69901291112595309585\n",
      "Iteration 40905 => Loss: 6.69901279860545972866\n",
      "Iteration 40906 => Loss: 6.69901268610013200799\n",
      "Iteration 40907 => Loss: 6.69901257360995927570\n",
      "Iteration 40908 => Loss: 6.69901246113494419632\n",
      "Iteration 40909 => Loss: 6.69901234867508943438\n",
      "Iteration 40910 => Loss: 6.69901223623039143718\n",
      "Iteration 40911 => Loss: 6.69901212380083954656\n",
      "Iteration 40912 => Loss: 6.69901201138643820343\n",
      "Iteration 40913 => Loss: 6.69901189898719007232\n",
      "Iteration 40914 => Loss: 6.69901178660308360691\n",
      "Iteration 40915 => Loss: 6.69901167423411525448\n",
      "Iteration 40916 => Loss: 6.69901156188029744953\n",
      "Iteration 40917 => Loss: 6.69901144954161420486\n",
      "Iteration 40918 => Loss: 6.69901133721807706678\n",
      "Iteration 40919 => Loss: 6.69901122490966649536\n",
      "Iteration 40920 => Loss: 6.69901111261639314876\n",
      "Iteration 40921 => Loss: 6.69901100033824903335\n",
      "Iteration 40922 => Loss: 6.69901088807523770186\n",
      "Iteration 40923 => Loss: 6.69901077582735204885\n",
      "Iteration 40924 => Loss: 6.69901066359459118615\n",
      "Iteration 40925 => Loss: 6.69901055137695689012\n",
      "Iteration 40926 => Loss: 6.69901043917443494990\n",
      "Iteration 40927 => Loss: 6.69901032698704135271\n",
      "Iteration 40928 => Loss: 6.69901021481476011132\n",
      "Iteration 40929 => Loss: 6.69901010265760188389\n",
      "Iteration 40930 => Loss: 6.69900999051554979502\n",
      "Iteration 40931 => Loss: 6.69900987838860917378\n",
      "Iteration 40932 => Loss: 6.69900976627677824382\n",
      "Iteration 40933 => Loss: 6.69900965418006055785\n",
      "Iteration 40934 => Loss: 6.69900954209843479958\n",
      "Iteration 40935 => Loss: 6.69900943003192228531\n",
      "Iteration 40936 => Loss: 6.69900931798050613963\n",
      "Iteration 40937 => Loss: 6.69900920594419257981\n",
      "Iteration 40938 => Loss: 6.69900909392297094769\n",
      "Iteration 40939 => Loss: 6.69900898191685101324\n",
      "Iteration 40940 => Loss: 6.69900886992582034196\n",
      "Iteration 40941 => Loss: 6.69900875794987715750\n",
      "Iteration 40942 => Loss: 6.69900864598903300617\n",
      "Iteration 40943 => Loss: 6.69900853404326301899\n",
      "Iteration 40944 => Loss: 6.69900842211258229497\n",
      "Iteration 40945 => Loss: 6.69900831019698816959\n",
      "Iteration 40946 => Loss: 6.69900819829647087289\n",
      "Iteration 40947 => Loss: 6.69900808641103306940\n",
      "Iteration 40948 => Loss: 6.69900797454067475911\n",
      "Iteration 40949 => Loss: 6.69900786268538350754\n",
      "Iteration 40950 => Loss: 6.69900775084517352553\n",
      "Iteration 40951 => Loss: 6.69900763902002704953\n",
      "Iteration 40952 => Loss: 6.69900752720995296130\n",
      "Iteration 40953 => Loss: 6.69900741541494149089\n",
      "Iteration 40954 => Loss: 6.69900730363499707920\n",
      "Iteration 40955 => Loss: 6.69900719187011439715\n",
      "Iteration 40956 => Loss: 6.69900708012029522109\n",
      "Iteration 40957 => Loss: 6.69900696838553066925\n",
      "Iteration 40958 => Loss: 6.69900685666582607070\n",
      "Iteration 40959 => Loss: 6.69900674496117076728\n",
      "Iteration 40960 => Loss: 6.69900663327157364080\n",
      "Iteration 40961 => Loss: 6.69900652159701959221\n",
      "Iteration 40962 => Loss: 6.69900640993751395058\n",
      "Iteration 40963 => Loss: 6.69900629829306293317\n",
      "Iteration 40964 => Loss: 6.69900618666365144094\n",
      "Iteration 40965 => Loss: 6.69900607504928213842\n",
      "Iteration 40966 => Loss: 6.69900596344994703202\n",
      "Iteration 40967 => Loss: 6.69900585186565855622\n",
      "Iteration 40968 => Loss: 6.69900574029640338836\n",
      "Iteration 40969 => Loss: 6.69900562874218330478\n",
      "Iteration 40970 => Loss: 6.69900551720298853553\n",
      "Iteration 40971 => Loss: 6.69900540567882352150\n",
      "Iteration 40972 => Loss: 6.69900529416969625629\n",
      "Iteration 40973 => Loss: 6.69900518267558897634\n",
      "Iteration 40974 => Loss: 6.69900507119651322796\n",
      "Iteration 40975 => Loss: 6.69900495973244414216\n",
      "Iteration 40976 => Loss: 6.69900484828341014065\n",
      "Iteration 40977 => Loss: 6.69900473684938457808\n",
      "Iteration 40978 => Loss: 6.69900462543037367169\n",
      "Iteration 40979 => Loss: 6.69900451402638363874\n",
      "Iteration 40980 => Loss: 6.69900440263739849200\n",
      "Iteration 40981 => Loss: 6.69900429126342888964\n",
      "Iteration 40982 => Loss: 6.69900417990446594985\n",
      "Iteration 40983 => Loss: 6.69900406856050345539\n",
      "Iteration 40984 => Loss: 6.69900395723154851169\n",
      "Iteration 40985 => Loss: 6.69900384591759134878\n",
      "Iteration 40986 => Loss: 6.69900373461863996027\n",
      "Iteration 40987 => Loss: 6.69900362333468546439\n",
      "Iteration 40988 => Loss: 6.69900351206571897933\n",
      "Iteration 40989 => Loss: 6.69900340081175293960\n",
      "Iteration 40990 => Loss: 6.69900328957277668707\n",
      "Iteration 40991 => Loss: 6.69900317834879199808\n",
      "Iteration 40992 => Loss: 6.69900306713979087903\n",
      "Iteration 40993 => Loss: 6.69900295594577599445\n",
      "Iteration 40994 => Loss: 6.69900284476674379164\n",
      "Iteration 40995 => Loss: 6.69900273360269782330\n",
      "Iteration 40996 => Loss: 6.69900262245363098401\n",
      "Iteration 40997 => Loss: 6.69900251131953883288\n",
      "Iteration 40998 => Loss: 6.69900240020041781719\n",
      "Iteration 40999 => Loss: 6.69900228909627148965\n",
      "Iteration 41000 => Loss: 6.69900217800709718574\n",
      "Iteration 41001 => Loss: 6.69900206693289490545\n",
      "Iteration 41002 => Loss: 6.69900195587365843153\n",
      "Iteration 41003 => Loss: 6.69900184482939042852\n",
      "Iteration 41004 => Loss: 6.69900173380008379098\n",
      "Iteration 41005 => Loss: 6.69900162278573585439\n",
      "Iteration 41006 => Loss: 6.69900151178635017146\n",
      "Iteration 41007 => Loss: 6.69900140080192407765\n",
      "Iteration 41008 => Loss: 6.69900128983244336212\n",
      "Iteration 41009 => Loss: 6.69900117887792401206\n",
      "Iteration 41010 => Loss: 6.69900106793834648755\n",
      "Iteration 41011 => Loss: 6.69900095701373121670\n",
      "Iteration 41012 => Loss: 6.69900084610405865959\n",
      "Iteration 41013 => Loss: 6.69900073520932970439\n",
      "Iteration 41014 => Loss: 6.69900062432953635749\n",
      "Iteration 41015 => Loss: 6.69900051346469371794\n",
      "Iteration 41016 => Loss: 6.69900040261479023940\n",
      "Iteration 41017 => Loss: 6.69900029177982236916\n",
      "Iteration 41018 => Loss: 6.69900018095978833088\n",
      "Iteration 41019 => Loss: 6.69900007015468812455\n",
      "Iteration 41020 => Loss: 6.69899995936451730927\n",
      "Iteration 41021 => Loss: 6.69899984858927233233\n",
      "Iteration 41022 => Loss: 6.69899973782896118735\n",
      "Iteration 41023 => Loss: 6.69899962708357143981\n",
      "Iteration 41024 => Loss: 6.69899951635310575426\n",
      "Iteration 41025 => Loss: 6.69899940563756324252\n",
      "Iteration 41026 => Loss: 6.69899929493693591098\n",
      "Iteration 41027 => Loss: 6.69899918425122464782\n",
      "Iteration 41028 => Loss: 6.69899907358043034122\n",
      "Iteration 41029 => Loss: 6.69899896292454499758\n",
      "Iteration 41030 => Loss: 6.69899885228357483413\n",
      "Iteration 41031 => Loss: 6.69899874165751718635\n",
      "Iteration 41032 => Loss: 6.69899863104635873157\n",
      "Iteration 41033 => Loss: 6.69899852045010746338\n",
      "Iteration 41034 => Loss: 6.69899840986876249360\n",
      "Iteration 41035 => Loss: 6.69899829930231671682\n",
      "Iteration 41036 => Loss: 6.69899818875077013303\n",
      "Iteration 41037 => Loss: 6.69899807821411652498\n",
      "Iteration 41038 => Loss: 6.69899796769236122174\n",
      "Iteration 41039 => Loss: 6.69899785718549445335\n",
      "Iteration 41040 => Loss: 6.69899774669352687795\n",
      "Iteration 41041 => Loss: 6.69899763621644250833\n",
      "Iteration 41042 => Loss: 6.69899752575424223267\n",
      "Iteration 41043 => Loss: 6.69899741530693848546\n",
      "Iteration 41044 => Loss: 6.69899730487450728589\n",
      "Iteration 41045 => Loss: 6.69899719445695662756\n",
      "Iteration 41046 => Loss: 6.69899708405428917501\n",
      "Iteration 41047 => Loss: 6.69899697366649693464\n",
      "Iteration 41048 => Loss: 6.69899686329358079462\n",
      "Iteration 41049 => Loss: 6.69899675293553364952\n",
      "Iteration 41050 => Loss: 6.69899664259235994024\n",
      "Iteration 41051 => Loss: 6.69899653226405344952\n",
      "Iteration 41052 => Loss: 6.69899642195061417738\n",
      "Iteration 41053 => Loss: 6.69899631165204212380\n",
      "Iteration 41054 => Loss: 6.69899620136832929518\n",
      "Iteration 41055 => Loss: 6.69899609109947657970\n",
      "Iteration 41056 => Loss: 6.69899598084548753008\n",
      "Iteration 41057 => Loss: 6.69899587060634882363\n",
      "Iteration 41058 => Loss: 6.69899576038207023032\n",
      "Iteration 41059 => Loss: 6.69899565017264464473\n",
      "Iteration 41060 => Loss: 6.69899553997806584960\n",
      "Iteration 41061 => Loss: 6.69899542979833384493\n",
      "Iteration 41062 => Loss: 6.69899531963345307162\n",
      "Iteration 41063 => Loss: 6.69899520948341908877\n",
      "Iteration 41064 => Loss: 6.69899509934822301460\n",
      "Iteration 41065 => Loss: 6.69899498922786662547\n",
      "Iteration 41066 => Loss: 6.69899487912235169773\n",
      "Iteration 41067 => Loss: 6.69899476903168000774\n",
      "Iteration 41068 => Loss: 6.69899465895583112740\n",
      "Iteration 41069 => Loss: 6.69899454889482104392\n",
      "Iteration 41070 => Loss: 6.69899443884864354004\n",
      "Iteration 41071 => Loss: 6.69899432881729062217\n",
      "Iteration 41072 => Loss: 6.69899421880076850755\n",
      "Iteration 41073 => Loss: 6.69899410879906831440\n",
      "Iteration 41074 => Loss: 6.69899399881219004271\n",
      "Iteration 41075 => Loss: 6.69899388884013280432\n",
      "Iteration 41076 => Loss: 6.69899377888289926375\n",
      "Iteration 41077 => Loss: 6.69899366894047343379\n",
      "Iteration 41078 => Loss: 6.69899355901287485437\n",
      "Iteration 41079 => Loss: 6.69899344910007599196\n",
      "Iteration 41080 => Loss: 6.69899333920209816284\n",
      "Iteration 41081 => Loss: 6.69899322931892005073\n",
      "Iteration 41082 => Loss: 6.69899311945054876105\n",
      "Iteration 41083 => Loss: 6.69899300959698784652\n",
      "Iteration 41084 => Loss: 6.69899289975823197807\n",
      "Iteration 41085 => Loss: 6.69899278993426960938\n",
      "Iteration 41086 => Loss: 6.69899268012511317494\n",
      "Iteration 41087 => Loss: 6.69899257033075379297\n",
      "Iteration 41088 => Loss: 6.69899246055118702259\n",
      "Iteration 41089 => Loss: 6.69899235078641108743\n",
      "Iteration 41090 => Loss: 6.69899224103642509931\n",
      "Iteration 41091 => Loss: 6.69899213130122728188\n",
      "Iteration 41092 => Loss: 6.69899202158081852332\n",
      "Iteration 41093 => Loss: 6.69899191187519349455\n",
      "Iteration 41094 => Loss: 6.69899180218435663647\n",
      "Iteration 41095 => Loss: 6.69899169250829107369\n",
      "Iteration 41096 => Loss: 6.69899158284701368160\n",
      "Iteration 41097 => Loss: 6.69899147320050492027\n",
      "Iteration 41098 => Loss: 6.69899136356877900056\n",
      "Iteration 41099 => Loss: 6.69899125395182259979\n",
      "Iteration 41100 => Loss: 6.69899114434963838249\n",
      "Iteration 41101 => Loss: 6.69899103476221746689\n",
      "Iteration 41102 => Loss: 6.69899092518956518205\n",
      "Iteration 41103 => Loss: 6.69899081563167797526\n",
      "Iteration 41104 => Loss: 6.69899070608856028741\n",
      "Iteration 41105 => Loss: 6.69899059656019790765\n",
      "Iteration 41106 => Loss: 6.69899048704659971776\n",
      "Iteration 41107 => Loss: 6.69899037754775594777\n",
      "Iteration 41108 => Loss: 6.69899026806366126863\n",
      "Iteration 41109 => Loss: 6.69899015859432367392\n",
      "Iteration 41110 => Loss: 6.69899004913973694642\n",
      "Iteration 41111 => Loss: 6.69898993969989842157\n",
      "Iteration 41112 => Loss: 6.69898983027480721120\n",
      "Iteration 41113 => Loss: 6.69898972086446242713\n",
      "Iteration 41114 => Loss: 6.69898961146886406937\n",
      "Iteration 41115 => Loss: 6.69898950208799970341\n",
      "Iteration 41116 => Loss: 6.69898939272187643468\n",
      "Iteration 41117 => Loss: 6.69898928337049515136\n",
      "Iteration 41118 => Loss: 6.69898917403384430713\n",
      "Iteration 41119 => Loss: 6.69898906471192656653\n",
      "Iteration 41120 => Loss: 6.69898895540474104138\n",
      "Iteration 41121 => Loss: 6.69898884611228417896\n",
      "Iteration 41122 => Loss: 6.69898873683455153838\n",
      "Iteration 41123 => Loss: 6.69898862757154844871\n",
      "Iteration 41124 => Loss: 6.69898851832327046907\n",
      "Iteration 41125 => Loss: 6.69898840908970960584\n",
      "Iteration 41126 => Loss: 6.69898829987086497084\n",
      "Iteration 41127 => Loss: 6.69898819066674455769\n",
      "Iteration 41128 => Loss: 6.69898808147734214913\n",
      "Iteration 41129 => Loss: 6.69898797230264264613\n",
      "Iteration 41130 => Loss: 6.69898786314265670683\n",
      "Iteration 41131 => Loss: 6.69898775399738433123\n",
      "Iteration 41132 => Loss: 6.69898764486681663755\n",
      "Iteration 41133 => Loss: 6.69898753575095629031\n",
      "Iteration 41134 => Loss: 6.69898742664979973682\n",
      "Iteration 41135 => Loss: 6.69898731756333631893\n",
      "Iteration 41136 => Loss: 6.69898720849158113566\n",
      "Iteration 41137 => Loss: 6.69898709943452086435\n",
      "Iteration 41138 => Loss: 6.69898699039215994588\n",
      "Iteration 41139 => Loss: 6.69898688136448239305\n",
      "Iteration 41140 => Loss: 6.69898677235150685760\n",
      "Iteration 41141 => Loss: 6.69898666335321912868\n",
      "Iteration 41142 => Loss: 6.69898655436962098264\n",
      "Iteration 41143 => Loss: 6.69898644540069732045\n",
      "Iteration 41144 => Loss: 6.69898633644646057661\n",
      "Iteration 41145 => Loss: 6.69898622750691696837\n",
      "Iteration 41146 => Loss: 6.69898611858204162672\n",
      "Iteration 41147 => Loss: 6.69898600967184965072\n",
      "Iteration 41148 => Loss: 6.69898590077632771767\n",
      "Iteration 41149 => Loss: 6.69898579189548293300\n",
      "Iteration 41150 => Loss: 6.69898568302931440854\n",
      "Iteration 41151 => Loss: 6.69898557417780704526\n",
      "Iteration 41152 => Loss: 6.69898546534097150129\n",
      "Iteration 41153 => Loss: 6.69898535651880155939\n",
      "Iteration 41154 => Loss: 6.69898524771129277866\n",
      "Iteration 41155 => Loss: 6.69898513891844338275\n",
      "Iteration 41156 => Loss: 6.69898503014026047708\n",
      "Iteration 41157 => Loss: 6.69898492137673873259\n",
      "Iteration 41158 => Loss: 6.69898481262786660295\n",
      "Iteration 41159 => Loss: 6.69898470389364408817\n",
      "Iteration 41160 => Loss: 6.69898459517408184638\n",
      "Iteration 41161 => Loss: 6.69898448646917099580\n",
      "Iteration 41162 => Loss: 6.69898437777889821376\n",
      "Iteration 41163 => Loss: 6.69898426910328126382\n",
      "Iteration 41164 => Loss: 6.69898416044230415878\n",
      "Iteration 41165 => Loss: 6.69898405179596423409\n",
      "Iteration 41166 => Loss: 6.69898394316426948336\n",
      "Iteration 41167 => Loss: 6.69898383454721635388\n",
      "Iteration 41168 => Loss: 6.69898372594479418751\n",
      "Iteration 41169 => Loss: 6.69898361735701008968\n",
      "Iteration 41170 => Loss: 6.69898350878385784313\n",
      "Iteration 41171 => Loss: 6.69898340022533300697\n",
      "Iteration 41172 => Loss: 6.69898329168143469303\n",
      "Iteration 41173 => Loss: 6.69898318315216734220\n",
      "Iteration 41174 => Loss: 6.69898307463751940816\n",
      "Iteration 41175 => Loss: 6.69898296613749355544\n",
      "Iteration 41176 => Loss: 6.69898285765209688947\n",
      "Iteration 41177 => Loss: 6.69898274918131342304\n",
      "Iteration 41178 => Loss: 6.69898264072514315615\n",
      "Iteration 41179 => Loss: 6.69898253228359408240\n",
      "Iteration 41180 => Loss: 6.69898242385665110277\n",
      "Iteration 41181 => Loss: 6.69898231544432132267\n",
      "Iteration 41182 => Loss: 6.69898220704659941305\n",
      "Iteration 41183 => Loss: 6.69898209866348359753\n",
      "Iteration 41184 => Loss: 6.69898199029497387613\n",
      "Iteration 41185 => Loss: 6.69898188194106758431\n",
      "Iteration 41186 => Loss: 6.69898177360176116935\n",
      "Iteration 41187 => Loss: 6.69898166527705551943\n",
      "Iteration 41188 => Loss: 6.69898155696694175276\n",
      "Iteration 41189 => Loss: 6.69898144867142431025\n",
      "Iteration 41190 => Loss: 6.69898134039049875099\n",
      "Iteration 41191 => Loss: 6.69898123212416685135\n",
      "Iteration 41192 => Loss: 6.69898112387242328225\n",
      "Iteration 41193 => Loss: 6.69898101563526715552\n",
      "Iteration 41194 => Loss: 6.69898090741269314208\n",
      "Iteration 41195 => Loss: 6.69898079920470657100\n",
      "Iteration 41196 => Loss: 6.69898069101129589598\n",
      "Iteration 41197 => Loss: 6.69898058283246733424\n",
      "Iteration 41198 => Loss: 6.69898047466821555673\n",
      "Iteration 41199 => Loss: 6.69898036651853789891\n",
      "Iteration 41200 => Loss: 6.69898025838343702532\n",
      "Iteration 41201 => Loss: 6.69898015026290227780\n",
      "Iteration 41202 => Loss: 6.69898004215694342633\n",
      "Iteration 41203 => Loss: 6.69897993406554626006\n",
      "Iteration 41204 => Loss: 6.69897982598871166715\n",
      "Iteration 41205 => Loss: 6.69897971792644764122\n",
      "Iteration 41206 => Loss: 6.69897960987874352412\n",
      "Iteration 41207 => Loss: 6.69897950184559753950\n",
      "Iteration 41208 => Loss: 6.69897939382701146371\n",
      "Iteration 41209 => Loss: 6.69897928582297819133\n",
      "Iteration 41210 => Loss: 6.69897917783349328147\n",
      "Iteration 41211 => Loss: 6.69897906985856650408\n",
      "Iteration 41212 => Loss: 6.69897896189819075374\n",
      "Iteration 41213 => Loss: 6.69897885395235359596\n",
      "Iteration 41214 => Loss: 6.69897874602106835340\n",
      "Iteration 41215 => Loss: 6.69897863810432703247\n",
      "Iteration 41216 => Loss: 6.69897853020213140951\n",
      "Iteration 41217 => Loss: 6.69897842231446816186\n",
      "Iteration 41218 => Loss: 6.69897831444134439494\n",
      "Iteration 41219 => Loss: 6.69897820658276366146\n",
      "Iteration 41220 => Loss: 6.69897809873871086239\n",
      "Iteration 41221 => Loss: 6.69897799090919132681\n",
      "Iteration 41222 => Loss: 6.69897788309419794928\n",
      "Iteration 41223 => Loss: 6.69897777529373605887\n",
      "Iteration 41224 => Loss: 6.69897766750780387923\n",
      "Iteration 41225 => Loss: 6.69897755973638897586\n",
      "Iteration 41226 => Loss: 6.69897745197949934237\n",
      "Iteration 41227 => Loss: 6.69897734423712787333\n",
      "Iteration 41228 => Loss: 6.69897723650927812145\n",
      "Iteration 41229 => Loss: 6.69897712879594831037\n",
      "Iteration 41230 => Loss: 6.69897702109712422924\n",
      "Iteration 41231 => Loss: 6.69897691341282186528\n",
      "Iteration 41232 => Loss: 6.69897680574302079037\n",
      "Iteration 41233 => Loss: 6.69897669808773610356\n",
      "Iteration 41234 => Loss: 6.69897659044694826491\n",
      "Iteration 41235 => Loss: 6.69897648282067148529\n",
      "Iteration 41236 => Loss: 6.69897637520889599472\n",
      "Iteration 41237 => Loss: 6.69897626761162268139\n",
      "Iteration 41238 => Loss: 6.69897616002885243347\n",
      "Iteration 41239 => Loss: 6.69897605246056926376\n",
      "Iteration 41240 => Loss: 6.69897594490678649493\n",
      "Iteration 41241 => Loss: 6.69897583736749702155\n",
      "Iteration 41242 => Loss: 6.69897572984269640273\n",
      "Iteration 41243 => Loss: 6.69897562233238286211\n",
      "Iteration 41244 => Loss: 6.69897551483655639970\n",
      "Iteration 41245 => Loss: 6.69897540735522234456\n",
      "Iteration 41246 => Loss: 6.69897529988836204495\n",
      "Iteration 41247 => Loss: 6.69897519243598971173\n",
      "Iteration 41248 => Loss: 6.69897508499809468674\n",
      "Iteration 41249 => Loss: 6.69897497757467696999\n",
      "Iteration 41250 => Loss: 6.69897487016573656149\n",
      "Iteration 41251 => Loss: 6.69897476277126369126\n",
      "Iteration 41252 => Loss: 6.69897465539126368839\n",
      "Iteration 41253 => Loss: 6.69897454802573921739\n",
      "Iteration 41254 => Loss: 6.69897444067467517925\n",
      "Iteration 41255 => Loss: 6.69897433333808045575\n",
      "Iteration 41256 => Loss: 6.69897422601594705327\n",
      "Iteration 41257 => Loss: 6.69897411870828118907\n",
      "Iteration 41258 => Loss: 6.69897401141506865230\n",
      "Iteration 41259 => Loss: 6.69897390413631743655\n",
      "Iteration 41260 => Loss: 6.69897379687202221277\n",
      "Iteration 41261 => Loss: 6.69897368962217587551\n",
      "Iteration 41262 => Loss: 6.69897358238678375386\n",
      "Iteration 41263 => Loss: 6.69897347516584407146\n",
      "Iteration 41264 => Loss: 6.69897336795935061105\n",
      "Iteration 41265 => Loss: 6.69897326076729804356\n",
      "Iteration 41266 => Loss: 6.69897315358970146804\n",
      "Iteration 41267 => Loss: 6.69897304642654223272\n",
      "Iteration 41268 => Loss: 6.69897293927781767309\n",
      "Iteration 41269 => Loss: 6.69897283214353311820\n",
      "Iteration 41270 => Loss: 6.69897272502368945624\n",
      "Iteration 41271 => Loss: 6.69897261791827691724\n",
      "Iteration 41272 => Loss: 6.69897251082729905391\n",
      "Iteration 41273 => Loss: 6.69897240375075053720\n",
      "Iteration 41274 => Loss: 6.69897229668862603802\n",
      "Iteration 41275 => Loss: 6.69897218964093976723\n",
      "Iteration 41276 => Loss: 6.69897208260767218491\n",
      "Iteration 41277 => Loss: 6.69897197558882151469\n",
      "Iteration 41278 => Loss: 6.69897186858439663837\n",
      "Iteration 41279 => Loss: 6.69897176159439133869\n",
      "Iteration 41280 => Loss: 6.69897165461880117476\n",
      "Iteration 41281 => Loss: 6.69897154765763058748\n",
      "Iteration 41282 => Loss: 6.69897144071086980688\n",
      "Iteration 41283 => Loss: 6.69897133377851172753\n",
      "Iteration 41284 => Loss: 6.69897122686057588936\n",
      "Iteration 41285 => Loss: 6.69897111995704452880\n",
      "Iteration 41286 => Loss: 6.69897101306791320496\n",
      "Iteration 41287 => Loss: 6.69897090619319168781\n",
      "Iteration 41288 => Loss: 6.69897079933286665465\n",
      "Iteration 41289 => Loss: 6.69897069248693899368\n",
      "Iteration 41290 => Loss: 6.69897058565540870489\n",
      "Iteration 41291 => Loss: 6.69897047883828200554\n",
      "Iteration 41292 => Loss: 6.69897037203554379658\n",
      "Iteration 41293 => Loss: 6.69897026524719851892\n",
      "Iteration 41294 => Loss: 6.69897015847324173166\n",
      "Iteration 41295 => Loss: 6.69897005171367165843\n",
      "Iteration 41296 => Loss: 6.69896994496849274014\n",
      "Iteration 41297 => Loss: 6.69896983823768810140\n",
      "Iteration 41298 => Loss: 6.69896973152127372941\n",
      "Iteration 41299 => Loss: 6.69896962481923630151\n",
      "Iteration 41300 => Loss: 6.69896951813157848221\n",
      "Iteration 41301 => Loss: 6.69896941145829227793\n",
      "Iteration 41302 => Loss: 6.69896930479938923497\n",
      "Iteration 41303 => Loss: 6.69896919815484626071\n",
      "Iteration 41304 => Loss: 6.69896909152468200688\n",
      "Iteration 41305 => Loss: 6.69896898490888581534\n",
      "Iteration 41306 => Loss: 6.69896887830745857428\n",
      "Iteration 41307 => Loss: 6.69896877172039317827\n",
      "Iteration 41308 => Loss: 6.69896866514768785095\n",
      "Iteration 41309 => Loss: 6.69896855858934170413\n",
      "Iteration 41310 => Loss: 6.69896845204535917873\n",
      "Iteration 41311 => Loss: 6.69896834551573139294\n",
      "Iteration 41312 => Loss: 6.69896823900046278766\n",
      "Iteration 41313 => Loss: 6.69896813249954092839\n",
      "Iteration 41314 => Loss: 6.69896802601297558510\n",
      "Iteration 41315 => Loss: 6.69896791954075165876\n",
      "Iteration 41316 => Loss: 6.69896781308287891932\n",
      "Iteration 41317 => Loss: 6.69896770663935114953\n",
      "Iteration 41318 => Loss: 6.69896760021016923758\n",
      "Iteration 41319 => Loss: 6.69896749379533051894\n",
      "Iteration 41320 => Loss: 6.69896738739482611180\n",
      "Iteration 41321 => Loss: 6.69896728100865779254\n",
      "Iteration 41322 => Loss: 6.69896717463682467297\n",
      "Iteration 41323 => Loss: 6.69896706827933119399\n",
      "Iteration 41324 => Loss: 6.69896696193616758563\n",
      "Iteration 41325 => Loss: 6.69896685560733828879\n",
      "Iteration 41326 => Loss: 6.69896674929282998079\n",
      "Iteration 41327 => Loss: 6.69896664299264177345\n",
      "Iteration 41328 => Loss: 6.69896653670678521308\n",
      "Iteration 41329 => Loss: 6.69896643043525141792\n",
      "Iteration 41330 => Loss: 6.69896632417803061799\n",
      "Iteration 41331 => Loss: 6.69896621793513880050\n",
      "Iteration 41332 => Loss: 6.69896611170655997824\n",
      "Iteration 41333 => Loss: 6.69896600549229326305\n",
      "Iteration 41334 => Loss: 6.69896589929233599037\n",
      "Iteration 41335 => Loss: 6.69896579310668993656\n",
      "Iteration 41336 => Loss: 6.69896568693536131889\n",
      "Iteration 41337 => Loss: 6.69896558077832882105\n",
      "Iteration 41338 => Loss: 6.69896547463560754210\n",
      "Iteration 41339 => Loss: 6.69896536850718593570\n",
      "Iteration 41340 => Loss: 6.69896526239306222550\n",
      "Iteration 41341 => Loss: 6.69896515629324618146\n",
      "Iteration 41342 => Loss: 6.69896505020771826366\n",
      "Iteration 41343 => Loss: 6.69896494413648824207\n",
      "Iteration 41344 => Loss: 6.69896483807955789302\n",
      "Iteration 41345 => Loss: 6.69896473203690856479\n",
      "Iteration 41346 => Loss: 6.69896462600855535641\n",
      "Iteration 41347 => Loss: 6.69896451999448405701\n",
      "Iteration 41348 => Loss: 6.69896441399470177203\n",
      "Iteration 41349 => Loss: 6.69896430800920317239\n",
      "Iteration 41350 => Loss: 6.69896420203798648174\n",
      "Iteration 41351 => Loss: 6.69896409608104370648\n",
      "Iteration 41352 => Loss: 6.69896399013838372838\n",
      "Iteration 41353 => Loss: 6.69896388421000210656\n",
      "Iteration 41354 => Loss: 6.69896377829588463015\n",
      "Iteration 41355 => Loss: 6.69896367239605083910\n",
      "Iteration 41356 => Loss: 6.69896356651047941710\n",
      "Iteration 41357 => Loss: 6.69896346063917480507\n",
      "Iteration 41358 => Loss: 6.69896335478214322023\n",
      "Iteration 41359 => Loss: 6.69896324893937222811\n",
      "Iteration 41360 => Loss: 6.69896314311086449322\n",
      "Iteration 41361 => Loss: 6.69896303729661379833\n",
      "Iteration 41362 => Loss: 6.69896293149662014343\n",
      "Iteration 41363 => Loss: 6.69896282571089507485\n",
      "Iteration 41364 => Loss: 6.69896271993941727629\n",
      "Iteration 41365 => Loss: 6.69896261418218763595\n",
      "Iteration 41366 => Loss: 6.69896250843921503559\n",
      "Iteration 41367 => Loss: 6.69896240271048970527\n",
      "Iteration 41368 => Loss: 6.69896229699601164498\n",
      "Iteration 41369 => Loss: 6.69896219129577641382\n",
      "Iteration 41370 => Loss: 6.69896208560978312363\n",
      "Iteration 41371 => Loss: 6.69896197993803621529\n",
      "Iteration 41372 => Loss: 6.69896187428052769519\n",
      "Iteration 41373 => Loss: 6.69896176863724956974\n",
      "Iteration 41374 => Loss: 6.69896166300821427342\n",
      "Iteration 41375 => Loss: 6.69896155739340848356\n",
      "Iteration 41376 => Loss: 6.69896145179284374649\n",
      "Iteration 41377 => Loss: 6.69896134620649252867\n",
      "Iteration 41378 => Loss: 6.69896124063438058727\n",
      "Iteration 41379 => Loss: 6.69896113507649015872\n",
      "Iteration 41380 => Loss: 6.69896102953282479575\n",
      "Iteration 41381 => Loss: 6.69896092400338449835\n",
      "Iteration 41382 => Loss: 6.69896081848816660198\n",
      "Iteration 41383 => Loss: 6.69896071298715511944\n",
      "Iteration 41384 => Loss: 6.69896060750036426157\n",
      "Iteration 41385 => Loss: 6.69896050202778603477\n",
      "Iteration 41386 => Loss: 6.69896039656943287355\n",
      "Iteration 41387 => Loss: 6.69896029112528346161\n",
      "Iteration 41388 => Loss: 6.69896018569533779896\n",
      "Iteration 41389 => Loss: 6.69896008027960121467\n",
      "Iteration 41390 => Loss: 6.69895997487807282056\n",
      "Iteration 41391 => Loss: 6.69895986949074373484\n",
      "Iteration 41392 => Loss: 6.69895976411761484570\n",
      "Iteration 41393 => Loss: 6.69895965875868171224\n",
      "Iteration 41394 => Loss: 6.69895955341395143989\n",
      "Iteration 41395 => Loss: 6.69895944808341692323\n",
      "Iteration 41396 => Loss: 6.69895934276707194499\n",
      "Iteration 41397 => Loss: 6.69895923746492005790\n",
      "Iteration 41398 => Loss: 6.69895913217696303832\n",
      "Iteration 41399 => Loss: 6.69895902690318667538\n",
      "Iteration 41400 => Loss: 6.69895892164359896270\n",
      "Iteration 41401 => Loss: 6.69895881639819101849\n",
      "Iteration 41402 => Loss: 6.69895871116696817182\n",
      "Iteration 41403 => Loss: 6.69895860594992686998\n",
      "Iteration 41404 => Loss: 6.69895850074706444843\n",
      "Iteration 41405 => Loss: 6.69895839555836758450\n",
      "Iteration 41406 => Loss: 6.69895829038385492993\n",
      "Iteration 41407 => Loss: 6.69895818522351316204\n",
      "Iteration 41408 => Loss: 6.69895808007734316902\n",
      "Iteration 41409 => Loss: 6.69895797494533606908\n",
      "Iteration 41410 => Loss: 6.69895786982750518490\n",
      "Iteration 41411 => Loss: 6.69895776472382653566\n",
      "Iteration 41412 => Loss: 6.69895765963431966128\n",
      "Iteration 41413 => Loss: 6.69895755455897035091\n",
      "Iteration 41414 => Loss: 6.69895744949778126909\n",
      "Iteration 41415 => Loss: 6.69895734445074353403\n",
      "Iteration 41416 => Loss: 6.69895723941786780387\n",
      "Iteration 41417 => Loss: 6.69895713439914608500\n",
      "Iteration 41418 => Loss: 6.69895702939456860747\n",
      "Iteration 41419 => Loss: 6.69895692440414425306\n",
      "Iteration 41420 => Loss: 6.69895681942786591634\n",
      "Iteration 41421 => Loss: 6.69895671446574247909\n",
      "Iteration 41422 => Loss: 6.69895660951774907232\n",
      "Iteration 41423 => Loss: 6.69895650458390168325\n",
      "Iteration 41424 => Loss: 6.69895639966420031186\n",
      "Iteration 41425 => Loss: 6.69895629475863163549\n",
      "Iteration 41426 => Loss: 6.69895618986720275956\n",
      "Iteration 41427 => Loss: 6.69895608498989769686\n",
      "Iteration 41428 => Loss: 6.69895598012673865185\n",
      "Iteration 41429 => Loss: 6.69895587527770075553\n",
      "Iteration 41430 => Loss: 6.69895577044279111334\n",
      "Iteration 41431 => Loss: 6.69895566562201505434\n",
      "Iteration 41432 => Loss: 6.69895556081535215043\n",
      "Iteration 41433 => Loss: 6.69895545602282371789\n",
      "Iteration 41434 => Loss: 6.69895535124441021679\n",
      "Iteration 41435 => Loss: 6.69895524648011519986\n",
      "Iteration 41436 => Loss: 6.69895514172993955526\n",
      "Iteration 41437 => Loss: 6.69895503699387973029\n",
      "Iteration 41438 => Loss: 6.69895493227192950769\n",
      "Iteration 41439 => Loss: 6.69895482756409332836\n",
      "Iteration 41440 => Loss: 6.69895472287036586323\n",
      "Iteration 41441 => Loss: 6.69895461819074800047\n",
      "Iteration 41442 => Loss: 6.69895451352523707556\n",
      "Iteration 41443 => Loss: 6.69895440887382598305\n",
      "Iteration 41444 => Loss: 6.69895430423651383478\n",
      "Iteration 41445 => Loss: 6.69895419961330595982\n",
      "Iteration 41446 => Loss: 6.69895409500419436455\n",
      "Iteration 41447 => Loss: 6.69895399040917816080\n",
      "Iteration 41448 => Loss: 6.69895388582826267765\n",
      "Iteration 41449 => Loss: 6.69895378126143103970\n",
      "Iteration 41450 => Loss: 6.69895367670868857601\n",
      "Iteration 41451 => Loss: 6.69895357217004416839\n",
      "Iteration 41452 => Loss: 6.69895346764547827689\n",
      "Iteration 41453 => Loss: 6.69895336313500155967\n",
      "Iteration 41454 => Loss: 6.69895325863860247040\n",
      "Iteration 41455 => Loss: 6.69895315415628722633\n",
      "Iteration 41456 => Loss: 6.69895304968805405110\n",
      "Iteration 41457 => Loss: 6.69895294523389406294\n",
      "Iteration 41458 => Loss: 6.69895284079381614362\n",
      "Iteration 41459 => Loss: 6.69895273636780697046\n",
      "Iteration 41460 => Loss: 6.69895263195586565530\n",
      "Iteration 41461 => Loss: 6.69895252755799663902\n",
      "Iteration 41462 => Loss: 6.69895242317419281619\n",
      "Iteration 41463 => Loss: 6.69895231880445241046\n",
      "Iteration 41464 => Loss: 6.69895221444878607997\n",
      "Iteration 41465 => Loss: 6.69895211010717783751\n",
      "Iteration 41466 => Loss: 6.69895200577963123578\n",
      "Iteration 41467 => Loss: 6.69895190146613739302\n",
      "Iteration 41468 => Loss: 6.69895179716669630920\n",
      "Iteration 41469 => Loss: 6.69895169288132219521\n",
      "Iteration 41470 => Loss: 6.69895158860999639927\n",
      "Iteration 41471 => Loss: 6.69895148435271003962\n",
      "Iteration 41472 => Loss: 6.69895138010948176799\n",
      "Iteration 41473 => Loss: 6.69895127588030092625\n",
      "Iteration 41474 => Loss: 6.69895117166516396168\n",
      "Iteration 41475 => Loss: 6.69895106746406554521\n",
      "Iteration 41476 => Loss: 6.69895096327701633498\n",
      "Iteration 41477 => Loss: 6.69895085910399856743\n",
      "Iteration 41478 => Loss: 6.69895075494502201252\n",
      "Iteration 41479 => Loss: 6.69895065080008222935\n",
      "Iteration 41480 => Loss: 6.69895054666917300068\n",
      "Iteration 41481 => Loss: 6.69895044255230054375\n",
      "Iteration 41482 => Loss: 6.69895033844945508861\n",
      "Iteration 41483 => Loss: 6.69895023436063308253\n",
      "Iteration 41484 => Loss: 6.69895013028583807824\n",
      "Iteration 41485 => Loss: 6.69895002622506829937\n",
      "Iteration 41486 => Loss: 6.69894992217832108139\n",
      "Iteration 41487 => Loss: 6.69894981814559553612\n",
      "Iteration 41488 => Loss: 6.69894971412689255175\n",
      "Iteration 41489 => Loss: 6.69894961012219525287\n",
      "Iteration 41490 => Loss: 6.69894950613151785035\n",
      "Iteration 41491 => Loss: 6.69894940215485679147\n",
      "Iteration 41492 => Loss: 6.69894929819220052991\n",
      "Iteration 41493 => Loss: 6.69894919424355705928\n",
      "Iteration 41494 => Loss: 6.69894909030892282686\n",
      "Iteration 41495 => Loss: 6.69894898638829339177\n",
      "Iteration 41496 => Loss: 6.69894888248165898403\n",
      "Iteration 41497 => Loss: 6.69894877858903736723\n",
      "Iteration 41498 => Loss: 6.69894867471040544871\n",
      "Iteration 41499 => Loss: 6.69894857084578010387\n",
      "Iteration 41500 => Loss: 6.69894846699514179278\n",
      "Iteration 41501 => Loss: 6.69894836315850294994\n",
      "Iteration 41502 => Loss: 6.69894825933585469357\n",
      "Iteration 41503 => Loss: 6.69894815552719702367\n",
      "Iteration 41504 => Loss: 6.69894805173252727570\n",
      "Iteration 41505 => Loss: 6.69894794795184278513\n",
      "Iteration 41506 => Loss: 6.69894784418514532831\n",
      "Iteration 41507 => Loss: 6.69894774043242691164\n",
      "Iteration 41508 => Loss: 6.69894763669368842329\n",
      "Iteration 41509 => Loss: 6.69894753296893252781\n",
      "Iteration 41510 => Loss: 6.69894742925815300794\n",
      "Iteration 41511 => Loss: 6.69894732556134808732\n",
      "Iteration 41512 => Loss: 6.69894722187851687778\n",
      "Iteration 41513 => Loss: 6.69894711820965760296\n",
      "Iteration 41514 => Loss: 6.69894701455476671015\n",
      "Iteration 41515 => Loss: 6.69894691091383887027\n",
      "Iteration 41516 => Loss: 6.69894680728688651783\n",
      "Iteration 41517 => Loss: 6.69894670367389100107\n",
      "Iteration 41518 => Loss: 6.69894660007485764908\n",
      "Iteration 41519 => Loss: 6.69894649648978557366\n",
      "Iteration 41520 => Loss: 6.69894639291867388664\n",
      "Iteration 41521 => Loss: 6.69894628936151015353\n",
      "Iteration 41522 => Loss: 6.69894618581830503246\n",
      "Iteration 41523 => Loss: 6.69894608228905674707\n",
      "Iteration 41524 => Loss: 6.69894597877375286288\n",
      "Iteration 41525 => Loss: 6.69894587527240226166\n",
      "Iteration 41526 => Loss: 6.69894577178499339709\n",
      "Iteration 41527 => Loss: 6.69894566831153870368\n",
      "Iteration 41528 => Loss: 6.69894556485202219420\n",
      "Iteration 41529 => Loss: 6.69894546140644120413\n",
      "Iteration 41530 => Loss: 6.69894535797480994432\n",
      "Iteration 41531 => Loss: 6.69894525455710176942\n",
      "Iteration 41532 => Loss: 6.69894515115334687749\n",
      "Iteration 41533 => Loss: 6.69894504776350974140\n",
      "Iteration 41534 => Loss: 6.69894494438760901289\n",
      "Iteration 41535 => Loss: 6.69894484102564202743\n",
      "Iteration 41536 => Loss: 6.69894473767760612049\n",
      "Iteration 41537 => Loss: 6.69894463434348974573\n",
      "Iteration 41538 => Loss: 6.69894453102329734406\n",
      "Iteration 41539 => Loss: 6.69894442771703424455\n",
      "Iteration 41540 => Loss: 6.69894432442468357181\n",
      "Iteration 41541 => Loss: 6.69894422114625687215\n",
      "Iteration 41542 => Loss: 6.69894411788174704014\n",
      "Iteration 41543 => Loss: 6.69894401463114785855\n",
      "Iteration 41544 => Loss: 6.69894391139446909733\n",
      "Iteration 41545 => Loss: 6.69894380817169210474\n",
      "Iteration 41546 => Loss: 6.69894370496282931526\n",
      "Iteration 41547 => Loss: 6.69894360176787095895\n",
      "Iteration 41548 => Loss: 6.69894349858682325305\n",
      "Iteration 41549 => Loss: 6.69894339541967553942\n",
      "Iteration 41550 => Loss: 6.69894329226642870623\n",
      "Iteration 41551 => Loss: 6.69894318912708452984\n",
      "Iteration 41552 => Loss: 6.69894308600163856937\n",
      "Iteration 41553 => Loss: 6.69894298289008460756\n",
      "Iteration 41554 => Loss: 6.69894287979243063802\n",
      "Iteration 41555 => Loss: 6.69894277670866689078\n",
      "Iteration 41556 => Loss: 6.69894267363879425403\n",
      "Iteration 41557 => Loss: 6.69894257058281006323\n",
      "Iteration 41558 => Loss: 6.69894246754070898930\n",
      "Iteration 41559 => Loss: 6.69894236451249547315\n",
      "Iteration 41560 => Loss: 6.69894226149816685023\n",
      "Iteration 41561 => Loss: 6.69894215849771779148\n",
      "Iteration 41562 => Loss: 6.69894205551114296782\n",
      "Iteration 41563 => Loss: 6.69894195253845570193\n",
      "Iteration 41564 => Loss: 6.69894184957963645388\n",
      "Iteration 41565 => Loss: 6.69894174663469588182\n",
      "Iteration 41566 => Loss: 6.69894164370362332761\n",
      "Iteration 41567 => Loss: 6.69894154078642323213\n",
      "Iteration 41568 => Loss: 6.69894143788308671361\n",
      "Iteration 41569 => Loss: 6.69894133499362176565\n",
      "Iteration 41570 => Loss: 6.69894123211801773010\n",
      "Iteration 41571 => Loss: 6.69894112925627727151\n",
      "Iteration 41572 => Loss: 6.69894102640840127805\n",
      "Iteration 41573 => Loss: 6.69894092357437287433\n",
      "Iteration 41574 => Loss: 6.69894082075421870570\n",
      "Iteration 41575 => Loss: 6.69894071794790946228\n",
      "Iteration 41576 => Loss: 6.69894061515544869678\n",
      "Iteration 41577 => Loss: 6.69894051237684706734\n",
      "Iteration 41578 => Loss: 6.69894040961209125129\n",
      "Iteration 41579 => Loss: 6.69894030686118391316\n",
      "Iteration 41580 => Loss: 6.69894020412411972387\n",
      "Iteration 41581 => Loss: 6.69894010140090578886\n",
      "Iteration 41582 => Loss: 6.69893999869152612092\n",
      "Iteration 41583 => Loss: 6.69893989599599581908\n",
      "Iteration 41584 => Loss: 6.69893979331429534341\n",
      "Iteration 41585 => Loss: 6.69893969064643535205\n",
      "Iteration 41586 => Loss: 6.69893958799240785140\n",
      "Iteration 41587 => Loss: 6.69893948535221728235\n",
      "Iteration 41588 => Loss: 6.69893938272585121041\n",
      "Iteration 41589 => Loss: 6.69893928011331762917\n",
      "Iteration 41590 => Loss: 6.69893917751461120957\n",
      "Iteration 41591 => Loss: 6.69893907492972839890\n",
      "Iteration 41592 => Loss: 6.69893897235867186168\n",
      "Iteration 41593 => Loss: 6.69893886980143271614\n",
      "Iteration 41594 => Loss: 6.69893876725801540317\n",
      "Iteration 41595 => Loss: 6.69893866472841548187\n",
      "Iteration 41596 => Loss: 6.69893856221263206407\n",
      "Iteration 41597 => Loss: 6.69893845971066248524\n",
      "Iteration 41598 => Loss: 6.69893835722250052811\n",
      "Iteration 41599 => Loss: 6.69893825474815507448\n",
      "Iteration 41600 => Loss: 6.69893815228761901892\n",
      "Iteration 41601 => Loss: 6.69893804984088436782\n",
      "Iteration 41602 => Loss: 6.69893794740795467391\n",
      "Iteration 41603 => Loss: 6.69893784498883260170\n",
      "Iteration 41604 => Loss: 6.69893774258350127582\n",
      "Iteration 41605 => Loss: 6.69893764019198201254\n",
      "Iteration 41606 => Loss: 6.69893753781424816651\n",
      "Iteration 41607 => Loss: 6.69893743545031927766\n",
      "Iteration 41608 => Loss: 6.69893733310018202332\n",
      "Iteration 41609 => Loss: 6.69893723076383551529\n",
      "Iteration 41610 => Loss: 6.69893712844127797723\n",
      "Iteration 41611 => Loss: 6.69893702613250940914\n",
      "Iteration 41612 => Loss: 6.69893692383752714647\n",
      "Iteration 41613 => Loss: 6.69893682155632852471\n",
      "Iteration 41614 => Loss: 6.69893671928891176748\n",
      "Iteration 41615 => Loss: 6.69893661703526976936\n",
      "Iteration 41616 => Loss: 6.69893651479542029392\n",
      "Iteration 41617 => Loss: 6.69893641256933580763\n",
      "Iteration 41618 => Loss: 6.69893631035702963317\n",
      "Iteration 41619 => Loss: 6.69893620815850621142\n",
      "Iteration 41620 => Loss: 6.69893610597373534432\n",
      "Iteration 41621 => Loss: 6.69893600380274456541\n",
      "Iteration 41622 => Loss: 6.69893590164552055199\n",
      "Iteration 41623 => Loss: 6.69893579950206241591\n",
      "Iteration 41624 => Loss: 6.69893569737236926898\n",
      "Iteration 41625 => Loss: 6.69893559525643844665\n",
      "Iteration 41626 => Loss: 6.69893549315426106716\n",
      "Iteration 41627 => Loss: 6.69893539106584601228\n",
      "Iteration 41628 => Loss: 6.69893528899119150566\n",
      "Iteration 41629 => Loss: 6.69893518693029488276\n",
      "Iteration 41630 => Loss: 6.69893508488313571547\n",
      "Iteration 41631 => Loss: 6.69893498284973532009\n",
      "Iteration 41632 => Loss: 6.69893488083008836753\n",
      "Iteration 41633 => Loss: 6.69893477882418597602\n",
      "Iteration 41634 => Loss: 6.69893467683202725738\n",
      "Iteration 41635 => Loss: 6.69893457485361221160\n",
      "Iteration 41636 => Loss: 6.69893447288893728597\n",
      "Iteration 41637 => Loss: 6.69893437093800603321\n",
      "Iteration 41638 => Loss: 6.69893426900080690700\n",
      "Iteration 41639 => Loss: 6.69893416707735323001\n",
      "Iteration 41640 => Loss: 6.69893406516762812686\n",
      "Iteration 41641 => Loss: 6.69893396327163337389\n",
      "Iteration 41642 => Loss: 6.69893386138936808294\n",
      "Iteration 41643 => Loss: 6.69893375952083669489\n",
      "Iteration 41644 => Loss: 6.69893365766603210432\n",
      "Iteration 41645 => Loss: 6.69893355582494542944\n",
      "Iteration 41646 => Loss: 6.69893345399758732839\n",
      "Iteration 41647 => Loss: 6.69893335218395069575\n",
      "Iteration 41648 => Loss: 6.69893325038402753790\n",
      "Iteration 41649 => Loss: 6.69893314859783028936\n",
      "Iteration 41650 => Loss: 6.69893304682534740380\n",
      "Iteration 41651 => Loss: 6.69893294506657976939\n",
      "Iteration 41652 => Loss: 6.69893284332151761618\n",
      "Iteration 41653 => Loss: 6.69893274159016893776\n",
      "Iteration 41654 => Loss: 6.69893263987252485236\n",
      "Iteration 41655 => Loss: 6.69893253816859512995\n",
      "Iteration 41656 => Loss: 6.69893243647836111876\n",
      "Iteration 41657 => Loss: 6.69893233480183614148\n",
      "Iteration 41658 => Loss: 6.69893223313901309268\n",
      "Iteration 41659 => Loss: 6.69893213148988397876\n",
      "Iteration 41660 => Loss: 6.69893202985445679332\n",
      "Iteration 41661 => Loss: 6.69893192823272087821\n",
      "Iteration 41662 => Loss: 6.69893182662467889799\n",
      "Iteration 41663 => Loss: 6.69893172503032996445\n",
      "Iteration 41664 => Loss: 6.69893162344966963673\n",
      "Iteration 41665 => Loss: 6.69893152188270146752\n",
      "Iteration 41666 => Loss: 6.69893142032941568687\n",
      "Iteration 41667 => Loss: 6.69893131878981407112\n",
      "Iteration 41668 => Loss: 6.69893121726389395576\n",
      "Iteration 41669 => Loss: 6.69893111575166066984\n",
      "Iteration 41670 => Loss: 6.69893101425309911434\n",
      "Iteration 41671 => Loss: 6.69893091276822261193\n",
      "Iteration 41672 => Loss: 6.69893081129701428722\n",
      "Iteration 41673 => Loss: 6.69893070983947769292\n",
      "Iteration 41674 => Loss: 6.69893060839561904629\n",
      "Iteration 41675 => Loss: 6.69893050696542413647\n",
      "Iteration 41676 => Loss: 6.69893040554889651617\n",
      "Iteration 41677 => Loss: 6.69893030414603973810\n",
      "Iteration 41678 => Loss: 6.69893020275684136777\n",
      "Iteration 41679 => Loss: 6.69893010138131117515\n",
      "Iteration 41680 => Loss: 6.69893000001944116661\n",
      "Iteration 41681 => Loss: 6.69892989867122246039\n",
      "Iteration 41682 => Loss: 6.69892979733666393827\n",
      "Iteration 41683 => Loss: 6.69892969601576737659\n",
      "Iteration 41684 => Loss: 6.69892959470851323545\n",
      "Iteration 41685 => Loss: 6.69892949341491661386\n",
      "Iteration 41686 => Loss: 6.69892939213496685369\n",
      "Iteration 41687 => Loss: 6.69892929086866750765\n",
      "Iteration 41688 => Loss: 6.69892918961601591121\n",
      "Iteration 41689 => Loss: 6.69892908837699785352\n",
      "Iteration 41690 => Loss: 6.69892898715162932177\n",
      "Iteration 41691 => Loss: 6.69892888593990143420\n",
      "Iteration 41692 => Loss: 6.69892878474181330262\n",
      "Iteration 41693 => Loss: 6.69892868355735693342\n",
      "Iteration 41694 => Loss: 6.69892858238653410297\n",
      "Iteration 41695 => Loss: 6.69892848122934569943\n",
      "Iteration 41696 => Loss: 6.69892838008578728193\n",
      "Iteration 41697 => Loss: 6.69892827895585796227\n",
      "Iteration 41698 => Loss: 6.69892817783956129318\n",
      "Iteration 41699 => Loss: 6.69892807673688661652\n",
      "Iteration 41700 => Loss: 6.69892797564783748498\n",
      "Iteration 41701 => Loss: 6.69892787457240324045\n",
      "Iteration 41702 => Loss: 6.69892777351059542923\n",
      "Iteration 41703 => Loss: 6.69892767246240339318\n",
      "Iteration 41704 => Loss: 6.69892757142782624413\n",
      "Iteration 41705 => Loss: 6.69892747040686842297\n",
      "Iteration 41706 => Loss: 6.69892736939951927155\n",
      "Iteration 41707 => Loss: 6.69892726840577878988\n",
      "Iteration 41708 => Loss: 6.69892716742565141885\n",
      "Iteration 41709 => Loss: 6.69892706645912650032\n",
      "Iteration 41710 => Loss: 6.69892696550621558060\n",
      "Iteration 41711 => Loss: 6.69892686456690533703\n",
      "Iteration 41712 => Loss: 6.69892676364119310506\n",
      "Iteration 41713 => Loss: 6.69892666272907977287\n",
      "Iteration 41714 => Loss: 6.69892656183056534047\n",
      "Iteration 41715 => Loss: 6.69892646094564714332\n",
      "Iteration 41716 => Loss: 6.69892636007432251688\n",
      "Iteration 41717 => Loss: 6.69892625921659057298\n",
      "Iteration 41718 => Loss: 6.69892615837244953525\n",
      "Iteration 41719 => Loss: 6.69892605754190473277\n",
      "Iteration 41720 => Loss: 6.69892595672493484926\n",
      "Iteration 41721 => Loss: 6.69892585592155587193\n",
      "Iteration 41722 => Loss: 6.69892575513175891899\n",
      "Iteration 41723 => Loss: 6.69892565435554576680\n",
      "Iteration 41724 => Loss: 6.69892555359291108630\n",
      "Iteration 41725 => Loss: 6.69892545284385221294\n",
      "Iteration 41726 => Loss: 6.69892535210837269943\n",
      "Iteration 41727 => Loss: 6.69892525138646099947\n",
      "Iteration 41728 => Loss: 6.69892515067812510665\n",
      "Iteration 41729 => Loss: 6.69892504998336946187\n",
      "Iteration 41730 => Loss: 6.69892494930216830795\n",
      "Iteration 41731 => Loss: 6.69892484863453763211\n",
      "Iteration 41732 => Loss: 6.69892474798047476980\n",
      "Iteration 41733 => Loss: 6.69892464733997083925\n",
      "Iteration 41734 => Loss: 6.69892454671303028135\n",
      "Iteration 41735 => Loss: 6.69892444609965487246\n",
      "Iteration 41736 => Loss: 6.69892434549983040171\n",
      "Iteration 41737 => Loss: 6.69892424491356752725\n",
      "Iteration 41738 => Loss: 6.69892414434085292640\n",
      "Iteration 41739 => Loss: 6.69892404378169104007\n",
      "Iteration 41740 => Loss: 6.69892394323607920370\n",
      "Iteration 41741 => Loss: 6.69892384270401120006\n",
      "Iteration 41742 => Loss: 6.69892374218550568088\n",
      "Iteration 41743 => Loss: 6.69892364168053067175\n",
      "Iteration 41744 => Loss: 6.69892354118910571259\n",
      "Iteration 41745 => Loss: 6.69892344071122280980\n",
      "Iteration 41746 => Loss: 6.69892334024687663430\n",
      "Iteration 41747 => Loss: 6.69892323979606274520\n",
      "Iteration 41748 => Loss: 6.69892313935878469522\n",
      "Iteration 41749 => Loss: 6.69892303893505136614\n",
      "Iteration 41750 => Loss: 6.69892293852484321803\n",
      "Iteration 41751 => Loss: 6.69892283812816380362\n",
      "Iteration 41752 => Loss: 6.69892273774501934014\n",
      "Iteration 41753 => Loss: 6.69892263737539739310\n",
      "Iteration 41754 => Loss: 6.69892253701930151522\n",
      "Iteration 41755 => Loss: 6.69892243667673081831\n",
      "Iteration 41756 => Loss: 6.69892233634767197969\n",
      "Iteration 41757 => Loss: 6.69892223603214098659\n",
      "Iteration 41758 => Loss: 6.69892213573012629269\n",
      "Iteration 41759 => Loss: 6.69892203544162079254\n",
      "Iteration 41760 => Loss: 6.69892193516663514430\n",
      "Iteration 41761 => Loss: 6.69892183490515957800\n",
      "Iteration 41762 => Loss: 6.69892173465719942271\n",
      "Iteration 41763 => Loss: 6.69892163442274402030\n",
      "Iteration 41764 => Loss: 6.69892153420179603529\n",
      "Iteration 41765 => Loss: 6.69892143399435280315\n",
      "Iteration 41766 => Loss: 6.69892133380041254753\n",
      "Iteration 41767 => Loss: 6.69892123361996727482\n",
      "Iteration 41768 => Loss: 6.69892113345302853133\n",
      "Iteration 41769 => Loss: 6.69892103329958299440\n",
      "Iteration 41770 => Loss: 6.69892093315963776945\n",
      "Iteration 41771 => Loss: 6.69892083303318663923\n",
      "Iteration 41772 => Loss: 6.69892073292022072195\n",
      "Iteration 41773 => Loss: 6.69892063282075245212\n",
      "Iteration 41774 => Loss: 6.69892053273476584252\n",
      "Iteration 41775 => Loss: 6.69892043266227332765\n",
      "Iteration 41776 => Loss: 6.69892033260325625577\n",
      "Iteration 41777 => Loss: 6.69892023255772972590\n",
      "Iteration 41778 => Loss: 6.69892013252568130355\n",
      "Iteration 41779 => Loss: 6.69892003250711454143\n",
      "Iteration 41780 => Loss: 6.69891993250202677501\n",
      "Iteration 41781 => Loss: 6.69891983251040645797\n",
      "Iteration 41782 => Loss: 6.69891973253226513663\n",
      "Iteration 41783 => Loss: 6.69891963256759837009\n",
      "Iteration 41784 => Loss: 6.69891953261639461203\n",
      "Iteration 41785 => Loss: 6.69891943267866984968\n",
      "Iteration 41786 => Loss: 6.69891933275440631945\n",
      "Iteration 41787 => Loss: 6.69891923284360668589\n",
      "Iteration 41788 => Loss: 6.69891913294627094899\n",
      "Iteration 41789 => Loss: 6.69891903306239200333\n",
      "Iteration 41790 => Loss: 6.69891893319198228340\n",
      "Iteration 41791 => Loss: 6.69891883333502224929\n",
      "Iteration 41792 => Loss: 6.69891873349151989459\n",
      "Iteration 41793 => Loss: 6.69891863366147255476\n",
      "Iteration 41794 => Loss: 6.69891853384487756529\n",
      "Iteration 41795 => Loss: 6.69891843404173048526\n",
      "Iteration 41796 => Loss: 6.69891833425203309105\n",
      "Iteration 41797 => Loss: 6.69891823447578005357\n",
      "Iteration 41798 => Loss: 6.69891813471297759008\n",
      "Iteration 41799 => Loss: 6.69891803496361326609\n",
      "Iteration 41800 => Loss: 6.69891793522769152247\n",
      "Iteration 41801 => Loss: 6.69891783550521502377\n",
      "Iteration 41802 => Loss: 6.69891773579616600642\n",
      "Iteration 41803 => Loss: 6.69891763610055690492\n",
      "Iteration 41804 => Loss: 6.69891753641837706112\n",
      "Iteration 41805 => Loss: 6.69891743674963002775\n",
      "Iteration 41806 => Loss: 6.69891733709432912747\n",
      "Iteration 41807 => Loss: 6.69891723745244238586\n",
      "Iteration 41808 => Loss: 6.69891713782398490196\n",
      "Iteration 41809 => Loss: 6.69891703820895312305\n",
      "Iteration 41810 => Loss: 6.69891693860734349641\n",
      "Iteration 41811 => Loss: 6.69891683901915691024\n",
      "Iteration 41812 => Loss: 6.69891673944438892363\n",
      "Iteration 41813 => Loss: 6.69891663988304042476\n",
      "Iteration 41814 => Loss: 6.69891654033510430821\n",
      "Iteration 41815 => Loss: 6.69891644080058146216\n",
      "Iteration 41816 => Loss: 6.69891634127947188659\n",
      "Iteration 41817 => Loss: 6.69891624177177735788\n",
      "Iteration 41818 => Loss: 6.69891614227749165877\n",
      "Iteration 41819 => Loss: 6.69891604279660501930\n",
      "Iteration 41820 => Loss: 6.69891594332912987397\n",
      "Iteration 41821 => Loss: 6.69891584387505734099\n",
      "Iteration 41822 => Loss: 6.69891574443437942676\n",
      "Iteration 41823 => Loss: 6.69891564500710856578\n",
      "Iteration 41824 => Loss: 6.69891554559323321172\n",
      "Iteration 41825 => Loss: 6.69891544619275869366\n",
      "Iteration 41826 => Loss: 6.69891534680566902438\n",
      "Iteration 41827 => Loss: 6.69891524743198196745\n",
      "Iteration 41828 => Loss: 6.69891514807168064749\n",
      "Iteration 41829 => Loss: 6.69891504872477216992\n",
      "Iteration 41830 => Loss: 6.69891494939123877117\n",
      "Iteration 41831 => Loss: 6.69891485007109999117\n",
      "Iteration 41832 => Loss: 6.69891475076434605995\n",
      "Iteration 41833 => Loss: 6.69891465147096720756\n",
      "Iteration 41834 => Loss: 6.69891455219097142759\n",
      "Iteration 41835 => Loss: 6.69891445292435339098\n",
      "Iteration 41836 => Loss: 6.69891435367110954502\n",
      "Iteration 41837 => Loss: 6.69891425443124433059\n",
      "Iteration 41838 => Loss: 6.69891415520474708956\n",
      "Iteration 41839 => Loss: 6.69891405599162670370\n",
      "Iteration 41840 => Loss: 6.69891395679186807399\n",
      "Iteration 41841 => Loss: 6.69891385760548363493\n",
      "Iteration 41842 => Loss: 6.69891375843246006383\n",
      "Iteration 41843 => Loss: 6.69891365927280091341\n",
      "Iteration 41844 => Loss: 6.69891356012650351914\n",
      "Iteration 41845 => Loss: 6.69891346099357143373\n",
      "Iteration 41846 => Loss: 6.69891336187399133451\n",
      "Iteration 41847 => Loss: 6.69891326276777210325\n",
      "Iteration 41848 => Loss: 6.69891316367490219363\n",
      "Iteration 41849 => Loss: 6.69891306459538338203\n",
      "Iteration 41850 => Loss: 6.69891296552921922114\n",
      "Iteration 41851 => Loss: 6.69891286647640704643\n",
      "Iteration 41852 => Loss: 6.69891276743693708795\n",
      "Iteration 41853 => Loss: 6.69891266841081556294\n",
      "Iteration 41854 => Loss: 6.69891256939803358961\n",
      "Iteration 41855 => Loss: 6.69891247039859827339\n",
      "Iteration 41856 => Loss: 6.69891237141250606157\n",
      "Iteration 41857 => Loss: 6.69891227243974540784\n",
      "Iteration 41858 => Loss: 6.69891217348032430579\n",
      "Iteration 41859 => Loss: 6.69891207453424275542\n",
      "Iteration 41860 => Loss: 6.69891197560148654588\n",
      "Iteration 41861 => Loss: 6.69891187668206367078\n",
      "Iteration 41862 => Loss: 6.69891177777597235377\n",
      "Iteration 41863 => Loss: 6.69891167888320460122\n",
      "Iteration 41864 => Loss: 6.69891158000377107129\n",
      "Iteration 41865 => Loss: 6.69891148113765222405\n",
      "Iteration 41866 => Loss: 6.69891138228485960582\n",
      "Iteration 41867 => Loss: 6.69891128344538344663\n",
      "Iteration 41868 => Loss: 6.69891118461923085192\n",
      "Iteration 41869 => Loss: 6.69891108580639649261\n",
      "Iteration 41870 => Loss: 6.69891098700687770418\n",
      "Iteration 41871 => Loss: 6.69891088822066116393\n",
      "Iteration 41872 => Loss: 6.69891078944776552362\n",
      "Iteration 41873 => Loss: 6.69891069068817746057\n",
      "Iteration 41874 => Loss: 6.69891059194189786297\n",
      "Iteration 41875 => Loss: 6.69891049320892317809\n",
      "Iteration 41876 => Loss: 6.69891039448925607047\n",
      "Iteration 41877 => Loss: 6.69891029578288677016\n",
      "Iteration 41878 => Loss: 6.69891019708982504710\n",
      "Iteration 41879 => Loss: 6.69891009841005846681\n",
      "Iteration 41880 => Loss: 6.69890999974358525293\n",
      "Iteration 41881 => Loss: 6.69890990109041073453\n",
      "Iteration 41882 => Loss: 6.69890980245052958253\n",
      "Iteration 41883 => Loss: 6.69890970382394002058\n",
      "Iteration 41884 => Loss: 6.69890960521063849598\n",
      "Iteration 41885 => Loss: 6.69890950661062678506\n",
      "Iteration 41886 => Loss: 6.69890940802389955877\n",
      "Iteration 41887 => Loss: 6.69890930945046481071\n",
      "Iteration 41888 => Loss: 6.69890921089030566549\n",
      "Iteration 41889 => Loss: 6.69890911234342745217\n",
      "Iteration 41890 => Loss: 6.69890901380983017077\n",
      "Iteration 41891 => Loss: 6.69890891528950671585\n",
      "Iteration 41892 => Loss: 6.69890881678246419284\n",
      "Iteration 41893 => Loss: 6.69890871828869371996\n",
      "Iteration 41894 => Loss: 6.69890861980819618537\n",
      "Iteration 41895 => Loss: 6.69890852134096714821\n",
      "Iteration 41896 => Loss: 6.69890842288700305573\n",
      "Iteration 41897 => Loss: 6.69890832444630923703\n",
      "Iteration 41898 => Loss: 6.69890822601887769849\n",
      "Iteration 41899 => Loss: 6.69890812760471288101\n",
      "Iteration 41900 => Loss: 6.69890802920381478458\n",
      "Iteration 41901 => Loss: 6.69890793081617008653\n",
      "Iteration 41902 => Loss: 6.69890783244177878686\n",
      "Iteration 41903 => Loss: 6.69890773408064799099\n",
      "Iteration 41904 => Loss: 6.69890763573277059351\n",
      "Iteration 41905 => Loss: 6.69890753739814392986\n",
      "Iteration 41906 => Loss: 6.69890743907677510549\n",
      "Iteration 41907 => Loss: 6.69890734076864546864\n",
      "Iteration 41908 => Loss: 6.69890724247377011835\n",
      "Iteration 41909 => Loss: 6.69890714419213573194\n",
      "Iteration 41910 => Loss: 6.69890704592375030302\n",
      "Iteration 41911 => Loss: 6.69890694766860139708\n",
      "Iteration 41912 => Loss: 6.69890684942668990232\n",
      "Iteration 41913 => Loss: 6.69890675119802470050\n",
      "Iteration 41914 => Loss: 6.69890665298258625171\n",
      "Iteration 41915 => Loss: 6.69890655478038432591\n",
      "Iteration 41916 => Loss: 6.69890645659142514035\n",
      "Iteration 41917 => Loss: 6.69890635841568737874\n",
      "Iteration 41918 => Loss: 6.69890626025318081105\n",
      "Iteration 41919 => Loss: 6.69890616210390454910\n",
      "Iteration 41920 => Loss: 6.69890606396785504018\n",
      "Iteration 41921 => Loss: 6.69890596584502251432\n",
      "Iteration 41922 => Loss: 6.69890586773541674148\n",
      "Iteration 41923 => Loss: 6.69890576963902795171\n",
      "Iteration 41924 => Loss: 6.69890567155586147408\n",
      "Iteration 41925 => Loss: 6.69890557348591375586\n",
      "Iteration 41926 => Loss: 6.69890547542917769164\n",
      "Iteration 41927 => Loss: 6.69890537738564795234\n",
      "Iteration 41928 => Loss: 6.69890527935533608428\n",
      "Iteration 41929 => Loss: 6.69890518133823853475\n",
      "Iteration 41930 => Loss: 6.69890508333434109289\n",
      "Iteration 41931 => Loss: 6.69890498534365619321\n",
      "Iteration 41932 => Loss: 6.69890488736616607213\n",
      "Iteration 41933 => Loss: 6.69890478940188582868\n",
      "Iteration 41934 => Loss: 6.69890469145080302837\n",
      "Iteration 41935 => Loss: 6.69890459351292300028\n",
      "Iteration 41936 => Loss: 6.69890449558823775078\n",
      "Iteration 41937 => Loss: 6.69890439767674550353\n",
      "Iteration 41938 => Loss: 6.69890429977844892306\n",
      "Iteration 41939 => Loss: 6.69890420189334978573\n",
      "Iteration 41940 => Loss: 6.69890410402143565705\n",
      "Iteration 41941 => Loss: 6.69890400616270742518\n",
      "Iteration 41942 => Loss: 6.69890390831716686648\n",
      "Iteration 41943 => Loss: 6.69890381048481042825\n",
      "Iteration 41944 => Loss: 6.69890371266563811048\n",
      "Iteration 41945 => Loss: 6.69890361485965168953\n",
      "Iteration 41946 => Loss: 6.69890351706683961908\n",
      "Iteration 41947 => Loss: 6.69890341928720634002\n",
      "Iteration 41948 => Loss: 6.69890332152074829963\n",
      "Iteration 41949 => Loss: 6.69890322376746549793\n",
      "Iteration 41950 => Loss: 6.69890312602735615854\n",
      "Iteration 41951 => Loss: 6.69890302830040873516\n",
      "Iteration 41952 => Loss: 6.69890293058663655046\n",
      "Iteration 41953 => Loss: 6.69890283288603516354\n",
      "Iteration 41954 => Loss: 6.69890273519859302809\n",
      "Iteration 41955 => Loss: 6.69890263752431458499\n",
      "Iteration 41956 => Loss: 6.69890253986319805790\n",
      "Iteration 41957 => Loss: 6.69890244221524255863\n",
      "Iteration 41958 => Loss: 6.69890234458044631083\n",
      "Iteration 41959 => Loss: 6.69890224695879954453\n",
      "Iteration 41960 => Loss: 6.69890214935031647059\n",
      "Iteration 41961 => Loss: 6.69890205175498554269\n",
      "Iteration 41962 => Loss: 6.69890195417279965540\n",
      "Iteration 41963 => Loss: 6.69890185660376591414\n",
      "Iteration 41964 => Loss: 6.69890175904788609529\n",
      "Iteration 41965 => Loss: 6.69890166150513710619\n",
      "Iteration 41966 => Loss: 6.69890156397554115131\n",
      "Iteration 41967 => Loss: 6.69890146645909023704\n",
      "Iteration 41968 => Loss: 6.69890136895577015252\n",
      "Iteration 41969 => Loss: 6.69890127146560043769\n",
      "Iteration 41970 => Loss: 6.69890117398855888808\n",
      "Iteration 41971 => Loss: 6.69890107652465438548\n",
      "Iteration 41972 => Loss: 6.69890097907388426535\n",
      "Iteration 41973 => Loss: 6.69890088163625030404\n",
      "Iteration 41974 => Loss: 6.69890078421173296164\n",
      "Iteration 41975 => Loss: 6.69890068680035533077\n",
      "Iteration 41976 => Loss: 6.69890058940209698335\n",
      "Iteration 41977 => Loss: 6.69890049201696591297\n",
      "Iteration 41978 => Loss: 6.69890039464496389598\n",
      "Iteration 41979 => Loss: 6.69890029728607316883\n",
      "Iteration 41980 => Loss: 6.69890019994030527783\n",
      "Iteration 41981 => Loss: 6.69890010260765489392\n",
      "Iteration 41982 => Loss: 6.69890000528812201708\n",
      "Iteration 41983 => Loss: 6.69889990798169421282\n",
      "Iteration 41984 => Loss: 6.69889981068838658018\n",
      "Iteration 41985 => Loss: 6.69889971340818668466\n",
      "Iteration 41986 => Loss: 6.69889961614109896715\n",
      "Iteration 41987 => Loss: 6.69889951888711454586\n",
      "Iteration 41988 => Loss: 6.69889942164623342080\n",
      "Iteration 41989 => Loss: 6.69889932441845914468\n",
      "Iteration 41990 => Loss: 6.69889922720378550025\n",
      "Iteration 41991 => Loss: 6.69889913000220715844\n",
      "Iteration 41992 => Loss: 6.69889903281373655375\n",
      "Iteration 41993 => Loss: 6.69889893563835148171\n",
      "Iteration 41994 => Loss: 6.69889883847606970591\n",
      "Iteration 41995 => Loss: 6.69889874132687257458\n",
      "Iteration 41996 => Loss: 6.69889864419077252222\n",
      "Iteration 41997 => Loss: 6.69889854706776066706\n",
      "Iteration 41998 => Loss: 6.69889844995782990367\n",
      "Iteration 41999 => Loss: 6.69889835286099266654\n",
      "Iteration 42000 => Loss: 6.69889825577723119210\n",
      "Iteration 42001 => Loss: 6.69889815870656057939\n",
      "Iteration 42002 => Loss: 6.69889806164896306484\n",
      "Iteration 42003 => Loss: 6.69889796460445285931\n",
      "Iteration 42004 => Loss: 6.69889786757301042286\n",
      "Iteration 42005 => Loss: 6.69889777055464641364\n",
      "Iteration 42006 => Loss: 6.69889767354935550259\n",
      "Iteration 42007 => Loss: 6.69889757655713413698\n",
      "Iteration 42008 => Loss: 6.69889747957798764588\n",
      "Iteration 42009 => Loss: 6.69889738261190448299\n",
      "Iteration 42010 => Loss: 6.69889728565888642464\n",
      "Iteration 42011 => Loss: 6.69889718871893613539\n",
      "Iteration 42012 => Loss: 6.69889709179205006251\n",
      "Iteration 42013 => Loss: 6.69889699487822110058\n",
      "Iteration 42014 => Loss: 6.69889689797744836142\n",
      "Iteration 42015 => Loss: 6.69889680108974250317\n",
      "Iteration 42016 => Loss: 6.69889670421508842679\n",
      "Iteration 42017 => Loss: 6.69889660735348080323\n",
      "Iteration 42018 => Loss: 6.69889651050493473150\n",
      "Iteration 42019 => Loss: 6.69889641366943422440\n",
      "Iteration 42020 => Loss: 6.69889631684698905190\n",
      "Iteration 42021 => Loss: 6.69889622003757967406\n",
      "Iteration 42022 => Loss: 6.69889612324122207809\n",
      "Iteration 42023 => Loss: 6.69889602645790205315\n",
      "Iteration 42024 => Loss: 6.69889592968763647463\n",
      "Iteration 42025 => Loss: 6.69889583293039958534\n",
      "Iteration 42026 => Loss: 6.69889573618620381978\n",
      "Iteration 42027 => Loss: 6.69889563945504651343\n",
      "Iteration 42028 => Loss: 6.69889554273691789632\n",
      "Iteration 42029 => Loss: 6.69889544603182507387\n",
      "Iteration 42030 => Loss: 6.69889534933976715791\n",
      "Iteration 42031 => Loss: 6.69889525266073793119\n",
      "Iteration 42032 => Loss: 6.69889515599472940011\n",
      "Iteration 42033 => Loss: 6.69889505934174689372\n",
      "Iteration 42034 => Loss: 6.69889496270179662929\n",
      "Iteration 42035 => Loss: 6.69889486607486439596\n",
      "Iteration 42036 => Loss: 6.69889476946095197007\n",
      "Iteration 42037 => Loss: 6.69889467286006023983\n",
      "Iteration 42038 => Loss: 6.69889457627218298796\n",
      "Iteration 42039 => Loss: 6.69889447969732021448\n",
      "Iteration 42040 => Loss: 6.69889438313547369575\n",
      "Iteration 42041 => Loss: 6.69889428658663277361\n",
      "Iteration 42042 => Loss: 6.69889419005081077074\n",
      "Iteration 42043 => Loss: 6.69889409352799436448\n",
      "Iteration 42044 => Loss: 6.69889399701817822574\n",
      "Iteration 42045 => Loss: 6.69889390052137478904\n",
      "Iteration 42046 => Loss: 6.69889380403757161986\n",
      "Iteration 42047 => Loss: 6.69889370756676516550\n",
      "Iteration 42048 => Loss: 6.69889361110896608409\n",
      "Iteration 42049 => Loss: 6.69889351466416016478\n",
      "Iteration 42050 => Loss: 6.69889341823235273665\n",
      "Iteration 42051 => Loss: 6.69889332181353314155\n",
      "Iteration 42052 => Loss: 6.69889322540771026127\n",
      "Iteration 42053 => Loss: 6.69889312901488498397\n",
      "Iteration 42054 => Loss: 6.69889303263503688157\n",
      "Iteration 42055 => Loss: 6.69889293626818105309\n",
      "Iteration 42056 => Loss: 6.69889283991431394583\n",
      "Iteration 42057 => Loss: 6.69889274357342578980\n",
      "Iteration 42058 => Loss: 6.69889264724551924957\n",
      "Iteration 42059 => Loss: 6.69889255093059698964\n",
      "Iteration 42060 => Loss: 6.69889245462864568736\n",
      "Iteration 42061 => Loss: 6.69889235833967511269\n",
      "Iteration 42062 => Loss: 6.69889226206368793015\n",
      "Iteration 42063 => Loss: 6.69889216580066282347\n",
      "Iteration 42064 => Loss: 6.69889206955061489168\n",
      "Iteration 42065 => Loss: 6.69889197331353170028\n",
      "Iteration 42066 => Loss: 6.69889187708942035471\n",
      "Iteration 42067 => Loss: 6.69889178087827108499\n",
      "Iteration 42068 => Loss: 6.69889168468009099655\n",
      "Iteration 42069 => Loss: 6.69889158849487653669\n",
      "Iteration 42070 => Loss: 6.69889149232261349454\n",
      "Iteration 42071 => Loss: 6.69889139616331874549\n",
      "Iteration 42072 => Loss: 6.69889130001697719052\n",
      "Iteration 42073 => Loss: 6.69889120388359504688\n",
      "Iteration 42074 => Loss: 6.69889110776316076823\n",
      "Iteration 42075 => Loss: 6.69889101165568057183\n",
      "Iteration 42076 => Loss: 6.69889091556115090498\n",
      "Iteration 42077 => Loss: 6.69889081947956732677\n",
      "Iteration 42078 => Loss: 6.69889072341093783081\n",
      "Iteration 42079 => Loss: 6.69889062735524642989\n",
      "Iteration 42080 => Loss: 6.69889053131250999940\n",
      "Iteration 42081 => Loss: 6.69889043528270100580\n",
      "Iteration 42082 => Loss: 6.69889033926584254175\n",
      "Iteration 42083 => Loss: 6.69889024326191684366\n",
      "Iteration 42084 => Loss: 6.69889014727093279333\n",
      "Iteration 42085 => Loss: 6.69889005129287351537\n",
      "Iteration 42086 => Loss: 6.69888995532775144426\n",
      "Iteration 42087 => Loss: 6.69888985937556924455\n",
      "Iteration 42088 => Loss: 6.69888976343631448174\n",
      "Iteration 42089 => Loss: 6.69888966750997916222\n",
      "Iteration 42090 => Loss: 6.69888957159657660867\n",
      "Iteration 42091 => Loss: 6.69888947569609438659\n",
      "Iteration 42092 => Loss: 6.69888937980853782506\n",
      "Iteration 42093 => Loss: 6.69888928393389360139\n",
      "Iteration 42094 => Loss: 6.69888918807218214369\n",
      "Iteration 42095 => Loss: 6.69888909222337591842\n",
      "Iteration 42096 => Loss: 6.69888899638749535370\n",
      "Iteration 42097 => Loss: 6.69888890056452535049\n",
      "Iteration 42098 => Loss: 6.69888880475446324425\n",
      "Iteration 42099 => Loss: 6.69888870895731702859\n",
      "Iteration 42100 => Loss: 6.69888861317307249266\n",
      "Iteration 42101 => Loss: 6.69888851740174384730\n",
      "Iteration 42102 => Loss: 6.69888842164331332896\n",
      "Iteration 42103 => Loss: 6.69888832589778893123\n",
      "Iteration 42104 => Loss: 6.69888823016516177233\n",
      "Iteration 42105 => Loss: 6.69888813444544872766\n",
      "Iteration 42106 => Loss: 6.69888803873862048732\n",
      "Iteration 42107 => Loss: 6.69888794304469303853\n",
      "Iteration 42108 => Loss: 6.69888784736366016404\n",
      "Iteration 42109 => Loss: 6.69888775169551475841\n",
      "Iteration 42110 => Loss: 6.69888765604027103251\n",
      "Iteration 42111 => Loss: 6.69888756039791299912\n",
      "Iteration 42112 => Loss: 6.69888746476843621735\n",
      "Iteration 42113 => Loss: 6.69888736915185134535\n",
      "Iteration 42114 => Loss: 6.69888727354815127768\n",
      "Iteration 42115 => Loss: 6.69888717795733423799\n",
      "Iteration 42116 => Loss: 6.69888708237939756174\n",
      "Iteration 42117 => Loss: 6.69888698681433414350\n",
      "Iteration 42118 => Loss: 6.69888689126215908232\n",
      "Iteration 42119 => Loss: 6.69888679572284839736\n",
      "Iteration 42120 => Loss: 6.69888670019641541131\n",
      "Iteration 42121 => Loss: 6.69888660468285834781\n",
      "Iteration 42122 => Loss: 6.69888650918217187780\n",
      "Iteration 42123 => Loss: 6.69888641369435156037\n",
      "Iteration 42124 => Loss: 6.69888631821939650735\n",
      "Iteration 42125 => Loss: 6.69888622275731027145\n",
      "Iteration 42126 => Loss: 6.69888612730808841178\n",
      "Iteration 42127 => Loss: 6.69888603187172027020\n",
      "Iteration 42128 => Loss: 6.69888593644822183393\n",
      "Iteration 42129 => Loss: 6.69888584103757978028\n",
      "Iteration 42130 => Loss: 6.69888574563978878018\n",
      "Iteration 42131 => Loss: 6.69888565025485416271\n",
      "Iteration 42132 => Loss: 6.69888555488278125694\n",
      "Iteration 42133 => Loss: 6.69888545952355141111\n",
      "Iteration 42134 => Loss: 6.69888536417716728977\n",
      "Iteration 42135 => Loss: 6.69888526884364221559\n",
      "Iteration 42136 => Loss: 6.69888517352295576046\n",
      "Iteration 42137 => Loss: 6.69888507821511414164\n",
      "Iteration 42138 => Loss: 6.69888498292011025370\n",
      "Iteration 42139 => Loss: 6.69888488763796274839\n",
      "Iteration 42140 => Loss: 6.69888479236864053945\n",
      "Iteration 42141 => Loss: 6.69888469711216671953\n",
      "Iteration 42142 => Loss: 6.69888460186851997236\n",
      "Iteration 42143 => Loss: 6.69888450663771628513\n",
      "Iteration 42144 => Loss: 6.69888441141973611792\n",
      "Iteration 42145 => Loss: 6.69888431621458924070\n",
      "Iteration 42146 => Loss: 6.69888422102227032440\n",
      "Iteration 42147 => Loss: 6.69888412584277936901\n",
      "Iteration 42148 => Loss: 6.69888403067611726271\n",
      "Iteration 42149 => Loss: 6.69888393552227601191\n",
      "Iteration 42150 => Loss: 6.69888384038125739295\n",
      "Iteration 42151 => Loss: 6.69888374525305785312\n",
      "Iteration 42152 => Loss: 6.69888365013767650424\n",
      "Iteration 42153 => Loss: 6.69888355503511068179\n",
      "Iteration 42154 => Loss: 6.69888345994536038575\n",
      "Iteration 42155 => Loss: 6.69888336486842828066\n",
      "Iteration 42156 => Loss: 6.69888326980430637292\n",
      "Iteration 42157 => Loss: 6.69888317475298933346\n",
      "Iteration 42158 => Loss: 6.69888307971448782041\n",
      "Iteration 42159 => Loss: 6.69888298468878762293\n",
      "Iteration 42160 => Loss: 6.69888288967589407008\n",
      "Iteration 42161 => Loss: 6.69888279467579650372\n",
      "Iteration 42162 => Loss: 6.69888269968851091107\n",
      "Iteration 42163 => Loss: 6.69888260471402308127\n",
      "Iteration 42164 => Loss: 6.69888250975232768525\n",
      "Iteration 42165 => Loss: 6.69888241480343182843\n",
      "Iteration 42166 => Loss: 6.69888231986733106993\n",
      "Iteration 42167 => Loss: 6.69888222494402096885\n",
      "Iteration 42168 => Loss: 6.69888213003349886066\n",
      "Iteration 42169 => Loss: 6.69888203513576829806\n",
      "Iteration 42170 => Loss: 6.69888194025082750471\n",
      "Iteration 42171 => Loss: 6.69888184537867559243\n",
      "Iteration 42172 => Loss: 6.69888175051929923853\n",
      "Iteration 42173 => Loss: 6.69888165567270643663\n",
      "Iteration 42174 => Loss: 6.69888156083890695669\n",
      "Iteration 42175 => Loss: 6.69888146601786615975\n",
      "Iteration 42176 => Loss: 6.69888137120961513205\n",
      "Iteration 42177 => Loss: 6.69888127641413788638\n",
      "Iteration 42178 => Loss: 6.69888118163143353456\n",
      "Iteration 42179 => Loss: 6.69888108686150474114\n",
      "Iteration 42180 => Loss: 6.69888099210433640707\n",
      "Iteration 42181 => Loss: 6.69888089735995073681\n",
      "Iteration 42182 => Loss: 6.69888080262832374956\n",
      "Iteration 42183 => Loss: 6.69888070790945899802\n",
      "Iteration 42184 => Loss: 6.69888061320335737037\n",
      "Iteration 42185 => Loss: 6.69888051851002241932\n",
      "Iteration 42186 => Loss: 6.69888042382944881581\n",
      "Iteration 42187 => Loss: 6.69888032916162945440\n",
      "Iteration 42188 => Loss: 6.69888023450657144053\n",
      "Iteration 42189 => Loss: 6.69888013986426233970\n",
      "Iteration 42190 => Loss: 6.69888004523470304008\n",
      "Iteration 42191 => Loss: 6.69887995061790242346\n",
      "Iteration 42192 => Loss: 6.69887985601385160805\n",
      "Iteration 42193 => Loss: 6.69887976142254615297\n",
      "Iteration 42194 => Loss: 6.69887966684398694639\n",
      "Iteration 42195 => Loss: 6.69887957227817665284\n",
      "Iteration 42196 => Loss: 6.69887947772509928512\n",
      "Iteration 42197 => Loss: 6.69887938318476905408\n",
      "Iteration 42198 => Loss: 6.69887928865717707794\n",
      "Iteration 42199 => Loss: 6.69887919414232690940\n",
      "Iteration 42200 => Loss: 6.69887909964020700215\n",
      "Iteration 42201 => Loss: 6.69887900515082623798\n",
      "Iteration 42202 => Loss: 6.69887891067417129420\n",
      "Iteration 42203 => Loss: 6.69887881621025016443\n",
      "Iteration 42204 => Loss: 6.69887872175906018413\n",
      "Iteration 42205 => Loss: 6.69887862732059780058\n",
      "Iteration 42206 => Loss: 6.69887853289485679653\n",
      "Iteration 42207 => Loss: 6.69887843848184427742\n",
      "Iteration 42208 => Loss: 6.69887834408154692056\n",
      "Iteration 42209 => Loss: 6.69887824969397627228\n",
      "Iteration 42210 => Loss: 6.69887815531912345079\n",
      "Iteration 42211 => Loss: 6.69887806095698223885\n",
      "Iteration 42212 => Loss: 6.69887796660756329459\n",
      "Iteration 42213 => Loss: 6.69887787227086040076\n",
      "Iteration 42214 => Loss: 6.69887777794686467558\n",
      "Iteration 42215 => Loss: 6.69887768363557523088\n",
      "Iteration 42216 => Loss: 6.69887758933700183661\n",
      "Iteration 42217 => Loss: 6.69887749505112939374\n",
      "Iteration 42218 => Loss: 6.69887740077796411953\n",
      "Iteration 42219 => Loss: 6.69887730651750068489\n",
      "Iteration 42220 => Loss: 6.69887721226974353073\n",
      "Iteration 42221 => Loss: 6.69887711803468111071\n",
      "Iteration 42222 => Loss: 6.69887702381232408300\n",
      "Iteration 42223 => Loss: 6.69887692960265290765\n",
      "Iteration 42224 => Loss: 6.69887683540568357188\n",
      "Iteration 42225 => Loss: 6.69887674122140097666\n",
      "Iteration 42226 => Loss: 6.69887664704981844466\n",
      "Iteration 42227 => Loss: 6.69887655289091821231\n",
      "Iteration 42228 => Loss: 6.69887645874471804319\n",
      "Iteration 42229 => Loss: 6.69887636461118685105\n",
      "Iteration 42230 => Loss: 6.69887627049035572213\n",
      "Iteration 42231 => Loss: 6.69887617638219801108\n",
      "Iteration 42232 => Loss: 6.69887608228672526423\n",
      "Iteration 42233 => Loss: 6.69887598820392682342\n",
      "Iteration 42234 => Loss: 6.69887589413380979408\n",
      "Iteration 42235 => Loss: 6.69887580007636707080\n",
      "Iteration 42236 => Loss: 6.69887570603159865357\n",
      "Iteration 42237 => Loss: 6.69887561199950365420\n",
      "Iteration 42238 => Loss: 6.69887551798007585546\n",
      "Iteration 42239 => Loss: 6.69887542397332769184\n",
      "Iteration 42240 => Loss: 6.69887532997923429434\n",
      "Iteration 42241 => Loss: 6.69887523599781875561\n",
      "Iteration 42242 => Loss: 6.69887514202905620664\n",
      "Iteration 42243 => Loss: 6.69887504807296263465\n",
      "Iteration 42244 => Loss: 6.69887495412953004603\n",
      "Iteration 42245 => Loss: 6.69887486019874867083\n",
      "Iteration 42246 => Loss: 6.69887476628062916717\n",
      "Iteration 42247 => Loss: 6.69887467237517153507\n",
      "Iteration 42248 => Loss: 6.69887457848236689273\n",
      "Iteration 42249 => Loss: 6.69887448460220813473\n",
      "Iteration 42250 => Loss: 6.69887439073469881379\n",
      "Iteration 42251 => Loss: 6.69887429687984070625\n",
      "Iteration 42252 => Loss: 6.69887420303763025942\n",
      "Iteration 42253 => Loss: 6.69887410920806036785\n",
      "Iteration 42254 => Loss: 6.69887401539113991333\n",
      "Iteration 42255 => Loss: 6.69887392158685646137\n",
      "Iteration 42256 => Loss: 6.69887382779521978193\n",
      "Iteration 42257 => Loss: 6.69887373401621388780\n",
      "Iteration 42258 => Loss: 6.69887364024985298983\n",
      "Iteration 42259 => Loss: 6.69887354649612465352\n",
      "Iteration 42260 => Loss: 6.69887345275502710251\n",
      "Iteration 42261 => Loss: 6.69887335902655767228\n",
      "Iteration 42262 => Loss: 6.69887326531072169189\n",
      "Iteration 42263 => Loss: 6.69887317160751472045\n",
      "Iteration 42264 => Loss: 6.69887307791693231707\n",
      "Iteration 42265 => Loss: 6.69887298423897714628\n",
      "Iteration 42266 => Loss: 6.69887289057364476719\n",
      "Iteration 42267 => Loss: 6.69887279692093162708\n",
      "Iteration 42268 => Loss: 6.69887270328083683779\n",
      "Iteration 42269 => Loss: 6.69887260965336839291\n",
      "Iteration 42270 => Loss: 6.69887251603851208159\n",
      "Iteration 42271 => Loss: 6.69887242243626701566\n",
      "Iteration 42272 => Loss: 6.69887232884663941235\n",
      "Iteration 42273 => Loss: 6.69887223526962038989\n",
      "Iteration 42274 => Loss: 6.69887214170520906009\n",
      "Iteration 42275 => Loss: 6.69887204815340986386\n",
      "Iteration 42276 => Loss: 6.69887195461421303122\n",
      "Iteration 42277 => Loss: 6.69887186108762211489\n",
      "Iteration 42278 => Loss: 6.69887176757363445034\n",
      "Iteration 42279 => Loss: 6.69887167407224204396\n",
      "Iteration 42280 => Loss: 6.69887158058345644207\n",
      "Iteration 42281 => Loss: 6.69887148710726609835\n",
      "Iteration 42282 => Loss: 6.69887139364367367733\n",
      "Iteration 42283 => Loss: 6.69887130019267651448\n",
      "Iteration 42284 => Loss: 6.69887120675426128713\n",
      "Iteration 42285 => Loss: 6.69887111332845019973\n",
      "Iteration 42286 => Loss: 6.69887101991522104782\n",
      "Iteration 42287 => Loss: 6.69887092651457827230\n",
      "Iteration 42288 => Loss: 6.69887083312652364953\n",
      "Iteration 42289 => Loss: 6.69887073975105717949\n",
      "Iteration 42290 => Loss: 6.69887064638816642770\n",
      "Iteration 42291 => Loss: 6.69887055303785938776\n",
      "Iteration 42292 => Loss: 6.69887045970013250695\n",
      "Iteration 42293 => Loss: 6.69887036637498134439\n",
      "Iteration 42294 => Loss: 6.69887027306240767643\n",
      "Iteration 42295 => Loss: 6.69887017976240795036\n",
      "Iteration 42296 => Loss: 6.69887008647497328440\n",
      "Iteration 42297 => Loss: 6.69886999320011877757\n",
      "Iteration 42298 => Loss: 6.69886989993782133723\n",
      "Iteration 42299 => Loss: 6.69886980668810316786\n",
      "Iteration 42300 => Loss: 6.69886971345094561769\n",
      "Iteration 42301 => Loss: 6.69886962022634868674\n",
      "Iteration 42302 => Loss: 6.69886952701431592772\n",
      "Iteration 42303 => Loss: 6.69886943381484911697\n",
      "Iteration 42304 => Loss: 6.69886934062793049094\n",
      "Iteration 42305 => Loss: 6.69886924745357781319\n",
      "Iteration 42306 => Loss: 6.69886915429177243198\n",
      "Iteration 42307 => Loss: 6.69886906114253033451\n",
      "Iteration 42308 => Loss: 6.69886896800583642175\n",
      "Iteration 42309 => Loss: 6.69886887488168714100\n",
      "Iteration 42310 => Loss: 6.69886878177009048585\n",
      "Iteration 42311 => Loss: 6.69886868867103846270\n",
      "Iteration 42312 => Loss: 6.69886859558453995334\n",
      "Iteration 42313 => Loss: 6.69886850251057541783\n",
      "Iteration 42314 => Loss: 6.69886840944915284979\n",
      "Iteration 42315 => Loss: 6.69886831640027846646\n",
      "Iteration 42316 => Loss: 6.69886822336393183974\n",
      "Iteration 42317 => Loss: 6.69886813034013162138\n",
      "Iteration 42318 => Loss: 6.69886803732885915963\n",
      "Iteration 42319 => Loss: 6.69886794433012422445\n",
      "Iteration 42320 => Loss: 6.69886785134391615770\n",
      "Iteration 42321 => Loss: 6.69886775837024472935\n",
      "Iteration 42322 => Loss: 6.69886766540909395218\n",
      "Iteration 42323 => Loss: 6.69886757246047537251\n",
      "Iteration 42324 => Loss: 6.69886747952438099674\n",
      "Iteration 42325 => Loss: 6.69886738660080904850\n",
      "Iteration 42326 => Loss: 6.69886729368975952781\n",
      "Iteration 42327 => Loss: 6.69886720079122799376\n",
      "Iteration 42328 => Loss: 6.69886710790521267000\n",
      "Iteration 42329 => Loss: 6.69886701503171977379\n",
      "Iteration 42330 => Loss: 6.69886692217073775879\n",
      "Iteration 42331 => Loss: 6.69886682932226928955\n",
      "Iteration 42332 => Loss: 6.69886673648631170153\n",
      "Iteration 42333 => Loss: 6.69886664366286321837\n",
      "Iteration 42334 => Loss: 6.69886655085192117554\n",
      "Iteration 42335 => Loss: 6.69886645805348734939\n",
      "Iteration 42336 => Loss: 6.69886636526755463450\n",
      "Iteration 42337 => Loss: 6.69886627249413457719\n",
      "Iteration 42338 => Loss: 6.69886617973320586117\n",
      "Iteration 42339 => Loss: 6.69886608698478536184\n",
      "Iteration 42340 => Loss: 6.69886599424885709197\n",
      "Iteration 42341 => Loss: 6.69886590152542371612\n",
      "Iteration 42342 => Loss: 6.69886580881448789881\n",
      "Iteration 42343 => Loss: 6.69886571611604608734\n",
      "Iteration 42344 => Loss: 6.69886562343008584719\n",
      "Iteration 42345 => Loss: 6.69886553075662405377\n",
      "Iteration 42346 => Loss: 6.69886543809565182528\n",
      "Iteration 42347 => Loss: 6.69886534544716383266\n",
      "Iteration 42348 => Loss: 6.69886525281115563502\n",
      "Iteration 42349 => Loss: 6.69886516018763522595\n",
      "Iteration 42350 => Loss: 6.69886506757659194733\n",
      "Iteration 42351 => Loss: 6.69886497497802846368\n",
      "Iteration 42352 => Loss: 6.69886488239194655137\n",
      "Iteration 42353 => Loss: 6.69886478981833644042\n",
      "Iteration 42354 => Loss: 6.69886469725720523627\n",
      "Iteration 42355 => Loss: 6.69886460470854494531\n",
      "Iteration 42356 => Loss: 6.69886451217235112665\n",
      "Iteration 42357 => Loss: 6.69886441964862644483\n",
      "Iteration 42358 => Loss: 6.69886432713737534073\n",
      "Iteration 42359 => Loss: 6.69886423463859070893\n",
      "Iteration 42360 => Loss: 6.69886414215226277946\n",
      "Iteration 42361 => Loss: 6.69886404967840221047\n",
      "Iteration 42362 => Loss: 6.69886395721700100836\n",
      "Iteration 42363 => Loss: 6.69886386476806183765\n",
      "Iteration 42364 => Loss: 6.69886377233157848110\n",
      "Iteration 42365 => Loss: 6.69886367990754827417\n",
      "Iteration 42366 => Loss: 6.69886358749598009865\n",
      "Iteration 42367 => Loss: 6.69886349509685352643\n",
      "Iteration 42368 => Loss: 6.69886340271018720927\n",
      "Iteration 42369 => Loss: 6.69886331033596604811\n",
      "Iteration 42370 => Loss: 6.69886321797418737845\n",
      "Iteration 42371 => Loss: 6.69886312562486185840\n",
      "Iteration 42372 => Loss: 6.69886303328797794165\n",
      "Iteration 42373 => Loss: 6.69886294096353740457\n",
      "Iteration 42374 => Loss: 6.69886284865154024715\n",
      "Iteration 42375 => Loss: 6.69886275635197225853\n",
      "Iteration 42376 => Loss: 6.69886266406485653135\n",
      "Iteration 42377 => Loss: 6.69886257179015931484\n",
      "Iteration 42378 => Loss: 6.69886247952790725435\n",
      "Iteration 42379 => Loss: 6.69886238727808258631\n",
      "Iteration 42380 => Loss: 6.69886229504069508067\n",
      "Iteration 42381 => Loss: 6.69886220281572875024\n",
      "Iteration 42382 => Loss: 6.69886211060319336497\n",
      "Iteration 42383 => Loss: 6.69886201840308626032\n",
      "Iteration 42384 => Loss: 6.69886192621540299541\n",
      "Iteration 42385 => Loss: 6.69886183404013824116\n",
      "Iteration 42386 => Loss: 6.69886174187729288576\n",
      "Iteration 42387 => Loss: 6.69886164972687137009\n",
      "Iteration 42388 => Loss: 6.69886155758886570055\n",
      "Iteration 42389 => Loss: 6.69886146546327676532\n",
      "Iteration 42390 => Loss: 6.69886137335010012350\n",
      "Iteration 42391 => Loss: 6.69886128124933311057\n",
      "Iteration 42392 => Loss: 6.69886118916097572651\n",
      "Iteration 42393 => Loss: 6.69886109708503685312\n",
      "Iteration 42394 => Loss: 6.69886100502149428593\n",
      "Iteration 42395 => Loss: 6.69886091297036134762\n",
      "Iteration 42396 => Loss: 6.69886082093163182094\n",
      "Iteration 42397 => Loss: 6.69886072890530392954\n",
      "Iteration 42398 => Loss: 6.69886063689137767341\n",
      "Iteration 42399 => Loss: 6.69886054488984861166\n",
      "Iteration 42400 => Loss: 6.69886045290072296154\n",
      "Iteration 42401 => Loss: 6.69886036092398651220\n",
      "Iteration 42402 => Loss: 6.69886026895964814543\n",
      "Iteration 42403 => Loss: 6.69886017700770164396\n",
      "Iteration 42404 => Loss: 6.69886008506814523145\n",
      "Iteration 42405 => Loss: 6.69885999314097446700\n",
      "Iteration 42406 => Loss: 6.69885990122618757425\n",
      "Iteration 42407 => Loss: 6.69885980932379609953\n",
      "Iteration 42408 => Loss: 6.69885971743378583199\n",
      "Iteration 42409 => Loss: 6.69885962555616121250\n",
      "Iteration 42410 => Loss: 6.69885953369090447751\n",
      "Iteration 42411 => Loss: 6.69885944183803783147\n",
      "Iteration 42412 => Loss: 6.69885934999753995811\n",
      "Iteration 42413 => Loss: 6.69885925816942151556\n",
      "Iteration 42414 => Loss: 6.69885916635368072747\n",
      "Iteration 42415 => Loss: 6.69885907455031048841\n",
      "Iteration 42416 => Loss: 6.69885898275930902201\n",
      "Iteration 42417 => Loss: 6.69885889098067277558\n",
      "Iteration 42418 => Loss: 6.69885879921440885454\n",
      "Iteration 42419 => Loss: 6.69885870746050748892\n",
      "Iteration 42420 => Loss: 6.69885861571897311961\n",
      "Iteration 42421 => Loss: 6.69885852398980041755\n",
      "Iteration 42422 => Loss: 6.69885843227298583002\n",
      "Iteration 42423 => Loss: 6.69885834056853290974\n",
      "Iteration 42424 => Loss: 6.69885824887643543946\n",
      "Iteration 42425 => Loss: 6.69885815719668809010\n",
      "Iteration 42426 => Loss: 6.69885806552930507252\n",
      "Iteration 42427 => Loss: 6.69885797387427039951\n",
      "Iteration 42428 => Loss: 6.69885788223158407106\n",
      "Iteration 42429 => Loss: 6.69885779060124253448\n",
      "Iteration 42430 => Loss: 6.69885769898325733607\n",
      "Iteration 42431 => Loss: 6.69885760737761426498\n",
      "Iteration 42432 => Loss: 6.69885751578431243303\n",
      "Iteration 42433 => Loss: 6.69885742420335894565\n",
      "Iteration 42434 => Loss: 6.69885733263474225652\n",
      "Iteration 42435 => Loss: 6.69885724107846325381\n",
      "Iteration 42436 => Loss: 6.69885714953452193754\n",
      "Iteration 42437 => Loss: 6.69885705800291653134\n",
      "Iteration 42438 => Loss: 6.69885696648364437067\n",
      "Iteration 42439 => Loss: 6.69885687497670723189\n",
      "Iteration 42440 => Loss: 6.69885678348210067412\n",
      "Iteration 42441 => Loss: 6.69885669199981936828\n",
      "Iteration 42442 => Loss: 6.69885660052986509072\n",
      "Iteration 42443 => Loss: 6.69885650907224228234\n",
      "Iteration 42444 => Loss: 6.69885641762693762047\n",
      "Iteration 42445 => Loss: 6.69885632619395288145\n",
      "Iteration 42446 => Loss: 6.69885623477329339437\n",
      "Iteration 42447 => Loss: 6.69885614336495205379\n",
      "Iteration 42448 => Loss: 6.69885605196893063606\n",
      "Iteration 42449 => Loss: 6.69885596058522203577\n",
      "Iteration 42450 => Loss: 6.69885586921382625292\n",
      "Iteration 42451 => Loss: 6.69885577785474595203\n",
      "Iteration 42452 => Loss: 6.69885568650797402768\n",
      "Iteration 42453 => Loss: 6.69885559517351136805\n",
      "Iteration 42454 => Loss: 6.69885550385135974949\n",
      "Iteration 42455 => Loss: 6.69885541254150673751\n",
      "Iteration 42456 => Loss: 6.69885532124396032572\n",
      "Iteration 42457 => Loss: 6.69885522995871873775\n",
      "Iteration 42458 => Loss: 6.69885513868577664454\n",
      "Iteration 42459 => Loss: 6.69885504742513226972\n",
      "Iteration 42460 => Loss: 6.69885495617679360691\n",
      "Iteration 42461 => Loss: 6.69885486494073667529\n",
      "Iteration 42462 => Loss: 6.69885477371698190296\n",
      "Iteration 42463 => Loss: 6.69885468250551863179\n",
      "Iteration 42464 => Loss: 6.69885459130634686176\n",
      "Iteration 42465 => Loss: 6.69885450011945859927\n",
      "Iteration 42466 => Loss: 6.69885440894486006158\n",
      "Iteration 42467 => Loss: 6.69885431778255657775\n",
      "Iteration 42468 => Loss: 6.69885422663253127240\n",
      "Iteration 42469 => Loss: 6.69885413549478325734\n",
      "Iteration 42470 => Loss: 6.69885404436932585526\n",
      "Iteration 42471 => Loss: 6.69885395325613597350\n",
      "Iteration 42472 => Loss: 6.69885386215522782294\n",
      "Iteration 42473 => Loss: 6.69885377106659518631\n",
      "Iteration 42474 => Loss: 6.69885367999024161634\n",
      "Iteration 42475 => Loss: 6.69885358892615467852\n",
      "Iteration 42476 => Loss: 6.69885349787434325464\n",
      "Iteration 42477 => Loss: 6.69885340683479846291\n",
      "Iteration 42478 => Loss: 6.69885331580752563241\n",
      "Iteration 42479 => Loss: 6.69885322479251232863\n",
      "Iteration 42480 => Loss: 6.69885313378976920973\n",
      "Iteration 42481 => Loss: 6.69885304279928472937\n",
      "Iteration 42482 => Loss: 6.69885295182105888756\n",
      "Iteration 42483 => Loss: 6.69885286085510056608\n",
      "Iteration 42484 => Loss: 6.69885276990139733044\n",
      "Iteration 42485 => Loss: 6.69885267895994562792\n",
      "Iteration 42486 => Loss: 6.69885258803074634670\n",
      "Iteration 42487 => Loss: 6.69885249711380925675\n",
      "Iteration 42488 => Loss: 6.69885240620911748266\n",
      "Iteration 42489 => Loss: 6.69885231531667368898\n",
      "Iteration 42490 => Loss: 6.69885222443647876389\n",
      "Iteration 42491 => Loss: 6.69885213356853803646\n",
      "Iteration 42492 => Loss: 6.69885204271282930222\n",
      "Iteration 42493 => Loss: 6.69885195186937210110\n",
      "Iteration 42494 => Loss: 6.69885186103814778136\n",
      "Iteration 42495 => Loss: 6.69885177021916877749\n",
      "Iteration 42496 => Loss: 6.69885167941242976042\n",
      "Iteration 42497 => Loss: 6.69885158861791651930\n",
      "Iteration 42498 => Loss: 6.69885149783564770587\n",
      "Iteration 42499 => Loss: 6.69885140706560733292\n",
      "Iteration 42500 => Loss: 6.69885131630780605860\n",
      "Iteration 42501 => Loss: 6.69885122556222611934\n",
      "Iteration 42502 => Loss: 6.69885113482887373237\n",
      "Iteration 42503 => Loss: 6.69885104410775511496\n",
      "Iteration 42504 => Loss: 6.69885095339885516807\n",
      "Iteration 42505 => Loss: 6.69885086270218277349\n",
      "Iteration 42506 => Loss: 6.69885077201772549671\n",
      "Iteration 42507 => Loss: 6.69885068134548955499\n",
      "Iteration 42508 => Loss: 6.69885059068547583649\n",
      "Iteration 42509 => Loss: 6.69885050003767634763\n",
      "Iteration 42510 => Loss: 6.69885040940208931204\n",
      "Iteration 42511 => Loss: 6.69885031877872005879\n",
      "Iteration 42512 => Loss: 6.69885022816755881792\n",
      "Iteration 42513 => Loss: 6.69885013756860736578\n",
      "Iteration 42514 => Loss: 6.69885004698186392602\n",
      "Iteration 42515 => Loss: 6.69884995640733116318\n",
      "Iteration 42516 => Loss: 6.69884986584500108364\n",
      "Iteration 42517 => Loss: 6.69884977529487013470\n",
      "Iteration 42518 => Loss: 6.69884968475694453360\n",
      "Iteration 42519 => Loss: 6.69884959423121895128\n",
      "Iteration 42520 => Loss: 6.69884950371769072319\n",
      "Iteration 42521 => Loss: 6.69884941321635629663\n",
      "Iteration 42522 => Loss: 6.69884932272722455338\n",
      "Iteration 42523 => Loss: 6.69884923225028217075\n",
      "Iteration 42524 => Loss: 6.69884914178553358965\n",
      "Iteration 42525 => Loss: 6.69884905133296904012\n",
      "Iteration 42526 => Loss: 6.69884896089260095664\n",
      "Iteration 42527 => Loss: 6.69884887046441068748\n",
      "Iteration 42528 => Loss: 6.69884878004841421983\n",
      "Iteration 42529 => Loss: 6.69884868964459556651\n",
      "Iteration 42530 => Loss: 6.69884859925296716199\n",
      "Iteration 42531 => Loss: 6.69884850887351124271\n",
      "Iteration 42532 => Loss: 6.69884841850623580228\n",
      "Iteration 42533 => Loss: 6.69884832815113551163\n",
      "Iteration 42534 => Loss: 6.69884823780820770622\n",
      "Iteration 42535 => Loss: 6.69884814747746304420\n",
      "Iteration 42536 => Loss: 6.69884805715888198563\n",
      "Iteration 42537 => Loss: 6.69884796685247696502\n",
      "Iteration 42538 => Loss: 6.69884787655823910058\n",
      "Iteration 42539 => Loss: 6.69884778627616928048\n",
      "Iteration 42540 => Loss: 6.69884769600626217567\n",
      "Iteration 42541 => Loss: 6.69884760574852311521\n",
      "Iteration 42542 => Loss: 6.69884751550294232914\n",
      "Iteration 42543 => Loss: 6.69884742526952781105\n",
      "Iteration 42544 => Loss: 6.69884733504826446193\n",
      "Iteration 42545 => Loss: 6.69884724483915938720\n",
      "Iteration 42546 => Loss: 6.69884715464221347503\n",
      "Iteration 42547 => Loss: 6.69884706445741429093\n",
      "Iteration 42548 => Loss: 6.69884697428478048664\n",
      "Iteration 42549 => Loss: 6.69884688412429252224\n",
      "Iteration 42550 => Loss: 6.69884679397595039774\n",
      "Iteration 42551 => Loss: 6.69884670383975411312\n",
      "Iteration 42552 => Loss: 6.69884661371570722110\n",
      "Iteration 42553 => Loss: 6.69884652360380794534\n",
      "Iteration 42554 => Loss: 6.69884643350404651585\n",
      "Iteration 42555 => Loss: 6.69884634341642382083\n",
      "Iteration 42556 => Loss: 6.69884625334094696569\n",
      "Iteration 42557 => Loss: 6.69884616327760618049\n",
      "Iteration 42558 => Loss: 6.69884607322639613614\n",
      "Iteration 42559 => Loss: 6.69884598318732304989\n",
      "Iteration 42560 => Loss: 6.69884589316038780993\n",
      "Iteration 42561 => Loss: 6.69884580314557798175\n",
      "Iteration 42562 => Loss: 6.69884571314290155897\n",
      "Iteration 42563 => Loss: 6.69884562315234877161\n",
      "Iteration 42564 => Loss: 6.69884553317392938965\n",
      "Iteration 42565 => Loss: 6.69884544320762653768\n",
      "Iteration 42566 => Loss: 6.69884535325344909751\n",
      "Iteration 42567 => Loss: 6.69884526331139174005\n",
      "Iteration 42568 => Loss: 6.69884517338145979437\n",
      "Iteration 42569 => Loss: 6.69884508346363816145\n",
      "Iteration 42570 => Loss: 6.69884499355793838760\n",
      "Iteration 42571 => Loss: 6.69884490366435070285\n",
      "Iteration 42572 => Loss: 6.69884481378288132447\n",
      "Iteration 42573 => Loss: 6.69884472391351515341\n",
      "Iteration 42574 => Loss: 6.69884463405626640053\n",
      "Iteration 42575 => Loss: 6.69884454421111819045\n",
      "Iteration 42576 => Loss: 6.69884445437808206947\n",
      "Iteration 42577 => Loss: 6.69884436455715448488\n",
      "Iteration 42578 => Loss: 6.69884427474832477856\n",
      "Iteration 42579 => Loss: 6.69884418495159561502\n",
      "Iteration 42580 => Loss: 6.69884409516696699427\n",
      "Iteration 42581 => Loss: 6.69884400539444335720\n",
      "Iteration 42582 => Loss: 6.69884391563401049297\n",
      "Iteration 42583 => Loss: 6.69884382588567373062\n",
      "Iteration 42584 => Loss: 6.69884373614943218200\n",
      "Iteration 42585 => Loss: 6.69884364642528229439\n",
      "Iteration 42586 => Loss: 6.69884355671321696235\n",
      "Iteration 42587 => Loss: 6.69884346701325128492\n",
      "Iteration 42588 => Loss: 6.69884337732536572219\n",
      "Iteration 42589 => Loss: 6.69884328764956826774\n",
      "Iteration 42590 => Loss: 6.69884319798584648709\n",
      "Iteration 42591 => Loss: 6.69884310833421903197\n",
      "Iteration 42592 => Loss: 6.69884301869466280976\n",
      "Iteration 42593 => Loss: 6.69884292906719291949\n",
      "Iteration 42594 => Loss: 6.69884283945179603847\n",
      "Iteration 42595 => Loss: 6.69884274984847660761\n",
      "Iteration 42596 => Loss: 6.69884266025722929783\n",
      "Iteration 42597 => Loss: 6.69884257067805499730\n",
      "Iteration 42598 => Loss: 6.69884248111094837697\n",
      "Iteration 42599 => Loss: 6.69884239155591387771\n",
      "Iteration 42600 => Loss: 6.69884230201295149953\n",
      "Iteration 42601 => Loss: 6.69884221248205413701\n",
      "Iteration 42602 => Loss: 6.69884212296321379654\n",
      "Iteration 42603 => Loss: 6.69884203345644202443\n",
      "Iteration 42604 => Loss: 6.69884194396172727437\n",
      "Iteration 42605 => Loss: 6.69884185447907487543\n",
      "Iteration 42606 => Loss: 6.69884176500847949853\n",
      "Iteration 42607 => Loss: 6.69884167554994380822\n",
      "Iteration 42608 => Loss: 6.69884158610345181728\n",
      "Iteration 42609 => Loss: 6.69884149666902839471\n",
      "Iteration 42610 => Loss: 6.69884140724664334243\n",
      "Iteration 42611 => Loss: 6.69884131783631087131\n",
      "Iteration 42612 => Loss: 6.69884122843803275771\n",
      "Iteration 42613 => Loss: 6.69884113905179301440\n",
      "Iteration 42614 => Loss: 6.69884104967759963500\n",
      "Iteration 42615 => Loss: 6.69884096031545350769\n",
      "Iteration 42616 => Loss: 6.69884087096534663885\n",
      "Iteration 42617 => Loss: 6.69884078162728080486\n",
      "Iteration 42618 => Loss: 6.69884069230125245298\n",
      "Iteration 42619 => Loss: 6.69884060298726158322\n",
      "Iteration 42620 => Loss: 6.69884051368530286652\n",
      "Iteration 42621 => Loss: 6.69884042439538074376\n",
      "Iteration 42622 => Loss: 6.69884033511748722134\n",
      "Iteration 42623 => Loss: 6.69884024585162496379\n",
      "Iteration 42624 => Loss: 6.69884015659779308294\n",
      "Iteration 42625 => Loss: 6.69884006735598802607\n",
      "Iteration 42626 => Loss: 6.69883997812620535228\n",
      "Iteration 42627 => Loss: 6.69883988890844683795\n",
      "Iteration 42628 => Loss: 6.69883979970271870030\n",
      "Iteration 42629 => Loss: 6.69883971050899784672\n",
      "Iteration 42630 => Loss: 6.69883962132730736982\n",
      "Iteration 42631 => Loss: 6.69883953215763217059\n",
      "Iteration 42632 => Loss: 6.69883944299996869631\n",
      "Iteration 42633 => Loss: 6.69883935385431605880\n",
      "Iteration 42634 => Loss: 6.69883926472068402802\n",
      "Iteration 42635 => Loss: 6.69883917559905572858\n",
      "Iteration 42636 => Loss: 6.69883908648944359499\n",
      "Iteration 42637 => Loss: 6.69883899739183164002\n",
      "Iteration 42638 => Loss: 6.69883890830622874546\n",
      "Iteration 42639 => Loss: 6.69883881923263402314\n",
      "Iteration 42640 => Loss: 6.69883873017103592673\n",
      "Iteration 42641 => Loss: 6.69883864112143712077\n",
      "Iteration 42642 => Loss: 6.69883855208384293434\n",
      "Iteration 42643 => Loss: 6.69883846305824626199\n",
      "Iteration 42644 => Loss: 6.69883837404464355103\n",
      "Iteration 42645 => Loss: 6.69883828504303924234\n",
      "Iteration 42646 => Loss: 6.69883819605342711867\n",
      "Iteration 42647 => Loss: 6.69883810707579829824\n",
      "Iteration 42648 => Loss: 6.69883801811016965644\n",
      "Iteration 42649 => Loss: 6.69883792915653142330\n",
      "Iteration 42650 => Loss: 6.69883784021487205251\n",
      "Iteration 42651 => Loss: 6.69883775128519243225\n",
      "Iteration 42652 => Loss: 6.69883766236750677336\n",
      "Iteration 42653 => Loss: 6.69883757346179553593\n",
      "Iteration 42654 => Loss: 6.69883748456807293081\n",
      "Iteration 42655 => Loss: 6.69883739568631764172\n",
      "Iteration 42656 => Loss: 6.69883730681654654404\n",
      "Iteration 42657 => Loss: 6.69883721795875342053\n",
      "Iteration 42658 => Loss: 6.69883712911292494852\n",
      "Iteration 42659 => Loss: 6.69883704027907533884\n",
      "Iteration 42660 => Loss: 6.69883695145719393338\n",
      "Iteration 42661 => Loss: 6.69883686264728339665\n",
      "Iteration 42662 => Loss: 6.69883677384933573506\n",
      "Iteration 42663 => Loss: 6.69883668506335627768\n",
      "Iteration 42664 => Loss: 6.69883659628933614272\n",
      "Iteration 42665 => Loss: 6.69883650752728865285\n",
      "Iteration 42666 => Loss: 6.69883641877719426816\n",
      "Iteration 42667 => Loss: 6.69883633003905654135\n",
      "Iteration 42668 => Loss: 6.69883624131288346604\n",
      "Iteration 42669 => Loss: 6.69883615259865994318\n",
      "Iteration 42670 => Loss: 6.69883606389639663092\n",
      "Iteration 42671 => Loss: 6.69883597520608020659\n",
      "Iteration 42672 => Loss: 6.69883588652771777561\n",
      "Iteration 42673 => Loss: 6.69883579786130045619\n",
      "Iteration 42674 => Loss: 6.69883570920683535377\n",
      "Iteration 42675 => Loss: 6.69883562056431891563\n",
      "Iteration 42676 => Loss: 6.69883553193374048362\n",
      "Iteration 42677 => Loss: 6.69883544331511426861\n",
      "Iteration 42678 => Loss: 6.69883535470841806614\n",
      "Iteration 42679 => Loss: 6.69883526611367408066\n",
      "Iteration 42680 => Loss: 6.69883517753085833135\n",
      "Iteration 42681 => Loss: 6.69883508895998236454\n",
      "Iteration 42682 => Loss: 6.69883500040103907480\n",
      "Iteration 42683 => Loss: 6.69883491185403290302\n",
      "Iteration 42684 => Loss: 6.69883482331896029649\n",
      "Iteration 42685 => Loss: 6.69883473479581148524\n",
      "Iteration 42686 => Loss: 6.69883464628459179835\n",
      "Iteration 42687 => Loss: 6.69883455778530034763\n",
      "Iteration 42688 => Loss: 6.69883446929793358038\n",
      "Iteration 42689 => Loss: 6.69883438082249771384\n",
      "Iteration 42690 => Loss: 6.69883429235897231990\n",
      "Iteration 42691 => Loss: 6.69883420390738137939\n",
      "Iteration 42692 => Loss: 6.69883411546769380607\n",
      "Iteration 42693 => Loss: 6.69883402703993446892\n",
      "Iteration 42694 => Loss: 6.69883393862409182162\n",
      "Iteration 42695 => Loss: 6.69883385022015342969\n",
      "Iteration 42696 => Loss: 6.69883376182812906308\n",
      "Iteration 42697 => Loss: 6.69883367344802138632\n",
      "Iteration 42698 => Loss: 6.69883358507981885310\n",
      "Iteration 42699 => Loss: 6.69883349672352856885\n",
      "Iteration 42700 => Loss: 6.69883340837914165178\n",
      "Iteration 42701 => Loss: 6.69883332004665899007\n",
      "Iteration 42702 => Loss: 6.69883323172607969553\n",
      "Iteration 42703 => Loss: 6.69883314341740376818\n",
      "Iteration 42704 => Loss: 6.69883305512062499076\n",
      "Iteration 42705 => Loss: 6.69883296683573892238\n",
      "Iteration 42706 => Loss: 6.69883287856275355665\n",
      "Iteration 42707 => Loss: 6.69883279030166534085\n",
      "Iteration 42708 => Loss: 6.69883270205246983409\n",
      "Iteration 42709 => Loss: 6.69883261381516170729\n",
      "Iteration 42710 => Loss: 6.69883252558974628954\n",
      "Iteration 42711 => Loss: 6.69883243737622269265\n",
      "Iteration 42712 => Loss: 6.69883234917458025848\n",
      "Iteration 42713 => Loss: 6.69883226098482698063\n",
      "Iteration 42714 => Loss: 6.69883217280695575369\n",
      "Iteration 42715 => Loss: 6.69883208464096036039\n",
      "Iteration 42716 => Loss: 6.69883199648685412342\n",
      "Iteration 42717 => Loss: 6.69883190834462460828\n",
      "Iteration 42718 => Loss: 6.69883182021427536768\n",
      "Iteration 42719 => Loss: 6.69883173209579485530\n",
      "Iteration 42720 => Loss: 6.69883164398919817017\n",
      "Iteration 42721 => Loss: 6.69883155589446133149\n",
      "Iteration 42722 => Loss: 6.69883146781159677374\n",
      "Iteration 42723 => Loss: 6.69883137974060982600\n",
      "Iteration 42724 => Loss: 6.69883129168148361288\n",
      "Iteration 42725 => Loss: 6.69883120363422790433\n",
      "Iteration 42726 => Loss: 6.69883111559883470676\n",
      "Iteration 42727 => Loss: 6.69883102757530402016\n",
      "Iteration 42728 => Loss: 6.69883093956363762089\n",
      "Iteration 42729 => Loss: 6.69883085156382485081\n",
      "Iteration 42730 => Loss: 6.69883076357587903260\n",
      "Iteration 42731 => Loss: 6.69883067559977884997\n",
      "Iteration 42732 => Loss: 6.69883058763553673742\n",
      "Iteration 42733 => Loss: 6.69883049968315091860\n",
      "Iteration 42734 => Loss: 6.69883041174261606443\n",
      "Iteration 42735 => Loss: 6.69883032381393039856\n",
      "Iteration 42736 => Loss: 6.69883023589709392098\n",
      "Iteration 42737 => Loss: 6.69883014799210307899\n",
      "Iteration 42738 => Loss: 6.69883006009895698440\n",
      "Iteration 42739 => Loss: 6.69882997221765030815\n",
      "Iteration 42740 => Loss: 6.69882988434819814927\n",
      "Iteration 42741 => Loss: 6.69882979649057563876\n",
      "Iteration 42742 => Loss: 6.69882970864479698747\n",
      "Iteration 42743 => Loss: 6.69882962081085153727\n",
      "Iteration 42744 => Loss: 6.69882953298874461723\n",
      "Iteration 42745 => Loss: 6.69882944517846734556\n",
      "Iteration 42746 => Loss: 6.69882935738003126858\n",
      "Iteration 42747 => Loss: 6.69882926959341684636\n",
      "Iteration 42748 => Loss: 6.69882918181863651341\n",
      "Iteration 42749 => Loss: 6.69882909405568316430\n",
      "Iteration 42750 => Loss: 6.69882900630455768720\n",
      "Iteration 42751 => Loss: 6.69882891856525564123\n",
      "Iteration 42752 => Loss: 6.69882883083777436184\n",
      "Iteration 42753 => Loss: 6.69882874312211562540\n",
      "Iteration 42754 => Loss: 6.69882865541827676736\n",
      "Iteration 42755 => Loss: 6.69882856772625601138\n",
      "Iteration 42756 => Loss: 6.69882848004605246928\n",
      "Iteration 42757 => Loss: 6.69882839237766081197\n",
      "Iteration 42758 => Loss: 6.69882830472108370401\n",
      "Iteration 42759 => Loss: 6.69882821707632203356\n",
      "Iteration 42760 => Loss: 6.69882812944336691885\n",
      "Iteration 42761 => Loss: 6.69882804182222280076\n",
      "Iteration 42762 => Loss: 6.69882795421287902116\n",
      "Iteration 42763 => Loss: 6.69882786661534712636\n",
      "Iteration 42764 => Loss: 6.69882777902962178729\n",
      "Iteration 42765 => Loss: 6.69882769145569056946\n",
      "Iteration 42766 => Loss: 6.69882760389356413100\n",
      "Iteration 42767 => Loss: 6.69882751634323891921\n",
      "Iteration 42768 => Loss: 6.69882742880470782865\n",
      "Iteration 42769 => Loss: 6.69882734127796641843\n",
      "Iteration 42770 => Loss: 6.69882725376303156395\n",
      "Iteration 42771 => Loss: 6.69882716625988727799\n",
      "Iteration 42772 => Loss: 6.69882707876852911966\n",
      "Iteration 42773 => Loss: 6.69882699128896419438\n",
      "Iteration 42774 => Loss: 6.69882690382118095584\n",
      "Iteration 42775 => Loss: 6.69882681636518118040\n",
      "Iteration 42776 => Loss: 6.69882672892097819073\n",
      "Iteration 42777 => Loss: 6.69882664148855688779\n",
      "Iteration 42778 => Loss: 6.69882655406790572528\n",
      "Iteration 42779 => Loss: 6.69882646665904601946\n",
      "Iteration 42780 => Loss: 6.69882637926196178313\n",
      "Iteration 42781 => Loss: 6.69882629187664857540\n",
      "Iteration 42782 => Loss: 6.69882620450312149529\n",
      "Iteration 42783 => Loss: 6.69882611714135478564\n",
      "Iteration 42784 => Loss: 6.69882602979136887456\n",
      "Iteration 42785 => Loss: 6.69882594245315221571\n",
      "Iteration 42786 => Loss: 6.69882585512671013817\n",
      "Iteration 42787 => Loss: 6.69882576781202665472\n",
      "Iteration 42788 => Loss: 6.69882568050910087720\n",
      "Iteration 42789 => Loss: 6.69882559321795145735\n",
      "Iteration 42790 => Loss: 6.69882550593856684884\n",
      "Iteration 42791 => Loss: 6.69882541867094083443\n",
      "Iteration 42792 => Loss: 6.69882533141506808505\n",
      "Iteration 42793 => Loss: 6.69882524417095215341\n",
      "Iteration 42794 => Loss: 6.69882515693860192130\n",
      "Iteration 42795 => Loss: 6.69882506971799962514\n",
      "Iteration 42796 => Loss: 6.69882498250915503490\n",
      "Iteration 42797 => Loss: 6.69882489531205926880\n",
      "Iteration 42798 => Loss: 6.69882480812671410320\n",
      "Iteration 42799 => Loss: 6.69882472095311332083\n",
      "Iteration 42800 => Loss: 6.69882463379125514535\n",
      "Iteration 42801 => Loss: 6.69882454664116089305\n",
      "Iteration 42802 => Loss: 6.69882445950279770130\n",
      "Iteration 42803 => Loss: 6.69882437237617534009\n",
      "Iteration 42804 => Loss: 6.69882428526129380941\n",
      "Iteration 42805 => Loss: 6.69882419815815044473\n",
      "Iteration 42806 => Loss: 6.69882411106673991696\n",
      "Iteration 42807 => Loss: 6.69882402398707466062\n",
      "Iteration 42808 => Loss: 6.69882393691914046485\n",
      "Iteration 42809 => Loss: 6.69882384986293821783\n",
      "Iteration 42810 => Loss: 6.69882376281846703137\n",
      "Iteration 42811 => Loss: 6.69882367578572335276\n",
      "Iteration 42812 => Loss: 6.69882358876470984654\n",
      "Iteration 42813 => Loss: 6.69882350175541940729\n",
      "Iteration 42814 => Loss: 6.69882341475785292317\n",
      "Iteration 42815 => Loss: 6.69882332777201394691\n",
      "Iteration 42816 => Loss: 6.69882324079789626126\n",
      "Iteration 42817 => Loss: 6.69882315383549276078\n",
      "Iteration 42818 => Loss: 6.69882306688481055090\n",
      "Iteration 42819 => Loss: 6.69882297994584696710\n",
      "Iteration 42820 => Loss: 6.69882289301859668029\n",
      "Iteration 42821 => Loss: 6.69882280610305613777\n",
      "Iteration 42822 => Loss: 6.69882271919923422132\n",
      "Iteration 42823 => Loss: 6.69882263230711760826\n",
      "Iteration 42824 => Loss: 6.69882254542670807496\n",
      "Iteration 42825 => Loss: 6.69882245855801450318\n",
      "Iteration 42826 => Loss: 6.69882237170101912938\n",
      "Iteration 42827 => Loss: 6.69882228485572728260\n",
      "Iteration 42828 => Loss: 6.69882219802214340376\n",
      "Iteration 42829 => Loss: 6.69882211120025505835\n",
      "Iteration 42830 => Loss: 6.69882202439006402273\n",
      "Iteration 42831 => Loss: 6.69882193759158095503\n",
      "Iteration 42832 => Loss: 6.69882185080478631534\n",
      "Iteration 42833 => Loss: 6.69882176402968454454\n",
      "Iteration 42834 => Loss: 6.69882167726627830717\n",
      "Iteration 42835 => Loss: 6.69882159051456493870\n",
      "Iteration 42836 => Loss: 6.69882150377454088641\n",
      "Iteration 42837 => Loss: 6.69882141704620881484\n",
      "Iteration 42838 => Loss: 6.69882133032955628948\n",
      "Iteration 42839 => Loss: 6.69882124362458775124\n",
      "Iteration 42840 => Loss: 6.69882115693131297007\n",
      "Iteration 42841 => Loss: 6.69882107024971240605\n",
      "Iteration 42842 => Loss: 6.69882098357979227643\n",
      "Iteration 42843 => Loss: 6.69882089692155435756\n",
      "Iteration 42844 => Loss: 6.69882081027498621495\n",
      "Iteration 42845 => Loss: 6.69882072364010117127\n",
      "Iteration 42846 => Loss: 6.69882063701688768020\n",
      "Iteration 42847 => Loss: 6.69882055040535195900\n",
      "Iteration 42848 => Loss: 6.69882046380547802045\n",
      "Iteration 42849 => Loss: 6.69882037721727829904\n",
      "Iteration 42850 => Loss: 6.69882029064074124847\n",
      "Iteration 42851 => Loss: 6.69882020407588107958\n",
      "Iteration 42852 => Loss: 6.69882011752267469973\n",
      "Iteration 42853 => Loss: 6.69882003098113987249\n",
      "Iteration 42854 => Loss: 6.69881994445126061066\n",
      "Iteration 42855 => Loss: 6.69881985793304490784\n",
      "Iteration 42856 => Loss: 6.69881977142648654677\n",
      "Iteration 42857 => Loss: 6.69881968493158019839\n",
      "Iteration 42858 => Loss: 6.69881959844833740902\n",
      "Iteration 42859 => Loss: 6.69881951197674041509\n",
      "Iteration 42860 => Loss: 6.69881942551680431563\n",
      "Iteration 42861 => Loss: 6.69881933906851134708\n",
      "Iteration 42862 => Loss: 6.69881925263186772668\n",
      "Iteration 42863 => Loss: 6.69881916620687434261\n",
      "Iteration 42864 => Loss: 6.69881907979351964855\n",
      "Iteration 42865 => Loss: 6.69881899339181696718\n",
      "Iteration 42866 => Loss: 6.69881890700175652853\n",
      "Iteration 42867 => Loss: 6.69881882062333300354\n",
      "Iteration 42868 => Loss: 6.69881873425655793852\n",
      "Iteration 42869 => Loss: 6.69881864790140824084\n",
      "Iteration 42870 => Loss: 6.69881856155790345042\n",
      "Iteration 42871 => Loss: 6.69881847522603379730\n",
      "Iteration 42872 => Loss: 6.69881838890579484058\n",
      "Iteration 42873 => Loss: 6.69881830259717947484\n",
      "Iteration 42874 => Loss: 6.69881821630020812819\n",
      "Iteration 42875 => Loss: 6.69881813001485948433\n",
      "Iteration 42876 => Loss: 6.69881804374113620781\n",
      "Iteration 42877 => Loss: 6.69881795747903829863\n",
      "Iteration 42878 => Loss: 6.69881787122856131589\n",
      "Iteration 42879 => Loss: 6.69881778498971591773\n",
      "Iteration 42880 => Loss: 6.69881769876248434059\n",
      "Iteration 42881 => Loss: 6.69881761254687191354\n",
      "Iteration 42882 => Loss: 6.69881752634288218928\n",
      "Iteration 42883 => Loss: 6.69881744015050450969\n",
      "Iteration 42884 => Loss: 6.69881735396974598018\n",
      "Iteration 42885 => Loss: 6.69881726780059327808\n",
      "Iteration 42886 => Loss: 6.69881718164305528518\n",
      "Iteration 42887 => Loss: 6.69881709549712578422\n",
      "Iteration 42888 => Loss: 6.69881700936280477521\n",
      "Iteration 42889 => Loss: 6.69881692324008959361\n",
      "Iteration 42890 => Loss: 6.69881683712898290395\n",
      "Iteration 42891 => Loss: 6.69881675102947138356\n",
      "Iteration 42892 => Loss: 6.69881666494157279601\n",
      "Iteration 42893 => Loss: 6.69881657886527026591\n",
      "Iteration 42894 => Loss: 6.69881649280056290507\n",
      "Iteration 42895 => Loss: 6.69881640674745426622\n",
      "Iteration 42896 => Loss: 6.69881632070594523753\n",
      "Iteration 42897 => Loss: 6.69881623467602604904\n",
      "Iteration 42898 => Loss: 6.69881614865770380618\n",
      "Iteration 42899 => Loss: 6.69881606265097140351\n",
      "Iteration 42900 => Loss: 6.69881597665582528833\n",
      "Iteration 42901 => Loss: 6.69881589067226901335\n",
      "Iteration 42902 => Loss: 6.69881580470029813767\n",
      "Iteration 42903 => Loss: 6.69881571873991710220\n",
      "Iteration 42904 => Loss: 6.69881563279110814335\n",
      "Iteration 42905 => Loss: 6.69881554685388902470\n",
      "Iteration 42906 => Loss: 6.69881546092824997629\n",
      "Iteration 42907 => Loss: 6.69881537501418122815\n",
      "Iteration 42908 => Loss: 6.69881528911169876750\n",
      "Iteration 42909 => Loss: 6.69881520322078838348\n",
      "Iteration 42910 => Loss: 6.69881511734144918790\n",
      "Iteration 42911 => Loss: 6.69881503147369095075\n",
      "Iteration 42912 => Loss: 6.69881494561749590844\n",
      "Iteration 42913 => Loss: 6.69881485977287471911\n",
      "Iteration 42914 => Loss: 6.69881477393981317192\n",
      "Iteration 42915 => Loss: 6.69881468811832903043\n",
      "Iteration 42916 => Loss: 6.69881460230840186654\n",
      "Iteration 42917 => Loss: 6.69881451651003878567\n",
      "Iteration 42918 => Loss: 6.69881443072323357057\n",
      "Iteration 42919 => Loss: 6.69881434494798888579\n",
      "Iteration 42920 => Loss: 6.69881425918430473132\n",
      "Iteration 42921 => Loss: 6.69881417343218021898\n",
      "Iteration 42922 => Loss: 6.69881408769160557881\n",
      "Iteration 42923 => Loss: 6.69881400196259324531\n",
      "Iteration 42924 => Loss: 6.69881391624512634309\n",
      "Iteration 42925 => Loss: 6.69881383053920398396\n",
      "Iteration 42926 => Loss: 6.69881374484484215515\n",
      "Iteration 42927 => Loss: 6.69881365916202309307\n",
      "Iteration 42928 => Loss: 6.69881357349074413321\n",
      "Iteration 42929 => Loss: 6.69881348783101593369\n",
      "Iteration 42930 => Loss: 6.69881340218283050092\n",
      "Iteration 42931 => Loss: 6.69881331654618072946\n",
      "Iteration 42932 => Loss: 6.69881323092107283657\n",
      "Iteration 42933 => Loss: 6.69881314530750948677\n",
      "Iteration 42934 => Loss: 6.69881305970547469286\n",
      "Iteration 42935 => Loss: 6.69881297411497378391\n",
      "Iteration 42936 => Loss: 6.69881288853601297717\n",
      "Iteration 42937 => Loss: 6.69881280296858161449\n",
      "Iteration 42938 => Loss: 6.69881271741267969588\n",
      "Iteration 42939 => Loss: 6.69881263186830544498\n",
      "Iteration 42940 => Loss: 6.69881254633545886179\n",
      "Iteration 42941 => Loss: 6.69881246081413639359\n",
      "Iteration 42942 => Loss: 6.69881237530434070493\n",
      "Iteration 42943 => Loss: 6.69881228980606469037\n",
      "Iteration 42944 => Loss: 6.69881220431930923809\n",
      "Iteration 42945 => Loss: 6.69881211884407790080\n",
      "Iteration 42946 => Loss: 6.69881203338036623762\n",
      "Iteration 42947 => Loss: 6.69881194792816270223\n",
      "Iteration 42948 => Loss: 6.69881186248747795275\n",
      "Iteration 42949 => Loss: 6.69881177705830133107\n",
      "Iteration 42950 => Loss: 6.69881169164064615984\n",
      "Iteration 42951 => Loss: 6.69881160623449378733\n",
      "Iteration 42952 => Loss: 6.69881152083985131895\n",
      "Iteration 42953 => Loss: 6.69881143545671520201\n",
      "Iteration 42954 => Loss: 6.69881135008508366013\n",
      "Iteration 42955 => Loss: 6.69881126472496379876\n",
      "Iteration 42956 => Loss: 6.69881117937634051884\n",
      "Iteration 42957 => Loss: 6.69881109403921648493\n",
      "Iteration 42958 => Loss: 6.69881100871359169702\n",
      "Iteration 42959 => Loss: 6.69881092339946349057\n",
      "Iteration 42960 => Loss: 6.69881083809683097741\n",
      "Iteration 42961 => Loss: 6.69881075280569771024\n",
      "Iteration 42962 => Loss: 6.69881066752605924819\n",
      "Iteration 42963 => Loss: 6.69881058225790759764\n",
      "Iteration 42964 => Loss: 6.69881049700124631130\n",
      "Iteration 42965 => Loss: 6.69881041175607361282\n",
      "Iteration 42966 => Loss: 6.69881032652238683767\n",
      "Iteration 42967 => Loss: 6.69881024130018865037\n",
      "Iteration 42968 => Loss: 6.69881015608946484008\n",
      "Iteration 42969 => Loss: 6.69881007089023405854\n",
      "Iteration 42970 => Loss: 6.69880998570247854218\n",
      "Iteration 42971 => Loss: 6.69880990052620184372\n",
      "Iteration 42972 => Loss: 6.69880981536140573951\n",
      "Iteration 42973 => Loss: 6.69880973020808223595\n",
      "Iteration 42974 => Loss: 6.69880964506623666210\n",
      "Iteration 42975 => Loss: 6.69880955993586368891\n",
      "Iteration 42976 => Loss: 6.69880947481695798729\n",
      "Iteration 42977 => Loss: 6.69880938970952310996\n",
      "Iteration 42978 => Loss: 6.69880930461356083327\n",
      "Iteration 42979 => Loss: 6.69880921952906316363\n",
      "Iteration 42980 => Loss: 6.69880913445602921286\n",
      "Iteration 42981 => Loss: 6.69880904939445454005\n",
      "Iteration 42982 => Loss: 6.69880896434435157971\n",
      "Iteration 42983 => Loss: 6.69880887930571145006\n",
      "Iteration 42984 => Loss: 6.69880879427851727570\n",
      "Iteration 42985 => Loss: 6.69880870926278593203\n",
      "Iteration 42986 => Loss: 6.69880862425851386632\n",
      "Iteration 42987 => Loss: 6.69880853926568953227\n",
      "Iteration 42988 => Loss: 6.69880845428432092348\n",
      "Iteration 42989 => Loss: 6.69880836931440271087\n",
      "Iteration 42990 => Loss: 6.69880828435593755898\n",
      "Iteration 42991 => Loss: 6.69880819940891569786\n",
      "Iteration 42992 => Loss: 6.69880811447334956199\n",
      "Iteration 42993 => Loss: 6.69880802954922227599\n",
      "Iteration 42994 => Loss: 6.69880794463654005710\n",
      "Iteration 42995 => Loss: 6.69880785973529757626\n",
      "Iteration 42996 => Loss: 6.69880777484549394529\n",
      "Iteration 42997 => Loss: 6.69880768996713005237\n",
      "Iteration 42998 => Loss: 6.69880760510021033838\n",
      "Iteration 42999 => Loss: 6.69880752024472236883\n",
      "Iteration 43000 => Loss: 6.69880743540066436736\n",
      "Iteration 43001 => Loss: 6.69880735056804343941\n",
      "Iteration 43002 => Loss: 6.69880726574685425589\n",
      "Iteration 43003 => Loss: 6.69880718093709237593\n",
      "Iteration 43004 => Loss: 6.69880709613875513497\n",
      "Iteration 43005 => Loss: 6.69880701135185763206\n",
      "Iteration 43006 => Loss: 6.69880692657637144549\n",
      "Iteration 43007 => Loss: 6.69880684181231433882\n",
      "Iteration 43008 => Loss: 6.69880675705967565392\n",
      "Iteration 43009 => Loss: 6.69880667231846249621\n",
      "Iteration 43010 => Loss: 6.69880658758866953661\n",
      "Iteration 43011 => Loss: 6.69880650287028789336\n",
      "Iteration 43012 => Loss: 6.69880641816331934280\n",
      "Iteration 43013 => Loss: 6.69880633346777276671\n",
      "Iteration 43014 => Loss: 6.69880624878363928332\n",
      "Iteration 43015 => Loss: 6.69880616411091178719\n",
      "Iteration 43016 => Loss: 6.69880607944959827194\n",
      "Iteration 43017 => Loss: 6.69880599479968452670\n",
      "Iteration 43018 => Loss: 6.69880591016118476233\n",
      "Iteration 43019 => Loss: 6.69880582553409187341\n",
      "Iteration 43020 => Loss: 6.69880574091840230722\n",
      "Iteration 43021 => Loss: 6.69880565631411162286\n",
      "Iteration 43022 => Loss: 6.69880557172121982035\n",
      "Iteration 43023 => Loss: 6.69880548713973222874\n",
      "Iteration 43024 => Loss: 6.69880540256963818990\n",
      "Iteration 43025 => Loss: 6.69880531801093592748\n",
      "Iteration 43026 => Loss: 6.69880523346363698778\n",
      "Iteration 43027 => Loss: 6.69880514892772183089\n",
      "Iteration 43028 => Loss: 6.69880506440320733219\n",
      "Iteration 43029 => Loss: 6.69880497989007483994\n",
      "Iteration 43030 => Loss: 6.69880489538833145957\n",
      "Iteration 43031 => Loss: 6.69880481089797807925\n",
      "Iteration 43032 => Loss: 6.69880472641901114628\n",
      "Iteration 43033 => Loss: 6.69880464195142444339\n",
      "Iteration 43034 => Loss: 6.69880455749521352971\n",
      "Iteration 43035 => Loss: 6.69880447305039350425\n",
      "Iteration 43036 => Loss: 6.69880438861694926800\n",
      "Iteration 43037 => Loss: 6.69880430419487460370\n",
      "Iteration 43038 => Loss: 6.69880421978418727491\n",
      "Iteration 43039 => Loss: 6.69880413538487218261\n",
      "Iteration 43040 => Loss: 6.69880405099692932680\n",
      "Iteration 43041 => Loss: 6.69880396662035959565\n",
      "Iteration 43042 => Loss: 6.69880388225514966649\n",
      "Iteration 43043 => Loss: 6.69880379790131463835\n",
      "Iteration 43044 => Loss: 6.69880371355885095852\n",
      "Iteration 43045 => Loss: 6.69880362922774441614\n",
      "Iteration 43046 => Loss: 6.69880354490800655753\n",
      "Iteration 43047 => Loss: 6.69880346059962583638\n",
      "Iteration 43048 => Loss: 6.69880337630260935811\n",
      "Iteration 43049 => Loss: 6.69880329201694735275\n",
      "Iteration 43050 => Loss: 6.69880320774265403116\n",
      "Iteration 43051 => Loss: 6.69880312347970363618\n",
      "Iteration 43052 => Loss: 6.69880303922811926043\n",
      "Iteration 43053 => Loss: 6.69880295498788047581\n",
      "Iteration 43054 => Loss: 6.69880287075899349958\n",
      "Iteration 43055 => Loss: 6.69880278654145389083\n",
      "Iteration 43056 => Loss: 6.69880270233527230772\n",
      "Iteration 43057 => Loss: 6.69880261814043098667\n",
      "Iteration 43058 => Loss: 6.69880253395693525675\n",
      "Iteration 43059 => Loss: 6.69880244978477890072\n",
      "Iteration 43060 => Loss: 6.69880236562396724764\n",
      "Iteration 43061 => Loss: 6.69880228147450118570\n",
      "Iteration 43062 => Loss: 6.69880219733637183310\n",
      "Iteration 43063 => Loss: 6.69880211320957918986\n",
      "Iteration 43064 => Loss: 6.69880202909412236778\n",
      "Iteration 43065 => Loss: 6.69880194499000580777\n",
      "Iteration 43066 => Loss: 6.69880186089721441078\n",
      "Iteration 43067 => Loss: 6.69880177681575617044\n",
      "Iteration 43068 => Loss: 6.69880169274562842219\n",
      "Iteration 43069 => Loss: 6.69880160868682583697\n",
      "Iteration 43070 => Loss: 6.69880152463935463203\n",
      "Iteration 43071 => Loss: 6.69880144060321303101\n",
      "Iteration 43072 => Loss: 6.69880135657838948759\n",
      "Iteration 43073 => Loss: 6.69880127256489199539\n",
      "Iteration 43074 => Loss: 6.69880118856270989625\n",
      "Iteration 43075 => Loss: 6.69880110457185118378\n",
      "Iteration 43076 => Loss: 6.69880102059230786438\n",
      "Iteration 43077 => Loss: 6.69880093662408260258\n",
      "Iteration 43078 => Loss: 6.69880085266717451020\n",
      "Iteration 43079 => Loss: 6.69880076872156671186\n",
      "Iteration 43080 => Loss: 6.69880068478728674108\n",
      "Iteration 43081 => Loss: 6.69880060086430795252\n",
      "Iteration 43082 => Loss: 6.69880051695264366884\n",
      "Iteration 43083 => Loss: 6.69880043305228145556\n",
      "Iteration 43084 => Loss: 6.69880034916323108263\n",
      "Iteration 43085 => Loss: 6.69880026528547567466\n",
      "Iteration 43086 => Loss: 6.69880018141902766615\n",
      "Iteration 43087 => Loss: 6.69880009756388350439\n",
      "Iteration 43088 => Loss: 6.69880001372003253124\n",
      "Iteration 43089 => Loss: 6.69879992988748362848\n",
      "Iteration 43090 => Loss: 6.69879984606622791432\n",
      "Iteration 43091 => Loss: 6.69879976225626894148\n",
      "Iteration 43092 => Loss: 6.69879967845760404543\n",
      "Iteration 43093 => Loss: 6.69879959467023411435\n",
      "Iteration 43094 => Loss: 6.69879951089415115462\n",
      "Iteration 43095 => Loss: 6.69879942712935694260\n",
      "Iteration 43096 => Loss: 6.69879934337584792559\n",
      "Iteration 43097 => Loss: 6.69879925963362143904\n",
      "Iteration 43098 => Loss: 6.69879917590268725291\n",
      "Iteration 43099 => Loss: 6.69879909218303737362\n",
      "Iteration 43100 => Loss: 6.69879900847466380753\n",
      "Iteration 43101 => Loss: 6.69879892477757188374\n",
      "Iteration 43102 => Loss: 6.69879884109175183227\n",
      "Iteration 43103 => Loss: 6.69879875741721431126\n",
      "Iteration 43104 => Loss: 6.69879867375395665618\n",
      "Iteration 43105 => Loss: 6.69879859010196820890\n",
      "Iteration 43106 => Loss: 6.69879850646124719304\n",
      "Iteration 43107 => Loss: 6.69879842283180337859\n",
      "Iteration 43108 => Loss: 6.69879833921362166649\n",
      "Iteration 43109 => Loss: 6.69879825560671360307\n",
      "Iteration 43110 => Loss: 6.69879817201106941837\n",
      "Iteration 43111 => Loss: 6.69879808842669088875\n",
      "Iteration 43112 => Loss: 6.69879800485357534967\n",
      "Iteration 43113 => Loss: 6.69879792129172191295\n",
      "Iteration 43114 => Loss: 6.69879783774112524952\n",
      "Iteration 43115 => Loss: 6.69879775420179246481\n",
      "Iteration 43116 => Loss: 6.69879767067370668343\n",
      "Iteration 43117 => Loss: 6.69879758715688389259\n",
      "Iteration 43118 => Loss: 6.69879750365131432233\n",
      "Iteration 43119 => Loss: 6.69879742015699708446\n",
      "Iteration 43120 => Loss: 6.69879733667392951446\n",
      "Iteration 43121 => Loss: 6.69879725320211960593\n",
      "Iteration 43122 => Loss: 6.69879716974153893716\n",
      "Iteration 43123 => Loss: 6.69879708629221681804\n",
      "Iteration 43124 => Loss: 6.69879700285413726135\n",
      "Iteration 43125 => Loss: 6.69879691942730026710\n",
      "Iteration 43126 => Loss: 6.69879683601171116436\n",
      "Iteration 43127 => Loss: 6.69879675260735574227\n",
      "Iteration 43128 => Loss: 6.69879666921424377080\n",
      "Iteration 43129 => Loss: 6.69879658583236725633\n",
      "Iteration 43130 => Loss: 6.69879650246172886341\n",
      "Iteration 43131 => Loss: 6.69879641910231971025\n",
      "Iteration 43132 => Loss: 6.69879633575414334956\n",
      "Iteration 43133 => Loss: 6.69879625241719800499\n",
      "Iteration 43134 => Loss: 6.69879616909148811743\n",
      "Iteration 43135 => Loss: 6.69879608577700214056\n",
      "Iteration 43136 => Loss: 6.69879600247373918620\n",
      "Iteration 43137 => Loss: 6.69879591918170547160\n",
      "Iteration 43138 => Loss: 6.69879583590089300316\n",
      "Iteration 43139 => Loss: 6.69879575263130622176\n",
      "Iteration 43140 => Loss: 6.69879566937293713380\n",
      "Iteration 43141 => Loss: 6.69879558612578929200\n",
      "Iteration 43142 => Loss: 6.69879550288985914364\n",
      "Iteration 43143 => Loss: 6.69879541966514491236\n",
      "Iteration 43144 => Loss: 6.69879533645164126909\n",
      "Iteration 43145 => Loss: 6.69879525324935531927\n",
      "Iteration 43146 => Loss: 6.69879517005827462839\n",
      "Iteration 43147 => Loss: 6.69879508687841163095\n",
      "Iteration 43148 => Loss: 6.69879500370975922152\n",
      "Iteration 43149 => Loss: 6.69879492055230496561\n",
      "Iteration 43150 => Loss: 6.69879483740605952136\n",
      "Iteration 43151 => Loss: 6.69879475427101755969\n",
      "Iteration 43152 => Loss: 6.69879467114718085696\n",
      "Iteration 43153 => Loss: 6.69879458803454230775\n",
      "Iteration 43154 => Loss: 6.69879450493310457659\n",
      "Iteration 43155 => Loss: 6.69879442184286411077\n",
      "Iteration 43156 => Loss: 6.69879433876382357482\n",
      "Iteration 43157 => Loss: 6.69879425569596964607\n",
      "Iteration 43158 => Loss: 6.69879417263931653537\n",
      "Iteration 43159 => Loss: 6.69879408959384825550\n",
      "Iteration 43160 => Loss: 6.69879400655958079369\n",
      "Iteration 43161 => Loss: 6.69879392353649283365\n",
      "Iteration 43162 => Loss: 6.69879384052459592169\n",
      "Iteration 43163 => Loss: 6.69879375752388384058\n",
      "Iteration 43164 => Loss: 6.69879367453435836666\n",
      "Iteration 43165 => Loss: 6.69879359155601505904\n",
      "Iteration 43166 => Loss: 6.69879350858885125319\n",
      "Iteration 43167 => Loss: 6.69879342563286606094\n",
      "Iteration 43168 => Loss: 6.69879334268806392316\n",
      "Iteration 43169 => Loss: 6.69879325975443773444\n",
      "Iteration 43170 => Loss: 6.69879317683198571842\n",
      "Iteration 43171 => Loss: 6.69879309392070521056\n",
      "Iteration 43172 => Loss: 6.69879301102059621087\n",
      "Iteration 43173 => Loss: 6.69879292813166582476\n",
      "Iteration 43174 => Loss: 6.69879284525389451233\n",
      "Iteration 43175 => Loss: 6.69879276238729648441\n",
      "Iteration 43176 => Loss: 6.69879267953186463558\n",
      "Iteration 43177 => Loss: 6.69879259668760251856\n",
      "Iteration 43178 => Loss: 6.69879251385449148160\n",
      "Iteration 43179 => Loss: 6.69879243103255284097\n",
      "Iteration 43180 => Loss: 6.69879234822177060948\n",
      "Iteration 43181 => Loss: 6.69879226542214567530\n",
      "Iteration 43182 => Loss: 6.69879218263368247932\n",
      "Iteration 43183 => Loss: 6.69879209985637125158\n",
      "Iteration 43184 => Loss: 6.69879201709021376843\n",
      "Iteration 43185 => Loss: 6.69879193433521002987\n",
      "Iteration 43186 => Loss: 6.69879185159136003591\n",
      "Iteration 43187 => Loss: 6.69879176885865490476\n",
      "Iteration 43188 => Loss: 6.69879168613710263003\n",
      "Iteration 43189 => Loss: 6.69879160342669077721\n",
      "Iteration 43190 => Loss: 6.69879152072743355717\n",
      "Iteration 43191 => Loss: 6.69879143803931409451\n",
      "Iteration 43192 => Loss: 6.69879135536234127102\n",
      "Iteration 43193 => Loss: 6.69879127269650442855\n",
      "Iteration 43194 => Loss: 6.69879119004180445529\n",
      "Iteration 43195 => Loss: 6.69879110739824845666\n",
      "Iteration 43196 => Loss: 6.69879102476582577452\n",
      "Iteration 43197 => Loss: 6.69879094214453818523\n",
      "Iteration 43198 => Loss: 6.69879085953438568879\n",
      "Iteration 43199 => Loss: 6.69879077693536029159\n",
      "Iteration 43200 => Loss: 6.69879069434746643452\n",
      "Iteration 43201 => Loss: 6.69879061177070145305\n",
      "Iteration 43202 => Loss: 6.69879052920506534718\n",
      "Iteration 43203 => Loss: 6.69879044665055722874\n",
      "Iteration 43204 => Loss: 6.69879036410717176864\n",
      "Iteration 43205 => Loss: 6.69879028157490896689\n",
      "Iteration 43206 => Loss: 6.69879019905376260624\n",
      "Iteration 43207 => Loss: 6.69879011654374245666\n",
      "Iteration 43208 => Loss: 6.69879003404483785999\n",
      "Iteration 43209 => Loss: 6.69878995155704881626\n",
      "Iteration 43210 => Loss: 6.69878986908037621362\n",
      "Iteration 43211 => Loss: 6.69878978661482005208\n",
      "Iteration 43212 => Loss: 6.69878970416037322622\n",
      "Iteration 43213 => Loss: 6.69878962171703395967\n",
      "Iteration 43214 => Loss: 6.69878953928480491697\n",
      "Iteration 43215 => Loss: 6.69878945686369053902\n",
      "Iteration 43216 => Loss: 6.69878937445367839132\n",
      "Iteration 43217 => Loss: 6.69878929205476847386\n",
      "Iteration 43218 => Loss: 6.69878920966696345118\n",
      "Iteration 43219 => Loss: 6.69878912729026421147\n",
      "Iteration 43220 => Loss: 6.69878904492466098475\n",
      "Iteration 43221 => Loss: 6.69878896257015377103\n",
      "Iteration 43222 => Loss: 6.69878888022674789937\n",
      "Iteration 43223 => Loss: 6.69878879789443892889\n",
      "Iteration 43224 => Loss: 6.69878871557322508323\n",
      "Iteration 43225 => Loss: 6.69878863326310103332\n",
      "Iteration 43226 => Loss: 6.69878855096406677916\n",
      "Iteration 43227 => Loss: 6.69878846867611876803\n",
      "Iteration 43228 => Loss: 6.69878838639926588172\n",
      "Iteration 43229 => Loss: 6.69878830413350456752\n",
      "Iteration 43230 => Loss: 6.69878822187882327910\n",
      "Iteration 43231 => Loss: 6.69878813963522112829\n",
      "Iteration 43232 => Loss: 6.69878805740270522051\n",
      "Iteration 43233 => Loss: 6.69878797518127200306\n",
      "Iteration 43234 => Loss: 6.69878789297091525867\n",
      "Iteration 43235 => Loss: 6.69878781077163054647\n",
      "Iteration 43236 => Loss: 6.69878772858343563001\n",
      "Iteration 43237 => Loss: 6.69878764640630830485\n",
      "Iteration 43238 => Loss: 6.69878756424026011729\n",
      "Iteration 43239 => Loss: 6.69878748208527063923\n",
      "Iteration 43240 => Loss: 6.69878739994136029878\n",
      "Iteration 43241 => Loss: 6.69878731780852554323\n",
      "Iteration 43242 => Loss: 6.69878723568674239175\n",
      "Iteration 43243 => Loss: 6.69878715357604015423\n",
      "Iteration 43244 => Loss: 6.69878707147639307351\n",
      "Iteration 43245 => Loss: 6.69878698938781180772\n",
      "Iteration 43246 => Loss: 6.69878690731029635685\n",
      "Iteration 43247 => Loss: 6.69878682524382629282\n",
      "Iteration 43248 => Loss: 6.69878674318842737279\n",
      "Iteration 43249 => Loss: 6.69878666114408716226\n",
      "Iteration 43250 => Loss: 6.69878657911079589127\n",
      "Iteration 43251 => Loss: 6.69878649708855800071\n",
      "Iteration 43252 => Loss: 6.69878641507737615512\n",
      "Iteration 43253 => Loss: 6.69878633307724591361\n",
      "Iteration 43254 => Loss: 6.69878625108816017075\n",
      "Iteration 43255 => Loss: 6.69878616911013402557\n",
      "Iteration 43256 => Loss: 6.69878608714313816819\n",
      "Iteration 43257 => Loss: 6.69878600518719569124\n",
      "Iteration 43258 => Loss: 6.69878592324229771293\n",
      "Iteration 43259 => Loss: 6.69878584130844423328\n",
      "Iteration 43260 => Loss: 6.69878575938562459413\n",
      "Iteration 43261 => Loss: 6.69878567747384856546\n",
      "Iteration 43262 => Loss: 6.69878559557310993000\n",
      "Iteration 43263 => Loss: 6.69878551368340691141\n",
      "Iteration 43264 => Loss: 6.69878543180474039787\n",
      "Iteration 43265 => Loss: 6.69878534993710328393\n",
      "Iteration 43266 => Loss: 6.69878526808050356323\n",
      "Iteration 43267 => Loss: 6.69878518623492613671\n",
      "Iteration 43268 => Loss: 6.69878510440037988616\n",
      "Iteration 43269 => Loss: 6.69878502257686658794\n",
      "Iteration 43270 => Loss: 6.69878494076437203120\n",
      "Iteration 43271 => Loss: 6.69878485896290509771\n",
      "Iteration 43272 => Loss: 6.69878477717245424117\n",
      "Iteration 43273 => Loss: 6.69878469539303544877\n",
      "Iteration 43274 => Loss: 6.69878461362462829243\n",
      "Iteration 43275 => Loss: 6.69878453186724254209\n",
      "Iteration 43276 => Loss: 6.69878445012087642141\n",
      "Iteration 43277 => Loss: 6.69878436838551927224\n",
      "Iteration 43278 => Loss: 6.69878428666117642365\n",
      "Iteration 43279 => Loss: 6.69878420494785498107\n",
      "Iteration 43280 => Loss: 6.69878412324553718094\n",
      "Iteration 43281 => Loss: 6.69878404155423012867\n",
      "Iteration 43282 => Loss: 6.69878395987393382427\n",
      "Iteration 43283 => Loss: 6.69878387820463760960\n",
      "Iteration 43284 => Loss: 6.69878379654634681373\n",
      "Iteration 43285 => Loss: 6.69878371489906232483\n",
      "Iteration 43286 => Loss: 6.69878363326278236656\n",
      "Iteration 43287 => Loss: 6.69878355163749450440\n",
      "Iteration 43288 => Loss: 6.69878347002321472559\n",
      "Iteration 43289 => Loss: 6.69878338841993592467\n",
      "Iteration 43290 => Loss: 6.69878330682764033810\n",
      "Iteration 43291 => Loss: 6.69878322524635372304\n",
      "Iteration 43292 => Loss: 6.69878314367604499324\n",
      "Iteration 43293 => Loss: 6.69878306211673724135\n",
      "Iteration 43294 => Loss: 6.69878298056841892105\n",
      "Iteration 43295 => Loss: 6.69878289903108381509\n",
      "Iteration 43296 => Loss: 6.69878281750473991707\n",
      "Iteration 43297 => Loss: 6.69878273598938367428\n",
      "Iteration 43298 => Loss: 6.69878265448500886947\n",
      "Iteration 43299 => Loss: 6.69878257299161639082\n",
      "Iteration 43300 => Loss: 6.69878249150920712651\n",
      "Iteration 43301 => Loss: 6.69878241003777308293\n",
      "Iteration 43302 => Loss: 6.69878232857732403005\n",
      "Iteration 43303 => Loss: 6.69878224712784930972\n",
      "Iteration 43304 => Loss: 6.69878216568935158648\n",
      "Iteration 43305 => Loss: 6.69878208426182020219\n",
      "Iteration 43306 => Loss: 6.69878200284526403863\n",
      "Iteration 43307 => Loss: 6.69878192143968487215\n",
      "Iteration 43308 => Loss: 6.69878184004507293281\n",
      "Iteration 43309 => Loss: 6.69878175866142466788\n",
      "Iteration 43310 => Loss: 6.69878167728874895914\n",
      "Iteration 43311 => Loss: 6.69878159592702893121\n",
      "Iteration 43312 => Loss: 6.69878151457628145948\n",
      "Iteration 43313 => Loss: 6.69878143323649233309\n",
      "Iteration 43314 => Loss: 6.69878135190766599294\n",
      "Iteration 43315 => Loss: 6.69878127058979533359\n",
      "Iteration 43316 => Loss: 6.69878118928288035505\n",
      "Iteration 43317 => Loss: 6.69878110798693171546\n",
      "Iteration 43318 => Loss: 6.69878102670192721035\n",
      "Iteration 43319 => Loss: 6.69878094542787660970\n",
      "Iteration 43320 => Loss: 6.69878086416478524256\n",
      "Iteration 43321 => Loss: 6.69878078291263889810\n",
      "Iteration 43322 => Loss: 6.69878070167143935265\n",
      "Iteration 43323 => Loss: 6.69878062044119282348\n",
      "Iteration 43324 => Loss: 6.69878053922188243519\n",
      "Iteration 43325 => Loss: 6.69878045801352417499\n",
      "Iteration 43326 => Loss: 6.69878037681610827292\n",
      "Iteration 43327 => Loss: 6.69878029562963384080\n",
      "Iteration 43328 => Loss: 6.69878021445410176682\n",
      "Iteration 43329 => Loss: 6.69878013328950050465\n",
      "Iteration 43330 => Loss: 6.69878005213584337696\n",
      "Iteration 43331 => Loss: 6.69877997099311794926\n",
      "Iteration 43332 => Loss: 6.69877988986132599791\n",
      "Iteration 43333 => Loss: 6.69877980874046485837\n",
      "Iteration 43334 => Loss: 6.69877972763053985972\n",
      "Iteration 43335 => Loss: 6.69877964653153945562\n",
      "Iteration 43336 => Loss: 6.69877956544347341605\n",
      "Iteration 43337 => Loss: 6.69877948436632753015\n",
      "Iteration 43338 => Loss: 6.69877940330010801517\n",
      "Iteration 43339 => Loss: 6.69877932224481842383\n",
      "Iteration 43340 => Loss: 6.69877924120044276890\n",
      "Iteration 43341 => Loss: 6.69877916016699437307\n",
      "Iteration 43342 => Loss: 6.69877907914446169002\n",
      "Iteration 43343 => Loss: 6.69877899813285004882\n",
      "Iteration 43344 => Loss: 6.69877891713215323222\n",
      "Iteration 43345 => Loss: 6.69877883614237124021\n",
      "Iteration 43346 => Loss: 6.69877875516350052010\n",
      "Iteration 43347 => Loss: 6.69877867419554018369\n",
      "Iteration 43348 => Loss: 6.69877859323849200734\n",
      "Iteration 43349 => Loss: 6.69877851229235776742\n",
      "Iteration 43350 => Loss: 6.69877843135712325306\n",
      "Iteration 43351 => Loss: 6.69877835043279734606\n",
      "Iteration 43352 => Loss: 6.69877826951937827005\n",
      "Iteration 43353 => Loss: 6.69877818861685803142\n",
      "Iteration 43354 => Loss: 6.69877810772524728833\n",
      "Iteration 43355 => Loss: 6.69877802684453360627\n",
      "Iteration 43356 => Loss: 6.69877794597471787341\n",
      "Iteration 43357 => Loss: 6.69877786511579120798\n",
      "Iteration 43358 => Loss: 6.69877778426776693266\n",
      "Iteration 43359 => Loss: 6.69877770343064060654\n",
      "Iteration 43360 => Loss: 6.69877762260440778874\n",
      "Iteration 43361 => Loss: 6.69877754178905870930\n",
      "Iteration 43362 => Loss: 6.69877746098460402635\n",
      "Iteration 43363 => Loss: 6.69877738019103752265\n",
      "Iteration 43364 => Loss: 6.69877729940835475730\n",
      "Iteration 43365 => Loss: 6.69877721863656194756\n",
      "Iteration 43366 => Loss: 6.69877713787565998160\n",
      "Iteration 43367 => Loss: 6.69877705712562487861\n",
      "Iteration 43368 => Loss: 6.69877697638647884304\n",
      "Iteration 43369 => Loss: 6.69877689565820944040\n",
      "Iteration 43370 => Loss: 6.69877681494081844704\n",
      "Iteration 43371 => Loss: 6.69877673423430497479\n",
      "Iteration 43372 => Loss: 6.69877665353866991182\n",
      "Iteration 43373 => Loss: 6.69877657285390615272\n",
      "Iteration 43374 => Loss: 6.69877649218001547382\n",
      "Iteration 43375 => Loss: 6.69877641151699876332\n",
      "Iteration 43376 => Loss: 6.69877633086484269853\n",
      "Iteration 43377 => Loss: 6.69877625022356149032\n",
      "Iteration 43378 => Loss: 6.69877616959314181599\n",
      "Iteration 43379 => Loss: 6.69877608897359344553\n",
      "Iteration 43380 => Loss: 6.69877600836490127989\n",
      "Iteration 43381 => Loss: 6.69877592776707864175\n",
      "Iteration 43382 => Loss: 6.69877584718011309661\n",
      "Iteration 43383 => Loss: 6.69877576660400375630\n",
      "Iteration 43384 => Loss: 6.69877568603875417352\n",
      "Iteration 43385 => Loss: 6.69877560548436079557\n",
      "Iteration 43386 => Loss: 6.69877552494082273427\n",
      "Iteration 43387 => Loss: 6.69877544440814087778\n",
      "Iteration 43388 => Loss: 6.69877536388631078523\n",
      "Iteration 43389 => Loss: 6.69877528337532535119\n",
      "Iteration 43390 => Loss: 6.69877520287518724018\n",
      "Iteration 43391 => Loss: 6.69877512238590622218\n",
      "Iteration 43392 => Loss: 6.69877504190745920454\n",
      "Iteration 43393 => Loss: 6.69877496143986661536\n",
      "Iteration 43394 => Loss: 6.69877488098311335563\n",
      "Iteration 43395 => Loss: 6.69877480053720297803\n",
      "Iteration 43396 => Loss: 6.69877472010213192988\n",
      "Iteration 43397 => Loss: 6.69877463967789754662\n",
      "Iteration 43398 => Loss: 6.69877455926449894008\n",
      "Iteration 43399 => Loss: 6.69877447886194143933\n",
      "Iteration 43400 => Loss: 6.69877439847022149166\n",
      "Iteration 43401 => Loss: 6.69877431808932133350\n",
      "Iteration 43402 => Loss: 6.69877423771925872842\n",
      "Iteration 43403 => Loss: 6.69877415736002657098\n",
      "Iteration 43404 => Loss: 6.69877407701162574938\n",
      "Iteration 43405 => Loss: 6.69877399667405271089\n",
      "Iteration 43406 => Loss: 6.69877391634729590919\n",
      "Iteration 43407 => Loss: 6.69877383603137399604\n",
      "Iteration 43408 => Loss: 6.69877375572627009603\n",
      "Iteration 43409 => Loss: 6.69877367543198420918\n",
      "Iteration 43410 => Loss: 6.69877359514852255273\n",
      "Iteration 43411 => Loss: 6.69877351487588068579\n",
      "Iteration 43412 => Loss: 6.69877343461404883840\n",
      "Iteration 43413 => Loss: 6.69877335436303678051\n",
      "Iteration 43414 => Loss: 6.69877327412283385399\n",
      "Iteration 43415 => Loss: 6.69877319389345249334\n",
      "Iteration 43416 => Loss: 6.69877311367487049409\n",
      "Iteration 43417 => Loss: 6.69877303346711006071\n",
      "Iteration 43418 => Loss: 6.69877295327014898874\n",
      "Iteration 43419 => Loss: 6.69877287308399793631\n",
      "Iteration 43420 => Loss: 6.69877279290864446892\n",
      "Iteration 43421 => Loss: 6.69877271274410279744\n",
      "Iteration 43422 => Loss: 6.69877263259036315191\n",
      "Iteration 43423 => Loss: 6.69877255244742375595\n",
      "Iteration 43424 => Loss: 6.69877247231528194504\n",
      "Iteration 43425 => Loss: 6.69877239219393683101\n",
      "Iteration 43426 => Loss: 6.69877231208339463109\n",
      "Iteration 43427 => Loss: 6.69877223198363758172\n",
      "Iteration 43428 => Loss: 6.69877215189468344647\n",
      "Iteration 43429 => Loss: 6.69877207181651090906\n",
      "Iteration 43430 => Loss: 6.69877199174913950941\n",
      "Iteration 43431 => Loss: 6.69877191169254881942\n",
      "Iteration 43432 => Loss: 6.69877183164675660265\n",
      "Iteration 43433 => Loss: 6.69877175161173976647\n",
      "Iteration 43434 => Loss: 6.69877167158750808085\n",
      "Iteration 43435 => Loss: 6.69877159157406332213\n",
      "Iteration 43436 => Loss: 6.69877151157140193760\n",
      "Iteration 43437 => Loss: 6.69877143157951326913\n",
      "Iteration 43438 => Loss: 6.69877135159841774481\n",
      "Iteration 43439 => Loss: 6.69877127162808960747\n",
      "Iteration 43440 => Loss: 6.69877119166853685073\n",
      "Iteration 43441 => Loss: 6.69877111171975858639\n",
      "Iteration 43442 => Loss: 6.69877103178175659082\n",
      "Iteration 43443 => Loss: 6.69877095185452553494\n",
      "Iteration 43444 => Loss: 6.69877087193805831333\n",
      "Iteration 43445 => Loss: 6.69877079203236913685\n",
      "Iteration 43446 => Loss: 6.69877071213744734735\n",
      "Iteration 43447 => Loss: 6.69877063225329116847\n",
      "Iteration 43448 => Loss: 6.69877055237989527114\n",
      "Iteration 43449 => Loss: 6.69877047251725521448\n",
      "Iteration 43450 => Loss: 6.69877039266539320295\n",
      "Iteration 43451 => Loss: 6.69877031282428259118\n",
      "Iteration 43452 => Loss: 6.69877023299393137279\n",
      "Iteration 43453 => Loss: 6.69877015317433510688\n",
      "Iteration 43454 => Loss: 6.69877007336549468164\n",
      "Iteration 43455 => Loss: 6.69876999356741187341\n",
      "Iteration 43456 => Loss: 6.69876991378007957678\n",
      "Iteration 43457 => Loss: 6.69876983400349423903\n",
      "Iteration 43458 => Loss: 6.69876975423766474194\n",
      "Iteration 43459 => Loss: 6.69876967448258220372\n",
      "Iteration 43460 => Loss: 6.69876959473824840074\n",
      "Iteration 43461 => Loss: 6.69876951500465978029\n",
      "Iteration 43462 => Loss: 6.69876943528181545418\n",
      "Iteration 43463 => Loss: 6.69876935556970654062\n",
      "Iteration 43464 => Loss: 6.69876927586834813866\n",
      "Iteration 43465 => Loss: 6.69876919617772514925\n",
      "Iteration 43466 => Loss: 6.69876911649784734237\n",
      "Iteration 43467 => Loss: 6.69876903682869784262\n",
      "Iteration 43468 => Loss: 6.69876895717028819632\n",
      "Iteration 43469 => Loss: 6.69876887752260952169\n",
      "Iteration 43470 => Loss: 6.69876879788566270690\n",
      "Iteration 43471 => Loss: 6.69876871825944952832\n",
      "Iteration 43472 => Loss: 6.69876863864396643322\n",
      "Iteration 43473 => Loss: 6.69876855903920809254\n",
      "Iteration 43474 => Loss: 6.69876847944518072353\n",
      "Iteration 43475 => Loss: 6.69876839986187633258\n",
      "Iteration 43476 => Loss: 6.69876832028929758422\n",
      "Iteration 43477 => Loss: 6.69876824072743826122\n",
      "Iteration 43478 => Loss: 6.69876816117630546898\n",
      "Iteration 43479 => Loss: 6.69876808163588766121\n",
      "Iteration 43480 => Loss: 6.69876800210618572606\n",
      "Iteration 43481 => Loss: 6.69876792258720055173\n",
      "Iteration 43482 => Loss: 6.69876784307893835546\n",
      "Iteration 43483 => Loss: 6.69876776358138492640\n",
      "Iteration 43484 => Loss: 6.69876768409454736997\n",
      "Iteration 43485 => Loss: 6.69876760461841236349\n",
      "Iteration 43486 => Loss: 6.69876752515299589419\n",
      "Iteration 43487 => Loss: 6.69876744569828108666\n",
      "Iteration 43488 => Loss: 6.69876736625427149363\n",
      "Iteration 43489 => Loss: 6.69876728682096711509\n",
      "Iteration 43490 => Loss: 6.69876720739837505647\n",
      "Iteration 43491 => Loss: 6.69876712798647666602\n",
      "Iteration 43492 => Loss: 6.69876704858528349007\n",
      "Iteration 43493 => Loss: 6.69876696919479108772\n",
      "Iteration 43494 => Loss: 6.69876688981499412989\n",
      "Iteration 43495 => Loss: 6.69876681044588639935\n",
      "Iteration 43496 => Loss: 6.69876673108747944241\n",
      "Iteration 43497 => Loss: 6.69876665173976615364\n",
      "Iteration 43498 => Loss: 6.69876657240274919758\n",
      "Iteration 43499 => Loss: 6.69876649307641880426\n",
      "Iteration 43500 => Loss: 6.69876641376077675005\n",
      "Iteration 43501 => Loss: 6.69876633445582925219\n",
      "Iteration 43502 => Loss: 6.69876625516156121165\n",
      "Iteration 43503 => Loss: 6.69876617587798151021\n",
      "Iteration 43504 => Loss: 6.69876609660508304245\n",
      "Iteration 43505 => Loss: 6.69876601734286314382\n",
      "Iteration 43506 => Loss: 6.69876593809132803159\n",
      "Iteration 43507 => Loss: 6.69876585885047326485\n",
      "Iteration 43508 => Loss: 6.69876577962029617908\n",
      "Iteration 43509 => Loss: 6.69876570040079766244\n",
      "Iteration 43510 => Loss: 6.69876562119196794498\n",
      "Iteration 43511 => Loss: 6.69876554199381324395\n",
      "Iteration 43512 => Loss: 6.69876546280633000663\n",
      "Iteration 43513 => Loss: 6.69876538362951734484\n",
      "Iteration 43514 => Loss: 6.69876530446337792313\n",
      "Iteration 43515 => Loss: 6.69876522530790463605\n",
      "Iteration 43516 => Loss: 6.69876514616308949002\n",
      "Iteration 43517 => Loss: 6.69876506702895024858\n",
      "Iteration 43518 => Loss: 6.69876498790546559547\n",
      "Iteration 43519 => Loss: 6.69876490879265062972\n",
      "Iteration 43520 => Loss: 6.69876482969049291682\n",
      "Iteration 43521 => Loss: 6.69876475059899245679\n",
      "Iteration 43522 => Loss: 6.69876467151814924961\n",
      "Iteration 43523 => Loss: 6.69876459244796329529\n",
      "Iteration 43524 => Loss: 6.69876451338843637018\n",
      "Iteration 43525 => Loss: 6.69876443433955515161\n",
      "Iteration 43526 => Loss: 6.69876435530133296226\n",
      "Iteration 43527 => Loss: 6.69876427627375559126\n",
      "Iteration 43528 => Loss: 6.69876419725682659134\n",
      "Iteration 43529 => Loss: 6.69876411825055217975\n",
      "Iteration 43530 => Loss: 6.69876403925492258651\n",
      "Iteration 43531 => Loss: 6.69876396026993070620\n",
      "Iteration 43532 => Loss: 6.69876388129558986151\n",
      "Iteration 43533 => Loss: 6.69876380233188495339\n",
      "Iteration 43534 => Loss: 6.69876372337882397545\n",
      "Iteration 43535 => Loss: 6.69876364443640071045\n",
      "Iteration 43536 => Loss: 6.69876356550460894113\n",
      "Iteration 43537 => Loss: 6.69876348658346021381\n",
      "Iteration 43538 => Loss: 6.69876340767294298217\n",
      "Iteration 43539 => Loss: 6.69876332877306346347\n",
      "Iteration 43540 => Loss: 6.69876324988381011138\n",
      "Iteration 43541 => Loss: 6.69876317100519980130\n",
      "Iteration 43542 => Loss: 6.69876309213720233515\n",
      "Iteration 43543 => Loss: 6.69876301327984169376\n",
      "Iteration 43544 => Loss: 6.69876293443309922537\n",
      "Iteration 43545 => Loss: 6.69876285559699091721\n",
      "Iteration 43546 => Loss: 6.69876277677149900569\n",
      "Iteration 43547 => Loss: 6.69876269795662793172\n",
      "Iteration 43548 => Loss: 6.69876261915238657707\n",
      "Iteration 43549 => Loss: 6.69876254035875362547\n",
      "Iteration 43550 => Loss: 6.69876246157574328777\n",
      "Iteration 43551 => Loss: 6.69876238280334579400\n",
      "Iteration 43552 => Loss: 6.69876230404156558507\n",
      "Iteration 43553 => Loss: 6.69876222529039555553\n",
      "Iteration 43554 => Loss: 6.69876214654984192265\n",
      "Iteration 43555 => Loss: 6.69876206781989758099\n",
      "Iteration 43556 => Loss: 6.69876198910056075420\n",
      "Iteration 43557 => Loss: 6.69876191039182788955\n",
      "Iteration 43558 => Loss: 6.69876183169370431614\n",
      "Iteration 43559 => Loss: 6.69876175300618736941\n",
      "Iteration 43560 => Loss: 6.69876167432927438483\n",
      "Iteration 43561 => Loss: 6.69876159566295292791\n",
      "Iteration 43562 => Loss: 6.69876151700724431493\n",
      "Iteration 43563 => Loss: 6.69876143836213167049\n",
      "Iteration 43564 => Loss: 6.69876135972761233006\n",
      "Iteration 43565 => Loss: 6.69876128110368807000\n",
      "Iteration 43566 => Loss: 6.69876120249036777210\n",
      "Iteration 43567 => Loss: 6.69876112388762567917\n",
      "Iteration 43568 => Loss: 6.69876104529548666022\n",
      "Iteration 43569 => Loss: 6.69876096671393561621\n",
      "Iteration 43570 => Loss: 6.69876088814297876439\n",
      "Iteration 43571 => Loss: 6.69876080958259922937\n",
      "Iteration 43572 => Loss: 6.69876073103281033383\n",
      "Iteration 43573 => Loss: 6.69876065249361030141\n",
      "Iteration 43574 => Loss: 6.69876057396498492125\n",
      "Iteration 43575 => Loss: 6.69876049544694218696\n",
      "Iteration 43576 => Loss: 6.69876041693948209854\n",
      "Iteration 43577 => Loss: 6.69876033844260643235\n",
      "Iteration 43578 => Loss: 6.69876025995630186571\n",
      "Iteration 43579 => Loss: 6.69876018148057816859\n",
      "Iteration 43580 => Loss: 6.69876010301541757741\n",
      "Iteration 43581 => Loss: 6.69876002456084229664\n",
      "Iteration 43582 => Loss: 6.69875994611683722724\n",
      "Iteration 43583 => Loss: 6.69875986768340148103\n",
      "Iteration 43584 => Loss: 6.69875978926052528806\n",
      "Iteration 43585 => Loss: 6.69875971084823174095\n",
      "Iteration 43586 => Loss: 6.69875963244649597073\n",
      "Iteration 43587 => Loss: 6.69875955405532330644\n",
      "Iteration 43588 => Loss: 6.69875947567471996535\n",
      "Iteration 43589 => Loss: 6.69875939730467084843\n",
      "Iteration 43590 => Loss: 6.69875931894518927834\n",
      "Iteration 43591 => Loss: 6.69875924059626459695\n",
      "Iteration 43592 => Loss: 6.69875916225789058700\n",
      "Iteration 43593 => Loss: 6.69875908393007701846\n",
      "Iteration 43594 => Loss: 6.69875900561282300316\n",
      "Iteration 43595 => Loss: 6.69875892730611965931\n",
      "Iteration 43596 => Loss: 6.69875884900996698690\n",
      "Iteration 43597 => Loss: 6.69875877072436498594\n",
      "Iteration 43598 => Loss: 6.69875869244931099189\n",
      "Iteration 43599 => Loss: 6.69875861418480589293\n",
      "Iteration 43600 => Loss: 6.69875853593084436000\n",
      "Iteration 43601 => Loss: 6.69875845768743172215\n",
      "Iteration 43602 => Loss: 6.69875837945456087397\n",
      "Iteration 43603 => Loss: 6.69875830123223359180\n",
      "Iteration 43604 => Loss: 6.69875822302044188206\n",
      "Iteration 43605 => Loss: 6.69875814481918929744\n",
      "Iteration 43606 => Loss: 6.69875806662848383155\n",
      "Iteration 43607 => Loss: 6.69875798844830239176\n",
      "Iteration 43608 => Loss: 6.69875791027866274163\n",
      "Iteration 43609 => Loss: 6.69875783211955333485\n",
      "Iteration 43610 => Loss: 6.69875775397097772412\n",
      "Iteration 43611 => Loss: 6.69875767583293324492\n",
      "Iteration 43612 => Loss: 6.69875759770542078542\n",
      "Iteration 43613 => Loss: 6.69875751958842791112\n",
      "Iteration 43614 => Loss: 6.69875744148196972105\n",
      "Iteration 43615 => Loss: 6.69875736338603555708\n",
      "Iteration 43616 => Loss: 6.69875728530062009014\n",
      "Iteration 43617 => Loss: 6.69875720722572687293\n",
      "Iteration 43618 => Loss: 6.69875712916135590547\n",
      "Iteration 43619 => Loss: 6.69875705110750363502\n",
      "Iteration 43620 => Loss: 6.69875697306416562071\n",
      "Iteration 43621 => Loss: 6.69875689503134896796\n",
      "Iteration 43622 => Loss: 6.69875681700904390681\n",
      "Iteration 43623 => Loss: 6.69875673899725487814\n",
      "Iteration 43624 => Loss: 6.69875666099597921743\n",
      "Iteration 43625 => Loss: 6.69875658300521248378\n",
      "Iteration 43626 => Loss: 6.69875650502495467720\n",
      "Iteration 43627 => Loss: 6.69875642705520313314\n",
      "Iteration 43628 => Loss: 6.69875634909595962796\n",
      "Iteration 43629 => Loss: 6.69875627114722416167\n",
      "Iteration 43630 => Loss: 6.69875619320898429976\n",
      "Iteration 43631 => Loss: 6.69875611528125336491\n",
      "Iteration 43632 => Loss: 6.69875603736402513988\n",
      "Iteration 43633 => Loss: 6.69875595945729251923\n",
      "Iteration 43634 => Loss: 6.69875588156105639115\n",
      "Iteration 43635 => Loss: 6.69875580367531764381\n",
      "Iteration 43636 => Loss: 6.69875572580007894175\n",
      "Iteration 43637 => Loss: 6.69875564793532785046\n",
      "Iteration 43638 => Loss: 6.69875557008106969903\n",
      "Iteration 43639 => Loss: 6.69875549223730271109\n",
      "Iteration 43640 => Loss: 6.69875541440402777482\n",
      "Iteration 43641 => Loss: 6.69875533658123956116\n",
      "Iteration 43642 => Loss: 6.69875525876893895827\n",
      "Iteration 43643 => Loss: 6.69875518096711797256\n",
      "Iteration 43644 => Loss: 6.69875510317578637398\n",
      "Iteration 43645 => Loss: 6.69875502539493439258\n",
      "Iteration 43646 => Loss: 6.69875494762456380471\n",
      "Iteration 43647 => Loss: 6.69875486986467638673\n",
      "Iteration 43648 => Loss: 6.69875479211526503320\n",
      "Iteration 43649 => Loss: 6.69875471437632619143\n",
      "Iteration 43650 => Loss: 6.69875463664786430229\n",
      "Iteration 43651 => Loss: 6.69875455892988647122\n",
      "Iteration 43652 => Loss: 6.69875448122237049375\n",
      "Iteration 43653 => Loss: 6.69875440352532880439\n",
      "Iteration 43654 => Loss: 6.69875432583875429771\n",
      "Iteration 43655 => Loss: 6.69875424816264875005\n",
      "Iteration 43656 => Loss: 6.69875417049701482597\n",
      "Iteration 43657 => Loss: 6.69875409284184364367\n",
      "Iteration 43658 => Loss: 6.69875401519713253862\n",
      "Iteration 43659 => Loss: 6.69875393756288772806\n",
      "Iteration 43660 => Loss: 6.69875385993910832383\n",
      "Iteration 43661 => Loss: 6.69875378232577833870\n",
      "Iteration 43662 => Loss: 6.69875370472291375989\n",
      "Iteration 43663 => Loss: 6.69875362713051103469\n",
      "Iteration 43664 => Loss: 6.69875354954855506406\n",
      "Iteration 43665 => Loss: 6.69875347197705295343\n",
      "Iteration 43666 => Loss: 6.69875339441600647916\n",
      "Iteration 43667 => Loss: 6.69875331686541652942\n",
      "Iteration 43668 => Loss: 6.69875323932526978155\n",
      "Iteration 43669 => Loss: 6.69875316179556801188\n",
      "Iteration 43670 => Loss: 6.69875308427631743768\n",
      "Iteration 43671 => Loss: 6.69875300676752072349\n",
      "Iteration 43672 => Loss: 6.69875292926915832936\n",
      "Iteration 43673 => Loss: 6.69875285178124446617\n",
      "Iteration 43674 => Loss: 6.69875277430376581123\n",
      "Iteration 43675 => Loss: 6.69875269683673479904\n",
      "Iteration 43676 => Loss: 6.69875261938013455421\n",
      "Iteration 43677 => Loss: 6.69875254193398017577\n",
      "Iteration 43678 => Loss: 6.69875246449825301198\n",
      "Iteration 43679 => Loss: 6.69875238707296283280\n",
      "Iteration 43680 => Loss: 6.69875230965810608552\n",
      "Iteration 43681 => Loss: 6.69875223225368987556\n",
      "Iteration 43682 => Loss: 6.69875215485969288665\n",
      "Iteration 43683 => Loss: 6.69875207747613199416\n",
      "Iteration 43684 => Loss: 6.69875200010298943454\n",
      "Iteration 43685 => Loss: 6.69875192274028030681\n",
      "Iteration 43686 => Loss: 6.69875184538799217648\n",
      "Iteration 43687 => Loss: 6.69875176804612859627\n",
      "Iteration 43688 => Loss: 6.69875169071468690163\n",
      "Iteration 43689 => Loss: 6.69875161339366620439\n",
      "Iteration 43690 => Loss: 6.69875153608306472819\n",
      "Iteration 43691 => Loss: 6.69875145878287892032\n",
      "Iteration 43692 => Loss: 6.69875138149311144531\n",
      "Iteration 43693 => Loss: 6.69875130421376141499\n",
      "Iteration 43694 => Loss: 6.69875122694481905938\n",
      "Iteration 43695 => Loss: 6.69875114968629148393\n",
      "Iteration 43696 => Loss: 6.69875107243817158320\n",
      "Iteration 43697 => Loss: 6.69875099520046646262\n",
      "Iteration 43698 => Loss: 6.69875091797316546405\n",
      "Iteration 43699 => Loss: 6.69875084075627302838\n",
      "Iteration 43700 => Loss: 6.69875076354978471471\n",
      "Iteration 43701 => Loss: 6.69875068635370052306\n",
      "Iteration 43702 => Loss: 6.69875060916801334798\n",
      "Iteration 43703 => Loss: 6.69875053199272763038\n",
      "Iteration 43704 => Loss: 6.69875045482785047568\n",
      "Iteration 43705 => Loss: 6.69875037767335879124\n",
      "Iteration 43706 => Loss: 6.69875030052927389335\n",
      "Iteration 43707 => Loss: 6.69875022339557535389\n",
      "Iteration 43708 => Loss: 6.69875014627227916009\n",
      "Iteration 43709 => Loss: 6.69875006915937198926\n",
      "Iteration 43710 => Loss: 6.69874999205685739412\n",
      "Iteration 43711 => Loss: 6.69874991496473182195\n",
      "Iteration 43712 => Loss: 6.69874983788298994369\n",
      "Iteration 43713 => Loss: 6.69874976081163975294\n",
      "Iteration 43714 => Loss: 6.69874968375067059156\n",
      "Iteration 43715 => Loss: 6.69874960670009045316\n",
      "Iteration 43716 => Loss: 6.69874952965988956777\n",
      "Iteration 43717 => Loss: 6.69874945263007415264\n",
      "Iteration 43718 => Loss: 6.69874937561062999691\n",
      "Iteration 43719 => Loss: 6.69874929860157042327\n",
      "Iteration 43720 => Loss: 6.69874922160289099082\n",
      "Iteration 43721 => Loss: 6.69874914461458104142\n",
      "Iteration 43722 => Loss: 6.69874906763664501597\n",
      "Iteration 43723 => Loss: 6.69874899066908557899\n",
      "Iteration 43724 => Loss: 6.69874891371189562506\n",
      "Iteration 43725 => Loss: 6.69874883676507515418\n",
      "Iteration 43726 => Loss: 6.69874875982862238999\n",
      "Iteration 43727 => Loss: 6.69874868290253822067\n",
      "Iteration 43728 => Loss: 6.69874860598682264623\n",
      "Iteration 43729 => Loss: 6.69874852908146589670\n",
      "Iteration 43730 => Loss: 6.69874845218648218292\n",
      "Iteration 43731 => Loss: 6.69874837530184930046\n",
      "Iteration 43732 => Loss: 6.69874829842757790743\n",
      "Iteration 43733 => Loss: 6.69874822156366800385\n",
      "Iteration 43734 => Loss: 6.69874814471012047790\n",
      "Iteration 43735 => Loss: 6.69874806786692200689\n",
      "Iteration 43736 => Loss: 6.69874799103407880807\n",
      "Iteration 43737 => Loss: 6.69874791421159354599\n",
      "Iteration 43738 => Loss: 6.69874783739944668071\n",
      "Iteration 43739 => Loss: 6.69874776059766130487\n",
      "Iteration 43740 => Loss: 6.69874768380622764852\n",
      "Iteration 43741 => Loss: 6.69874760702513949440\n",
      "Iteration 43742 => Loss: 6.69874753025439417797\n",
      "Iteration 43743 => Loss: 6.69874745349399169925\n",
      "Iteration 43744 => Loss: 6.69874737674394005182\n",
      "Iteration 43745 => Loss: 6.69874730000422680121\n",
      "Iteration 43746 => Loss: 6.69874722327485194739\n",
      "Iteration 43747 => Loss: 6.69874714655581993128\n",
      "Iteration 43748 => Loss: 6.69874706984712720015\n",
      "Iteration 43749 => Loss: 6.69874699314876753675\n",
      "Iteration 43750 => Loss: 6.69874691646074271745\n",
      "Iteration 43751 => Loss: 6.69874683978305451859\n",
      "Iteration 43752 => Loss: 6.69874676311569405840\n",
      "Iteration 43753 => Loss: 6.69874668645866844230\n",
      "Iteration 43754 => Loss: 6.69874660981196434761\n",
      "Iteration 43755 => Loss: 6.69874653317560220245\n",
      "Iteration 43756 => Loss: 6.69874645654956335505\n",
      "Iteration 43757 => Loss: 6.69874637993384247636\n",
      "Iteration 43758 => Loss: 6.69874630332845111269\n",
      "Iteration 43759 => Loss: 6.69874622673338038226\n",
      "Iteration 43760 => Loss: 6.69874615014862762052\n",
      "Iteration 43761 => Loss: 6.69874607357420526199\n",
      "Iteration 43762 => Loss: 6.69874599701008932584\n",
      "Iteration 43763 => Loss: 6.69874592045629757564\n",
      "Iteration 43764 => Loss: 6.69874584391281402418\n",
      "Iteration 43765 => Loss: 6.69874576737965465867\n",
      "Iteration 43766 => Loss: 6.69874569085679993918\n",
      "Iteration 43767 => Loss: 6.69874561434426230022\n",
      "Iteration 43768 => Loss: 6.69874553784203374818\n",
      "Iteration 43769 => Loss: 6.69874546135011339487\n",
      "Iteration 43770 => Loss: 6.69874538486849768759\n",
      "Iteration 43771 => Loss: 6.69874530839718307362\n",
      "Iteration 43772 => Loss: 6.69874523193618909289\n",
      "Iteration 43773 => Loss: 6.69874515548548821187\n",
      "Iteration 43774 => Loss: 6.69874507904508753597\n",
      "Iteration 43775 => Loss: 6.69874500261498884157\n",
      "Iteration 43776 => Loss: 6.69874492619519479319\n",
      "Iteration 43777 => Loss: 6.69874484978568851545\n",
      "Iteration 43778 => Loss: 6.69874477338648333102\n",
      "Iteration 43779 => Loss: 6.69874469699757302266\n",
      "Iteration 43780 => Loss: 6.69874462061895314946\n",
      "Iteration 43781 => Loss: 6.69874454425063081686\n",
      "Iteration 43782 => Loss: 6.69874446789259803126\n",
      "Iteration 43783 => Loss: 6.69874439154485390446\n",
      "Iteration 43784 => Loss: 6.69874431520739399559\n",
      "Iteration 43785 => Loss: 6.69874423888022452189\n",
      "Iteration 43786 => Loss: 6.69874416256333837794\n",
      "Iteration 43787 => Loss: 6.69874408625673556372\n",
      "Iteration 43788 => Loss: 6.69874400996041341472\n",
      "Iteration 43789 => Loss: 6.69874393367438081270\n",
      "Iteration 43790 => Loss: 6.69874385739861999411\n",
      "Iteration 43791 => Loss: 6.69874378113313539984\n",
      "Iteration 43792 => Loss: 6.69874370487793147078\n",
      "Iteration 43793 => Loss: 6.69874362863300287785\n",
      "Iteration 43794 => Loss: 6.69874355239834340381\n",
      "Iteration 43795 => Loss: 6.69874347617396459498\n",
      "Iteration 43796 => Loss: 6.69874339995985845775\n",
      "Iteration 43797 => Loss: 6.69874332375601166945\n",
      "Iteration 43798 => Loss: 6.69874324756244288182\n",
      "Iteration 43799 => Loss: 6.69874317137913699582\n",
      "Iteration 43800 => Loss: 6.69874309520609845237\n",
      "Iteration 43801 => Loss: 6.69874301904332281055\n",
      "Iteration 43802 => Loss: 6.69874294289080740583\n",
      "Iteration 43803 => Loss: 6.69874286674855401458\n",
      "Iteration 43804 => Loss: 6.69874279061656263679\n",
      "Iteration 43805 => Loss: 6.69874271449483593699\n",
      "Iteration 43806 => Loss: 6.69874263838335703980\n",
      "Iteration 43807 => Loss: 6.69874256228214015607\n",
      "Iteration 43808 => Loss: 6.69874248619117640402\n",
      "Iteration 43809 => Loss: 6.69874241011046756000\n",
      "Iteration 43810 => Loss: 6.69874233404000740677\n",
      "Iteration 43811 => Loss: 6.69874225797979772068\n",
      "Iteration 43812 => Loss: 6.69874218192983938991\n",
      "Iteration 43813 => Loss: 6.69874210589012530903\n",
      "Iteration 43814 => Loss: 6.69874202986065991894\n",
      "Iteration 43815 => Loss: 6.69874195384143877874\n",
      "Iteration 43816 => Loss: 6.69874187783245922390\n",
      "Iteration 43817 => Loss: 6.69874180183373368891\n",
      "Iteration 43818 => Loss: 6.69874172584523019935\n",
      "Iteration 43819 => Loss: 6.69874164986697984148\n",
      "Iteration 43820 => Loss: 6.69874157389896218717\n",
      "Iteration 43821 => Loss: 6.69874149794118700640\n",
      "Iteration 43822 => Loss: 6.69874142199364275285\n",
      "Iteration 43823 => Loss: 6.69874134605633209105\n",
      "Iteration 43824 => Loss: 6.69874127012925324465\n",
      "Iteration 43825 => Loss: 6.69874119421240710182\n",
      "Iteration 43826 => Loss: 6.69874111830579632709\n",
      "Iteration 43827 => Loss: 6.69874104240940493327\n",
      "Iteration 43828 => Loss: 6.69874096652323736123\n",
      "Iteration 43829 => Loss: 6.69874089064730871002\n",
      "Iteration 43830 => Loss: 6.69874081478159588698\n",
      "Iteration 43831 => Loss: 6.69874073892610777392\n",
      "Iteration 43832 => Loss: 6.69874066308084614718\n",
      "Iteration 43833 => Loss: 6.69874058724579235502\n",
      "Iteration 43834 => Loss: 6.69874051142096771372\n",
      "Iteration 43835 => Loss: 6.69874043560635268335\n",
      "Iteration 43836 => Loss: 6.69874035980195881024\n",
      "Iteration 43837 => Loss: 6.69874028400778165349\n",
      "Iteration 43838 => Loss: 6.69874020822381410767\n",
      "Iteration 43839 => Loss: 6.69874013245006150186\n",
      "Iteration 43840 => Loss: 6.69874005668651406609\n",
      "Iteration 43841 => Loss: 6.69873998093318245850\n",
      "Iteration 43842 => Loss: 6.69873990519005513278\n",
      "Iteration 43843 => Loss: 6.69873982945713120074\n",
      "Iteration 43844 => Loss: 6.69873975373442043235\n",
      "Iteration 43845 => Loss: 6.69873967802190417586\n",
      "Iteration 43846 => Loss: 6.69873960231959930667\n",
      "Iteration 43847 => Loss: 6.69873952662748539666\n",
      "Iteration 43848 => Loss: 6.69873945094557488034\n",
      "Iteration 43849 => Loss: 6.69873937527386598134\n",
      "Iteration 43850 => Loss: 6.69873929961234715336\n",
      "Iteration 43851 => Loss: 6.69873922396103349541\n",
      "Iteration 43852 => Loss: 6.69873914831991079666\n",
      "Iteration 43853 => Loss: 6.69873907268897461620\n",
      "Iteration 43854 => Loss: 6.69873899706823205946\n",
      "Iteration 43855 => Loss: 6.69873892145768579098\n",
      "Iteration 43856 => Loss: 6.69873884585731804719\n",
      "Iteration 43857 => Loss: 6.69873877026714303895\n",
      "Iteration 43858 => Loss: 6.69873869468715366082\n",
      "Iteration 43859 => Loss: 6.69873861911734991281\n",
      "Iteration 43860 => Loss: 6.69873854355772468949\n",
      "Iteration 43861 => Loss: 6.69873846800828953718\n",
      "Iteration 43862 => Loss: 6.69873839246902580413\n",
      "Iteration 43863 => Loss: 6.69873831693994237213\n",
      "Iteration 43864 => Loss: 6.69873824142104368207\n",
      "Iteration 43865 => Loss: 6.69873816591231285855\n",
      "Iteration 43866 => Loss: 6.69873809041375967155\n",
      "Iteration 43867 => Loss: 6.69873801492537968016\n",
      "Iteration 43868 => Loss: 6.69873793944717288440\n",
      "Iteration 43869 => Loss: 6.69873786397913661972\n",
      "Iteration 43870 => Loss: 6.69873778852127088612\n",
      "Iteration 43871 => Loss: 6.69873771307357035454\n",
      "Iteration 43872 => Loss: 6.69873763763603502497\n",
      "Iteration 43873 => Loss: 6.69873756220866667377\n",
      "Iteration 43874 => Loss: 6.69873748679146352458\n",
      "Iteration 43875 => Loss: 6.69873741138442646559\n",
      "Iteration 43876 => Loss: 6.69873733598754572682\n",
      "Iteration 43877 => Loss: 6.69873726060082397282\n",
      "Iteration 43878 => Loss: 6.69873718522425765087\n",
      "Iteration 43879 => Loss: 6.69873710985785386640\n",
      "Iteration 43880 => Loss: 6.69873703450160640216\n",
      "Iteration 43881 => Loss: 6.69873695915551703450\n",
      "Iteration 43882 => Loss: 6.69873688381956533533\n",
      "Iteration 43883 => Loss: 6.69873680849377528546\n",
      "Iteration 43884 => Loss: 6.69873673317813711492\n",
      "Iteration 43885 => Loss: 6.69873665787264638283\n",
      "Iteration 43886 => Loss: 6.69873658257729864829\n",
      "Iteration 43887 => Loss: 6.69873650729210101673\n",
      "Iteration 43888 => Loss: 6.69873643201704727090\n",
      "Iteration 43889 => Loss: 6.69873635675213829899\n",
      "Iteration 43890 => Loss: 6.69873628149736877191\n",
      "Iteration 43891 => Loss: 6.69873620625274135421\n",
      "Iteration 43892 => Loss: 6.69873613101825693406\n",
      "Iteration 43893 => Loss: 6.69873605579390574150\n",
      "Iteration 43894 => Loss: 6.69873598057968955288\n",
      "Iteration 43895 => Loss: 6.69873590537561103275\n",
      "Iteration 43896 => Loss: 6.69873583018166574021\n",
      "Iteration 43897 => Loss: 6.69873575499785456344\n",
      "Iteration 43898 => Loss: 6.69873567982416862066\n",
      "Iteration 43899 => Loss: 6.69873560466061501728\n",
      "Iteration 43900 => Loss: 6.69873552950719286514\n",
      "Iteration 43901 => Loss: 6.69873545436389594698\n",
      "Iteration 43902 => Loss: 6.69873537923072248645\n",
      "Iteration 43903 => Loss: 6.69873530410767337173\n",
      "Iteration 43904 => Loss: 6.69873522899474860282\n",
      "Iteration 43905 => Loss: 6.69873515389194640335\n",
      "Iteration 43906 => Loss: 6.69873507879926588515\n",
      "Iteration 43907 => Loss: 6.69873500371669994280\n",
      "Iteration 43908 => Loss: 6.69873492864425568172\n",
      "Iteration 43909 => Loss: 6.69873485358192066741\n",
      "Iteration 43910 => Loss: 6.69873477852970733437\n",
      "Iteration 43911 => Loss: 6.69873470348760857718\n",
      "Iteration 43912 => Loss: 6.69873462845561018497\n",
      "Iteration 43913 => Loss: 6.69873455343373169768\n",
      "Iteration 43914 => Loss: 6.69873447842196423352\n",
      "Iteration 43915 => Loss: 6.69873440342029713435\n",
      "Iteration 43916 => Loss: 6.69873432842874194648\n",
      "Iteration 43917 => Loss: 6.69873425344728889996\n",
      "Iteration 43918 => Loss: 6.69873417847594243568\n",
      "Iteration 43919 => Loss: 6.69873410351469544821\n",
      "Iteration 43920 => Loss: 6.69873402856355060209\n",
      "Iteration 43921 => Loss: 6.69873395362251056184\n",
      "Iteration 43922 => Loss: 6.69873387869156200480\n",
      "Iteration 43923 => Loss: 6.69873380377071292457\n",
      "Iteration 43924 => Loss: 6.69873372885995710391\n",
      "Iteration 43925 => Loss: 6.69873365395929720734\n",
      "Iteration 43926 => Loss: 6.69873357906873145851\n",
      "Iteration 43927 => Loss: 6.69873350418825630470\n",
      "Iteration 43928 => Loss: 6.69873342931787263410\n",
      "Iteration 43929 => Loss: 6.69873335445757156492\n",
      "Iteration 43930 => Loss: 6.69873327960736464348\n",
      "Iteration 43931 => Loss: 6.69873320476724476435\n",
      "Iteration 43932 => Loss: 6.69873312993720482211\n",
      "Iteration 43933 => Loss: 6.69873305511725369854\n",
      "Iteration 43934 => Loss: 6.69873298030737718278\n",
      "Iteration 43935 => Loss: 6.69873290550758326845\n",
      "Iteration 43936 => Loss: 6.69873283071787373188\n",
      "Iteration 43937 => Loss: 6.69873275593823347407\n",
      "Iteration 43938 => Loss: 6.69873268116867670585\n",
      "Iteration 43939 => Loss: 6.69873260640919188091\n",
      "Iteration 43940 => Loss: 6.69873253165978432833\n",
      "Iteration 43941 => Loss: 6.69873245692044516630\n",
      "Iteration 43942 => Loss: 6.69873238219118416481\n",
      "Iteration 43943 => Loss: 6.69873230747198444845\n",
      "Iteration 43944 => Loss: 6.69873223276285845174\n",
      "Iteration 43945 => Loss: 6.69873215806379462833\n",
      "Iteration 43946 => Loss: 6.69873208337480363639\n",
      "Iteration 43947 => Loss: 6.69873200869587126505\n",
      "Iteration 43948 => Loss: 6.69873193402700373156\n",
      "Iteration 43949 => Loss: 6.69873185936820014774\n",
      "Iteration 43950 => Loss: 6.69873178471945607271\n",
      "Iteration 43951 => Loss: 6.69873171008076884192\n",
      "Iteration 43952 => Loss: 6.69873163545213490266\n",
      "Iteration 43953 => Loss: 6.69873156083356047219\n",
      "Iteration 43954 => Loss: 6.69873148622504199778\n",
      "Iteration 43955 => Loss: 6.69873141162657947945\n",
      "Iteration 43956 => Loss: 6.69873133703816669993\n",
      "Iteration 43957 => Loss: 6.69873126245980721194\n",
      "Iteration 43958 => Loss: 6.69873118789149657459\n",
      "Iteration 43959 => Loss: 6.69873111333323034700\n",
      "Iteration 43960 => Loss: 6.69873103878501474640\n",
      "Iteration 43961 => Loss: 6.69873096424683822647\n",
      "Iteration 43962 => Loss: 6.69873088971871322173\n",
      "Iteration 43963 => Loss: 6.69873081520062729766\n",
      "Iteration 43964 => Loss: 6.69873074069258400698\n",
      "Iteration 43965 => Loss: 6.69873066619457979698\n",
      "Iteration 43966 => Loss: 6.69873059170660933859\n",
      "Iteration 43967 => Loss: 6.69873051722869394808\n",
      "Iteration 43968 => Loss: 6.69873044276079632198\n",
      "Iteration 43969 => Loss: 6.69873036830293688837\n",
      "Iteration 43970 => Loss: 6.69873029385510854183\n",
      "Iteration 43971 => Loss: 6.69873021941731572326\n",
      "Iteration 43972 => Loss: 6.69873014498955665630\n",
      "Iteration 43973 => Loss: 6.69873007057182334734\n",
      "Iteration 43974 => Loss: 6.69872999616412023727\n",
      "Iteration 43975 => Loss: 6.69872992176644199702\n",
      "Iteration 43976 => Loss: 6.69872984737878685024\n",
      "Iteration 43977 => Loss: 6.69872977300115746147\n",
      "Iteration 43978 => Loss: 6.69872969863354850162\n",
      "Iteration 43979 => Loss: 6.69872962427596974067\n",
      "Iteration 43980 => Loss: 6.69872954992839542143\n",
      "Iteration 43981 => Loss: 6.69872947559084686020\n",
      "Iteration 43982 => Loss: 6.69872940126331783972\n",
      "Iteration 43983 => Loss: 6.69872932694580125457\n",
      "Iteration 43984 => Loss: 6.69872925263829710474\n",
      "Iteration 43985 => Loss: 6.69872917834081338384\n",
      "Iteration 43986 => Loss: 6.69872910405333410466\n",
      "Iteration 43987 => Loss: 6.69872902977586726081\n",
      "Iteration 43988 => Loss: 6.69872895550841018775\n",
      "Iteration 43989 => Loss: 6.69872888125095933276\n",
      "Iteration 43990 => Loss: 6.69872880700351380767\n",
      "Iteration 43991 => Loss: 6.69872873276607538884\n",
      "Iteration 43992 => Loss: 6.69872865853863874719\n",
      "Iteration 43993 => Loss: 6.69872858432120565908\n",
      "Iteration 43994 => Loss: 6.69872851011377345998\n",
      "Iteration 43995 => Loss: 6.69872843591634126170\n",
      "Iteration 43996 => Loss: 6.69872836172890551154\n",
      "Iteration 43997 => Loss: 6.69872828755146976221\n",
      "Iteration 43998 => Loss: 6.69872821338402513192\n",
      "Iteration 43999 => Loss: 6.69872813922657162067\n",
      "Iteration 44000 => Loss: 6.69872806507912166296\n",
      "Iteration 44001 => Loss: 6.69872799094165038980\n",
      "Iteration 44002 => Loss: 6.69872791681417911747\n",
      "Iteration 44003 => Loss: 6.69872784269668919421\n",
      "Iteration 44004 => Loss: 6.69872776858920015997\n",
      "Iteration 44005 => Loss: 6.69872769449168270484\n",
      "Iteration 44006 => Loss: 6.69872762040415192786\n",
      "Iteration 44007 => Loss: 6.69872754632660516450\n",
      "Iteration 44008 => Loss: 6.69872747225903975021\n",
      "Iteration 44009 => Loss: 6.69872739820146367862\n",
      "Iteration 44010 => Loss: 6.69872732415386273885\n",
      "Iteration 44011 => Loss: 6.69872725011623249003\n",
      "Iteration 44012 => Loss: 6.69872717608858359029\n",
      "Iteration 44013 => Loss: 6.69872710207091248691\n",
      "Iteration 44014 => Loss: 6.69872702806321562718\n",
      "Iteration 44015 => Loss: 6.69872695406548235297\n",
      "Iteration 44016 => Loss: 6.69872688007773131602\n",
      "Iteration 44017 => Loss: 6.69872680609994475276\n",
      "Iteration 44018 => Loss: 6.69872673213212799226\n",
      "Iteration 44019 => Loss: 6.69872665817427748181\n",
      "Iteration 44020 => Loss: 6.69872658422639588593\n",
      "Iteration 44021 => Loss: 6.69872651028847521104\n",
      "Iteration 44022 => Loss: 6.69872643636051989802\n",
      "Iteration 44023 => Loss: 6.69872636244251928872\n",
      "Iteration 44024 => Loss: 6.69872628853448937036\n",
      "Iteration 44025 => Loss: 6.69872621463641149120\n",
      "Iteration 44026 => Loss: 6.69872614074829630937\n",
      "Iteration 44027 => Loss: 6.69872606687012872584\n",
      "Iteration 44028 => Loss: 6.69872599300192916871\n",
      "Iteration 44029 => Loss: 6.69872591914367365717\n",
      "Iteration 44030 => Loss: 6.69872584529537551390\n",
      "Iteration 44031 => Loss: 6.69872577145702585710\n",
      "Iteration 44032 => Loss: 6.69872569762862823950\n",
      "Iteration 44033 => Loss: 6.69872562381017733202\n",
      "Iteration 44034 => Loss: 6.69872555000167224648\n",
      "Iteration 44035 => Loss: 6.69872547620311653560\n",
      "Iteration 44036 => Loss: 6.69872540241450220577\n",
      "Iteration 44037 => Loss: 6.69872532863583192153\n",
      "Iteration 44038 => Loss: 6.69872525486710301834\n",
      "Iteration 44039 => Loss: 6.69872518110831460803\n",
      "Iteration 44040 => Loss: 6.69872510735946224969\n",
      "Iteration 44041 => Loss: 6.69872503362055038423\n",
      "Iteration 44042 => Loss: 6.69872495989157634710\n",
      "Iteration 44043 => Loss: 6.69872488617253569743\n",
      "Iteration 44044 => Loss: 6.69872481246343021155\n",
      "Iteration 44045 => Loss: 6.69872473876425367223\n",
      "Iteration 44046 => Loss: 6.69872466507501052035\n",
      "Iteration 44047 => Loss: 6.69872459139569720321\n",
      "Iteration 44048 => Loss: 6.69872451772631372080\n",
      "Iteration 44049 => Loss: 6.69872444406685563223\n",
      "Iteration 44050 => Loss: 6.69872437041731760843\n",
      "Iteration 44051 => Loss: 6.69872429677771119572\n",
      "Iteration 44052 => Loss: 6.69872422314802573595\n",
      "Iteration 44053 => Loss: 6.69872414952825856460\n",
      "Iteration 44054 => Loss: 6.69872407591841501073\n",
      "Iteration 44055 => Loss: 6.69872400231848974528\n",
      "Iteration 44056 => Loss: 6.69872392872848099188\n",
      "Iteration 44057 => Loss: 6.69872385514838963871\n",
      "Iteration 44058 => Loss: 6.69872378157821479761\n",
      "Iteration 44059 => Loss: 6.69872370801795291584\n",
      "Iteration 44060 => Loss: 6.69872363446760132888\n",
      "Iteration 44061 => Loss: 6.69872356092716092490\n",
      "Iteration 44062 => Loss: 6.69872348739663436845\n",
      "Iteration 44063 => Loss: 6.69872341387601011320\n",
      "Iteration 44064 => Loss: 6.69872334036529348822\n",
      "Iteration 44065 => Loss: 6.69872326686449071076\n",
      "Iteration 44066 => Loss: 6.69872319337358046454\n",
      "Iteration 44067 => Loss: 6.69872311989257518405\n",
      "Iteration 44068 => Loss: 6.69872304642148108655\n",
      "Iteration 44069 => Loss: 6.69872297296027774394\n",
      "Iteration 44070 => Loss: 6.69872289950897936706\n",
      "Iteration 44071 => Loss: 6.69872282606757618595\n",
      "Iteration 44072 => Loss: 6.69872275263606109519\n",
      "Iteration 44073 => Loss: 6.69872267921445185834\n",
      "Iteration 44074 => Loss: 6.69872260580273515274\n",
      "Iteration 44075 => Loss: 6.69872253240090564930\n",
      "Iteration 44076 => Loss: 6.69872245900897400617\n",
      "Iteration 44077 => Loss: 6.69872238562692334796\n",
      "Iteration 44078 => Loss: 6.69872231225476433281\n",
      "Iteration 44079 => Loss: 6.69872223889249429618\n",
      "Iteration 44080 => Loss: 6.69872216554010879719\n",
      "Iteration 44081 => Loss: 6.69872209219760428311\n",
      "Iteration 44082 => Loss: 6.69872201886499052392\n",
      "Iteration 44083 => Loss: 6.69872194554225242058\n",
      "Iteration 44084 => Loss: 6.69872187222939707851\n",
      "Iteration 44085 => Loss: 6.69872179892641828047\n",
      "Iteration 44086 => Loss: 6.69872172563331602646\n",
      "Iteration 44087 => Loss: 6.69872165235008765194\n",
      "Iteration 44088 => Loss: 6.69872157907674203869\n",
      "Iteration 44089 => Loss: 6.69872150581326852858\n",
      "Iteration 44090 => Loss: 6.69872143255966356890\n",
      "Iteration 44091 => Loss: 6.69872135931593071234\n",
      "Iteration 44092 => Loss: 6.69872128608206729439\n",
      "Iteration 44093 => Loss: 6.69872121285807242685\n",
      "Iteration 44094 => Loss: 6.69872113964395232699\n",
      "Iteration 44095 => Loss: 6.69872106643968567852\n",
      "Iteration 44096 => Loss: 6.69872099324528313957\n",
      "Iteration 44097 => Loss: 6.69872092006075270376\n",
      "Iteration 44098 => Loss: 6.69872084688607749570\n",
      "Iteration 44099 => Loss: 6.69872077372127172623\n",
      "Iteration 44100 => Loss: 6.69872070056631674362\n",
      "Iteration 44101 => Loss: 6.69872062742121521239\n",
      "Iteration 44102 => Loss: 6.69872055428597956706\n",
      "Iteration 44103 => Loss: 6.69872048116059026768\n",
      "Iteration 44104 => Loss: 6.69872040804505886058\n",
      "Iteration 44105 => Loss: 6.69872033493938090487\n",
      "Iteration 44106 => Loss: 6.69872026184355373601\n",
      "Iteration 44107 => Loss: 6.69872018875757735401\n",
      "Iteration 44108 => Loss: 6.69872011568144554161\n",
      "Iteration 44109 => Loss: 6.69872004261516273971\n",
      "Iteration 44110 => Loss: 6.69871996955872628376\n",
      "Iteration 44111 => Loss: 6.69871989651213350925\n",
      "Iteration 44112 => Loss: 6.69871982347538263980\n",
      "Iteration 44113 => Loss: 6.69871975044847545178\n",
      "Iteration 44114 => Loss: 6.69871967743140661611\n",
      "Iteration 44115 => Loss: 6.69871960442417968551\n",
      "Iteration 44116 => Loss: 6.69871953142678844273\n",
      "Iteration 44117 => Loss: 6.69871945843923111141\n",
      "Iteration 44118 => Loss: 6.69871938546151479699\n",
      "Iteration 44119 => Loss: 6.69871931249362706495\n",
      "Iteration 44120 => Loss: 6.69871923953557324438\n",
      "Iteration 44121 => Loss: 6.69871916658735688799\n",
      "Iteration 44122 => Loss: 6.69871909364896289674\n",
      "Iteration 44123 => Loss: 6.69871902072039926423\n",
      "Iteration 44124 => Loss: 6.69871894780166154959\n",
      "Iteration 44125 => Loss: 6.69871887489274975280\n",
      "Iteration 44126 => Loss: 6.69871880199366298569\n",
      "Iteration 44127 => Loss: 6.69871872910439325466\n",
      "Iteration 44128 => Loss: 6.69871865622495743509\n",
      "Iteration 44129 => Loss: 6.69871858335533332252\n",
      "Iteration 44130 => Loss: 6.69871851049553423962\n",
      "Iteration 44131 => Loss: 6.69871843764554775191\n",
      "Iteration 44132 => Loss: 6.69871836480538718206\n",
      "Iteration 44133 => Loss: 6.69871829197502766107\n",
      "Iteration 44134 => Loss: 6.69871821915449139340\n",
      "Iteration 44135 => Loss: 6.69871814634376594455\n",
      "Iteration 44136 => Loss: 6.69871807354285309088\n",
      "Iteration 44137 => Loss: 6.69871800075174306244\n",
      "Iteration 44138 => Loss: 6.69871792797045539913\n",
      "Iteration 44139 => Loss: 6.69871785519896434380\n",
      "Iteration 44140 => Loss: 6.69871778243728144275\n",
      "Iteration 44141 => Loss: 6.69871770968540047875\n",
      "Iteration 44142 => Loss: 6.69871763694332411632\n",
      "Iteration 44143 => Loss: 6.69871756421105590817\n",
      "Iteration 44144 => Loss: 6.69871749148857986711\n",
      "Iteration 44145 => Loss: 6.69871741877591198033\n",
      "Iteration 44146 => Loss: 6.69871734607303448428\n",
      "Iteration 44147 => Loss: 6.69871727337995626073\n",
      "Iteration 44148 => Loss: 6.69871720069667642150\n",
      "Iteration 44149 => Loss: 6.69871712802318786117\n",
      "Iteration 44150 => Loss: 6.69871705535949235610\n",
      "Iteration 44151 => Loss: 6.69871698270558635357\n",
      "Iteration 44152 => Loss: 6.69871691006147251812\n",
      "Iteration 44153 => Loss: 6.69871683742714552068\n",
      "Iteration 44154 => Loss: 6.69871676480260713760\n",
      "Iteration 44155 => Loss: 6.69871669218786003341\n",
      "Iteration 44156 => Loss: 6.69871661958289354999\n",
      "Iteration 44157 => Loss: 6.69871654698770857550\n",
      "Iteration 44158 => Loss: 6.69871647440230599813\n",
      "Iteration 44159 => Loss: 6.69871640182668226515\n",
      "Iteration 44160 => Loss: 6.69871632926084803472\n",
      "Iteration 44161 => Loss: 6.69871625670478021419\n",
      "Iteration 44162 => Loss: 6.69871618415849034989\n",
      "Iteration 44163 => Loss: 6.69871611162198732359\n",
      "Iteration 44164 => Loss: 6.69871603909525337173\n",
      "Iteration 44165 => Loss: 6.69871596657828671795\n",
      "Iteration 44166 => Loss: 6.69871589407110068493\n",
      "Iteration 44167 => Loss: 6.69871582157367750909\n",
      "Iteration 44168 => Loss: 6.69871574908602873677\n",
      "Iteration 44169 => Loss: 6.69871567660814015710\n",
      "Iteration 44170 => Loss: 6.69871560414002420458\n",
      "Iteration 44171 => Loss: 6.69871553168167110925\n",
      "Iteration 44172 => Loss: 6.69871545923308442383\n",
      "Iteration 44173 => Loss: 6.69871538679425793106\n",
      "Iteration 44174 => Loss: 6.69871531436518985458\n",
      "Iteration 44175 => Loss: 6.69871524194588197076\n",
      "Iteration 44176 => Loss: 6.69871516953633872049\n",
      "Iteration 44177 => Loss: 6.69871509713655211016\n",
      "Iteration 44178 => Loss: 6.69871502474651681069\n",
      "Iteration 44179 => Loss: 6.69871495236623459846\n",
      "Iteration 44180 => Loss: 6.69871487999571435523\n",
      "Iteration 44181 => Loss: 6.69871480763494009381\n",
      "Iteration 44182 => Loss: 6.69871473528391536689\n",
      "Iteration 44183 => Loss: 6.69871466294264461538\n",
      "Iteration 44184 => Loss: 6.69871459061111362843\n",
      "Iteration 44185 => Loss: 6.69871451828933839323\n",
      "Iteration 44186 => Loss: 6.69871444597730025805\n",
      "Iteration 44187 => Loss: 6.69871437367500810467\n",
      "Iteration 44188 => Loss: 6.69871430138246903851\n",
      "Iteration 44189 => Loss: 6.69871422909965641423\n",
      "Iteration 44190 => Loss: 6.69871415682659687718\n",
      "Iteration 44191 => Loss: 6.69871408456326733472\n",
      "Iteration 44192 => Loss: 6.69871401230967933316\n",
      "Iteration 44193 => Loss: 6.69871394006582487890\n",
      "Iteration 44194 => Loss: 6.69871386783170574830\n",
      "Iteration 44195 => Loss: 6.69871379560732016500\n",
      "Iteration 44196 => Loss: 6.69871372339266812901\n",
      "Iteration 44197 => Loss: 6.69871365118774253489\n",
      "Iteration 44198 => Loss: 6.69871357899255226442\n",
      "Iteration 44199 => Loss: 6.69871350680709198855\n",
      "Iteration 44200 => Loss: 6.69871343463135549001\n",
      "Iteration 44201 => Loss: 6.69871336246534543335\n",
      "Iteration 44202 => Loss: 6.69871329030905826585\n",
      "Iteration 44203 => Loss: 6.69871321816249576386\n",
      "Iteration 44204 => Loss: 6.69871314602565526286\n",
      "Iteration 44205 => Loss: 6.69871307389853409830\n",
      "Iteration 44206 => Loss: 6.69871300178112871748\n",
      "Iteration 44207 => Loss: 6.69871292967344622582\n",
      "Iteration 44208 => Loss: 6.69871285757547862971\n",
      "Iteration 44209 => Loss: 6.69871278548722681734\n",
      "Iteration 44210 => Loss: 6.69871271340868545963\n",
      "Iteration 44211 => Loss: 6.69871264133986077383\n",
      "Iteration 44212 => Loss: 6.69871256928074476633\n",
      "Iteration 44213 => Loss: 6.69871249723134098986\n",
      "Iteration 44214 => Loss: 6.69871242519164766804\n",
      "Iteration 44215 => Loss: 6.69871235316165591911\n",
      "Iteration 44216 => Loss: 6.69871228114137018395\n",
      "Iteration 44217 => Loss: 6.69871220913079312709\n",
      "Iteration 44218 => Loss: 6.69871213712992119582\n",
      "Iteration 44219 => Loss: 6.69871206513874906108\n",
      "Iteration 44220 => Loss: 6.69871199315727849921\n",
      "Iteration 44221 => Loss: 6.69871192118550240480\n",
      "Iteration 44222 => Loss: 6.69871184922342788326\n",
      "Iteration 44223 => Loss: 6.69871177727105404642\n",
      "Iteration 44224 => Loss: 6.69871170532837023615\n",
      "Iteration 44225 => Loss: 6.69871163339538444603\n",
      "Iteration 44226 => Loss: 6.69871156147209401155\n",
      "Iteration 44227 => Loss: 6.69871148955848916273\n",
      "Iteration 44228 => Loss: 6.69871141765457789319\n",
      "Iteration 44229 => Loss: 6.69871134576035753838\n",
      "Iteration 44230 => Loss: 6.69871127387582188106\n",
      "Iteration 44231 => Loss: 6.69871120200096825670\n",
      "Iteration 44232 => Loss: 6.69871113013580021800\n",
      "Iteration 44233 => Loss: 6.69871105828032842311\n",
      "Iteration 44234 => Loss: 6.69871098643453155574\n",
      "Iteration 44235 => Loss: 6.69871091459840961591\n",
      "Iteration 44236 => Loss: 6.69871084277197237355\n",
      "Iteration 44237 => Loss: 6.69871077095521538780\n",
      "Iteration 44238 => Loss: 6.69871069914813510593\n",
      "Iteration 44239 => Loss: 6.69871062735073596883\n",
      "Iteration 44240 => Loss: 6.69871055556300465383\n",
      "Iteration 44241 => Loss: 6.69871048378494649000\n",
      "Iteration 44242 => Loss: 6.69871041201656236552\n",
      "Iteration 44243 => Loss: 6.69871034025785316857\n",
      "Iteration 44244 => Loss: 6.69871026850880202375\n",
      "Iteration 44245 => Loss: 6.69871019676943113552\n",
      "Iteration 44246 => Loss: 6.69871012503972540486\n",
      "Iteration 44247 => Loss: 6.69871005331968039087\n",
      "Iteration 44248 => Loss: 6.69870998160930053444\n",
      "Iteration 44249 => Loss: 6.69870990990858317105\n",
      "Iteration 44250 => Loss: 6.69870983821752918885\n",
      "Iteration 44251 => Loss: 6.69870976653614036422\n",
      "Iteration 44252 => Loss: 6.69870969486439982177\n",
      "Iteration 44253 => Loss: 6.69870962320231999598\n",
      "Iteration 44254 => Loss: 6.69870955154990443958\n",
      "Iteration 44255 => Loss: 6.69870947990714071807\n",
      "Iteration 44256 => Loss: 6.69870940827402616691\n",
      "Iteration 44257 => Loss: 6.69870933665056611517\n",
      "Iteration 44258 => Loss: 6.69870926503676233921\n",
      "Iteration 44259 => Loss: 6.69870919343260684542\n",
      "Iteration 44260 => Loss: 6.69870912183809341656\n",
      "Iteration 44261 => Loss: 6.69870905025323448712\n",
      "Iteration 44262 => Loss: 6.69870897867801762260\n",
      "Iteration 44263 => Loss: 6.69870890711244637572\n",
      "Iteration 44264 => Loss: 6.69870883555651985830\n",
      "Iteration 44265 => Loss: 6.69870876401022918856\n",
      "Iteration 44266 => Loss: 6.69870869247358680099\n",
      "Iteration 44267 => Loss: 6.69870862094658114927\n",
      "Iteration 44268 => Loss: 6.69870854942921756248\n",
      "Iteration 44269 => Loss: 6.69870847792148982336\n",
      "Iteration 44270 => Loss: 6.69870840642339437920\n",
      "Iteration 44271 => Loss: 6.69870833493493478272\n",
      "Iteration 44272 => Loss: 6.69870826345610925756\n",
      "Iteration 44273 => Loss: 6.69870819198691602736\n",
      "Iteration 44274 => Loss: 6.69870812052735331577\n",
      "Iteration 44275 => Loss: 6.69870804907741046463\n",
      "Iteration 44276 => Loss: 6.69870797763710879025\n",
      "Iteration 44277 => Loss: 6.69870790620642697633\n",
      "Iteration 44278 => Loss: 6.69870783478537124012\n",
      "Iteration 44279 => Loss: 6.69870776337393980526\n",
      "Iteration 44280 => Loss: 6.69870769197213444812\n",
      "Iteration 44281 => Loss: 6.69870762057994451055\n",
      "Iteration 44282 => Loss: 6.69870754919737709798\n",
      "Iteration 44283 => Loss: 6.69870747782443309859\n",
      "Iteration 44284 => Loss: 6.69870740646109652516\n",
      "Iteration 44285 => Loss: 6.69870733510738691763\n",
      "Iteration 44286 => Loss: 6.69870726376327940699\n",
      "Iteration 44287 => Loss: 6.69870719242879619770\n",
      "Iteration 44288 => Loss: 6.69870712110392130256\n",
      "Iteration 44289 => Loss: 6.69870704978866182699\n",
      "Iteration 44290 => Loss: 6.69870697848301155375\n",
      "Iteration 44291 => Loss: 6.69870690718696160104\n",
      "Iteration 44292 => Loss: 6.69870683590052795608\n",
      "Iteration 44293 => Loss: 6.69870676462369729620\n",
      "Iteration 44294 => Loss: 6.69870669335646873321\n",
      "Iteration 44295 => Loss: 6.69870662209884493166\n",
      "Iteration 44296 => Loss: 6.69870655085082944424\n",
      "Iteration 44297 => Loss: 6.69870647961240539559\n",
      "Iteration 44298 => Loss: 6.69870640838358433200\n",
      "Iteration 44299 => Loss: 6.69870633716436447713\n",
      "Iteration 44300 => Loss: 6.69870626595473073195\n",
      "Iteration 44301 => Loss: 6.69870619475469819548\n",
      "Iteration 44302 => Loss: 6.69870612356426420320\n",
      "Iteration 44303 => Loss: 6.69870605238341898513\n",
      "Iteration 44304 => Loss: 6.69870598121216964671\n",
      "Iteration 44305 => Loss: 6.69870591005050375344\n",
      "Iteration 44306 => Loss: 6.69870583889843107528\n",
      "Iteration 44307 => Loss: 6.69870576775595338859\n",
      "Iteration 44308 => Loss: 6.69870569662304937708\n",
      "Iteration 44309 => Loss: 6.69870562549973591615\n",
      "Iteration 44310 => Loss: 6.69870555438600412401\n",
      "Iteration 44311 => Loss: 6.69870548328186021791\n",
      "Iteration 44312 => Loss: 6.69870541218729087518\n",
      "Iteration 44313 => Loss: 6.69870534110231030667\n",
      "Iteration 44314 => Loss: 6.69870527002689986062\n",
      "Iteration 44315 => Loss: 6.69870519896106930702\n",
      "Iteration 44316 => Loss: 6.69870512790481331677\n",
      "Iteration 44317 => Loss: 6.69870505685813988350\n",
      "Iteration 44318 => Loss: 6.69870498582102857910\n",
      "Iteration 44319 => Loss: 6.69870491479349361441\n",
      "Iteration 44320 => Loss: 6.69870484377553676580\n",
      "Iteration 44321 => Loss: 6.69870477276714471060\n",
      "Iteration 44322 => Loss: 6.69870470176831744880\n",
      "Iteration 44323 => Loss: 6.69870463077906297400\n",
      "Iteration 44324 => Loss: 6.69870455979937329261\n",
      "Iteration 44325 => Loss: 6.69870448882925018097\n",
      "Iteration 44326 => Loss: 6.69870441786868298095\n",
      "Iteration 44327 => Loss: 6.69870434691768057434\n",
      "Iteration 44328 => Loss: 6.69870427597623940841\n",
      "Iteration 44329 => Loss: 6.69870420504436037135\n",
      "Iteration 44330 => Loss: 6.69870413412203458137\n",
      "Iteration 44331 => Loss: 6.69870406320926825572\n",
      "Iteration 44332 => Loss: 6.69870399230605340080\n",
      "Iteration 44333 => Loss: 6.69870392141239712203\n",
      "Iteration 44334 => Loss: 6.69870385052829320216\n",
      "Iteration 44335 => Loss: 6.69870377965374164120\n",
      "Iteration 44336 => Loss: 6.69870370878874332732\n",
      "Iteration 44337 => Loss: 6.69870363793329115509\n",
      "Iteration 44338 => Loss: 6.69870356708738157181\n",
      "Iteration 44339 => Loss: 6.69870349625102523561\n",
      "Iteration 44340 => Loss: 6.69870342542421060017\n",
      "Iteration 44341 => Loss: 6.69870335460694743546\n",
      "Iteration 44342 => Loss: 6.69870328379921353701\n",
      "Iteration 44343 => Loss: 6.69870321300103555018\n",
      "Iteration 44344 => Loss: 6.69870314221238505326\n",
      "Iteration 44345 => Loss: 6.69870307143327536892\n",
      "Iteration 44346 => Loss: 6.69870300066371360259\n",
      "Iteration 44347 => Loss: 6.69870292990367577346\n",
      "Iteration 44348 => Loss: 6.69870285915318230963\n",
      "Iteration 44349 => Loss: 6.69870278841221811206\n",
      "Iteration 44350 => Loss: 6.69870271768078318075\n",
      "Iteration 44351 => Loss: 6.69870264695888462114\n",
      "Iteration 44352 => Loss: 6.69870257624651443962\n",
      "Iteration 44353 => Loss: 6.69870250554366641893\n",
      "Iteration 44354 => Loss: 6.69870243485035476994\n",
      "Iteration 44355 => Loss: 6.69870236416655906453\n",
      "Iteration 44356 => Loss: 6.69870229349229528992\n",
      "Iteration 44357 => Loss: 6.69870222282755811705\n",
      "Iteration 44358 => Loss: 6.69870215217233866412\n",
      "Iteration 44359 => Loss: 6.69870208152663781931\n",
      "Iteration 44360 => Loss: 6.69870201089046002352\n",
      "Iteration 44361 => Loss: 6.69870194026379639496\n",
      "Iteration 44362 => Loss: 6.69870186964665581542\n",
      "Iteration 44363 => Loss: 6.69870179903902940310\n",
      "Iteration 44364 => Loss: 6.69870172844091360531\n",
      "Iteration 44365 => Loss: 6.69870165785231108657\n",
      "Iteration 44366 => Loss: 6.69870158727322362324\n",
      "Iteration 44367 => Loss: 6.69870151670364943897\n",
      "Iteration 44368 => Loss: 6.69870144614357876378\n",
      "Iteration 44369 => Loss: 6.69870137559301603858\n",
      "Iteration 44370 => Loss: 6.69870130505195859882\n",
      "Iteration 44371 => Loss: 6.69870123452040555634\n",
      "Iteration 44372 => Loss: 6.69870116399837023380\n",
      "Iteration 44373 => Loss: 6.69870109348582598585\n",
      "Iteration 44374 => Loss: 6.69870102298278258246\n",
      "Iteration 44375 => Loss: 6.69870095248924091180\n",
      "Iteration 44376 => Loss: 6.69870088200519919752\n",
      "Iteration 44377 => Loss: 6.69870081153065832780\n",
      "Iteration 44378 => Loss: 6.69870074106560942084\n",
      "Iteration 44379 => Loss: 6.69870067061005780573\n",
      "Iteration 44380 => Loss: 6.69870060016399904157\n",
      "Iteration 44381 => Loss: 6.69870052972743490471\n",
      "Iteration 44382 => Loss: 6.69870045930036006609\n",
      "Iteration 44383 => Loss: 6.69870038888277541389\n",
      "Iteration 44384 => Loss: 6.69870031847468005992\n",
      "Iteration 44385 => Loss: 6.69870024807607489237\n",
      "Iteration 44386 => Loss: 6.69870017768695014126\n",
      "Iteration 44387 => Loss: 6.69870010730732001747\n",
      "Iteration 44388 => Loss: 6.69870003693716320470\n",
      "Iteration 44389 => Loss: 6.69869996657648858474\n",
      "Iteration 44390 => Loss: 6.69869989622530326301\n",
      "Iteration 44391 => Loss: 6.69869982588359480502\n",
      "Iteration 44392 => Loss: 6.69869975555136143441\n",
      "Iteration 44393 => Loss: 6.69869968522861025662\n",
      "Iteration 44394 => Loss: 6.69869961491533061348\n",
      "Iteration 44395 => Loss: 6.69869954461152694591\n",
      "Iteration 44396 => Loss: 6.69869947431719925390\n",
      "Iteration 44397 => Loss: 6.69869940403234043202\n",
      "Iteration 44398 => Loss: 6.69869933375695492117\n",
      "Iteration 44399 => Loss: 6.69869926349103650409\n",
      "Iteration 44400 => Loss: 6.69869919323458784532\n",
      "Iteration 44401 => Loss: 6.69869912298760450398\n",
      "Iteration 44402 => Loss: 6.69869905275008914458\n",
      "Iteration 44403 => Loss: 6.69869898252203732625\n",
      "Iteration 44404 => Loss: 6.69869891230345348987\n",
      "Iteration 44405 => Loss: 6.69869884209432608912\n",
      "Iteration 44406 => Loss: 6.69869877189465867673\n",
      "Iteration 44407 => Loss: 6.69869870170445569357\n",
      "Iteration 44408 => Loss: 6.69869863152370825787\n",
      "Iteration 44409 => Loss: 6.69869856135241459327\n",
      "Iteration 44410 => Loss: 6.69869849119057914066\n",
      "Iteration 44411 => Loss: 6.69869842103820278822\n",
      "Iteration 44412 => Loss: 6.69869835089526777239\n",
      "Iteration 44413 => Loss: 6.69869828076179540943\n",
      "Iteration 44414 => Loss: 6.69869821063776882397\n",
      "Iteration 44415 => Loss: 6.69869814052319334508\n",
      "Iteration 44416 => Loss: 6.69869807041806453185\n",
      "Iteration 44417 => Loss: 6.69869800032238327248\n",
      "Iteration 44418 => Loss: 6.69869793023614690242\n",
      "Iteration 44419 => Loss: 6.69869786015935453349\n",
      "Iteration 44420 => Loss: 6.69869779009200172482\n",
      "Iteration 44421 => Loss: 6.69869772003409380545\n",
      "Iteration 44422 => Loss: 6.69869764998563077540\n",
      "Iteration 44423 => Loss: 6.69869757994660020017\n",
      "Iteration 44424 => Loss: 6.69869750991701007337\n",
      "Iteration 44425 => Loss: 6.69869743989685506591\n",
      "Iteration 44426 => Loss: 6.69869736988613340145\n",
      "Iteration 44427 => Loss: 6.69869729988485040906\n",
      "Iteration 44428 => Loss: 6.69869722989299809512\n",
      "Iteration 44429 => Loss: 6.69869715991057734783\n",
      "Iteration 44430 => Loss: 6.69869708993758283810\n",
      "Iteration 44431 => Loss: 6.69869701997401989502\n",
      "Iteration 44432 => Loss: 6.69869695001988940675\n",
      "Iteration 44433 => Loss: 6.69869688007518160333\n",
      "Iteration 44434 => Loss: 6.69869681013990181384\n",
      "Iteration 44435 => Loss: 6.69869674021403671560\n",
      "Iteration 44436 => Loss: 6.69869667029759785493\n",
      "Iteration 44437 => Loss: 6.69869660039058434364\n",
      "Iteration 44438 => Loss: 6.69869653049298641179\n",
      "Iteration 44439 => Loss: 6.69869646060480938843\n",
      "Iteration 44440 => Loss: 6.69869639072605416175\n",
      "Iteration 44441 => Loss: 6.69869632085671184996\n",
      "Iteration 44442 => Loss: 6.69869625099678422941\n",
      "Iteration 44443 => Loss: 6.69869618114626419469\n",
      "Iteration 44444 => Loss: 6.69869611130516240394\n",
      "Iteration 44445 => Loss: 6.69869604147347352807\n",
      "Iteration 44446 => Loss: 6.69869597165119312621\n",
      "Iteration 44447 => Loss: 6.69869590183831586927\n",
      "Iteration 44448 => Loss: 6.69869583203484886269\n",
      "Iteration 44449 => Loss: 6.69869576224079210647\n",
      "Iteration 44450 => Loss: 6.69869569245613227793\n",
      "Iteration 44451 => Loss: 6.69869562268088714063\n",
      "Iteration 44452 => Loss: 6.69869555291503537831\n",
      "Iteration 44453 => Loss: 6.69869548315858320819\n",
      "Iteration 44454 => Loss: 6.69869541341153951208\n",
      "Iteration 44455 => Loss: 6.69869534367389007912\n",
      "Iteration 44456 => Loss: 6.69869527394563757383\n",
      "Iteration 44457 => Loss: 6.69869520422678110805\n",
      "Iteration 44458 => Loss: 6.69869513451731801723\n",
      "Iteration 44459 => Loss: 6.69869506481725185409\n",
      "Iteration 44460 => Loss: 6.69869499512657196050\n",
      "Iteration 44461 => Loss: 6.69869492544528366551\n",
      "Iteration 44462 => Loss: 6.69869485577339407456\n",
      "Iteration 44463 => Loss: 6.69869478611087920683\n",
      "Iteration 44464 => Loss: 6.69869471645776481949\n",
      "Iteration 44465 => Loss: 6.69869464681402693174\n",
      "Iteration 44466 => Loss: 6.69869457717967531352\n",
      "Iteration 44467 => Loss: 6.69869450755470730030\n",
      "Iteration 44468 => Loss: 6.69869443793912289209\n",
      "Iteration 44469 => Loss: 6.69869436833291143074\n",
      "Iteration 44470 => Loss: 6.69869429873608712711\n",
      "Iteration 44471 => Loss: 6.69869422914863932306\n",
      "Iteration 44472 => Loss: 6.69869415957056801858\n",
      "Iteration 44473 => Loss: 6.69869409000187410186\n",
      "Iteration 44474 => Loss: 6.69869402044255846107\n",
      "Iteration 44475 => Loss: 6.69869395089260244447\n",
      "Iteration 44476 => Loss: 6.69869388135203269741\n",
      "Iteration 44477 => Loss: 6.69869381182082790360\n",
      "Iteration 44478 => Loss: 6.69869374229899339213\n",
      "Iteration 44479 => Loss: 6.69869367278652116937\n",
      "Iteration 44480 => Loss: 6.69869360328342633437\n",
      "Iteration 44481 => Loss: 6.69869353378968401813\n",
      "Iteration 44482 => Loss: 6.69869346430531553693\n",
      "Iteration 44483 => Loss: 6.69869339483030667992\n",
      "Iteration 44484 => Loss: 6.69869332536466455252\n",
      "Iteration 44485 => Loss: 6.69869325590838116113\n",
      "Iteration 44486 => Loss: 6.69869318646145028850\n",
      "Iteration 44487 => Loss: 6.69869311702388170460\n",
      "Iteration 44488 => Loss: 6.69869304759566919216\n",
      "Iteration 44489 => Loss: 6.69869297817681452756\n",
      "Iteration 44490 => Loss: 6.69869290876730971718\n",
      "Iteration 44491 => Loss: 6.69869283936716275463\n",
      "Iteration 44492 => Loss: 6.69869276997636564630\n",
      "Iteration 44493 => Loss: 6.69869270059492016856\n",
      "Iteration 44494 => Loss: 6.69869263122282099232\n",
      "Iteration 44495 => Loss: 6.69869256186007167031\n",
      "Iteration 44496 => Loss: 6.69869249250667131435\n",
      "Iteration 44497 => Loss: 6.69869242316261104264\n",
      "Iteration 44498 => Loss: 6.69869235382789263156\n",
      "Iteration 44499 => Loss: 6.69869228450252673923\n",
      "Iteration 44500 => Loss: 6.69869221518649471392\n",
      "Iteration 44501 => Loss: 6.69869214587980899012\n",
      "Iteration 44502 => Loss: 6.69869207658245802151\n",
      "Iteration 44503 => Loss: 6.69869200729444447262\n",
      "Iteration 44504 => Loss: 6.69869193801577189618\n",
      "Iteration 44505 => Loss: 6.69869186874642874585\n",
      "Iteration 44506 => Loss: 6.69869179948642035072\n",
      "Iteration 44507 => Loss: 6.69869173023575292802\n",
      "Iteration 44508 => Loss: 6.69869166099440693785\n",
      "Iteration 44509 => Loss: 6.69869159176239392650\n",
      "Iteration 44510 => Loss: 6.69869152253971389399\n",
      "Iteration 44511 => Loss: 6.69869145332635973489\n",
      "Iteration 44512 => Loss: 6.69869138412232967283\n",
      "Iteration 44513 => Loss: 6.69869131492762637237\n",
      "Iteration 44514 => Loss: 6.69869124574224450441\n",
      "Iteration 44515 => Loss: 6.69869117656619117440\n",
      "Iteration 44516 => Loss: 6.69869110739945572419\n",
      "Iteration 44517 => Loss: 6.69869103824204081832\n",
      "Iteration 44518 => Loss: 6.69869096909394734496\n",
      "Iteration 44519 => Loss: 6.69869089995516731051\n",
      "Iteration 44520 => Loss: 6.69869083082570515586\n",
      "Iteration 44521 => Loss: 6.69869076170555466376\n",
      "Iteration 44522 => Loss: 6.69869069259472826872\n",
      "Iteration 44523 => Loss: 6.69869062349320909533\n",
      "Iteration 44524 => Loss: 6.69869055440100602539\n",
      "Iteration 44525 => Loss: 6.69869048531810040714\n",
      "Iteration 44526 => Loss: 6.69869041624450822781\n",
      "Iteration 44527 => Loss: 6.69869034718023304009\n",
      "Iteration 44528 => Loss: 6.69869027812525708043\n",
      "Iteration 44529 => Loss: 6.69869020907958923061\n",
      "Iteration 44530 => Loss: 6.69869014004322416156\n",
      "Iteration 44531 => Loss: 6.69869007101615832056\n",
      "Iteration 44532 => Loss: 6.69869000199839703669\n",
      "Iteration 44533 => Loss: 6.69868993298993320451\n",
      "Iteration 44534 => Loss: 6.69868986399077037674\n",
      "Iteration 44535 => Loss: 6.69868979500090677703\n",
      "Iteration 44536 => Loss: 6.69868972602033707631\n",
      "Iteration 44537 => Loss: 6.69868965704906305092\n",
      "Iteration 44538 => Loss: 6.69868958808708558905\n",
      "Iteration 44539 => Loss: 6.69868951913439669710\n",
      "Iteration 44540 => Loss: 6.69868945019099992777\n",
      "Iteration 44541 => Loss: 6.69868938125689439289\n",
      "Iteration 44542 => Loss: 6.69868931233207742793\n",
      "Iteration 44543 => Loss: 6.69868924341654636834\n",
      "Iteration 44544 => Loss: 6.69868917451030299048\n",
      "Iteration 44545 => Loss: 6.69868910561334907072\n",
      "Iteration 44546 => Loss: 6.69868903672566862184\n",
      "Iteration 44547 => Loss: 6.69868896784728118377\n",
      "Iteration 44548 => Loss: 6.69868889897817254564\n",
      "Iteration 44549 => Loss: 6.69868883011834448382\n",
      "Iteration 44550 => Loss: 6.69868876126778989288\n",
      "Iteration 44551 => Loss: 6.69868869242652298368\n",
      "Iteration 44552 => Loss: 6.69868862359452421629\n",
      "Iteration 44553 => Loss: 6.69868855477180780156\n",
      "Iteration 44554 => Loss: 6.69868848595835419957\n",
      "Iteration 44555 => Loss: 6.69868841715418295024\n",
      "Iteration 44556 => Loss: 6.69868834835927362548\n",
      "Iteration 44557 => Loss: 6.69868827957364310066\n",
      "Iteration 44558 => Loss: 6.69868821079727272405\n",
      "Iteration 44559 => Loss: 6.69868814203018114739\n",
      "Iteration 44560 => Loss: 6.69868807327234527804\n",
      "Iteration 44561 => Loss: 6.69868800452378021504\n",
      "Iteration 44562 => Loss: 6.69868793578448684656\n",
      "Iteration 44563 => Loss: 6.69868786705444296814\n",
      "Iteration 44564 => Loss: 6.69868779833366101428\n",
      "Iteration 44565 => Loss: 6.69868772962214542588\n",
      "Iteration 44566 => Loss: 6.69868766091988643296\n",
      "Iteration 44567 => Loss: 6.69868759222688403554\n",
      "Iteration 44568 => Loss: 6.69868752354314000996\n",
      "Iteration 44569 => Loss: 6.69868745486865702077\n",
      "Iteration 44570 => Loss: 6.69868738620340931078\n",
      "Iteration 44571 => Loss: 6.69868731754742796625\n",
      "Iteration 44572 => Loss: 6.69868724890070144085\n",
      "Iteration 44573 => Loss: 6.69868718026321818826\n",
      "Iteration 44574 => Loss: 6.69868711163498442573\n",
      "Iteration 44575 => Loss: 6.69868704301600192963\n",
      "Iteration 44576 => Loss: 6.69868697440626270634\n",
      "Iteration 44577 => Loss: 6.69868690580576853222\n",
      "Iteration 44578 => Loss: 6.69868683721451585455\n",
      "Iteration 44579 => Loss: 6.69868676863251177878\n",
      "Iteration 44580 => Loss: 6.69868670005974742310\n",
      "Iteration 44581 => Loss: 6.69868663149621923480\n",
      "Iteration 44582 => Loss: 6.69868656294193165479\n",
      "Iteration 44583 => Loss: 6.69868649439688912395\n",
      "Iteration 44584 => Loss: 6.69868642586107299053\n",
      "Iteration 44585 => Loss: 6.69868635733449746539\n",
      "Iteration 44586 => Loss: 6.69868628881715277856\n",
      "Iteration 44587 => Loss: 6.69868622030904425912\n",
      "Iteration 44588 => Loss: 6.69868615181016480165\n",
      "Iteration 44589 => Loss: 6.69868608332051884702\n",
      "Iteration 44590 => Loss: 6.69868601484009840163\n",
      "Iteration 44591 => Loss: 6.69868594636890168914\n",
      "Iteration 44592 => Loss: 6.69868587790694292039\n",
      "Iteration 44593 => Loss: 6.69868580945419633821\n",
      "Iteration 44594 => Loss: 6.69868574101067792981\n",
      "Iteration 44595 => Loss: 6.69868567257638947154\n",
      "Iteration 44596 => Loss: 6.69868560415132030528\n",
      "Iteration 44597 => Loss: 6.69868553573546421376\n",
      "Iteration 44598 => Loss: 6.69868546732883540784\n",
      "Iteration 44599 => Loss: 6.69868539893141967667\n",
      "Iteration 44600 => Loss: 6.69868533054321790843\n",
      "Iteration 44601 => Loss: 6.69868526216423809672\n",
      "Iteration 44602 => Loss: 6.69868519379446869522\n",
      "Iteration 44603 => Loss: 6.69868512543391592118\n",
      "Iteration 44604 => Loss: 6.69868505708257089282\n",
      "Iteration 44605 => Loss: 6.69868498874043005742\n",
      "Iteration 44606 => Loss: 6.69868492040751384309\n",
      "Iteration 44607 => Loss: 6.69868485208378938722\n",
      "Iteration 44608 => Loss: 6.69868478376927800610\n",
      "Iteration 44609 => Loss: 6.69868471546397525884\n",
      "Iteration 44610 => Loss: 6.69868464716787670454\n",
      "Iteration 44611 => Loss: 6.69868457888098234321\n",
      "Iteration 44612 => Loss: 6.69868451060328240487\n",
      "Iteration 44613 => Loss: 6.69868444233478399497\n",
      "Iteration 44614 => Loss: 6.69868437407548622531\n",
      "Iteration 44615 => Loss: 6.69868430582539353679\n",
      "Iteration 44616 => Loss: 6.69868423758449171856\n",
      "Iteration 44617 => Loss: 6.69868416935278165880\n",
      "Iteration 44618 => Loss: 6.69868410113027046293\n",
      "Iteration 44619 => Loss: 6.69868403291694480828\n",
      "Iteration 44620 => Loss: 6.69868396471282245841\n",
      "Iteration 44621 => Loss: 6.69868389651789009065\n",
      "Iteration 44622 => Loss: 6.69868382833213793504\n",
      "Iteration 44623 => Loss: 6.69868376015557753789\n",
      "Iteration 44624 => Loss: 6.69868369198820357013\n",
      "Iteration 44625 => Loss: 6.69868362383002136085\n",
      "Iteration 44626 => Loss: 6.69868355568101581099\n",
      "Iteration 44627 => Loss: 6.69868348754119757871\n",
      "Iteration 44628 => Loss: 6.69868341941056044675\n",
      "Iteration 44629 => Loss: 6.69868335128909908605\n",
      "Iteration 44630 => Loss: 6.69868328317682415474\n",
      "Iteration 44631 => Loss: 6.69868321507372321832\n",
      "Iteration 44632 => Loss: 6.69868314697980160588\n",
      "Iteration 44633 => Loss: 6.69868307889505398833\n",
      "Iteration 44634 => Loss: 6.69868301081948036568\n",
      "Iteration 44635 => Loss: 6.69868294275307984975\n",
      "Iteration 44636 => Loss: 6.69868287469585066418\n",
      "Iteration 44637 => Loss: 6.69868280664779547351\n",
      "Iteration 44638 => Loss: 6.69868273860890717231\n",
      "Iteration 44639 => Loss: 6.69868267057918664875\n",
      "Iteration 44640 => Loss: 6.69868260255863390285\n",
      "Iteration 44641 => Loss: 6.69868253454724715823\n",
      "Iteration 44642 => Loss: 6.69868246654502463855\n",
      "Iteration 44643 => Loss: 6.69868239855196456745\n",
      "Iteration 44644 => Loss: 6.69868233056806694492\n",
      "Iteration 44645 => Loss: 6.69868226259333532369\n",
      "Iteration 44646 => Loss: 6.69868219462776171014\n",
      "Iteration 44647 => Loss: 6.69868212667133811067\n",
      "Iteration 44648 => Loss: 6.69868205872407873613\n",
      "Iteration 44649 => Loss: 6.69868199078597648111\n",
      "Iteration 44650 => Loss: 6.69868192285702601652\n",
      "Iteration 44651 => Loss: 6.69868185493723000690\n",
      "Iteration 44652 => Loss: 6.69868178702658312318\n",
      "Iteration 44653 => Loss: 6.69868171912509069443\n",
      "Iteration 44654 => Loss: 6.69868165123274650341\n",
      "Iteration 44655 => Loss: 6.69868158334954877375\n",
      "Iteration 44656 => Loss: 6.69868151547550105818\n",
      "Iteration 44657 => Loss: 6.69868144761059891579\n",
      "Iteration 44658 => Loss: 6.69868137975484412294\n",
      "Iteration 44659 => Loss: 6.69868131190822602150\n",
      "Iteration 44660 => Loss: 6.69868124407075438143\n",
      "Iteration 44661 => Loss: 6.69868117624242831454\n",
      "Iteration 44662 => Loss: 6.69868110842323449816\n",
      "Iteration 44663 => Loss: 6.69868104061318181408\n",
      "Iteration 44664 => Loss: 6.69868097281226759776\n",
      "Iteration 44665 => Loss: 6.69868090502049451374\n",
      "Iteration 44666 => Loss: 6.69868083723784746297\n",
      "Iteration 44667 => Loss: 6.69868076946433621544\n",
      "Iteration 44668 => Loss: 6.69868070169996165930\n",
      "Iteration 44669 => Loss: 6.69868063394471491279\n",
      "Iteration 44670 => Loss: 6.69868056619859952860\n",
      "Iteration 44671 => Loss: 6.69868049846161195404\n",
      "Iteration 44672 => Loss: 6.69868043073375130092\n",
      "Iteration 44673 => Loss: 6.69868036301502467467\n",
      "Iteration 44674 => Loss: 6.69868029530541253536\n",
      "Iteration 44675 => Loss: 6.69868022760493353474\n",
      "Iteration 44676 => Loss: 6.69868015991356458017\n",
      "Iteration 44677 => Loss: 6.69868009223132521157\n",
      "Iteration 44678 => Loss: 6.69868002455820654717\n",
      "Iteration 44679 => Loss: 6.69867995689420858696\n",
      "Iteration 44680 => Loss: 6.69867988923932600187\n",
      "Iteration 44681 => Loss: 6.69867982159355790373\n",
      "Iteration 44682 => Loss: 6.69867975395690606888\n",
      "Iteration 44683 => Loss: 6.69867968632936428008\n",
      "Iteration 44684 => Loss: 6.69867961871093964277\n",
      "Iteration 44685 => Loss: 6.69867955110162416332\n",
      "Iteration 44686 => Loss: 6.69867948350142050629\n",
      "Iteration 44687 => Loss: 6.69867941591033400073\n",
      "Iteration 44688 => Loss: 6.69867934832835043579\n",
      "Iteration 44689 => Loss: 6.69867928075546803512\n",
      "Iteration 44690 => Loss: 6.69867921319169834504\n",
      "Iteration 44691 => Loss: 6.69867914563702182562\n",
      "Iteration 44692 => Loss: 6.69867907809146156950\n",
      "Iteration 44693 => Loss: 6.69867901055499537222\n",
      "Iteration 44694 => Loss: 6.69867894302762589831\n",
      "Iteration 44695 => Loss: 6.69867887550936735863\n",
      "Iteration 44696 => Loss: 6.69867880800019932508\n",
      "Iteration 44697 => Loss: 6.69867874050013156761\n",
      "Iteration 44698 => Loss: 6.69867867300915342810\n",
      "Iteration 44699 => Loss: 6.69867860552728178192\n",
      "Iteration 44700 => Loss: 6.69867853805449176008\n",
      "Iteration 44701 => Loss: 6.69867847059079490890\n",
      "Iteration 44702 => Loss: 6.69867840313619122838\n",
      "Iteration 44703 => Loss: 6.69867833569068160671\n",
      "Iteration 44704 => Loss: 6.69867826825425005666\n",
      "Iteration 44705 => Loss: 6.69867820082691256545\n",
      "Iteration 44706 => Loss: 6.69867813340866380400\n",
      "Iteration 44707 => Loss: 6.69867806599948956148\n",
      "Iteration 44708 => Loss: 6.69867799859940493690\n",
      "Iteration 44709 => Loss: 6.69867793120840460119\n",
      "Iteration 44710 => Loss: 6.69867786382648322530\n",
      "Iteration 44711 => Loss: 6.69867779645363636831\n",
      "Iteration 44712 => Loss: 6.69867772908987824110\n",
      "Iteration 44713 => Loss: 6.69867766173518841555\n",
      "Iteration 44714 => Loss: 6.69867759438958110252\n",
      "Iteration 44715 => Loss: 6.69867752705304386751\n",
      "Iteration 44716 => Loss: 6.69867745972558292777\n",
      "Iteration 44717 => Loss: 6.69867739240719295424\n",
      "Iteration 44718 => Loss: 6.69867732509787305872\n",
      "Iteration 44719 => Loss: 6.69867725779762590577\n",
      "Iteration 44720 => Loss: 6.69867719050644705447\n",
      "Iteration 44721 => Loss: 6.69867712322433739303\n",
      "Iteration 44722 => Loss: 6.69867705595128892782\n",
      "Iteration 44723 => Loss: 6.69867698868730965245\n",
      "Iteration 44724 => Loss: 6.69867692143238890878\n",
      "Iteration 44725 => Loss: 6.69867685418653113771\n",
      "Iteration 44726 => Loss: 6.69867678694974255649\n",
      "Iteration 44727 => Loss: 6.69867671972200984243\n",
      "Iteration 44728 => Loss: 6.69867665250333477189\n",
      "Iteration 44729 => Loss: 6.69867658529372089760\n",
      "Iteration 44730 => Loss: 6.69867651809316022593\n",
      "Iteration 44731 => Loss: 6.69867645090165453325\n",
      "Iteration 44732 => Loss: 6.69867638371920559592\n",
      "Iteration 44733 => Loss: 6.69867631654580808487\n",
      "Iteration 44734 => Loss: 6.69867624938146022373\n",
      "Iteration 44735 => Loss: 6.69867618222616378887\n",
      "Iteration 44736 => Loss: 6.69867611507992410935\n",
      "Iteration 44737 => Loss: 6.69867604794271986890\n",
      "Iteration 44738 => Loss: 6.69867598081456705472\n",
      "Iteration 44739 => Loss: 6.69867591369545856139\n",
      "Iteration 44740 => Loss: 6.69867584658539438891\n",
      "Iteration 44741 => Loss: 6.69867577948438341906\n",
      "Iteration 44742 => Loss: 6.69867571239240522374\n",
      "Iteration 44743 => Loss: 6.69867564530946957291\n",
      "Iteration 44744 => Loss: 6.69867557823558001928\n",
      "Iteration 44745 => Loss: 6.69867551117071613476\n",
      "Iteration 44746 => Loss: 6.69867544411489124201\n",
      "Iteration 44747 => Loss: 6.69867537706810889375\n",
      "Iteration 44748 => Loss: 6.69867531003035399095\n",
      "Iteration 44749 => Loss: 6.69867524300163719175\n",
      "Iteration 44750 => Loss: 6.69867517598195139072\n",
      "Iteration 44751 => Loss: 6.69867510897129658787\n",
      "Iteration 44752 => Loss: 6.69867504196966656593\n",
      "Iteration 44753 => Loss: 6.69867497497707553578\n",
      "Iteration 44754 => Loss: 6.69867490799350484565\n",
      "Iteration 44755 => Loss: 6.69867484101896515369\n",
      "Iteration 44756 => Loss: 6.69867477405344668995\n",
      "Iteration 44757 => Loss: 6.69867470709694590170\n",
      "Iteration 44758 => Loss: 6.69867464014947877615\n",
      "Iteration 44759 => Loss: 6.69867457321101866796\n",
      "Iteration 44760 => Loss: 6.69867450628159399884\n",
      "Iteration 44761 => Loss: 6.69867443936117545888\n",
      "Iteration 44762 => Loss: 6.69867437244978614075\n",
      "Iteration 44763 => Loss: 6.69867430554740117543\n",
      "Iteration 44764 => Loss: 6.69867423865404454375\n",
      "Iteration 44765 => Loss: 6.69867417176969404125\n",
      "Iteration 44766 => Loss: 6.69867410489435410881\n",
      "Iteration 44767 => Loss: 6.69867403802802741097\n",
      "Iteration 44768 => Loss: 6.69867397117071750046\n",
      "Iteration 44769 => Loss: 6.69867390432240750187\n",
      "Iteration 44770 => Loss: 6.69867383748310718516\n",
      "Iteration 44771 => Loss: 6.69867377065281655035\n",
      "Iteration 44772 => Loss: 6.69867370383153026836\n",
      "Iteration 44773 => Loss: 6.69867363701924922736\n",
      "Iteration 44774 => Loss: 6.69867357021596898647\n",
      "Iteration 44775 => Loss: 6.69867350342168865751\n",
      "Iteration 44776 => Loss: 6.69867343663641356954\n",
      "Iteration 44777 => Loss: 6.69867336986013306444\n",
      "Iteration 44778 => Loss: 6.69867330309285691214\n",
      "Iteration 44779 => Loss: 6.69867323633457267817\n",
      "Iteration 44780 => Loss: 6.69867316958528480342\n",
      "Iteration 44781 => Loss: 6.69867310284499239970\n",
      "Iteration 44782 => Loss: 6.69867303611368924976\n",
      "Iteration 44783 => Loss: 6.69867296939137979450\n",
      "Iteration 44784 => Loss: 6.69867290267806936299\n",
      "Iteration 44785 => Loss: 6.69867283597374196802\n",
      "Iteration 44786 => Loss: 6.69867276927840116230\n",
      "Iteration 44787 => Loss: 6.69867270259205493943\n",
      "Iteration 44788 => Loss: 6.69867263591468464767\n",
      "Iteration 44789 => Loss: 6.69867256924630360970\n",
      "Iteration 44790 => Loss: 6.69867250258690294373\n",
      "Iteration 44791 => Loss: 6.69867243593649330791\n",
      "Iteration 44792 => Loss: 6.69867236929506226772\n",
      "Iteration 44793 => Loss: 6.69867230266260360594\n",
      "Iteration 44794 => Loss: 6.69867223603912975705\n",
      "Iteration 44795 => Loss: 6.69867216942463095108\n",
      "Iteration 44796 => Loss: 6.69867210281911606984\n",
      "Iteration 44797 => Loss: 6.69867203622256557338\n",
      "Iteration 44798 => Loss: 6.69867196963499544893\n",
      "Iteration 44799 => Loss: 6.69867190305639592651\n",
      "Iteration 44800 => Loss: 6.69867183648676522978\n",
      "Iteration 44801 => Loss: 6.69867176992611046416\n",
      "Iteration 44802 => Loss: 6.69867170337442008332\n",
      "Iteration 44803 => Loss: 6.69867163683170119270\n",
      "Iteration 44804 => Loss: 6.69867157029794579870\n",
      "Iteration 44805 => Loss: 6.69867150377315656584\n",
      "Iteration 44806 => Loss: 6.69867143725733171777\n",
      "Iteration 44807 => Loss: 6.69867137075046858996\n",
      "Iteration 44808 => Loss: 6.69867130425257695236\n",
      "Iteration 44809 => Loss: 6.69867123776363282417\n",
      "Iteration 44810 => Loss: 6.69867117128365396894\n",
      "Iteration 44811 => Loss: 6.69867110481262884036\n",
      "Iteration 44812 => Loss: 6.69867103835056543204\n",
      "Iteration 44813 => Loss: 6.69867097189745930308\n",
      "Iteration 44814 => Loss: 6.69867090545330423623\n",
      "Iteration 44815 => Loss: 6.69867083901810289603\n",
      "Iteration 44816 => Loss: 6.69867077259185794702\n",
      "Iteration 44817 => Loss: 6.69867070617455784287\n",
      "Iteration 44818 => Loss: 6.69867063976620702448\n",
      "Iteration 44819 => Loss: 6.69867057336681259727\n",
      "Iteration 44820 => Loss: 6.69867050697635857404\n",
      "Iteration 44821 => Loss: 6.69867044059485028384\n",
      "Iteration 44822 => Loss: 6.69867037422228950305\n",
      "Iteration 44823 => Loss: 6.69867030785867889620\n",
      "Iteration 44824 => Loss: 6.69867024150399981153\n",
      "Iteration 44825 => Loss: 6.69867017515826024265\n",
      "Iteration 44826 => Loss: 6.69867010882147084772\n",
      "Iteration 44827 => Loss: 6.69867004249361741586\n",
      "Iteration 44828 => Loss: 6.69866997617469817072\n",
      "Iteration 44829 => Loss: 6.69866990986471755321\n",
      "Iteration 44830 => Loss: 6.69866984356367467512\n",
      "Iteration 44831 => Loss: 6.69866977727156243105\n",
      "Iteration 44832 => Loss: 6.69866971098838437371\n",
      "Iteration 44833 => Loss: 6.69866964471413872673\n",
      "Iteration 44834 => Loss: 6.69866957844882104922\n",
      "Iteration 44835 => Loss: 6.69866951219243311755\n",
      "Iteration 44836 => Loss: 6.69866944594498026078\n",
      "Iteration 44837 => Loss: 6.69866937970644293898\n",
      "Iteration 44838 => Loss: 6.69866931347683802755\n",
      "Iteration 44839 => Loss: 6.69866924725616197378\n",
      "Iteration 44840 => Loss: 6.69866918104439790227\n",
      "Iteration 44841 => Loss: 6.69866911484156357659\n",
      "Iteration 44842 => Loss: 6.69866904864764833860\n",
      "Iteration 44843 => Loss: 6.69866898246264685923\n",
      "Iteration 44844 => Loss: 6.69866891628657334934\n",
      "Iteration 44845 => Loss: 6.69866885011940560446\n",
      "Iteration 44846 => Loss: 6.69866878396116849359\n",
      "Iteration 44847 => Loss: 6.69866871781183714774\n",
      "Iteration 44848 => Loss: 6.69866865167141778414\n",
      "Iteration 44849 => Loss: 6.69866858553991573189\n",
      "Iteration 44850 => Loss: 6.69866851941732388553\n",
      "Iteration 44851 => Loss: 6.69866845330364579780\n",
      "Iteration 44852 => Loss: 6.69866838719886992237\n",
      "Iteration 44853 => Loss: 6.69866832110300780556\n",
      "Iteration 44854 => Loss: 6.69866825501604346016\n",
      "Iteration 44855 => Loss: 6.69866818893799020884\n",
      "Iteration 44856 => Loss: 6.69866812286883916983\n",
      "Iteration 44857 => Loss: 6.69866805680859034311\n",
      "Iteration 44858 => Loss: 6.69866799075724461687\n",
      "Iteration 44859 => Loss: 6.69866792471480110294\n",
      "Iteration 44860 => Loss: 6.69866785868125536041\n",
      "Iteration 44861 => Loss: 6.69866779265660561293\n",
      "Iteration 44862 => Loss: 6.69866772664085896594\n",
      "Iteration 44863 => Loss: 6.69866766063400120856\n",
      "Iteration 44864 => Loss: 6.69866759463603678171\n",
      "Iteration 44865 => Loss: 6.69866752864697101444\n",
      "Iteration 44866 => Loss: 6.69866746266679591315\n",
      "Iteration 44867 => Loss: 6.69866739669551058967\n",
      "Iteration 44868 => Loss: 6.69866733073311593216\n",
      "Iteration 44869 => Loss: 6.69866726477961282882\n",
      "Iteration 44870 => Loss: 6.69866719883498884514\n",
      "Iteration 44871 => Loss: 6.69866713289925552743\n",
      "Iteration 44872 => Loss: 6.69866706697240754664\n",
      "Iteration 44873 => Loss: 6.69866700105444490276\n",
      "Iteration 44874 => Loss: 6.69866693514535427312\n",
      "Iteration 44875 => Loss: 6.69866686924515430945\n",
      "Iteration 44876 => Loss: 6.69866680335383612999\n",
      "Iteration 44877 => Loss: 6.69866673747138907657\n",
      "Iteration 44878 => Loss: 6.69866667159782647190\n",
      "Iteration 44879 => Loss: 6.69866660573313676963\n",
      "Iteration 44880 => Loss: 6.69866653987733062792\n",
      "Iteration 44881 => Loss: 6.69866647403039028319\n",
      "Iteration 44882 => Loss: 6.69866640819232195270\n",
      "Iteration 44883 => Loss: 6.69866634236312563644\n",
      "Iteration 44884 => Loss: 6.69866627654280577531\n",
      "Iteration 44885 => Loss: 6.69866621073134638209\n",
      "Iteration 44886 => Loss: 6.69866614492875900311\n",
      "Iteration 44887 => Loss: 6.69866607913503830929\n",
      "Iteration 44888 => Loss: 6.69866601335018874153\n",
      "Iteration 44889 => Loss: 6.69866594757419431261\n",
      "Iteration 44890 => Loss: 6.69866588180706745703\n",
      "Iteration 44891 => Loss: 6.69866581604880551026\n",
      "Iteration 44892 => Loss: 6.69866575029940047870\n",
      "Iteration 44893 => Loss: 6.69866568455885946776\n",
      "Iteration 44894 => Loss: 6.69866561882717537202\n",
      "Iteration 44895 => Loss: 6.69866555310434819148\n",
      "Iteration 44896 => Loss: 6.69866548739037526161\n",
      "Iteration 44897 => Loss: 6.69866542168525658241\n",
      "Iteration 44898 => Loss: 6.69866535598899925930\n",
      "Iteration 44899 => Loss: 6.69866529030159085778\n",
      "Iteration 44900 => Loss: 6.69866522462302782515\n",
      "Iteration 44901 => Loss: 6.69866515895331726682\n",
      "Iteration 44902 => Loss: 6.69866509329246184734\n",
      "Iteration 44903 => Loss: 6.69866502764044824403\n",
      "Iteration 44904 => Loss: 6.69866496199728622685\n",
      "Iteration 44905 => Loss: 6.69866489636296513766\n",
      "Iteration 44906 => Loss: 6.69866483073749119370\n",
      "Iteration 44907 => Loss: 6.69866476512085373685\n",
      "Iteration 44908 => Loss: 6.69866469951306431341\n",
      "Iteration 44909 => Loss: 6.69866463391411848249\n",
      "Iteration 44910 => Loss: 6.69866456832400558596\n",
      "Iteration 44911 => Loss: 6.69866450274273272925\n",
      "Iteration 44912 => Loss: 6.69866443717029991234\n",
      "Iteration 44913 => Loss: 6.69866437160669914164\n",
      "Iteration 44914 => Loss: 6.69866430605193219350\n",
      "Iteration 44915 => Loss: 6.69866424050600350881\n",
      "Iteration 44916 => Loss: 6.69866417496890331762\n",
      "Iteration 44917 => Loss: 6.69866410944063961352\n",
      "Iteration 44918 => Loss: 6.69866404392119640931\n",
      "Iteration 44919 => Loss: 6.69866397841059058038\n",
      "Iteration 44920 => Loss: 6.69866391290880969223\n",
      "Iteration 44921 => Loss: 6.69866384741585552121\n",
      "Iteration 44922 => Loss: 6.69866378193172717914\n",
      "Iteration 44923 => Loss: 6.69866371645641756061\n",
      "Iteration 44924 => Loss: 6.69866365098993465921\n",
      "Iteration 44925 => Loss: 6.69866358553227847494\n",
      "Iteration 44926 => Loss: 6.69866352008343390878\n",
      "Iteration 44927 => Loss: 6.69866345464341428340\n",
      "Iteration 44928 => Loss: 6.69866338921220805247\n",
      "Iteration 44929 => Loss: 6.69866332378982232143\n",
      "Iteration 44930 => Loss: 6.69866325837625176121\n",
      "Iteration 44931 => Loss: 6.69866319297149370726\n",
      "Iteration 44932 => Loss: 6.69866312757555171231\n",
      "Iteration 44933 => Loss: 6.69866306218842400000\n",
      "Iteration 44934 => Loss: 6.69866299681009724765\n",
      "Iteration 44935 => Loss: 6.69866293144058921882\n",
      "Iteration 44936 => Loss: 6.69866286607988925539\n",
      "Iteration 44937 => Loss: 6.69866280072799114009\n",
      "Iteration 44938 => Loss: 6.69866273538490375472\n",
      "Iteration 44939 => Loss: 6.69866267005062177020\n",
      "Iteration 44940 => Loss: 6.69866260472514429836\n",
      "Iteration 44941 => Loss: 6.69866253940846512194\n",
      "Iteration 44942 => Loss: 6.69866247410058690548\n",
      "Iteration 44943 => Loss: 6.69866240880151231352\n",
      "Iteration 44944 => Loss: 6.69866234351123868151\n",
      "Iteration 44945 => Loss: 6.69866227822975890405\n",
      "Iteration 44946 => Loss: 6.69866221295707475747\n",
      "Iteration 44947 => Loss: 6.69866214769318890632\n",
      "Iteration 44948 => Loss: 6.69866208243810046241\n",
      "Iteration 44949 => Loss: 6.69866201719180320850\n",
      "Iteration 44950 => Loss: 6.69866195195429092735\n",
      "Iteration 44951 => Loss: 6.69866188672557960615\n",
      "Iteration 44952 => Loss: 6.69866182150565236952\n",
      "Iteration 44953 => Loss: 6.69866175629451010565\n",
      "Iteration 44954 => Loss: 6.69866169109216169630\n",
      "Iteration 44955 => Loss: 6.69866162589859825971\n",
      "Iteration 44956 => Loss: 6.69866156071381269044\n",
      "Iteration 44957 => Loss: 6.69866149553781831116\n",
      "Iteration 44958 => Loss: 6.69866143037060535192\n",
      "Iteration 44959 => Loss: 6.69866136521217203637\n",
      "Iteration 44960 => Loss: 6.69866130006251925266\n",
      "Iteration 44961 => Loss: 6.69866123492164700082\n",
      "Iteration 44962 => Loss: 6.69866116978955350447\n",
      "Iteration 44963 => Loss: 6.69866110466623254638\n",
      "Iteration 44964 => Loss: 6.69866103955168412654\n",
      "Iteration 44965 => Loss: 6.69866097444591712673\n",
      "Iteration 44966 => Loss: 6.69866090934892177700\n",
      "Iteration 44967 => Loss: 6.69866084426069097191\n",
      "Iteration 44968 => Loss: 6.69866077918123714596\n",
      "Iteration 44969 => Loss: 6.69866071411055852280\n",
      "Iteration 44970 => Loss: 6.69866064904864000340\n",
      "Iteration 44971 => Loss: 6.69866058399549313407\n",
      "Iteration 44972 => Loss: 6.69866051895111080938\n",
      "Iteration 44973 => Loss: 6.69866045391548325938\n",
      "Iteration 44974 => Loss: 6.69866038888863624123\n",
      "Iteration 44975 => Loss: 6.69866032387053511599\n",
      "Iteration 44976 => Loss: 6.69866025886121008170\n",
      "Iteration 44977 => Loss: 6.69866019386063804575\n",
      "Iteration 44978 => Loss: 6.69866012886882433719\n",
      "Iteration 44979 => Loss: 6.69866006388576629149\n",
      "Iteration 44980 => Loss: 6.69865999891147012590\n",
      "Iteration 44981 => Loss: 6.69865993394592340593\n",
      "Iteration 44982 => Loss: 6.69865986898913412517\n",
      "Iteration 44983 => Loss: 6.69865980404110228363\n",
      "Iteration 44984 => Loss: 6.69865973910181722317\n",
      "Iteration 44985 => Loss: 6.69865967417128249650\n",
      "Iteration 44986 => Loss: 6.69865960924949721544\n",
      "Iteration 44987 => Loss: 6.69865954433645427457\n",
      "Iteration 44988 => Loss: 6.69865947943216788474\n",
      "Iteration 44989 => Loss: 6.69865941453662649963\n",
      "Iteration 44990 => Loss: 6.69865934964982479016\n",
      "Iteration 44991 => Loss: 6.69865928477177430267\n",
      "Iteration 44992 => Loss: 6.69865921990246082629\n",
      "Iteration 44993 => Loss: 6.69865915504188702556\n",
      "Iteration 44994 => Loss: 6.69865909019005734137\n",
      "Iteration 44995 => Loss: 6.69865902534696378012\n",
      "Iteration 44996 => Loss: 6.69865896051260811817\n",
      "Iteration 44997 => Loss: 6.69865889568698946732\n",
      "Iteration 44998 => Loss: 6.69865883087010072217\n",
      "Iteration 44999 => Loss: 6.69865876606195076448\n",
      "Iteration 45000 => Loss: 6.69865870126253604155\n",
      "Iteration 45001 => Loss: 6.69865863647185388885\n",
      "Iteration 45002 => Loss: 6.69865857168989897730\n",
      "Iteration 45003 => Loss: 6.69865850691667130690\n",
      "Iteration 45004 => Loss: 6.69865844215217354218\n",
      "Iteration 45005 => Loss: 6.69865837739640657134\n",
      "Iteration 45006 => Loss: 6.69865831264936328893\n",
      "Iteration 45007 => Loss: 6.69865824791104191860\n",
      "Iteration 45008 => Loss: 6.69865818318144512489\n",
      "Iteration 45009 => Loss: 6.69865811846057646051\n",
      "Iteration 45010 => Loss: 6.69865805374841816189\n",
      "Iteration 45011 => Loss: 6.69865798904498532806\n",
      "Iteration 45012 => Loss: 6.69865792435027529450\n",
      "Iteration 45013 => Loss: 6.69865785966428095577\n",
      "Iteration 45014 => Loss: 6.69865779498699787098\n",
      "Iteration 45015 => Loss: 6.69865773031843136920\n",
      "Iteration 45016 => Loss: 6.69865766565858589132\n",
      "Iteration 45017 => Loss: 6.69865760100744633831\n",
      "Iteration 45018 => Loss: 6.69865753636501626289\n",
      "Iteration 45019 => Loss: 6.69865747173130454684\n",
      "Iteration 45020 => Loss: 6.69865740710630408472\n",
      "Iteration 45021 => Loss: 6.69865734249000066569\n",
      "Iteration 45022 => Loss: 6.69865727788240938878\n",
      "Iteration 45023 => Loss: 6.69865721328352314856\n",
      "Iteration 45024 => Loss: 6.69865714869334460957\n",
      "Iteration 45025 => Loss: 6.69865708411187199545\n",
      "Iteration 45026 => Loss: 6.69865701953909731259\n",
      "Iteration 45027 => Loss: 6.69865695497502411371\n",
      "Iteration 45028 => Loss: 6.69865689041965595152\n",
      "Iteration 45029 => Loss: 6.69865682587297772699\n",
      "Iteration 45030 => Loss: 6.69865676133500009826\n",
      "Iteration 45031 => Loss: 6.69865669680571684808\n",
      "Iteration 45032 => Loss: 6.69865663228514041094\n",
      "Iteration 45033 => Loss: 6.69865656777324502968\n",
      "Iteration 45034 => Loss: 6.69865650327005202058\n",
      "Iteration 45035 => Loss: 6.69865643877554095553\n",
      "Iteration 45036 => Loss: 6.69865637428972604539\n",
      "Iteration 45037 => Loss: 6.69865630981260373744\n",
      "Iteration 45038 => Loss: 6.69865624534416248537\n",
      "Iteration 45039 => Loss: 6.69865618088441383549\n",
      "Iteration 45040 => Loss: 6.69865611643335245873\n",
      "Iteration 45041 => Loss: 6.69865605199097124967\n",
      "Iteration 45042 => Loss: 6.69865598755727376101\n",
      "Iteration 45043 => Loss: 6.69865592313226176913\n",
      "Iteration 45044 => Loss: 6.69865585871592994494\n",
      "Iteration 45045 => Loss: 6.69865579430828006480\n",
      "Iteration 45046 => Loss: 6.69865572990930857600\n",
      "Iteration 45047 => Loss: 6.69865566551901014947\n",
      "Iteration 45048 => Loss: 6.69865560113739189063\n",
      "Iteration 45049 => Loss: 6.69865553676444935860\n",
      "Iteration 45050 => Loss: 6.69865547240017988884\n",
      "Iteration 45051 => Loss: 6.69865540804458259316\n",
      "Iteration 45052 => Loss: 6.69865534369765835976\n",
      "Iteration 45053 => Loss: 6.69865527935940985316\n",
      "Iteration 45054 => Loss: 6.69865521502982641522\n",
      "Iteration 45055 => Loss: 6.69865515070891159866\n",
      "Iteration 45056 => Loss: 6.69865508639666007440\n",
      "Iteration 45057 => Loss: 6.69865502209307628334\n",
      "Iteration 45058 => Loss: 6.69865495779816377819\n",
      "Iteration 45059 => Loss: 6.69865489351191190082\n",
      "Iteration 45060 => Loss: 6.69865482923431532214\n",
      "Iteration 45061 => Loss: 6.69865476496539269391\n",
      "Iteration 45062 => Loss: 6.69865470070511914713\n",
      "Iteration 45063 => Loss: 6.69865463645351599808\n",
      "Iteration 45064 => Loss: 6.69865457221056281867\n",
      "Iteration 45065 => Loss: 6.69865450797626760249\n",
      "Iteration 45066 => Loss: 6.69865444375062590865\n",
      "Iteration 45067 => Loss: 6.69865437953364395440\n",
      "Iteration 45068 => Loss: 6.69865431532530930525\n",
      "Iteration 45069 => Loss: 6.69865425112562817844\n",
      "Iteration 45070 => Loss: 6.69865418693459879762\n",
      "Iteration 45071 => Loss: 6.69865412275222027461\n",
      "Iteration 45072 => Loss: 6.69865405857848905669\n",
      "Iteration 45073 => Loss: 6.69865399441341047293\n",
      "Iteration 45074 => Loss: 6.69865393025697297702\n",
      "Iteration 45075 => Loss: 6.69865386610918456256\n",
      "Iteration 45076 => Loss: 6.69865380197002924234\n",
      "Iteration 45077 => Loss: 6.69865373783952744446\n",
      "Iteration 45078 => Loss: 6.69865367371766051718\n",
      "Iteration 45079 => Loss: 6.69865360960443911864\n",
      "Iteration 45080 => Loss: 6.69865354549985436705\n",
      "Iteration 45081 => Loss: 6.69865348140390715059\n",
      "Iteration 45082 => Loss: 6.69865341731660191016\n",
      "Iteration 45083 => Loss: 6.69865335323793065214\n",
      "Iteration 45084 => Loss: 6.69865328916788360658\n",
      "Iteration 45085 => Loss: 6.69865322510648208976\n",
      "Iteration 45086 => Loss: 6.69865316105370922628\n",
      "Iteration 45087 => Loss: 6.69865309700957034522\n",
      "Iteration 45088 => Loss: 6.69865303297405656480\n",
      "Iteration 45089 => Loss: 6.69865296894717765497\n",
      "Iteration 45090 => Loss: 6.69865290492892206942\n",
      "Iteration 45091 => Loss: 6.69865284091929158450\n",
      "Iteration 45092 => Loss: 6.69865277691829064111\n",
      "Iteration 45093 => Loss: 6.69865271292590946928\n",
      "Iteration 45094 => Loss: 6.69865264894215162172\n",
      "Iteration 45095 => Loss: 6.69865258496701354574\n",
      "Iteration 45096 => Loss: 6.69865252100050501127\n",
      "Iteration 45097 => Loss: 6.69865245704261180748\n",
      "Iteration 45098 => Loss: 6.69865239309333393436\n",
      "Iteration 45099 => Loss: 6.69865232915267139191\n",
      "Iteration 45100 => Loss: 6.69865226522063572645\n",
      "Iteration 45101 => Loss: 6.69865220129720562170\n",
      "Iteration 45102 => Loss: 6.69865213738239351215\n",
      "Iteration 45103 => Loss: 6.69865207347619140421\n",
      "Iteration 45104 => Loss: 6.69865200957859752151\n",
      "Iteration 45105 => Loss: 6.69865194568962074584\n",
      "Iteration 45106 => Loss: 6.69865188180924508998\n",
      "Iteration 45107 => Loss: 6.69865181793748032391\n",
      "Iteration 45108 => Loss: 6.69865175407433088850\n",
      "Iteration 45109 => Loss: 6.69865169021977546748\n",
      "Iteration 45110 => Loss: 6.69865162637383004807\n",
      "Iteration 45111 => Loss: 6.69865156253648397211\n",
      "Iteration 45112 => Loss: 6.69865149870774523322\n",
      "Iteration 45113 => Loss: 6.69865143488760317325\n",
      "Iteration 45114 => Loss: 6.69865137107606312128\n",
      "Iteration 45115 => Loss: 6.69865130727312330094\n",
      "Iteration 45116 => Loss: 6.69865124347877660682\n",
      "Iteration 45117 => Loss: 6.69865117969303103251\n",
      "Iteration 45118 => Loss: 6.69865111591587503170\n",
      "Iteration 45119 => Loss: 6.69865105214731748617\n",
      "Iteration 45120 => Loss: 6.69865098838735395503\n",
      "Iteration 45121 => Loss: 6.69865092463597378014\n",
      "Iteration 45122 => Loss: 6.69865086089319294871\n",
      "Iteration 45123 => Loss: 6.69865079715900080259\n",
      "Iteration 45124 => Loss: 6.69865073343339201273\n",
      "Iteration 45125 => Loss: 6.69865066971637190818\n",
      "Iteration 45126 => Loss: 6.69865060600793427170\n",
      "Iteration 45127 => Loss: 6.69865054230808709690\n",
      "Iteration 45128 => Loss: 6.69865047861682327834\n",
      "Iteration 45129 => Loss: 6.69865041493413926332\n",
      "Iteration 45130 => Loss: 6.69865035126003682819\n",
      "Iteration 45131 => Loss: 6.69865028759451419660\n",
      "Iteration 45132 => Loss: 6.69865022393757314489\n",
      "Iteration 45133 => Loss: 6.69865016028921100855\n",
      "Iteration 45134 => Loss: 6.69865009664942245848\n",
      "Iteration 45135 => Loss: 6.69865003301820571835\n",
      "Iteration 45136 => Loss: 6.69864996939556611721\n",
      "Iteration 45137 => Loss: 6.69864990578150454326\n",
      "Iteration 45138 => Loss: 6.69864984217601033833\n",
      "Iteration 45139 => Loss: 6.69864977857908439063\n",
      "Iteration 45140 => Loss: 6.69864971499073202921\n",
      "Iteration 45141 => Loss: 6.69864965141094703682\n",
      "Iteration 45142 => Loss: 6.69864958783973030165\n",
      "Iteration 45143 => Loss: 6.69864952427708093552\n",
      "Iteration 45144 => Loss: 6.69864946072299449753\n",
      "Iteration 45145 => Loss: 6.69864939717747365222\n",
      "Iteration 45146 => Loss: 6.69864933364052106413\n",
      "Iteration 45147 => Loss: 6.69864927011211985786\n",
      "Iteration 45148 => Loss: 6.69864920659228335609\n",
      "Iteration 45149 => Loss: 6.69864914308100267704\n",
      "Iteration 45150 => Loss: 6.69864907957828403795\n",
      "Iteration 45151 => Loss: 6.69864901608412388612\n",
      "Iteration 45152 => Loss: 6.69864895259851600429\n",
      "Iteration 45153 => Loss: 6.69864888912146394517\n",
      "Iteration 45154 => Loss: 6.69864882565296770878\n",
      "Iteration 45155 => Loss: 6.69864876219301663696\n",
      "Iteration 45156 => Loss: 6.69864869874162405239\n",
      "Iteration 45157 => Loss: 6.69864863529877663240\n",
      "Iteration 45158 => Loss: 6.69864857186448237059\n",
      "Iteration 45159 => Loss: 6.69864850843873771424\n",
      "Iteration 45160 => Loss: 6.69864844502153466976\n",
      "Iteration 45161 => Loss: 6.69864838161288300711\n",
      "Iteration 45162 => Loss: 6.69864831821276673907\n",
      "Iteration 45163 => Loss: 6.69864825482119741196\n",
      "Iteration 45164 => Loss: 6.69864819143817324942\n",
      "Iteration 45165 => Loss: 6.69864812806368981057\n",
      "Iteration 45166 => Loss: 6.69864806469773998998\n",
      "Iteration 45167 => Loss: 6.69864800134033355761\n",
      "Iteration 45168 => Loss: 6.69864793799146340803\n",
      "Iteration 45169 => Loss: 6.69864787465113042941\n",
      "Iteration 45170 => Loss: 6.69864781131933373359\n",
      "Iteration 45171 => Loss: 6.69864774799606443878\n",
      "Iteration 45172 => Loss: 6.69864768468133764401\n",
      "Iteration 45173 => Loss: 6.69864762137513825024\n",
      "Iteration 45174 => Loss: 6.69864755807746714567\n",
      "Iteration 45175 => Loss: 6.69864749478832610663\n",
      "Iteration 45176 => Loss: 6.69864743150771069224\n",
      "Iteration 45177 => Loss: 6.69864736823563156065\n",
      "Iteration 45178 => Loss: 6.69864730497207006010\n",
      "Iteration 45179 => Loss: 6.69864724171703596056\n",
      "Iteration 45180 => Loss: 6.69864717847052570932\n",
      "Iteration 45181 => Loss: 6.69864711523253486547\n",
      "Iteration 45182 => Loss: 6.69864705200306875810\n",
      "Iteration 45183 => Loss: 6.69864698878212827537\n",
      "Iteration 45184 => Loss: 6.69864692556969920645\n",
      "Iteration 45185 => Loss: 6.69864686236578865675\n",
      "Iteration 45186 => Loss: 6.69864679917039218537\n",
      "Iteration 45187 => Loss: 6.69864673598351689776\n",
      "Iteration 45188 => Loss: 6.69864667280515657666\n",
      "Iteration 45189 => Loss: 6.69864660963530589299\n",
      "Iteration 45190 => Loss: 6.69864654647397017584\n",
      "Iteration 45191 => Loss: 6.69864648332113699070\n",
      "Iteration 45192 => Loss: 6.69864642017682410113\n",
      "Iteration 45193 => Loss: 6.69864635704101551994\n",
      "Iteration 45194 => Loss: 6.69864629391371568801\n",
      "Iteration 45195 => Loss: 6.69864623079492105262\n",
      "Iteration 45196 => Loss: 6.69864616768463339014\n",
      "Iteration 45197 => Loss: 6.69864610458284648331\n",
      "Iteration 45198 => Loss: 6.69864604148956566121\n",
      "Iteration 45199 => Loss: 6.69864597840478559476\n",
      "Iteration 45200 => Loss: 6.69864591532850894851\n",
      "Iteration 45201 => Loss: 6.69864585226072950519\n",
      "Iteration 45202 => Loss: 6.69864578920144637664\n",
      "Iteration 45203 => Loss: 6.69864572615065956285\n",
      "Iteration 45204 => Loss: 6.69864566310837883378\n",
      "Iteration 45205 => Loss: 6.69864560007458376134\n",
      "Iteration 45206 => Loss: 6.69864553704928322730\n",
      "Iteration 45207 => Loss: 6.69864547403247811985\n",
      "Iteration 45208 => Loss: 6.69864541102416488627\n",
      "Iteration 45209 => Loss: 6.69864534802433642113\n",
      "Iteration 45210 => Loss: 6.69864528503300515894\n",
      "Iteration 45211 => Loss: 6.69864522205015866518\n",
      "Iteration 45212 => Loss: 6.69864515907579693987\n",
      "Iteration 45213 => Loss: 6.69864509610991909483\n",
      "Iteration 45214 => Loss: 6.69864503315253045912\n",
      "Iteration 45215 => Loss: 6.69864497020362570368\n",
      "Iteration 45216 => Loss: 6.69864490726320394032\n",
      "Iteration 45217 => Loss: 6.69864484433125628726\n",
      "Iteration 45218 => Loss: 6.69864478140779873172\n",
      "Iteration 45219 => Loss: 6.69864471849281084559\n",
      "Iteration 45220 => Loss: 6.69864465558630506337\n",
      "Iteration 45221 => Loss: 6.69864459268827783234\n",
      "Iteration 45222 => Loss: 6.69864452979872115890\n",
      "Iteration 45223 => Loss: 6.69864446691764303665\n",
      "Iteration 45224 => Loss: 6.69864440404503902471\n",
      "Iteration 45225 => Loss: 6.69864434118090823489\n",
      "Iteration 45226 => Loss: 6.69864427832524356177\n",
      "Iteration 45227 => Loss: 6.69864421547804589352\n",
      "Iteration 45228 => Loss: 6.69864415263932322375\n",
      "Iteration 45229 => Loss: 6.69864408980906400615\n",
      "Iteration 45230 => Loss: 6.69864402698727978702\n",
      "Iteration 45231 => Loss: 6.69864396417395280281\n",
      "Iteration 45232 => Loss: 6.69864390136909015894\n",
      "Iteration 45233 => Loss: 6.69864383857269096723\n",
      "Iteration 45234 => Loss: 6.69864377578475433950\n",
      "Iteration 45235 => Loss: 6.69864371300527938757\n",
      "Iteration 45236 => Loss: 6.69864365023426255874\n",
      "Iteration 45237 => Loss: 6.69864358747170118846\n",
      "Iteration 45238 => Loss: 6.69864352471760060581\n",
      "Iteration 45239 => Loss: 6.69864346197195814625\n",
      "Iteration 45240 => Loss: 6.69864339923476936889\n",
      "Iteration 45241 => Loss: 6.69864333650603338555\n",
      "Iteration 45242 => Loss: 6.69864327378575019623\n",
      "Iteration 45243 => Loss: 6.69864321107391713639\n",
      "Iteration 45244 => Loss: 6.69864314837053953511\n",
      "Iteration 45245 => Loss: 6.69864308567560851060\n",
      "Iteration 45246 => Loss: 6.69864302298911962197\n",
      "Iteration 45247 => Loss: 6.69864296031108708007\n",
      "Iteration 45248 => Loss: 6.69864289764149667405\n",
      "Iteration 45249 => Loss: 6.69864283498035018027\n",
      "Iteration 45250 => Loss: 6.69864277232764759873\n",
      "Iteration 45251 => Loss: 6.69864270968338715306\n",
      "Iteration 45252 => Loss: 6.69864264704756351421\n",
      "Iteration 45253 => Loss: 6.69864258442018556394\n",
      "Iteration 45254 => Loss: 6.69864252180124797320\n",
      "Iteration 45255 => Loss: 6.69864245919074896563\n",
      "Iteration 45256 => Loss: 6.69864239658868410032\n",
      "Iteration 45257 => Loss: 6.69864233399505248912\n",
      "Iteration 45258 => Loss: 6.69864227140985857289\n",
      "Iteration 45259 => Loss: 6.69864220883309791077\n",
      "Iteration 45260 => Loss: 6.69864214626476517367\n",
      "Iteration 45261 => Loss: 6.69864208370487723698\n",
      "Iteration 45262 => Loss: 6.69864202115340212629\n",
      "Iteration 45263 => Loss: 6.69864195861036648694\n",
      "Iteration 45264 => Loss: 6.69864189607574989083\n",
      "Iteration 45265 => Loss: 6.69864183354957098970\n",
      "Iteration 45266 => Loss: 6.69864177103180491457\n",
      "Iteration 45267 => Loss: 6.69864170852247298171\n",
      "Iteration 45268 => Loss: 6.69864164602156009209\n",
      "Iteration 45269 => Loss: 6.69864158352906891025\n",
      "Iteration 45270 => Loss: 6.69864152104500565343\n",
      "Iteration 45271 => Loss: 6.69864145856935699896\n",
      "Iteration 45272 => Loss: 6.69864139610212205866\n",
      "Iteration 45273 => Loss: 6.69864133364330882614\n",
      "Iteration 45274 => Loss: 6.69864127119291197232\n",
      "Iteration 45275 => Loss: 6.69864120875092883267\n",
      "Iteration 45276 => Loss: 6.69864114631736295991\n",
      "Iteration 45277 => Loss: 6.69864108389220636042\n",
      "Iteration 45278 => Loss: 6.69864102147546880417\n",
      "Iteration 45279 => Loss: 6.69864095906712808670\n",
      "Iteration 45280 => Loss: 6.69864089666720641247\n",
      "Iteration 45281 => Loss: 6.69864083427569489970\n",
      "Iteration 45282 => Loss: 6.69864077189258466660\n",
      "Iteration 45283 => Loss: 6.69864070951788459496\n",
      "Iteration 45284 => Loss: 6.69864064715158757934\n",
      "Iteration 45285 => Loss: 6.69864058479370161336\n",
      "Iteration 45286 => Loss: 6.69864052244420804527\n",
      "Iteration 45287 => Loss: 6.69864046010312375046\n",
      "Iteration 45288 => Loss: 6.69864039777043362989\n",
      "Iteration 45289 => Loss: 6.69864033544614567717\n",
      "Iteration 45290 => Loss: 6.69864027313025456323\n",
      "Iteration 45291 => Loss: 6.69864021082276028807\n",
      "Iteration 45292 => Loss: 6.69864014852366551622\n",
      "Iteration 45293 => Loss: 6.69864008623296136591\n",
      "Iteration 45294 => Loss: 6.69864002395064961348\n",
      "Iteration 45295 => Loss: 6.69863996167673647619\n",
      "Iteration 45296 => Loss: 6.69863989941121040772\n",
      "Iteration 45297 => Loss: 6.69863983715407407260\n",
      "Iteration 45298 => Loss: 6.69863977490533191173\n",
      "Iteration 45299 => Loss: 6.69863971266497237878\n",
      "Iteration 45300 => Loss: 6.69863965043300524371\n",
      "Iteration 45301 => Loss: 6.69863958820942340111\n",
      "Iteration 45302 => Loss: 6.69863952599422329826\n",
      "Iteration 45303 => Loss: 6.69863946378740138243\n",
      "Iteration 45304 => Loss: 6.69863940158897186450\n",
      "Iteration 45305 => Loss: 6.69863933939891431635\n",
      "Iteration 45306 => Loss: 6.69863927721724561337\n",
      "Iteration 45307 => Loss: 6.69863921504395598561\n",
      "Iteration 45308 => Loss: 6.69863915287903655127\n",
      "Iteration 45309 => Loss: 6.69863909072249885668\n",
      "Iteration 45310 => Loss: 6.69863902857433579641\n",
      "Iteration 45311 => Loss: 6.69863896643454737045\n",
      "Iteration 45312 => Loss: 6.69863890430313002611\n",
      "Iteration 45313 => Loss: 6.69863884218008287519\n",
      "Iteration 45314 => Loss: 6.69863878006541657584\n",
      "Iteration 45315 => Loss: 6.69863871795911247631\n",
      "Iteration 45316 => Loss: 6.69863865586117945838\n",
      "Iteration 45317 => Loss: 6.69863859377161574571\n",
      "Iteration 45318 => Loss: 6.69863853169041600921\n",
      "Iteration 45319 => Loss: 6.69863846961758202525\n",
      "Iteration 45320 => Loss: 6.69863840755311112929\n",
      "Iteration 45321 => Loss: 6.69863834549700420951\n",
      "Iteration 45322 => Loss: 6.69863828344925948954\n",
      "Iteration 45323 => Loss: 6.69863822140987874576\n",
      "Iteration 45324 => Loss: 6.69863815937885309637\n",
      "Iteration 45325 => Loss: 6.69863809735618698227\n",
      "Iteration 45326 => Loss: 6.69863803534188129163\n",
      "Iteration 45327 => Loss: 6.69863797333592803085\n",
      "Iteration 45328 => Loss: 6.69863791133833785807\n",
      "Iteration 45329 => Loss: 6.69863784934909567426\n",
      "Iteration 45330 => Loss: 6.69863778736820325577\n",
      "Iteration 45331 => Loss: 6.69863772539567126074\n",
      "Iteration 45332 => Loss: 6.69863766343148014926\n",
      "Iteration 45333 => Loss: 6.69863760147564768488\n",
      "Iteration 45334 => Loss: 6.69863753952815965675\n",
      "Iteration 45335 => Loss: 6.69863747758901784124\n",
      "Iteration 45336 => Loss: 6.69863741565822934376\n",
      "Iteration 45337 => Loss: 6.69863735373578084165\n",
      "Iteration 45338 => Loss: 6.69863729182167677578\n",
      "Iteration 45339 => Loss: 6.69863722991591181710\n",
      "Iteration 45340 => Loss: 6.69863716801849484739\n",
      "Iteration 45341 => Loss: 6.69863710612941698486\n",
      "Iteration 45342 => Loss: 6.69863704424867645315\n",
      "Iteration 45343 => Loss: 6.69863698237627680498\n",
      "Iteration 45344 => Loss: 6.69863692051221537582\n",
      "Iteration 45345 => Loss: 6.69863685865648417206\n",
      "Iteration 45346 => Loss: 6.69863679680909562819\n",
      "Iteration 45347 => Loss: 6.69863673497003375701\n",
      "Iteration 45348 => Loss: 6.69863667313930832847\n",
      "Iteration 45349 => Loss: 6.69863661131691845441\n",
      "Iteration 45350 => Loss: 6.69863654950284903578\n",
      "Iteration 45351 => Loss: 6.69863648769711961251\n",
      "Iteration 45352 => Loss: 6.69863642589971508556\n",
      "Iteration 45353 => Loss: 6.69863636411063545495\n",
      "Iteration 45354 => Loss: 6.69863630232987983248\n",
      "Iteration 45355 => Loss: 6.69863624055745621177\n",
      "Iteration 45356 => Loss: 6.69863617879335304650\n",
      "Iteration 45357 => Loss: 6.69863611703757033666\n",
      "Iteration 45358 => Loss: 6.69863605529011252315\n",
      "Iteration 45359 => Loss: 6.69863599355096894783\n",
      "Iteration 45360 => Loss: 6.69863593182015204519\n",
      "Iteration 45361 => Loss: 6.69863587009764938074\n",
      "Iteration 45362 => Loss: 6.69863580838347161261\n",
      "Iteration 45363 => Loss: 6.69863574667760364179\n",
      "Iteration 45364 => Loss: 6.69863568498004724461\n",
      "Iteration 45365 => Loss: 6.69863562329081307922\n",
      "Iteration 45366 => Loss: 6.69863556160988604660\n",
      "Iteration 45367 => Loss: 6.69863549993727058762\n",
      "Iteration 45368 => Loss: 6.69863543827296048505\n",
      "Iteration 45369 => Loss: 6.69863537661696639702\n",
      "Iteration 45370 => Loss: 6.69863531496927588904\n",
      "Iteration 45371 => Loss: 6.69863525332990050742\n",
      "Iteration 45372 => Loss: 6.69863519169882160043\n",
      "Iteration 45373 => Loss: 6.69863513007605160254\n",
      "Iteration 45374 => Loss: 6.69863506846158962560\n",
      "Iteration 45375 => Loss: 6.69863500685542767599\n",
      "Iteration 45376 => Loss: 6.69863494525756042464\n",
      "Iteration 45377 => Loss: 6.69863488366800208240\n",
      "Iteration 45378 => Loss: 6.69863482208674287932\n",
      "Iteration 45379 => Loss: 6.69863476051377482179\n",
      "Iteration 45380 => Loss: 6.69863469894910679159\n",
      "Iteration 45381 => Loss: 6.69863463739273345965\n",
      "Iteration 45382 => Loss: 6.69863457584465304961\n",
      "Iteration 45383 => Loss: 6.69863451430487533145\n",
      "Iteration 45384 => Loss: 6.69863445277338698247\n",
      "Iteration 45385 => Loss: 6.69863439125018267362\n",
      "Iteration 45386 => Loss: 6.69863432973527928027\n",
      "Iteration 45387 => Loss: 6.69863426822865992705\n",
      "Iteration 45388 => Loss: 6.69863420673032816666\n",
      "Iteration 45389 => Loss: 6.69863414524028133457\n",
      "Iteration 45390 => Loss: 6.69863408375852031895\n",
      "Iteration 45391 => Loss: 6.69863402228504511982\n",
      "Iteration 45392 => Loss: 6.69863396081985751351\n",
      "Iteration 45393 => Loss: 6.69863389936294950644\n",
      "Iteration 45394 => Loss: 6.69863383791431488135\n",
      "Iteration 45395 => Loss: 6.69863377647397406633\n",
      "Iteration 45396 => Loss: 6.69863371504190485695\n",
      "Iteration 45397 => Loss: 6.69863365361811791132\n",
      "Iteration 45398 => Loss: 6.69863359220260168314\n",
      "Iteration 45399 => Loss: 6.69863353079536860690\n",
      "Iteration 45400 => Loss: 6.69863346939640447175\n",
      "Iteration 45401 => Loss: 6.69863340800570927769\n",
      "Iteration 45402 => Loss: 6.69863334662329457103\n",
      "Iteration 45403 => Loss: 6.69863328524914880546\n",
      "Iteration 45404 => Loss: 6.69863322388327464552\n",
      "Iteration 45405 => Loss: 6.69863316252566853848\n",
      "Iteration 45406 => Loss: 6.69863310117632959617\n",
      "Iteration 45407 => Loss: 6.69863303983525604224\n",
      "Iteration 45408 => Loss: 6.69863297850245409393\n",
      "Iteration 45409 => Loss: 6.69863291717790954038\n",
      "Iteration 45410 => Loss: 6.69863285586162948704\n",
      "Iteration 45411 => Loss: 6.69863279455361659842\n",
      "Iteration 45412 => Loss: 6.69863273325386288093\n",
      "Iteration 45413 => Loss: 6.69863267196237011092\n",
      "Iteration 45414 => Loss: 6.69863261067913473568\n",
      "Iteration 45415 => Loss: 6.69863254940415586702\n",
      "Iteration 45416 => Loss: 6.69863248813743439314\n",
      "Iteration 45417 => Loss: 6.69863242687896942584\n",
      "Iteration 45418 => Loss: 6.69863236562876807056\n",
      "Iteration 45419 => Loss: 6.69863230438680545831\n",
      "Iteration 45420 => Loss: 6.69863224315309846446\n",
      "Iteration 45421 => Loss: 6.69863218192765152992\n",
      "Iteration 45422 => Loss: 6.69863212071044600293\n",
      "Iteration 45423 => Loss: 6.69863205950149165346\n",
      "Iteration 45424 => Loss: 6.69863199830078759334\n",
      "Iteration 45425 => Loss: 6.69863193710833115802\n",
      "Iteration 45426 => Loss: 6.69863187592411701843\n",
      "Iteration 45427 => Loss: 6.69863181474815050365\n",
      "Iteration 45428 => Loss: 6.69863175358041917917\n",
      "Iteration 45429 => Loss: 6.69863169242094169675\n",
      "Iteration 45430 => Loss: 6.69863163126969585193\n",
      "Iteration 45431 => Loss: 6.69863157012669052648\n",
      "Iteration 45432 => Loss: 6.69863150899193371401\n",
      "Iteration 45433 => Loss: 6.69863144786541120368\n",
      "Iteration 45434 => Loss: 6.69863138674711944276\n",
      "Iteration 45435 => Loss: 6.69863132563706820122\n",
      "Iteration 45436 => Loss: 6.69863126453524948545\n",
      "Iteration 45437 => Loss: 6.69863120344166595999\n",
      "Iteration 45438 => Loss: 6.69863114235631051940\n",
      "Iteration 45439 => Loss: 6.69863108127919826273\n",
      "Iteration 45440 => Loss: 6.69863102021030254463\n",
      "Iteration 45441 => Loss: 6.69863095914964290500\n",
      "Iteration 45442 => Loss: 6.69863089809721135026\n",
      "Iteration 45443 => Loss: 6.69863083705300521586\n",
      "Iteration 45444 => Loss: 6.69863077601702450181\n",
      "Iteration 45445 => Loss: 6.69863071498926476721\n",
      "Iteration 45446 => Loss: 6.69863065396972956478\n",
      "Iteration 45447 => Loss: 6.69863059295842333540\n",
      "Iteration 45448 => Loss: 6.69863053195533897366\n",
      "Iteration 45449 => Loss: 6.69863047096046315687\n",
      "Iteration 45450 => Loss: 6.69863040997381986585\n",
      "Iteration 45451 => Loss: 6.69863034899538423161\n",
      "Iteration 45452 => Loss: 6.69863028802517312954\n",
      "Iteration 45453 => Loss: 6.69863022706317146060\n",
      "Iteration 45454 => Loss: 6.69863016610938277751\n",
      "Iteration 45455 => Loss: 6.69863010516381418569\n",
      "Iteration 45456 => Loss: 6.69863004422645591518\n",
      "Iteration 45457 => Loss: 6.69862998329731063052\n",
      "Iteration 45458 => Loss: 6.69862992237637300263\n",
      "Iteration 45459 => Loss: 6.69862986146364924878\n",
      "Iteration 45460 => Loss: 6.69862980055912426991\n",
      "Iteration 45461 => Loss: 6.69862973966280961235\n",
      "Iteration 45462 => Loss: 6.69862967877470349976\n",
      "Iteration 45463 => Loss: 6.69862961789480149122\n",
      "Iteration 45464 => Loss: 6.69862955702310269857\n",
      "Iteration 45465 => Loss: 6.69862949615960179273\n",
      "Iteration 45466 => Loss: 6.69862943530431120820\n",
      "Iteration 45467 => Loss: 6.69862937445721584595\n",
      "Iteration 45468 => Loss: 6.69862931361831748234\n",
      "Iteration 45469 => Loss: 6.69862925278761967007\n",
      "Iteration 45470 => Loss: 6.69862919196511974462\n",
      "Iteration 45471 => Loss: 6.69862913115081148874\n",
      "Iteration 45472 => Loss: 6.69862907034470822509\n",
      "Iteration 45473 => Loss: 6.69862900954678686105\n",
      "Iteration 45474 => Loss: 6.69862894875706338382\n",
      "Iteration 45475 => Loss: 6.69862888797553335252\n",
      "Iteration 45476 => Loss: 6.69862882720218433263\n",
      "Iteration 45477 => Loss: 6.69862876643703764046\n",
      "Iteration 45478 => Loss: 6.69862870568007462424\n",
      "Iteration 45479 => Loss: 6.69862864493129084309\n",
      "Iteration 45480 => Loss: 6.69862858419069961968\n",
      "Iteration 45481 => Loss: 6.69862852345829562495\n",
      "Iteration 45482 => Loss: 6.69862846273406731257\n",
      "Iteration 45483 => Loss: 6.69862840201802800522\n",
      "Iteration 45484 => Loss: 6.69862834131016793293\n",
      "Iteration 45485 => Loss: 6.69862828061048976025\n",
      "Iteration 45486 => Loss: 6.69862821991898460539\n",
      "Iteration 45487 => Loss: 6.69862815923566579102\n",
      "Iteration 45488 => Loss: 6.69862809856052798807\n",
      "Iteration 45489 => Loss: 6.69862803789355876205\n",
      "Iteration 45490 => Loss: 6.69862797723475988931\n",
      "Iteration 45491 => Loss: 6.69862791658414025164\n",
      "Iteration 45492 => Loss: 6.69862785594169185543\n",
      "Iteration 45493 => Loss: 6.69862779530741381251\n",
      "Iteration 45494 => Loss: 6.69862773468131145194\n",
      "Iteration 45495 => Loss: 6.69862767406337678011\n",
      "Iteration 45496 => Loss: 6.69862761345360890886\n",
      "Iteration 45497 => Loss: 6.69862755285200694999\n",
      "Iteration 45498 => Loss: 6.69862749225857179169\n",
      "Iteration 45499 => Loss: 6.69862743167330965122\n",
      "Iteration 45500 => Loss: 6.69862737109620187681\n",
      "Iteration 45501 => Loss: 6.69862731052725646208\n",
      "Iteration 45502 => Loss: 6.69862724996647251885\n",
      "Iteration 45503 => Loss: 6.69862718941385537619\n",
      "Iteration 45504 => Loss: 6.69862712886939171142\n",
      "Iteration 45505 => Loss: 6.69862706833309129451\n",
      "Iteration 45506 => Loss: 6.69862700780494080277\n",
      "Iteration 45507 => Loss: 6.69862694728495799978\n",
      "Iteration 45508 => Loss: 6.69862688677312068108\n",
      "Iteration 45509 => Loss: 6.69862682626944305753\n",
      "Iteration 45510 => Loss: 6.69862676577391535915\n",
      "Iteration 45511 => Loss: 6.69862670528653758595\n",
      "Iteration 45512 => Loss: 6.69862664480731417882\n",
      "Iteration 45513 => Loss: 6.69862658433623803234\n",
      "Iteration 45514 => Loss: 6.69862652387331003467\n",
      "Iteration 45515 => Loss: 6.69862646341853373855\n",
      "Iteration 45516 => Loss: 6.69862640297189848582\n",
      "Iteration 45517 => Loss: 6.69862634253341138191\n",
      "Iteration 45518 => Loss: 6.69862628210306443322\n",
      "Iteration 45519 => Loss: 6.69862622168086296881\n",
      "Iteration 45520 => Loss: 6.69862616126680521234\n",
      "Iteration 45521 => Loss: 6.69862610086088761108\n",
      "Iteration 45522 => Loss: 6.69862604046311016504\n",
      "Iteration 45523 => Loss: 6.69862598007346843332\n",
      "Iteration 45524 => Loss: 6.69862591969196685682\n",
      "Iteration 45525 => Loss: 6.69862585931859921828\n",
      "Iteration 45526 => Loss: 6.69862579895337084679\n",
      "Iteration 45527 => Loss: 6.69862573859627463690\n",
      "Iteration 45528 => Loss: 6.69862567824730970045\n",
      "Iteration 45529 => Loss: 6.69862561790647692561\n",
      "Iteration 45530 => Loss: 6.69862555757377986509\n",
      "Iteration 45531 => Loss: 6.69862549724921052530\n",
      "Iteration 45532 => Loss: 6.69862543693276890622\n",
      "Iteration 45533 => Loss: 6.69862537662445145514\n",
      "Iteration 45534 => Loss: 6.69862531632426616568\n",
      "Iteration 45535 => Loss: 6.69862525603220593240\n",
      "Iteration 45536 => Loss: 6.69862519574826364988\n",
      "Iteration 45537 => Loss: 6.69862513547244997625\n",
      "Iteration 45538 => Loss: 6.69862507520476135880\n",
      "Iteration 45539 => Loss: 6.69862501494519069212\n",
      "Iteration 45540 => Loss: 6.69862495469373353529\n",
      "Iteration 45541 => Loss: 6.69862489445040232283\n",
      "Iteration 45542 => Loss: 6.69862483421519083748\n",
      "Iteration 45543 => Loss: 6.69862477398809197382\n",
      "Iteration 45544 => Loss: 6.69862471376910928456\n",
      "Iteration 45545 => Loss: 6.69862465355823744062\n",
      "Iteration 45546 => Loss: 6.69862459335548798833\n",
      "Iteration 45547 => Loss: 6.69862453316084405230\n",
      "Iteration 45548 => Loss: 6.69862447297431451432\n",
      "Iteration 45549 => Loss: 6.69862441279589404530\n",
      "Iteration 45550 => Loss: 6.69862435262558264526\n",
      "Iteration 45551 => Loss: 6.69862429246337498512\n",
      "Iteration 45552 => Loss: 6.69862423230928349938\n",
      "Iteration 45553 => Loss: 6.69862417216329131264\n",
      "Iteration 45554 => Loss: 6.69862411202540464217\n",
      "Iteration 45555 => Loss: 6.69862405189562437613\n",
      "Iteration 45556 => Loss: 6.69862399177394518546\n",
      "Iteration 45557 => Loss: 6.69862393166036085290\n",
      "Iteration 45558 => Loss: 6.69862387155488470114\n",
      "Iteration 45559 => Loss: 6.69862381145750429567\n",
      "Iteration 45560 => Loss: 6.69862375136822851829\n",
      "Iteration 45561 => Loss: 6.69862369128703516452\n",
      "Iteration 45562 => Loss: 6.69862363121395354426\n",
      "Iteration 45563 => Loss: 6.69862357114895878851\n",
      "Iteration 45564 => Loss: 6.69862351109206066724\n",
      "Iteration 45565 => Loss: 6.69862345104325029865\n",
      "Iteration 45566 => Loss: 6.69862339100253745272\n",
      "Iteration 45567 => Loss: 6.69862333096990880676\n",
      "Iteration 45568 => Loss: 6.69862327094537768346\n",
      "Iteration 45569 => Loss: 6.69862321092892631924\n",
      "Iteration 45570 => Loss: 6.69862315092056714860\n",
      "Iteration 45571 => Loss: 6.69862309092029306612\n",
      "Iteration 45572 => Loss: 6.69862303092810407179\n",
      "Iteration 45573 => Loss: 6.69862297094400016562\n",
      "Iteration 45574 => Loss: 6.69862291096798045942\n",
      "Iteration 45575 => Loss: 6.69862285100004317684\n",
      "Iteration 45576 => Loss: 6.69862279104017854792\n",
      "Iteration 45577 => Loss: 6.69862273108839545444\n",
      "Iteration 45578 => Loss: 6.69862267114469656093\n",
      "Iteration 45579 => Loss: 6.69862261120907476197\n",
      "Iteration 45580 => Loss: 6.69862255128152916939\n",
      "Iteration 45581 => Loss: 6.69862249136205356592\n",
      "Iteration 45582 => Loss: 6.69862243145065328065\n",
      "Iteration 45583 => Loss: 6.69862237154732920175\n",
      "Iteration 45584 => Loss: 6.69862231165207422379\n",
      "Iteration 45585 => Loss: 6.69862225176489367584\n",
      "Iteration 45586 => Loss: 6.69862219188578045248\n",
      "Iteration 45587 => Loss: 6.69862213201473988278\n",
      "Iteration 45588 => Loss: 6.69862207215176308495\n",
      "Iteration 45589 => Loss: 6.69862201229685094717\n",
      "Iteration 45590 => Loss: 6.69862195245000791033\n",
      "Iteration 45591 => Loss: 6.69862189261123042172\n",
      "Iteration 45592 => Loss: 6.69862183278050959956\n",
      "Iteration 45593 => Loss: 6.69862177295785876652\n",
      "Iteration 45594 => Loss: 6.69862171314326459992\n",
      "Iteration 45595 => Loss: 6.69862165333673154066\n",
      "Iteration 45596 => Loss: 6.69862159353825514785\n",
      "Iteration 45597 => Loss: 6.69862153374783986237\n",
      "Iteration 45598 => Loss: 6.69862147396548213152\n",
      "Iteration 45599 => Loss: 6.69862141419117396168\n",
      "Iteration 45600 => Loss: 6.69862135442492512283\n",
      "Iteration 45601 => Loss: 6.69862129466673295042\n",
      "Iteration 45602 => Loss: 6.69862123491659566810\n",
      "Iteration 45603 => Loss: 6.69862117517449373594\n",
      "Iteration 45604 => Loss: 6.69862111544044935840\n",
      "Iteration 45605 => Loss: 6.69862105571446342367\n",
      "Iteration 45606 => Loss: 6.69862099599651639181\n",
      "Iteration 45607 => Loss: 6.69862093628661359190\n",
      "Iteration 45608 => Loss: 6.69862087658476657026\n",
      "Iteration 45609 => Loss: 6.69862081689096378057\n",
      "Iteration 45610 => Loss: 6.69862075720519900557\n",
      "Iteration 45611 => Loss: 6.69862069752747579798\n",
      "Iteration 45612 => Loss: 6.69862063785780392777\n",
      "Iteration 45613 => Loss: 6.69862057819615852594\n",
      "Iteration 45614 => Loss: 6.69862051854256268513\n",
      "Iteration 45615 => Loss: 6.69862045889700130630\n",
      "Iteration 45616 => Loss: 6.69862039925947438945\n",
      "Iteration 45617 => Loss: 6.69862033962999259273\n",
      "Iteration 45618 => Loss: 6.69862028000853815257\n",
      "Iteration 45619 => Loss: 6.69862022039511728622\n",
      "Iteration 45620 => Loss: 6.69862016078973265820\n",
      "Iteration 45621 => Loss: 6.69862010119237893946\n",
      "Iteration 45622 => Loss: 6.69862004160305790634\n",
      "Iteration 45623 => Loss: 6.69861998202176600614\n",
      "Iteration 45624 => Loss: 6.69861992244849879796\n",
      "Iteration 45625 => Loss: 6.69861986288326427541\n",
      "Iteration 45626 => Loss: 6.69861980332605089217\n",
      "Iteration 45627 => Loss: 6.69861974377686575366\n",
      "Iteration 45628 => Loss: 6.69861968423570708353\n",
      "Iteration 45629 => Loss: 6.69861962470256955271\n",
      "Iteration 45630 => Loss: 6.69861956517745849027\n",
      "Iteration 45631 => Loss: 6.69861950566035790899\n",
      "Iteration 45632 => Loss: 6.69861944615128468428\n",
      "Iteration 45633 => Loss: 6.69861938665022460526\n",
      "Iteration 45634 => Loss: 6.69861932715718744191\n",
      "Iteration 45635 => Loss: 6.69861926767216697698\n",
      "Iteration 45636 => Loss: 6.69861920819515965775\n",
      "Iteration 45637 => Loss: 6.69861914872617258965\n",
      "Iteration 45638 => Loss: 6.69861908926519156182\n",
      "Iteration 45639 => Loss: 6.69861902981222634423\n",
      "Iteration 45640 => Loss: 6.69861897036727160781\n",
      "Iteration 45641 => Loss: 6.69861891093032468802\n",
      "Iteration 45642 => Loss: 6.69861885150139801937\n",
      "Iteration 45643 => Loss: 6.69861879208046673284\n",
      "Iteration 45644 => Loss: 6.69861873266755125655\n",
      "Iteration 45645 => Loss: 6.69861867326263205058\n",
      "Iteration 45646 => Loss: 6.69861861386572954302\n",
      "Iteration 45647 => Loss: 6.69861855447681886488\n",
      "Iteration 45648 => Loss: 6.69861849509592222063\n",
      "Iteration 45649 => Loss: 6.69861843572301474126\n",
      "Iteration 45650 => Loss: 6.69861837635811685487\n",
      "Iteration 45651 => Loss: 6.69861831700121435063\n",
      "Iteration 45652 => Loss: 6.69861825765231255758\n",
      "Iteration 45653 => Loss: 6.69861819831141414028\n",
      "Iteration 45654 => Loss: 6.69861813897850044697\n",
      "Iteration 45655 => Loss: 6.69861807965358213579\n",
      "Iteration 45656 => Loss: 6.69861802033666275946\n",
      "Iteration 45657 => Loss: 6.69861796102773521255\n",
      "Iteration 45658 => Loss: 6.69861790172679771871\n",
      "Iteration 45659 => Loss: 6.69861784243385383064\n",
      "Iteration 45660 => Loss: 6.69861778314890532471\n",
      "Iteration 45661 => Loss: 6.69861772387193532552\n",
      "Iteration 45662 => Loss: 6.69861766460295271486\n",
      "Iteration 45663 => Loss: 6.69861760534196726269\n",
      "Iteration 45664 => Loss: 6.69861754608895587637\n",
      "Iteration 45665 => Loss: 6.69861748684394076037\n",
      "Iteration 45666 => Loss: 6.69861742760689526932\n",
      "Iteration 45667 => Loss: 6.69861736837784427223\n",
      "Iteration 45668 => Loss: 6.69861730915676201192\n",
      "Iteration 45669 => Loss: 6.69861724994366891650\n",
      "Iteration 45670 => Loss: 6.69861719073855610418\n",
      "Iteration 45671 => Loss: 6.69861713154140936410\n",
      "Iteration 45672 => Loss: 6.69861707235225090074\n",
      "Iteration 45673 => Loss: 6.69861701317106472686\n",
      "Iteration 45674 => Loss: 6.69861695399785261884\n",
      "Iteration 45675 => Loss: 6.69861689483261280031\n",
      "Iteration 45676 => Loss: 6.69861683567535148853\n",
      "Iteration 45677 => Loss: 6.69861677652605358446\n",
      "Iteration 45678 => Loss: 6.69861671738473329896\n",
      "Iteration 45679 => Loss: 6.69861665825137819752\n",
      "Iteration 45680 => Loss: 6.69861659912599449740\n",
      "Iteration 45681 => Loss: 6.69861654000856976410\n",
      "Iteration 45682 => Loss: 6.69861648089912087300\n",
      "Iteration 45683 => Loss: 6.69861642179763538962\n",
      "Iteration 45684 => Loss: 6.69861636270410620853\n",
      "Iteration 45685 => Loss: 6.69861630361854665239\n",
      "Iteration 45686 => Loss: 6.69861624454094783943\n",
      "Iteration 45687 => Loss: 6.69861618547130976964\n",
      "Iteration 45688 => Loss: 6.69861612640962711396\n",
      "Iteration 45689 => Loss: 6.69861606735590608963\n",
      "Iteration 45690 => Loss: 6.69861600831014580848\n",
      "Iteration 45691 => Loss: 6.69861594927233738872\n",
      "Iteration 45692 => Loss: 6.69861589024249060031\n",
      "Iteration 45693 => Loss: 6.69861583122059389694\n",
      "Iteration 45694 => Loss: 6.69861577220664727861\n",
      "Iteration 45695 => Loss: 6.69861571320065340984\n",
      "Iteration 45696 => Loss: 6.69861565420261673154\n",
      "Iteration 45697 => Loss: 6.69861559521251948013\n",
      "Iteration 45698 => Loss: 6.69861553623038030736\n",
      "Iteration 45699 => Loss: 6.69861547725619121962\n",
      "Iteration 45700 => Loss: 6.69861541828994599967\n",
      "Iteration 45701 => Loss: 6.69861535933164198298\n",
      "Iteration 45702 => Loss: 6.69861530038128627496\n",
      "Iteration 45703 => Loss: 6.69861524143887798743\n",
      "Iteration 45704 => Loss: 6.69861518250440113320\n",
      "Iteration 45705 => Loss: 6.69861512357787436400\n",
      "Iteration 45706 => Loss: 6.69861506465928346898\n",
      "Iteration 45707 => Loss: 6.69861500574863555357\n",
      "Iteration 45708 => Loss: 6.69861494684592173599\n",
      "Iteration 45709 => Loss: 6.69861488795114912165\n",
      "Iteration 45710 => Loss: 6.69861482906430971695\n",
      "Iteration 45711 => Loss: 6.69861477018541240369\n",
      "Iteration 45712 => Loss: 6.69861471131443853011\n",
      "Iteration 45713 => Loss: 6.69861465245140585978\n",
      "Iteration 45714 => Loss: 6.69861459359630107002\n",
      "Iteration 45715 => Loss: 6.69861453474912593720\n",
      "Iteration 45716 => Loss: 6.69861447590987957312\n",
      "Iteration 45717 => Loss: 6.69861441707856908323\n",
      "Iteration 45718 => Loss: 6.69861435825517670395\n",
      "Iteration 45719 => Loss: 6.69861429943971398160\n",
      "Iteration 45720 => Loss: 6.69861424063218091618\n",
      "Iteration 45721 => Loss: 6.69861418183257040226\n",
      "Iteration 45722 => Loss: 6.69861412304088066350\n",
      "Iteration 45723 => Loss: 6.69861406425711525259\n",
      "Iteration 45724 => Loss: 6.69861400548126972865\n",
      "Iteration 45725 => Loss: 6.69861394671334231532\n",
      "Iteration 45726 => Loss: 6.69861388795333745350\n",
      "Iteration 45727 => Loss: 6.69861382920124448503\n",
      "Iteration 45728 => Loss: 6.69861377045707406808\n",
      "Iteration 45729 => Loss: 6.69861371172082264991\n",
      "Iteration 45730 => Loss: 6.69861365299247957239\n",
      "Iteration 45731 => Loss: 6.69861359427205016459\n",
      "Iteration 45732 => Loss: 6.69861353555953886740\n",
      "Iteration 45733 => Loss: 6.69861347685493324633\n",
      "Iteration 45734 => Loss: 6.69861341815824573587\n",
      "Iteration 45735 => Loss: 6.69861335946946034881\n",
      "Iteration 45736 => Loss: 6.69861330078858863146\n",
      "Iteration 45737 => Loss: 6.69861324211561459663\n",
      "Iteration 45738 => Loss: 6.69861318345055511969\n",
      "Iteration 45739 => Loss: 6.69861312479339954251\n",
      "Iteration 45740 => Loss: 6.69861306614414342420\n",
      "Iteration 45741 => Loss: 6.69861300750280008742\n",
      "Iteration 45742 => Loss: 6.69861294886934821591\n",
      "Iteration 45743 => Loss: 6.69861289024380202051\n",
      "Iteration 45744 => Loss: 6.69861283162615261944\n",
      "Iteration 45745 => Loss: 6.69861277301641155901\n",
      "Iteration 45746 => Loss: 6.69861271441455663478\n",
      "Iteration 45747 => Loss: 6.69861265582060205759\n",
      "Iteration 45748 => Loss: 6.69861259723454605108\n",
      "Iteration 45749 => Loss: 6.69861253865638328620\n",
      "Iteration 45750 => Loss: 6.69861248008611198657\n",
      "Iteration 45751 => Loss: 6.69861242152373304037\n",
      "Iteration 45752 => Loss: 6.69861236296924911215\n",
      "Iteration 45753 => Loss: 6.69861230442265043195\n",
      "Iteration 45754 => Loss: 6.69861224588394588153\n",
      "Iteration 45755 => Loss: 6.69861218735312391459\n",
      "Iteration 45756 => Loss: 6.69861212883018719566\n",
      "Iteration 45757 => Loss: 6.69861207031514105381\n",
      "Iteration 45758 => Loss: 6.69861201180799081811\n",
      "Iteration 45759 => Loss: 6.69861195330870451414\n",
      "Iteration 45760 => Loss: 6.69861189481731322815\n",
      "Iteration 45761 => Loss: 6.69861183633379919655\n",
      "Iteration 45762 => Loss: 6.69861177785816419572\n",
      "Iteration 45763 => Loss: 6.69861171939041355472\n",
      "Iteration 45764 => Loss: 6.69861166093053572723\n",
      "Iteration 45765 => Loss: 6.69861160247854403593\n",
      "Iteration 45766 => Loss: 6.69861154403442338179\n",
      "Iteration 45767 => Loss: 6.69861148559817820569\n",
      "Iteration 45768 => Loss: 6.69861142716980406675\n",
      "Iteration 45769 => Loss: 6.69861136874931073493\n",
      "Iteration 45770 => Loss: 6.69861131033668311119\n",
      "Iteration 45771 => Loss: 6.69861125193193096550\n",
      "Iteration 45772 => Loss: 6.69861119353504896878\n",
      "Iteration 45773 => Loss: 6.69861113514603534469\n",
      "Iteration 45774 => Loss: 6.69861107676488209961\n",
      "Iteration 45775 => Loss: 6.69861101839159722715\n",
      "Iteration 45776 => Loss: 6.69861096002618960910\n",
      "Iteration 45777 => Loss: 6.69861090166863615281\n",
      "Iteration 45778 => Loss: 6.69861084331895106914\n",
      "Iteration 45779 => Loss: 6.69861078497712814084\n",
      "Iteration 45780 => Loss: 6.69861072664316559155\n",
      "Iteration 45781 => Loss: 6.69861066831706430946\n",
      "Iteration 45782 => Loss: 6.69861060999882518274\n",
      "Iteration 45783 => Loss: 6.69861055168844199414\n",
      "Iteration 45784 => Loss: 6.69861049338591385549\n",
      "Iteration 45785 => Loss: 6.69861043509124165496\n",
      "Iteration 45786 => Loss: 6.69861037680442716891\n",
      "Iteration 45787 => Loss: 6.69861031852546773280\n",
      "Iteration 45788 => Loss: 6.69861026025435712938\n",
      "Iteration 45789 => Loss: 6.69861020199110601681\n",
      "Iteration 45790 => Loss: 6.69861014373570373692\n",
      "Iteration 45791 => Loss: 6.69861008548814407249\n",
      "Iteration 45792 => Loss: 6.69861002724843945799\n",
      "Iteration 45793 => Loss: 6.69860996901658189984\n",
      "Iteration 45794 => Loss: 6.69860991079256784531\n",
      "Iteration 45795 => Loss: 6.69860985257640439983\n",
      "Iteration 45796 => Loss: 6.69860979436808356979\n",
      "Iteration 45797 => Loss: 6.69860973616760979610\n",
      "Iteration 45798 => Loss: 6.69860967797497242060\n",
      "Iteration 45799 => Loss: 6.69860961979017321966\n",
      "Iteration 45800 => Loss: 6.69860956161322196323\n",
      "Iteration 45801 => Loss: 6.69860950344410710500\n",
      "Iteration 45802 => Loss: 6.69860944528283663857\n",
      "Iteration 45803 => Loss: 6.69860938712939990580\n",
      "Iteration 45804 => Loss: 6.69860932898380045941\n",
      "Iteration 45805 => Loss: 6.69860927084603030579\n",
      "Iteration 45806 => Loss: 6.69860921271609566219\n",
      "Iteration 45807 => Loss: 6.69860915459399830496\n",
      "Iteration 45808 => Loss: 6.69860909647973112868\n",
      "Iteration 45809 => Loss: 6.69860903837329502153\n",
      "Iteration 45810 => Loss: 6.69860898027468376625\n",
      "Iteration 45811 => Loss: 6.69860892218390802100\n",
      "Iteration 45812 => Loss: 6.69860886410095535126\n",
      "Iteration 45813 => Loss: 6.69860880602583463883\n",
      "Iteration 45814 => Loss: 6.69860874795853433739\n",
      "Iteration 45815 => Loss: 6.69860868989906155235\n",
      "Iteration 45816 => Loss: 6.69860863184741539555\n",
      "Iteration 45817 => Loss: 6.69860857380358432067\n",
      "Iteration 45818 => Loss: 6.69860851576757898584\n",
      "Iteration 45819 => Loss: 6.69860845773939139747\n",
      "Iteration 45820 => Loss: 6.69860839971902688461\n",
      "Iteration 45821 => Loss: 6.69860834170647834185\n",
      "Iteration 45822 => Loss: 6.69860828370174399282\n",
      "Iteration 45823 => Loss: 6.69860822570483360749\n",
      "Iteration 45824 => Loss: 6.69860816771573475137\n",
      "Iteration 45825 => Loss: 6.69860810973444475991\n",
      "Iteration 45826 => Loss: 6.69860805176097251490\n",
      "Iteration 45827 => Loss: 6.69860799379531091091\n",
      "Iteration 45828 => Loss: 6.69860793583746438884\n",
      "Iteration 45829 => Loss: 6.69860787788741696147\n",
      "Iteration 45830 => Loss: 6.69860781994518994509\n",
      "Iteration 45831 => Loss: 6.69860776201076735248\n",
      "Iteration 45832 => Loss: 6.69860770408414829546\n",
      "Iteration 45833 => Loss: 6.69860764616533987947\n",
      "Iteration 45834 => Loss: 6.69860758825433411090\n",
      "Iteration 45835 => Loss: 6.69860753035112654885\n",
      "Iteration 45836 => Loss: 6.69860747245572873965\n",
      "Iteration 45837 => Loss: 6.69860741456813002515\n",
      "Iteration 45838 => Loss: 6.69860735668832951717\n",
      "Iteration 45839 => Loss: 6.69860729881633254479\n",
      "Iteration 45840 => Loss: 6.69860724095212844986\n",
      "Iteration 45841 => Loss: 6.69860718309572344964\n",
      "Iteration 45842 => Loss: 6.69860712524711221505\n",
      "Iteration 45843 => Loss: 6.69860706740630007516\n",
      "Iteration 45844 => Loss: 6.69860700957328614180\n",
      "Iteration 45845 => Loss: 6.69860695174805620411\n",
      "Iteration 45846 => Loss: 6.69860689393062180841\n",
      "Iteration 45847 => Loss: 6.69860683612097851380\n",
      "Iteration 45848 => Loss: 6.69860677831912720848\n",
      "Iteration 45849 => Loss: 6.69860672052506256335\n",
      "Iteration 45850 => Loss: 6.69860666273878191390\n",
      "Iteration 45851 => Loss: 6.69860660496029503008\n",
      "Iteration 45852 => Loss: 6.69860654718959214193\n",
      "Iteration 45853 => Loss: 6.69860648942666969674\n",
      "Iteration 45854 => Loss: 6.69860643167153124722\n",
      "Iteration 45855 => Loss: 6.69860637392418389879\n",
      "Iteration 45856 => Loss: 6.69860631618460899972\n",
      "Iteration 45857 => Loss: 6.69860625845281365542\n",
      "Iteration 45858 => Loss: 6.69860620072880852405\n",
      "Iteration 45859 => Loss: 6.69860614301257317749\n",
      "Iteration 45860 => Loss: 6.69860608530411472117\n",
      "Iteration 45861 => Loss: 6.69860602760343404327\n",
      "Iteration 45862 => Loss: 6.69860596991053114380\n",
      "Iteration 45863 => Loss: 6.69860591222539802914\n",
      "Iteration 45864 => Loss: 6.69860585454803825201\n",
      "Iteration 45865 => Loss: 6.69860579687845447694\n",
      "Iteration 45866 => Loss: 6.69860573921664403940\n",
      "Iteration 45867 => Loss: 6.69860568156259272854\n",
      "Iteration 45868 => Loss: 6.69860562391631564338\n",
      "Iteration 45869 => Loss: 6.69860556627780567851\n",
      "Iteration 45870 => Loss: 6.69860550864706105756\n",
      "Iteration 45871 => Loss: 6.69860545102408977414\n",
      "Iteration 45872 => Loss: 6.69860539340887761739\n",
      "Iteration 45873 => Loss: 6.69860533580143080457\n",
      "Iteration 45874 => Loss: 6.69860527820174489477\n",
      "Iteration 45875 => Loss: 6.69860522060982344072\n",
      "Iteration 45876 => Loss: 6.69860516302565578428\n",
      "Iteration 45877 => Loss: 6.69860510544925435994\n",
      "Iteration 45878 => Loss: 6.69860504788060762138\n",
      "Iteration 45879 => Loss: 6.69860499031972178585\n",
      "Iteration 45880 => Loss: 6.69860493276659152428\n",
      "Iteration 45881 => Loss: 6.69860487522121328396\n",
      "Iteration 45882 => Loss: 6.69860481768359239396\n",
      "Iteration 45883 => Loss: 6.69860476015372174885\n",
      "Iteration 45884 => Loss: 6.69860470263160756588\n",
      "Iteration 45885 => Loss: 6.69860464511724185144\n",
      "Iteration 45886 => Loss: 6.69860458761063082278\n",
      "Iteration 45887 => Loss: 6.69860453011176115723\n",
      "Iteration 45888 => Loss: 6.69860447262064262475\n",
      "Iteration 45889 => Loss: 6.69860441513726900808\n",
      "Iteration 45890 => Loss: 6.69860435766164119542\n",
      "Iteration 45891 => Loss: 6.69860430019376451583\n",
      "Iteration 45892 => Loss: 6.69860424273362742298\n",
      "Iteration 45893 => Loss: 6.69860418528123524595\n",
      "Iteration 45894 => Loss: 6.69860412783657732660\n",
      "Iteration 45895 => Loss: 6.69860407039966698761\n",
      "Iteration 45896 => Loss: 6.69860401297049534719\n",
      "Iteration 45897 => Loss: 6.69860395554906240534\n",
      "Iteration 45898 => Loss: 6.69860389813536905024\n",
      "Iteration 45899 => Loss: 6.69860384072940906464\n",
      "Iteration 45900 => Loss: 6.69860378333118333671\n",
      "Iteration 45901 => Loss: 6.69860372594069186647\n",
      "Iteration 45902 => Loss: 6.69860366855793465390\n",
      "Iteration 45903 => Loss: 6.69860361118291436355\n",
      "Iteration 45904 => Loss: 6.69860355381561944910\n",
      "Iteration 45905 => Loss: 6.69860349645605879232\n",
      "Iteration 45906 => Loss: 6.69860343910422706415\n",
      "Iteration 45907 => Loss: 6.69860338176011982370\n",
      "Iteration 45908 => Loss: 6.69860332442373707096\n",
      "Iteration 45909 => Loss: 6.69860326709509035226\n",
      "Iteration 45910 => Loss: 6.69860320977416012767\n",
      "Iteration 45911 => Loss: 6.69860315246096060804\n",
      "Iteration 45912 => Loss: 6.69860309515548202342\n",
      "Iteration 45913 => Loss: 6.69860303785772082108\n",
      "Iteration 45914 => Loss: 6.69860298056768677100\n",
      "Iteration 45915 => Loss: 6.69860292328536743867\n",
      "Iteration 45916 => Loss: 6.69860286601076992952\n",
      "Iteration 45917 => Loss: 6.69860280874389157901\n",
      "Iteration 45918 => Loss: 6.69860275148472705808\n",
      "Iteration 45919 => Loss: 6.69860269423327547855\n",
      "Iteration 45920 => Loss: 6.69860263698954483402\n",
      "Iteration 45921 => Loss: 6.69860257975352357818\n",
      "Iteration 45922 => Loss: 6.69860252252521704008\n",
      "Iteration 45923 => Loss: 6.69860246530461633796\n",
      "Iteration 45924 => Loss: 6.69860240809173834720\n",
      "Iteration 45925 => Loss: 6.69860235088655997515\n",
      "Iteration 45926 => Loss: 6.69860229368909187997\n",
      "Iteration 45927 => Loss: 6.69860223649933406165\n",
      "Iteration 45928 => Loss: 6.69860217931727852658\n",
      "Iteration 45929 => Loss: 6.69860212214292971566\n",
      "Iteration 45930 => Loss: 6.69860206497628141165\n",
      "Iteration 45931 => Loss: 6.69860200781734604902\n",
      "Iteration 45932 => Loss: 6.69860195066610586423\n",
      "Iteration 45933 => Loss: 6.69860189352256885087\n",
      "Iteration 45934 => Loss: 6.69860183638673145623\n",
      "Iteration 45935 => Loss: 6.69860177925858923942\n",
      "Iteration 45936 => Loss: 6.69860172213815374676\n",
      "Iteration 45937 => Loss: 6.69860166502540455014\n",
      "Iteration 45938 => Loss: 6.69860160792036296584\n",
      "Iteration 45939 => Loss: 6.69860155082300412488\n",
      "Iteration 45940 => Loss: 6.69860149373334756717\n",
      "Iteration 45941 => Loss: 6.69860143665137730551\n",
      "Iteration 45942 => Loss: 6.69860137957710488621\n",
      "Iteration 45943 => Loss: 6.69860132251052498020\n",
      "Iteration 45944 => Loss: 6.69860126545162959388\n",
      "Iteration 45945 => Loss: 6.69860120840042672086\n",
      "Iteration 45946 => Loss: 6.69860115135690481480\n",
      "Iteration 45947 => Loss: 6.69860109432107275751\n",
      "Iteration 45948 => Loss: 6.69860103729292610808\n",
      "Iteration 45949 => Loss: 6.69860098027246753105\n",
      "Iteration 45950 => Loss: 6.69860092325969080918\n",
      "Iteration 45951 => Loss: 6.69860086625459327792\n",
      "Iteration 45952 => Loss: 6.69860080925718381906\n",
      "Iteration 45953 => Loss: 6.69860075226744644539\n",
      "Iteration 45954 => Loss: 6.69860069528539803230\n",
      "Iteration 45955 => Loss: 6.69860063831101992804\n",
      "Iteration 45956 => Loss: 6.69860058134432279076\n",
      "Iteration 45957 => Loss: 6.69860052438531017316\n",
      "Iteration 45958 => Loss: 6.69860046743395365354\n",
      "Iteration 45959 => Loss: 6.69860041049028254179\n",
      "Iteration 45960 => Loss: 6.69860035355428529158\n",
      "Iteration 45961 => Loss: 6.69860029662595835021\n",
      "Iteration 45962 => Loss: 6.69860023970529994131\n",
      "Iteration 45963 => Loss: 6.69860018279231095306\n",
      "Iteration 45964 => Loss: 6.69860012588699849090\n",
      "Iteration 45965 => Loss: 6.69860006898935367303\n",
      "Iteration 45966 => Loss: 6.69860001209936761768\n",
      "Iteration 45967 => Loss: 6.69859995521705720023\n",
      "Iteration 45968 => Loss: 6.69859989834240288076\n",
      "Iteration 45969 => Loss: 6.69859984147541354105\n",
      "Iteration 45970 => Loss: 6.69859978461608474021\n",
      "Iteration 45971 => Loss: 6.69859972776442536002\n",
      "Iteration 45972 => Loss: 6.69859967092042829506\n",
      "Iteration 45973 => Loss: 6.69859961408408199901\n",
      "Iteration 45974 => Loss: 6.69859955725540068272\n",
      "Iteration 45975 => Loss: 6.69859950043437191169\n",
      "Iteration 45976 => Loss: 6.69859944362100456772\n",
      "Iteration 45977 => Loss: 6.69859938681529332172\n",
      "Iteration 45978 => Loss: 6.69859933001723195645\n",
      "Iteration 45979 => Loss: 6.69859927322682846551\n",
      "Iteration 45980 => Loss: 6.69859921644407840802\n",
      "Iteration 45981 => Loss: 6.69859915966897467854\n",
      "Iteration 45982 => Loss: 6.69859910290152882339\n",
      "Iteration 45983 => Loss: 6.69859904614172041448\n",
      "Iteration 45984 => Loss: 6.69859898938957165626\n",
      "Iteration 45985 => Loss: 6.69859893264506567334\n",
      "Iteration 45986 => Loss: 6.69859887590820157754\n",
      "Iteration 45987 => Loss: 6.69859881917899091519\n",
      "Iteration 45988 => Loss: 6.69859876245742036360\n",
      "Iteration 45989 => Loss: 6.69859870574349791639\n",
      "Iteration 45990 => Loss: 6.69859864903720847451\n",
      "Iteration 45991 => Loss: 6.69859859233857157790\n",
      "Iteration 45992 => Loss: 6.69859853564756235755\n",
      "Iteration 45993 => Loss: 6.69859847896420035340\n",
      "Iteration 45994 => Loss: 6.69859842228847313095\n",
      "Iteration 45995 => Loss: 6.69859836562038335472\n",
      "Iteration 45996 => Loss: 6.69859830895993191291\n",
      "Iteration 45997 => Loss: 6.69859825230711791733\n",
      "Iteration 45998 => Loss: 6.69859819566193515072\n",
      "Iteration 45999 => Loss: 6.69859813902438538946\n",
      "Iteration 46000 => Loss: 6.69859808239446241629\n",
      "Iteration 46001 => Loss: 6.69859802577217156028\n",
      "Iteration 46002 => Loss: 6.69859796915751992685\n",
      "Iteration 46003 => Loss: 6.69859791255048797609\n",
      "Iteration 46004 => Loss: 6.69859785595108370160\n",
      "Iteration 46005 => Loss: 6.69859779935931420880\n",
      "Iteration 46006 => Loss: 6.69859774277516173413\n",
      "Iteration 46007 => Loss: 6.69859768619863782391\n",
      "Iteration 46008 => Loss: 6.69859762962973448452\n",
      "Iteration 46009 => Loss: 6.69859757306845882141\n",
      "Iteration 46010 => Loss: 6.69859751651480550549\n",
      "Iteration 46011 => Loss: 6.69859745996876920771\n",
      "Iteration 46012 => Loss: 6.69859740343035170440\n",
      "Iteration 46013 => Loss: 6.69859734689955210740\n",
      "Iteration 46014 => Loss: 6.69859729037637219307\n",
      "Iteration 46015 => Loss: 6.69859723386080840868\n",
      "Iteration 46016 => Loss: 6.69859717735286253060\n",
      "Iteration 46017 => Loss: 6.69859712085252922975\n",
      "Iteration 46018 => Loss: 6.69859706435980850614\n",
      "Iteration 46019 => Loss: 6.69859700787469947159\n",
      "Iteration 46020 => Loss: 6.69859695139720390245\n",
      "Iteration 46021 => Loss: 6.69859689492731558147\n",
      "Iteration 46022 => Loss: 6.69859683846504072591\n",
      "Iteration 46023 => Loss: 6.69859678201037311851\n",
      "Iteration 46024 => Loss: 6.69859672556331631199\n",
      "Iteration 46025 => Loss: 6.69859666912385787185\n",
      "Iteration 46026 => Loss: 6.69859661269200490352\n",
      "Iteration 46027 => Loss: 6.69859655626776184789\n",
      "Iteration 46028 => Loss: 6.69859649985112337589\n",
      "Iteration 46029 => Loss: 6.69859644344208238209\n",
      "Iteration 46030 => Loss: 6.69859638704063797832\n",
      "Iteration 46031 => Loss: 6.69859633064680437542\n",
      "Iteration 46032 => Loss: 6.69859627426056025712\n",
      "Iteration 46033 => Loss: 6.69859621788192338698\n",
      "Iteration 46034 => Loss: 6.69859616151087244873\n",
      "Iteration 46035 => Loss: 6.69859610514742431775\n",
      "Iteration 46036 => Loss: 6.69859604879157277679\n",
      "Iteration 46037 => Loss: 6.69859599244331427315\n",
      "Iteration 46038 => Loss: 6.69859593610264969499\n",
      "Iteration 46039 => Loss: 6.69859587976957282507\n",
      "Iteration 46040 => Loss: 6.69859582344408810428\n",
      "Iteration 46041 => Loss: 6.69859576712619375627\n",
      "Iteration 46042 => Loss: 6.69859571081588622832\n",
      "Iteration 46043 => Loss: 6.69859565451316996132\n",
      "Iteration 46044 => Loss: 6.69859559821804229074\n",
      "Iteration 46045 => Loss: 6.69859554193049255844\n",
      "Iteration 46046 => Loss: 6.69859548565053763980\n",
      "Iteration 46047 => Loss: 6.69859542937816065944\n",
      "Iteration 46048 => Loss: 6.69859537311336605825\n",
      "Iteration 46049 => Loss: 6.69859531685615294805\n",
      "Iteration 46050 => Loss: 6.69859526060652044066\n",
      "Iteration 46051 => Loss: 6.69859520436447120062\n",
      "Iteration 46052 => Loss: 6.69859514812999545796\n",
      "Iteration 46053 => Loss: 6.69859509190310298266\n",
      "Iteration 46054 => Loss: 6.69859503568378134020\n",
      "Iteration 46055 => Loss: 6.69859497947203408330\n",
      "Iteration 46056 => Loss: 6.69859492326786831740\n",
      "Iteration 46057 => Loss: 6.69859486707127516070\n",
      "Iteration 46058 => Loss: 6.69859481088224928413\n",
      "Iteration 46059 => Loss: 6.69859475470079779313\n",
      "Iteration 46060 => Loss: 6.69859469852692157588\n",
      "Iteration 46061 => Loss: 6.69859464236060464515\n",
      "Iteration 46062 => Loss: 6.69859458620186387634\n",
      "Iteration 46063 => Loss: 6.69859453005069038767\n",
      "Iteration 46064 => Loss: 6.69859447390707263281\n",
      "Iteration 46065 => Loss: 6.69859441777103192805\n",
      "Iteration 46066 => Loss: 6.69859436164255672708\n",
      "Iteration 46067 => Loss: 6.69859430552163992445\n",
      "Iteration 46068 => Loss: 6.69859424940828329653\n",
      "Iteration 46069 => Loss: 6.69859419330248684332\n",
      "Iteration 46070 => Loss: 6.69859413720426388750\n",
      "Iteration 46071 => Loss: 6.69859408111358245463\n",
      "Iteration 46072 => Loss: 6.69859402503047007826\n",
      "Iteration 46073 => Loss: 6.69859396895491165935\n",
      "Iteration 46074 => Loss: 6.69859391288691430333\n",
      "Iteration 46075 => Loss: 6.69859385682645935844\n",
      "Iteration 46076 => Loss: 6.69859380077356725280\n",
      "Iteration 46077 => Loss: 6.69859374472823176916\n",
      "Iteration 46078 => Loss: 6.69859368869045024297\n",
      "Iteration 46079 => Loss: 6.69859363266020846339\n",
      "Iteration 46080 => Loss: 6.69859357663752597034\n",
      "Iteration 46081 => Loss: 6.69859352062239032932\n",
      "Iteration 46082 => Loss: 6.69859346461479798762\n",
      "Iteration 46083 => Loss: 6.69859340861475871520\n",
      "Iteration 46084 => Loss: 6.69859335262225652485\n",
      "Iteration 46085 => Loss: 6.69859329663730918014\n",
      "Iteration 46086 => Loss: 6.69859324065990069386\n",
      "Iteration 46087 => Loss: 6.69859318469003994778\n",
      "Iteration 46088 => Loss: 6.69859312872771628378\n",
      "Iteration 46089 => Loss: 6.69859307277293858363\n",
      "Iteration 46090 => Loss: 6.69859301682569796554\n",
      "Iteration 46091 => Loss: 6.69859296088599798225\n",
      "Iteration 46092 => Loss: 6.69859290495383064012\n",
      "Iteration 46093 => Loss: 6.69859284902920570914\n",
      "Iteration 46094 => Loss: 6.69859279311211341934\n",
      "Iteration 46095 => Loss: 6.69859273720255021800\n",
      "Iteration 46096 => Loss: 6.69859268130053298052\n",
      "Iteration 46097 => Loss: 6.69859262540603772607\n",
      "Iteration 46098 => Loss: 6.69859256951908133004\n",
      "Iteration 46099 => Loss: 6.69859251363965313431\n",
      "Iteration 46100 => Loss: 6.69859245776776113246\n",
      "Iteration 46101 => Loss: 6.69859240190338489640\n",
      "Iteration 46102 => Loss: 6.69859234604654218970\n",
      "Iteration 46103 => Loss: 6.69859229019722945964\n",
      "Iteration 46104 => Loss: 6.69859223435544137715\n",
      "Iteration 46105 => Loss: 6.69859217852117794223\n",
      "Iteration 46106 => Loss: 6.69859212269443826671\n",
      "Iteration 46107 => Loss: 6.69859206687522146240\n",
      "Iteration 46108 => Loss: 6.69859201106352397659\n",
      "Iteration 46109 => Loss: 6.69859195525934758564\n",
      "Iteration 46110 => Loss: 6.69859189946269406590\n",
      "Iteration 46111 => Loss: 6.69859184367355275924\n",
      "Iteration 46112 => Loss: 6.69859178789193965287\n",
      "Iteration 46113 => Loss: 6.69859173211783520685\n",
      "Iteration 46114 => Loss: 6.69859167635124475026\n",
      "Iteration 46115 => Loss: 6.69859162059217272400\n",
      "Iteration 46116 => Loss: 6.69859156484060935810\n",
      "Iteration 46117 => Loss: 6.69859150909656531070\n",
      "Iteration 46118 => Loss: 6.69859145336002637094\n",
      "Iteration 46119 => Loss: 6.69859139763100319698\n",
      "Iteration 46120 => Loss: 6.69859134190948779519\n",
      "Iteration 46121 => Loss: 6.69859128619547927741\n",
      "Iteration 46122 => Loss: 6.69859123048897853181\n",
      "Iteration 46123 => Loss: 6.69859117478998555839\n",
      "Iteration 46124 => Loss: 6.69859111909849858080\n",
      "Iteration 46125 => Loss: 6.69859106341451582267\n",
      "Iteration 46126 => Loss: 6.69859100773803906037\n",
      "Iteration 46127 => Loss: 6.69859095206905941211\n",
      "Iteration 46128 => Loss: 6.69859089640758575968\n",
      "Iteration 46129 => Loss: 6.69859084075360744492\n",
      "Iteration 46130 => Loss: 6.69859078510713068511\n",
      "Iteration 46131 => Loss: 6.69859072946815103933\n",
      "Iteration 46132 => Loss: 6.69859067383667916573\n",
      "Iteration 46133 => Loss: 6.69859061821268753079\n",
      "Iteration 46134 => Loss: 6.69859056259619922713\n",
      "Iteration 46135 => Loss: 6.69859050698721159023\n",
      "Iteration 46136 => Loss: 6.69859045138570685651\n",
      "Iteration 46137 => Loss: 6.69859039579169390777\n",
      "Iteration 46138 => Loss: 6.69859034020517807306\n",
      "Iteration 46139 => Loss: 6.69859028462614869426\n",
      "Iteration 46140 => Loss: 6.69859022905461465314\n",
      "Iteration 46141 => Loss: 6.69859017349056440338\n",
      "Iteration 46142 => Loss: 6.69859011793400149770\n",
      "Iteration 46143 => Loss: 6.69859006238492415974\n",
      "Iteration 46144 => Loss: 6.69859000684333238951\n",
      "Iteration 46145 => Loss: 6.69858995130922263428\n",
      "Iteration 46146 => Loss: 6.69858989578260377584\n",
      "Iteration 46147 => Loss: 6.69858984026346337970\n",
      "Iteration 46148 => Loss: 6.69858978475179878131\n",
      "Iteration 46149 => Loss: 6.69858972924762152701\n",
      "Iteration 46150 => Loss: 6.69858967375092007046\n",
      "Iteration 46151 => Loss: 6.69858961826169529985\n",
      "Iteration 46152 => Loss: 6.69858956277995343243\n",
      "Iteration 46153 => Loss: 6.69858950730568647458\n",
      "Iteration 46154 => Loss: 6.69858945183888820907\n",
      "Iteration 46155 => Loss: 6.69858939637957195856\n",
      "Iteration 46156 => Loss: 6.69858934092772351221\n",
      "Iteration 46157 => Loss: 6.69858928548334908726\n",
      "Iteration 46158 => Loss: 6.69858923004644690735\n",
      "Iteration 46159 => Loss: 6.69858917461700897888\n",
      "Iteration 46160 => Loss: 6.69858911919504684818\n",
      "Iteration 46161 => Loss: 6.69858906378054985709\n",
      "Iteration 46162 => Loss: 6.69858900837352067015\n",
      "Iteration 46163 => Loss: 6.69858895297395395829\n",
      "Iteration 46164 => Loss: 6.69858889758186126784\n",
      "Iteration 46165 => Loss: 6.69858884219722483522\n",
      "Iteration 46166 => Loss: 6.69858878682005620675\n",
      "Iteration 46167 => Loss: 6.69858873145034827701\n",
      "Iteration 46168 => Loss: 6.69858867608809926963\n",
      "Iteration 46169 => Loss: 6.69858862073331451370\n",
      "Iteration 46170 => Loss: 6.69858856538598068653\n",
      "Iteration 46171 => Loss: 6.69858851004610933444\n",
      "Iteration 46172 => Loss: 6.69858845471369956925\n",
      "Iteration 46173 => Loss: 6.69858839938874517372\n",
      "Iteration 46174 => Loss: 6.69858834407123637789\n",
      "Iteration 46175 => Loss: 6.69858828876119183349\n",
      "Iteration 46176 => Loss: 6.69858823345859821785\n",
      "Iteration 46177 => Loss: 6.69858817816345730733\n",
      "Iteration 46178 => Loss: 6.69858812287576199651\n",
      "Iteration 46179 => Loss: 6.69858806759552383170\n",
      "Iteration 46180 => Loss: 6.69858801232272593751\n",
      "Iteration 46181 => Loss: 6.69858795705738785387\n",
      "Iteration 46182 => Loss: 6.69858790179948115906\n",
      "Iteration 46183 => Loss: 6.69858784654903516298\n",
      "Iteration 46184 => Loss: 6.69858779130602677299\n",
      "Iteration 46185 => Loss: 6.69858773607046398268\n",
      "Iteration 46186 => Loss: 6.69858768084234323936\n",
      "Iteration 46187 => Loss: 6.69858762562166898391\n",
      "Iteration 46188 => Loss: 6.69858757040842522912\n",
      "Iteration 46189 => Loss: 6.69858751520263151491\n",
      "Iteration 46190 => Loss: 6.69858746000427274225\n",
      "Iteration 46191 => Loss: 6.69858740481335068750\n",
      "Iteration 46192 => Loss: 6.69858734962986979156\n",
      "Iteration 46193 => Loss: 6.69858729445382472534\n",
      "Iteration 46194 => Loss: 6.69858723928521104796\n",
      "Iteration 46195 => Loss: 6.69858718412403320031\n",
      "Iteration 46196 => Loss: 6.69858712897028762967\n",
      "Iteration 46197 => Loss: 6.69858707382397344787\n",
      "Iteration 46198 => Loss: 6.69858701868509243127\n",
      "Iteration 46199 => Loss: 6.69858696355364191533\n",
      "Iteration 46200 => Loss: 6.69858690842961301826\n",
      "Iteration 46201 => Loss: 6.69858685331302261545\n",
      "Iteration 46202 => Loss: 6.69858679820385294335\n",
      "Iteration 46203 => Loss: 6.69858674310210933101\n",
      "Iteration 46204 => Loss: 6.69858668800779266661\n",
      "Iteration 46205 => Loss: 6.69858663292090206198\n",
      "Iteration 46206 => Loss: 6.69858657784142597080\n",
      "Iteration 46207 => Loss: 6.69858652276938393300\n",
      "Iteration 46208 => Loss: 6.69858646770475374410\n",
      "Iteration 46209 => Loss: 6.69858641264754517408\n",
      "Iteration 46210 => Loss: 6.69858635759776177565\n",
      "Iteration 46211 => Loss: 6.69858630255538756160\n",
      "Iteration 46212 => Loss: 6.69858624752043052553\n",
      "Iteration 46213 => Loss: 6.69858619249289422015\n",
      "Iteration 46214 => Loss: 6.69858613747277065187\n",
      "Iteration 46215 => Loss: 6.69858608246006514975\n",
      "Iteration 46216 => Loss: 6.69858602745476794382\n",
      "Iteration 46217 => Loss: 6.69858597245688347499\n",
      "Iteration 46218 => Loss: 6.69858591746641174325\n",
      "Iteration 46219 => Loss: 6.69858586248334919588\n",
      "Iteration 46220 => Loss: 6.69858580750769494472\n",
      "Iteration 46221 => Loss: 6.69858575253944898975\n",
      "Iteration 46222 => Loss: 6.69858569757860689009\n",
      "Iteration 46223 => Loss: 6.69858564262517219845\n",
      "Iteration 46224 => Loss: 6.69858558767914491483\n",
      "Iteration 46225 => Loss: 6.69858553274051971016\n",
      "Iteration 46226 => Loss: 6.69858547780930191351\n",
      "Iteration 46227 => Loss: 6.69858542288547376131\n",
      "Iteration 46228 => Loss: 6.69858536796906545163\n",
      "Iteration 46229 => Loss: 6.69858531306004767458\n",
      "Iteration 46230 => Loss: 6.69858525815842931195\n",
      "Iteration 46231 => Loss: 6.69858520326420503466\n",
      "Iteration 46232 => Loss: 6.69858514837738372449\n",
      "Iteration 46233 => Loss: 6.69858509349795117060\n",
      "Iteration 46234 => Loss: 6.69858503862592069567\n",
      "Iteration 46235 => Loss: 6.69858498376127986518\n",
      "Iteration 46236 => Loss: 6.69858492890403223186\n",
      "Iteration 46237 => Loss: 6.69858487405418312477\n",
      "Iteration 46238 => Loss: 6.69858481921171655671\n",
      "Iteration 46239 => Loss: 6.69858476437664318581\n",
      "Iteration 46240 => Loss: 6.69858470954896567662\n",
      "Iteration 46241 => Loss: 6.69858465472866537738\n",
      "Iteration 46242 => Loss: 6.69858459991576093984\n",
      "Iteration 46243 => Loss: 6.69858454511023193589\n",
      "Iteration 46244 => Loss: 6.69858449031209790547\n",
      "Iteration 46245 => Loss: 6.69858443552133930865\n",
      "Iteration 46246 => Loss: 6.69858438073797390899\n",
      "Iteration 46247 => Loss: 6.69858432596198660747\n",
      "Iteration 46248 => Loss: 6.69858427119337473954\n",
      "Iteration 46249 => Loss: 6.69858421643214452246\n",
      "Iteration 46250 => Loss: 6.69858416167829862076\n",
      "Iteration 46251 => Loss: 6.69858410693182815265\n",
      "Iteration 46252 => Loss: 6.69858405219273844722\n",
      "Iteration 46253 => Loss: 6.69858399746101706995\n",
      "Iteration 46254 => Loss: 6.69858394273667823171\n",
      "Iteration 46255 => Loss: 6.69858388801970949800\n",
      "Iteration 46256 => Loss: 6.69858383331011530970\n",
      "Iteration 46257 => Loss: 6.69858377860789122593\n",
      "Iteration 46258 => Loss: 6.69858372391304346394\n",
      "Iteration 46259 => Loss: 6.69858366922555337197\n",
      "Iteration 46260 => Loss: 6.69858361454544493085\n",
      "Iteration 46261 => Loss: 6.69858355987270304155\n",
      "Iteration 46262 => Loss: 6.69858350520732503952\n",
      "Iteration 46263 => Loss: 6.69858345054931270113\n",
      "Iteration 46264 => Loss: 6.69858339589866957908\n",
      "Iteration 46265 => Loss: 6.69858334125538590342\n",
      "Iteration 46266 => Loss: 6.69858328661946522686\n",
      "Iteration 46267 => Loss: 6.69858323199091199029\n",
      "Iteration 46268 => Loss: 6.69858317736971464740\n",
      "Iteration 46269 => Loss: 6.69858312275588207996\n",
      "Iteration 46270 => Loss: 6.69858306814940540619\n",
      "Iteration 46271 => Loss: 6.69858301355028995516\n",
      "Iteration 46272 => Loss: 6.69858295895853039781\n",
      "Iteration 46273 => Loss: 6.69858290437412584595\n",
      "Iteration 46274 => Loss: 6.69858284979707185869\n",
      "Iteration 46275 => Loss: 6.69858279522738175871\n",
      "Iteration 46276 => Loss: 6.69858274066504133515\n",
      "Iteration 46277 => Loss: 6.69858268611005236437\n",
      "Iteration 46278 => Loss: 6.69858263156241484637\n",
      "Iteration 46279 => Loss: 6.69858257702212789297\n",
      "Iteration 46280 => Loss: 6.69858252248919150418\n",
      "Iteration 46281 => Loss: 6.69858246796359857456\n",
      "Iteration 46282 => Loss: 6.69858241344535709771\n",
      "Iteration 46283 => Loss: 6.69858235893446529730\n",
      "Iteration 46284 => Loss: 6.69858230443090807427\n",
      "Iteration 46285 => Loss: 6.69858224993470319220\n",
      "Iteration 46286 => Loss: 6.69858219544584354566\n",
      "Iteration 46287 => Loss: 6.69858214096432558193\n",
      "Iteration 46288 => Loss: 6.69858208649014663649\n",
      "Iteration 46289 => Loss: 6.69858203202330582116\n",
      "Iteration 46290 => Loss: 6.69858197756380491228\n",
      "Iteration 46291 => Loss: 6.69858192311164746258\n",
      "Iteration 46292 => Loss: 6.69858186866682281391\n",
      "Iteration 46293 => Loss: 6.69858181422933807170\n",
      "Iteration 46294 => Loss: 6.69858175979917902509\n",
      "Iteration 46295 => Loss: 6.69858170537636699038\n",
      "Iteration 46296 => Loss: 6.69858165096088065127\n",
      "Iteration 46297 => Loss: 6.69858159655273688315\n",
      "Iteration 46298 => Loss: 6.69858154215191259340\n",
      "Iteration 46299 => Loss: 6.69858148775842110467\n",
      "Iteration 46300 => Loss: 6.69858143337226508152\n",
      "Iteration 46301 => Loss: 6.69858137899343297761\n",
      "Iteration 46302 => Loss: 6.69858132462193101020\n",
      "Iteration 46303 => Loss: 6.69858127025775385022\n",
      "Iteration 46304 => Loss: 6.69858121590090149766\n",
      "Iteration 46305 => Loss: 6.69858116155137395253\n",
      "Iteration 46306 => Loss: 6.69858110720917299119\n",
      "Iteration 46307 => Loss: 6.69858105287428973185\n",
      "Iteration 46308 => Loss: 6.69858099854673394447\n",
      "Iteration 46309 => Loss: 6.69858094422649585908\n",
      "Iteration 46310 => Loss: 6.69858088991357192299\n",
      "Iteration 46311 => Loss: 6.69858083560797812339\n",
      "Iteration 46312 => Loss: 6.69858078130969225583\n",
      "Iteration 46313 => Loss: 6.69858072701872853116\n",
      "Iteration 46314 => Loss: 6.69858067273508162032\n",
      "Iteration 46315 => Loss: 6.69858061845873997697\n",
      "Iteration 46316 => Loss: 6.69858056418972314106\n",
      "Iteration 46317 => Loss: 6.69858050992801778989\n",
      "Iteration 46318 => Loss: 6.69858045567362037076\n",
      "Iteration 46319 => Loss: 6.69858040142653354820\n",
      "Iteration 46320 => Loss: 6.69858034718675465768\n",
      "Iteration 46321 => Loss: 6.69858029295428725192\n",
      "Iteration 46322 => Loss: 6.69858023872912156094\n",
      "Iteration 46323 => Loss: 6.69858018451127623649\n",
      "Iteration 46324 => Loss: 6.69858013030072907412\n",
      "Iteration 46325 => Loss: 6.69858007609748540290\n",
      "Iteration 46326 => Loss: 6.69858002190154877553\n",
      "Iteration 46327 => Loss: 6.69857996771291652749\n",
      "Iteration 46328 => Loss: 6.69857991353157888881\n",
      "Iteration 46329 => Loss: 6.69857985935754740581\n",
      "Iteration 46330 => Loss: 6.69857980519081852577\n",
      "Iteration 46331 => Loss: 6.69857975103138336692\n",
      "Iteration 46332 => Loss: 6.69857969687924725832\n",
      "Iteration 46333 => Loss: 6.69857964273441197633\n",
      "Iteration 46334 => Loss: 6.69857958859686686282\n",
      "Iteration 46335 => Loss: 6.69857953446661813501\n",
      "Iteration 46336 => Loss: 6.69857948034366401657\n",
      "Iteration 46337 => Loss: 6.69857942622800361931\n",
      "Iteration 46338 => Loss: 6.69857937211964404867\n",
      "Iteration 46339 => Loss: 6.69857931801856132381\n",
      "Iteration 46340 => Loss: 6.69857926392477676103\n",
      "Iteration 46341 => Loss: 6.69857920983827703765\n",
      "Iteration 46342 => Loss: 6.69857915575906748273\n",
      "Iteration 46343 => Loss: 6.69857910168714898447\n",
      "Iteration 46344 => Loss: 6.69857904762251088471\n",
      "Iteration 46345 => Loss: 6.69857899356515407163\n",
      "Iteration 46346 => Loss: 6.69857893951509719699\n",
      "Iteration 46347 => Loss: 6.69857888547231183907\n",
      "Iteration 46348 => Loss: 6.69857883143680687965\n",
      "Iteration 46349 => Loss: 6.69857877740858764781\n",
      "Iteration 46350 => Loss: 6.69857872338765236719\n",
      "Iteration 46351 => Loss: 6.69857866937398416241\n",
      "Iteration 46352 => Loss: 6.69857861536760434973\n",
      "Iteration 46353 => Loss: 6.69857856136850227102\n",
      "Iteration 46354 => Loss: 6.69857850737666904450\n",
      "Iteration 46355 => Loss: 6.69857845339211710467\n",
      "Iteration 46356 => Loss: 6.69857839941484023427\n",
      "Iteration 46357 => Loss: 6.69857834544483843331\n",
      "Iteration 46358 => Loss: 6.69857829148210637271\n",
      "Iteration 46359 => Loss: 6.69857823752664049977\n",
      "Iteration 46360 => Loss: 6.69857818357844880808\n",
      "Iteration 46361 => Loss: 6.69857812963752774493\n",
      "Iteration 46362 => Loss: 6.69857807570387908669\n",
      "Iteration 46363 => Loss: 6.69857802177749039885\n",
      "Iteration 46364 => Loss: 6.69857796785837500408\n",
      "Iteration 46365 => Loss: 6.69857791394652224426\n",
      "Iteration 46366 => Loss: 6.69857786004193478391\n",
      "Iteration 46367 => Loss: 6.69857780614460907032\n",
      "Iteration 46368 => Loss: 6.69857775225454776802\n",
      "Iteration 46369 => Loss: 6.69857769837175176519\n",
      "Iteration 46370 => Loss: 6.69857764449621129188\n",
      "Iteration 46371 => Loss: 6.69857759062793167715\n",
      "Iteration 46372 => Loss: 6.69857753676690759193\n",
      "Iteration 46373 => Loss: 6.69857748291314614164\n",
      "Iteration 46374 => Loss: 6.69857742906664199722\n",
      "Iteration 46375 => Loss: 6.69857737522739160596\n",
      "Iteration 46376 => Loss: 6.69857732139539585603\n",
      "Iteration 46377 => Loss: 6.69857726757065030654\n",
      "Iteration 46378 => Loss: 6.69857721375316295109\n",
      "Iteration 46379 => Loss: 6.69857715994292401973\n",
      "Iteration 46380 => Loss: 6.69857710613993795334\n",
      "Iteration 46381 => Loss: 6.69857705234420386375\n",
      "Iteration 46382 => Loss: 6.69857699855572086278\n",
      "Iteration 46383 => Loss: 6.69857694477447829229\n",
      "Iteration 46384 => Loss: 6.69857689100048503406\n",
      "Iteration 46385 => Loss: 6.69857683723373931173\n",
      "Iteration 46386 => Loss: 6.69857678347424023713\n",
      "Iteration 46387 => Loss: 6.69857672972198603389\n",
      "Iteration 46388 => Loss: 6.69857667597696693207\n",
      "Iteration 46389 => Loss: 6.69857662223919803068\n",
      "Iteration 46390 => Loss: 6.69857656850867133613\n",
      "Iteration 46391 => Loss: 6.69857651478537619028\n",
      "Iteration 46392 => Loss: 6.69857646106932680397\n",
      "Iteration 46393 => Loss: 6.69857640736051340724\n",
      "Iteration 46394 => Loss: 6.69857635365893600010\n",
      "Iteration 46395 => Loss: 6.69857629996459813526\n",
      "Iteration 46396 => Loss: 6.69857624627749537183\n",
      "Iteration 46397 => Loss: 6.69857619259762682162\n",
      "Iteration 46398 => Loss: 6.69857613892498715558\n",
      "Iteration 46399 => Loss: 6.69857608525958259094\n",
      "Iteration 46400 => Loss: 6.69857603160141312770\n",
      "Iteration 46401 => Loss: 6.69857597795046988409\n",
      "Iteration 46402 => Loss: 6.69857592430675730100\n",
      "Iteration 46403 => Loss: 6.69857587067027626659\n",
      "Iteration 46404 => Loss: 6.69857581704101701092\n",
      "Iteration 46405 => Loss: 6.69857576341898841576\n",
      "Iteration 46406 => Loss: 6.69857570980418337570\n",
      "Iteration 46407 => Loss: 6.69857565619660988432\n",
      "Iteration 46408 => Loss: 6.69857560259625284260\n",
      "Iteration 46409 => Loss: 6.69857554900311136237\n",
      "Iteration 46410 => Loss: 6.69857549541720409536\n",
      "Iteration 46411 => Loss: 6.69857544183851594255\n",
      "Iteration 46412 => Loss: 6.69857538826704068669\n",
      "Iteration 46413 => Loss: 6.69857533470278543319\n",
      "Iteration 46414 => Loss: 6.69857528114575284661\n",
      "Iteration 46415 => Loss: 6.69857522759593315698\n",
      "Iteration 46416 => Loss: 6.69857517405333524607\n",
      "Iteration 46417 => Loss: 6.69857512051794756758\n",
      "Iteration 46418 => Loss: 6.69857506698977456239\n",
      "Iteration 46419 => Loss: 6.69857501346881001325\n",
      "Iteration 46420 => Loss: 6.69857495995506457831\n",
      "Iteration 46421 => Loss: 6.69857490644852582307\n",
      "Iteration 46422 => Loss: 6.69857485294920351748\n",
      "Iteration 46423 => Loss: 6.69857479945708167435\n",
      "Iteration 46424 => Loss: 6.69857474597217095180\n",
      "Iteration 46425 => Loss: 6.69857469249446690895\n",
      "Iteration 46426 => Loss: 6.69857463902397132216\n",
      "Iteration 46427 => Loss: 6.69857458556067530964\n",
      "Iteration 46428 => Loss: 6.69857453210458686499\n",
      "Iteration 46429 => Loss: 6.69857447865570954093\n",
      "Iteration 46430 => Loss: 6.69857442521402912661\n",
      "Iteration 46431 => Loss: 6.69857437177954029295\n",
      "Iteration 46432 => Loss: 6.69857431835226346806\n",
      "Iteration 46433 => Loss: 6.69857426493218000019\n",
      "Iteration 46434 => Loss: 6.69857421151929877112\n",
      "Iteration 46435 => Loss: 6.69857415811361622815\n",
      "Iteration 46436 => Loss: 6.69857410471512437766\n",
      "Iteration 46437 => Loss: 6.69857405132383210145\n",
      "Iteration 46438 => Loss: 6.69857399793972874136\n",
      "Iteration 46439 => Loss: 6.69857394456282584372\n",
      "Iteration 46440 => Loss: 6.69857389119310919767\n",
      "Iteration 46441 => Loss: 6.69857383783058857318\n",
      "Iteration 46442 => Loss: 6.69857378447525952936\n",
      "Iteration 46443 => Loss: 6.69857373112711762531\n",
      "Iteration 46444 => Loss: 6.69857367778616197285\n",
      "Iteration 46445 => Loss: 6.69857362445239878923\n",
      "Iteration 46446 => Loss: 6.69857357112582540992\n",
      "Iteration 46447 => Loss: 6.69857351780642673589\n",
      "Iteration 46448 => Loss: 6.69857346449422585977\n",
      "Iteration 46449 => Loss: 6.69857341118919968892\n",
      "Iteration 46450 => Loss: 6.69857335789135355242\n",
      "Iteration 46451 => Loss: 6.69857330460069544387\n",
      "Iteration 46452 => Loss: 6.69857325131721470513\n",
      "Iteration 46453 => Loss: 6.69857319804091844162\n",
      "Iteration 46454 => Loss: 6.69857314477179777157\n",
      "Iteration 46455 => Loss: 6.69857309150985535950\n",
      "Iteration 46456 => Loss: 6.69857303825509564632\n",
      "Iteration 46457 => Loss: 6.69857298500750619752\n",
      "Iteration 46458 => Loss: 6.69857293176709767124\n",
      "Iteration 46459 => Loss: 6.69857287853385674481\n",
      "Iteration 46460 => Loss: 6.69857282530778430640\n",
      "Iteration 46461 => Loss: 6.69857277208889279052\n",
      "Iteration 46462 => Loss: 6.69857271887717065084\n",
      "Iteration 46463 => Loss: 6.69857266567262410462\n",
      "Iteration 46464 => Loss: 6.69857261247523538827\n",
      "Iteration 46465 => Loss: 6.69857255928502404174\n",
      "Iteration 46466 => Loss: 6.69857250610197585416\n",
      "Iteration 46467 => Loss: 6.69857245292609526643\n",
      "Iteration 46468 => Loss: 6.69857239975737961402\n",
      "Iteration 46469 => Loss: 6.69857234659582800873\n",
      "Iteration 46470 => Loss: 6.69857229344144133876\n",
      "Iteration 46471 => Loss: 6.69857224029421693956\n",
      "Iteration 46472 => Loss: 6.69857218715415214660\n",
      "Iteration 46473 => Loss: 6.69857213402125140078\n",
      "Iteration 46474 => Loss: 6.69857208089551292574\n",
      "Iteration 46475 => Loss: 6.69857202777692517515\n",
      "Iteration 46476 => Loss: 6.69857197466550058351\n",
      "Iteration 46477 => Loss: 6.69857192156122938087\n",
      "Iteration 46478 => Loss: 6.69857186846411511993\n",
      "Iteration 46479 => Loss: 6.69857181537416312977\n",
      "Iteration 46480 => Loss: 6.69857176229135298229\n",
      "Iteration 46481 => Loss: 6.69857170921569977651\n",
      "Iteration 46482 => Loss: 6.69857165614719818336\n",
      "Iteration 46483 => Loss: 6.69857160308585530828\n",
      "Iteration 46484 => Loss: 6.69857155003165427587\n",
      "Iteration 46485 => Loss: 6.69857149698460130338\n",
      "Iteration 46486 => Loss: 6.69857144394469905535\n",
      "Iteration 46487 => Loss: 6.69857139091194575542\n",
      "Iteration 46488 => Loss: 6.69857133788633607452\n",
      "Iteration 46489 => Loss: 6.69857128486787267718\n",
      "Iteration 46490 => Loss: 6.69857123185655289888\n",
      "Iteration 46491 => Loss: 6.69857117885237762778\n",
      "Iteration 46492 => Loss: 6.69857112585534419935\n",
      "Iteration 46493 => Loss: 6.69857107286545350178\n",
      "Iteration 46494 => Loss: 6.69857101988270731141\n",
      "Iteration 46495 => Loss: 6.69857096690708964104\n",
      "Iteration 46496 => Loss: 6.69857091393861470152\n",
      "Iteration 46497 => Loss: 6.69857086097728071650\n",
      "Iteration 46498 => Loss: 6.69857080802308146872\n",
      "Iteration 46499 => Loss: 6.69857075507601429365\n",
      "Iteration 46500 => Loss: 6.69857070213608452036\n",
      "Iteration 46501 => Loss: 6.69857064920328948432\n",
      "Iteration 46502 => Loss: 6.69857059627762740917\n",
      "Iteration 46503 => Loss: 6.69857054335909563036\n",
      "Iteration 46504 => Loss: 6.69857049044769414792\n",
      "Iteration 46505 => Loss: 6.69857043754342296182\n",
      "Iteration 46506 => Loss: 6.69857038464628029573\n",
      "Iteration 46507 => Loss: 6.69857033175626970234\n",
      "Iteration 46508 => Loss: 6.69857027887338229988\n",
      "Iteration 46509 => Loss: 6.69857022599761364745\n",
      "Iteration 46510 => Loss: 6.69857017312898417316\n",
      "Iteration 46511 => Loss: 6.69857012026746989619\n",
      "Iteration 46512 => Loss: 6.69857006741308147468\n",
      "Iteration 46513 => Loss: 6.69857001456581446774\n",
      "Iteration 46514 => Loss: 6.69856996172566798720\n",
      "Iteration 46515 => Loss: 6.69856990889264114486\n",
      "Iteration 46516 => Loss: 6.69856985606673571709\n",
      "Iteration 46517 => Loss: 6.69856980324795081572\n",
      "Iteration 46518 => Loss: 6.69856975043627844713\n",
      "Iteration 46519 => Loss: 6.69856969763172394039\n",
      "Iteration 46520 => Loss: 6.69856964483428196644\n",
      "Iteration 46521 => Loss: 6.69856959204395785434\n",
      "Iteration 46522 => Loss: 6.69856953926075338046\n",
      "Iteration 46523 => Loss: 6.69856948648464900486\n",
      "Iteration 46524 => Loss: 6.69856943371566426748\n",
      "Iteration 46525 => Loss: 6.69856938095378673381\n",
      "Iteration 46526 => Loss: 6.69856932819902350928\n",
      "Iteration 46527 => Loss: 6.69856927545136393576\n",
      "Iteration 46528 => Loss: 6.69856922271081423048\n",
      "Iteration 46529 => Loss: 6.69856916997736728803\n",
      "Iteration 46530 => Loss: 6.69856911725102932564\n",
      "Iteration 46531 => Loss: 6.69856906453179590244\n",
      "Iteration 46532 => Loss: 6.69856901181967057113\n",
      "Iteration 46533 => Loss: 6.69856895911464267357\n",
      "Iteration 46534 => Loss: 6.69856890641672553244\n",
      "Iteration 46535 => Loss: 6.69856885372589339056\n",
      "Iteration 46536 => Loss: 6.69856880104217644600\n",
      "Iteration 46537 => Loss: 6.69856874836554805341\n",
      "Iteration 46538 => Loss: 6.69856869569602686454\n",
      "Iteration 46539 => Loss: 6.69856864303359600399\n",
      "Iteration 46540 => Loss: 6.69856859037826435355\n",
      "Iteration 46541 => Loss: 6.69856853773002836050\n",
      "Iteration 46542 => Loss: 6.69856848508888802485\n",
      "Iteration 46543 => Loss: 6.69856843245483712934\n",
      "Iteration 46544 => Loss: 6.69856837982788011487\n",
      "Iteration 46545 => Loss: 6.69856832720801875780\n",
      "Iteration 46546 => Loss: 6.69856827459524684087\n",
      "Iteration 46547 => Loss: 6.69856822198955992320\n",
      "Iteration 46548 => Loss: 6.69856816939096155750\n",
      "Iteration 46549 => Loss: 6.69856811679945174376\n",
      "Iteration 46550 => Loss: 6.69856806421503048199\n",
      "Iteration 46551 => Loss: 6.69856801163769599583\n",
      "Iteration 46552 => Loss: 6.69856795906744562075\n",
      "Iteration 46553 => Loss: 6.69856790650428024492\n",
      "Iteration 46554 => Loss: 6.69856785394819720381\n",
      "Iteration 46555 => Loss: 6.69856780139918850381\n",
      "Iteration 46556 => Loss: 6.69856774885726924396\n",
      "Iteration 46557 => Loss: 6.69856769632242876611\n",
      "Iteration 46558 => Loss: 6.69856764379466174120\n",
      "Iteration 46559 => Loss: 6.69856759127398060372\n",
      "Iteration 46560 => Loss: 6.69856753876037558371\n",
      "Iteration 46561 => Loss: 6.69856748625384135210\n",
      "Iteration 46562 => Loss: 6.69856743375439389609\n",
      "Iteration 46563 => Loss: 6.69856738126201101124\n",
      "Iteration 46564 => Loss: 6.69856732877669980297\n",
      "Iteration 46565 => Loss: 6.69856727629846027128\n",
      "Iteration 46566 => Loss: 6.69856722382730307430\n",
      "Iteration 46567 => Loss: 6.69856717136320867212\n",
      "Iteration 46568 => Loss: 6.69856711890618150562\n",
      "Iteration 46569 => Loss: 6.69856706645622246299\n",
      "Iteration 46570 => Loss: 6.69856701401333776147\n",
      "Iteration 46571 => Loss: 6.69856696157751763110\n",
      "Iteration 46572 => Loss: 6.69856690914876029552\n",
      "Iteration 46573 => Loss: 6.69856685672707286017\n",
      "Iteration 46574 => Loss: 6.69856680431244466689\n",
      "Iteration 46575 => Loss: 6.69856675190487749205\n",
      "Iteration 46576 => Loss: 6.69856669950437488836\n",
      "Iteration 46577 => Loss: 6.69856664711093952036\n",
      "Iteration 46578 => Loss: 6.69856659472455095994\n",
      "Iteration 46579 => Loss: 6.69856654234523762881\n",
      "Iteration 46580 => Loss: 6.69856648997296311165\n",
      "Iteration 46581 => Loss: 6.69856643760776293561\n",
      "Iteration 46582 => Loss: 6.69856638524961134351\n",
      "Iteration 46583 => Loss: 6.69856633289851721713\n",
      "Iteration 46584 => Loss: 6.69856628055447966830\n",
      "Iteration 46585 => Loss: 6.69856622821748715069\n",
      "Iteration 46586 => Loss: 6.69856617588755387516\n",
      "Iteration 46587 => Loss: 6.69856612356467451264\n",
      "Iteration 46588 => Loss: 6.69856607124883751680\n",
      "Iteration 46589 => Loss: 6.69856601894005621034\n",
      "Iteration 46590 => Loss: 6.69856596663831282967\n",
      "Iteration 46591 => Loss: 6.69856591434362957926\n",
      "Iteration 46592 => Loss: 6.69856586205599224826\n",
      "Iteration 46593 => Loss: 6.69856580977539728394\n",
      "Iteration 46594 => Loss: 6.69856575750184468632\n",
      "Iteration 46595 => Loss: 6.69856570523533978445\n",
      "Iteration 46596 => Loss: 6.69856565297587636110\n",
      "Iteration 46597 => Loss: 6.69856560072345619261\n",
      "Iteration 46598 => Loss: 6.69856554847807572628\n",
      "Iteration 46599 => Loss: 6.69856549623973496210\n",
      "Iteration 46600 => Loss: 6.69856544400843567644\n",
      "Iteration 46601 => Loss: 6.69856539178417342839\n",
      "Iteration 46602 => Loss: 6.69856533956694466525\n",
      "Iteration 46603 => Loss: 6.69856528735675471609\n",
      "Iteration 46604 => Loss: 6.69856523515360091636\n",
      "Iteration 46605 => Loss: 6.69856518295748148972\n",
      "Iteration 46606 => Loss: 6.69856513076839732435\n",
      "Iteration 46607 => Loss: 6.69856507858634309116\n",
      "Iteration 46608 => Loss: 6.69856502641132145470\n",
      "Iteration 46609 => Loss: 6.69856497424332708590\n",
      "Iteration 46610 => Loss: 6.69856492208236886654\n",
      "Iteration 46611 => Loss: 6.69856486992842814487\n",
      "Iteration 46612 => Loss: 6.69856481778152712536\n",
      "Iteration 46613 => Loss: 6.69856476564164893261\n",
      "Iteration 46614 => Loss: 6.69856471350879267845\n",
      "Iteration 46615 => Loss: 6.69856466138296902102\n",
      "Iteration 46616 => Loss: 6.69856460926416641399\n",
      "Iteration 46617 => Loss: 6.69856455715238219284\n",
      "Iteration 46618 => Loss: 6.69856450504762435116\n",
      "Iteration 46619 => Loss: 6.69856445294988489536\n",
      "Iteration 46620 => Loss: 6.69856440085917004268\n",
      "Iteration 46621 => Loss: 6.69856434877546913498\n",
      "Iteration 46622 => Loss: 6.69856429669878572497\n",
      "Iteration 46623 => Loss: 6.69856424462912602991\n",
      "Iteration 46624 => Loss: 6.69856419256647939164\n",
      "Iteration 46625 => Loss: 6.69856414051084936290\n",
      "Iteration 46626 => Loss: 6.69856408846223150277\n",
      "Iteration 46627 => Loss: 6.69856403642062492310\n",
      "Iteration 46628 => Loss: 6.69856398438603672929\n",
      "Iteration 46629 => Loss: 6.69856393235845981593\n",
      "Iteration 46630 => Loss: 6.69856388033789151848\n",
      "Iteration 46631 => Loss: 6.69856382832433538965\n",
      "Iteration 46632 => Loss: 6.69856377631778254766\n",
      "Iteration 46633 => Loss: 6.69856372431824720337\n",
      "Iteration 46634 => Loss: 6.69856367232570892867\n",
      "Iteration 46635 => Loss: 6.69856362034018104623\n",
      "Iteration 46636 => Loss: 6.69856356836166089153\n",
      "Iteration 46637 => Loss: 6.69856351639013691823\n",
      "Iteration 46638 => Loss: 6.69856346442561978449\n",
      "Iteration 46639 => Loss: 6.69856341246810593759\n",
      "Iteration 46640 => Loss: 6.69856336051759360117\n",
      "Iteration 46641 => Loss: 6.69856330857408011070\n",
      "Iteration 46642 => Loss: 6.69856325663757168343\n",
      "Iteration 46643 => Loss: 6.69856320470805322032\n",
      "Iteration 46644 => Loss: 6.69856315278553715586\n",
      "Iteration 46645 => Loss: 6.69856310087001816100\n",
      "Iteration 46646 => Loss: 6.69856304896149090666\n",
      "Iteration 46647 => Loss: 6.69856299705996960370\n",
      "Iteration 46648 => Loss: 6.69856294516542316586\n",
      "Iteration 46649 => Loss: 6.69856289327788978483\n",
      "Iteration 46650 => Loss: 6.69856284139733126892\n",
      "Iteration 46651 => Loss: 6.69856278952377071079\n",
      "Iteration 46652 => Loss: 6.69856273765720366953\n",
      "Iteration 46653 => Loss: 6.69856268579761771065\n",
      "Iteration 46654 => Loss: 6.69856263394502260411\n",
      "Iteration 46655 => Loss: 6.69856258209941657356\n",
      "Iteration 46656 => Loss: 6.69856253026079517809\n",
      "Iteration 46657 => Loss: 6.69856247842916108226\n",
      "Iteration 46658 => Loss: 6.69856242660450984516\n",
      "Iteration 46659 => Loss: 6.69856237478684146680\n",
      "Iteration 46660 => Loss: 6.69856232297615683535\n",
      "Iteration 46661 => Loss: 6.69856227117245328628\n",
      "Iteration 46662 => Loss: 6.69856221937573081959\n",
      "Iteration 46663 => Loss: 6.69856216758599298799\n",
      "Iteration 46664 => Loss: 6.69856211580322646881\n",
      "Iteration 46665 => Loss: 6.69856206402743836748\n",
      "Iteration 46666 => Loss: 6.69856201225862424309\n",
      "Iteration 46667 => Loss: 6.69856196049678853655\n",
      "Iteration 46668 => Loss: 6.69856190874193213602\n",
      "Iteration 46669 => Loss: 6.69856185699404882428\n",
      "Iteration 46670 => Loss: 6.69856180525313593677\n",
      "Iteration 46671 => Loss: 6.69856175351919524985\n",
      "Iteration 46672 => Loss: 6.69856170179222409899\n",
      "Iteration 46673 => Loss: 6.69856165007222781327\n",
      "Iteration 46674 => Loss: 6.69856159835920106360\n",
      "Iteration 46675 => Loss: 6.69856154665313496821\n",
      "Iteration 46676 => Loss: 6.69856149495404284977\n",
      "Iteration 46677 => Loss: 6.69856144326191671468\n",
      "Iteration 46678 => Loss: 6.69856139157675567475\n",
      "Iteration 46679 => Loss: 6.69856133989855617727\n",
      "Iteration 46680 => Loss: 6.69856128822731911043\n",
      "Iteration 46681 => Loss: 6.69856123656304980329\n",
      "Iteration 46682 => Loss: 6.69856118490574115043\n",
      "Iteration 46683 => Loss: 6.69856113325539492820\n",
      "Iteration 46684 => Loss: 6.69856108161200491935\n",
      "Iteration 46685 => Loss: 6.69856102997557112388\n",
      "Iteration 46686 => Loss: 6.69856097834610331176\n",
      "Iteration 46687 => Loss: 6.69856092672358727214\n",
      "Iteration 46688 => Loss: 6.69856087510803366314\n",
      "Iteration 46689 => Loss: 6.69856082349942560938\n",
      "Iteration 46690 => Loss: 6.69856077189777732173\n",
      "Iteration 46691 => Loss: 6.69856072030308347109\n",
      "Iteration 46692 => Loss: 6.69856066871533339935\n",
      "Iteration 46693 => Loss: 6.69856061713454664641\n",
      "Iteration 46694 => Loss: 6.69856056556070900143\n",
      "Iteration 46695 => Loss: 6.69856051399381513534\n",
      "Iteration 46696 => Loss: 6.69856046243387037720\n",
      "Iteration 46697 => Loss: 6.69856041088087650337\n",
      "Iteration 46698 => Loss: 6.69856035933482640843\n",
      "Iteration 46699 => Loss: 6.69856030779572275691\n",
      "Iteration 46700 => Loss: 6.69856025626356466063\n",
      "Iteration 46701 => Loss: 6.69856020473835300777\n",
      "Iteration 46702 => Loss: 6.69856015322008335744\n",
      "Iteration 46703 => Loss: 6.69856010170875304510\n",
      "Iteration 46704 => Loss: 6.69856005020436473529\n",
      "Iteration 46705 => Loss: 6.69855999870691665166\n",
      "Iteration 46706 => Loss: 6.69855994721641145873\n",
      "Iteration 46707 => Loss: 6.69855989573283849836\n",
      "Iteration 46708 => Loss: 6.69855984425620487599\n",
      "Iteration 46709 => Loss: 6.69855979278651325615\n",
      "Iteration 46710 => Loss: 6.69855974132375031616\n",
      "Iteration 46711 => Loss: 6.69855968986792671416\n",
      "Iteration 46712 => Loss: 6.69855963841903534473\n",
      "Iteration 46713 => Loss: 6.69855958697707976057\n",
      "Iteration 46714 => Loss: 6.69855953554204930356\n",
      "Iteration 46715 => Loss: 6.69855948411395463182\n",
      "Iteration 46716 => Loss: 6.69855943269278686358\n",
      "Iteration 46717 => Loss: 6.69855938127854333430\n",
      "Iteration 46718 => Loss: 6.69855932987123647848\n",
      "Iteration 46719 => Loss: 6.69855927847085474980\n",
      "Iteration 46720 => Loss: 6.69855922707739992461\n",
      "Iteration 46721 => Loss: 6.69855917569087111474\n",
      "Iteration 46722 => Loss: 6.69855912431126210294\n",
      "Iteration 46723 => Loss: 6.69855907293857733009\n",
      "Iteration 46724 => Loss: 6.69855902157281679621\n",
      "Iteration 46725 => Loss: 6.69855897021397961311\n",
      "Iteration 46726 => Loss: 6.69855891886206222807\n",
      "Iteration 46727 => Loss: 6.69855886751705931204\n",
      "Iteration 46728 => Loss: 6.69855881617898152314\n",
      "Iteration 46729 => Loss: 6.69855876484781731506\n",
      "Iteration 46730 => Loss: 6.69855871352356757598\n",
      "Iteration 46731 => Loss: 6.69855866220624296403\n",
      "Iteration 46732 => Loss: 6.69855861089582749202\n",
      "Iteration 46733 => Loss: 6.69855855959232737717\n",
      "Iteration 46734 => Loss: 6.69855850829573906680\n",
      "Iteration 46735 => Loss: 6.69855845700606167270\n",
      "Iteration 46736 => Loss: 6.69855840572329608307\n",
      "Iteration 46737 => Loss: 6.69855835444744762697\n",
      "Iteration 46738 => Loss: 6.69855830317850209354\n",
      "Iteration 46739 => Loss: 6.69855825191646836458\n",
      "Iteration 46740 => Loss: 6.69855820066134111102\n",
      "Iteration 46741 => Loss: 6.69855814941311678012\n",
      "Iteration 46742 => Loss: 6.69855809817180158916\n",
      "Iteration 46743 => Loss: 6.69855804693739642630\n",
      "Iteration 46744 => Loss: 6.69855799570988263980\n",
      "Iteration 46745 => Loss: 6.69855794448927888141\n",
      "Iteration 46746 => Loss: 6.69855789327558515112\n",
      "Iteration 46747 => Loss: 6.69855784206877569176\n",
      "Iteration 46748 => Loss: 6.69855779086887981322\n",
      "Iteration 46749 => Loss: 6.69855773967587886375\n",
      "Iteration 46750 => Loss: 6.69855768848977284335\n",
      "Iteration 46751 => Loss: 6.69855763731056974564\n",
      "Iteration 46752 => Loss: 6.69855758613825624792\n",
      "Iteration 46753 => Loss: 6.69855753497283856746\n",
      "Iteration 46754 => Loss: 6.69855748381432647420\n",
      "Iteration 46755 => Loss: 6.69855743266269865188\n",
      "Iteration 46756 => Loss: 6.69855738151796220592\n",
      "Iteration 46757 => Loss: 6.69855733038011980085\n",
      "Iteration 46758 => Loss: 6.69855727924917054850\n",
      "Iteration 46759 => Loss: 6.69855722812511089614\n",
      "Iteration 46760 => Loss: 6.69855717700793551472\n",
      "Iteration 46761 => Loss: 6.69855712589765328602\n",
      "Iteration 46762 => Loss: 6.69855707479425355189\n",
      "Iteration 46763 => Loss: 6.69855702369774608229\n",
      "Iteration 46764 => Loss: 6.69855697260811755456\n",
      "Iteration 46765 => Loss: 6.69855692152537951500\n",
      "Iteration 46766 => Loss: 6.69855687044952219367\n",
      "Iteration 46767 => Loss: 6.69855681938054203783\n",
      "Iteration 46768 => Loss: 6.69855676831845059382\n",
      "Iteration 46769 => Loss: 6.69855671726324342075\n",
      "Iteration 46770 => Loss: 6.69855666621490808410\n",
      "Iteration 46771 => Loss: 6.69855661517345435385\n",
      "Iteration 46772 => Loss: 6.69855656413887778911\n",
      "Iteration 46773 => Loss: 6.69855651311117661351\n",
      "Iteration 46774 => Loss: 6.69855646209035260341\n",
      "Iteration 46775 => Loss: 6.69855641107640398246\n",
      "Iteration 46776 => Loss: 6.69855636006932808613\n",
      "Iteration 46777 => Loss: 6.69855630906912846712\n",
      "Iteration 46778 => Loss: 6.69855625807579624365\n",
      "Iteration 46779 => Loss: 6.69855620708933763296\n",
      "Iteration 46780 => Loss: 6.69855615610975441143\n",
      "Iteration 46781 => Loss: 6.69855610513703325637\n",
      "Iteration 46782 => Loss: 6.69855605417118127320\n",
      "Iteration 46783 => Loss: 6.69855600321220556737\n",
      "Iteration 46784 => Loss: 6.69855595226008837528\n",
      "Iteration 46785 => Loss: 6.69855590131484479599\n",
      "Iteration 46786 => Loss: 6.69855585037645351321\n",
      "Iteration 46787 => Loss: 6.69855579944493495503\n",
      "Iteration 46788 => Loss: 6.69855574852028112787\n",
      "Iteration 46789 => Loss: 6.69855569760248670264\n",
      "Iteration 46790 => Loss: 6.69855564669155700841\n",
      "Iteration 46791 => Loss: 6.69855559578747161709\n",
      "Iteration 46792 => Loss: 6.69855554489025895037\n",
      "Iteration 46793 => Loss: 6.69855549399990479742\n",
      "Iteration 46794 => Loss: 6.69855544311640649369\n",
      "Iteration 46795 => Loss: 6.69855539223976759189\n",
      "Iteration 46796 => Loss: 6.69855534136998276296\n",
      "Iteration 46797 => Loss: 6.69855529050704845417\n",
      "Iteration 46798 => Loss: 6.69855523965096999461\n",
      "Iteration 46799 => Loss: 6.69855518880174560792\n",
      "Iteration 46800 => Loss: 6.69855513795937174137\n",
      "Iteration 46801 => Loss: 6.69855508712384928316\n",
      "Iteration 46802 => Loss: 6.69855503629518089781\n",
      "Iteration 46803 => Loss: 6.69855498547335415083\n",
      "Iteration 46804 => Loss: 6.69855493465838058853\n",
      "Iteration 46805 => Loss: 6.69855488385025577003\n",
      "Iteration 46806 => Loss: 6.69855483304896903718\n",
      "Iteration 46807 => Loss: 6.69855478225454259444\n",
      "Iteration 46808 => Loss: 6.69855473146694802011\n",
      "Iteration 46809 => Loss: 6.69855468068620396593\n",
      "Iteration 46810 => Loss: 6.69855462991229888559\n",
      "Iteration 46811 => Loss: 6.69855457914522745000\n",
      "Iteration 46812 => Loss: 6.69855452838501186363\n",
      "Iteration 46813 => Loss: 6.69855447763163081021\n",
      "Iteration 46814 => Loss: 6.69855442688508517790\n",
      "Iteration 46815 => Loss: 6.69855437614538296032\n",
      "Iteration 46816 => Loss: 6.69855432541251438749\n",
      "Iteration 46817 => Loss: 6.69855427468648301215\n",
      "Iteration 46818 => Loss: 6.69855422396728616974\n",
      "Iteration 46819 => Loss: 6.69855417325492741298\n",
      "Iteration 46820 => Loss: 6.69855412254939430738\n",
      "Iteration 46821 => Loss: 6.69855407185069751108\n",
      "Iteration 46822 => Loss: 6.69855402115883613590\n",
      "Iteration 46823 => Loss: 6.69855397047380041187\n",
      "Iteration 46824 => Loss: 6.69855391979560188531\n",
      "Iteration 46825 => Loss: 6.69855386912423078627\n",
      "Iteration 46826 => Loss: 6.69855381845968000931\n",
      "Iteration 46827 => Loss: 6.69855376780196021258\n",
      "Iteration 46828 => Loss: 6.69855371715107139607\n",
      "Iteration 46829 => Loss: 6.69855366650700556619\n",
      "Iteration 46830 => Loss: 6.69855361586975917021\n",
      "Iteration 46831 => Loss: 6.69855356523933309632\n",
      "Iteration 46832 => Loss: 6.69855351461573977900\n",
      "Iteration 46833 => Loss: 6.69855346399896411924\n",
      "Iteration 46834 => Loss: 6.69855341338900611703\n",
      "Iteration 46835 => Loss: 6.69855336278587021326\n",
      "Iteration 46836 => Loss: 6.69855331218955107886\n",
      "Iteration 46837 => Loss: 6.69855326160005493108\n",
      "Iteration 46838 => Loss: 6.69855321101737199996\n",
      "Iteration 46839 => Loss: 6.69855316044150672639\n",
      "Iteration 46840 => Loss: 6.69855310987245733401\n",
      "Iteration 46841 => Loss: 6.69855305931022204646\n",
      "Iteration 46842 => Loss: 6.69855300875480175193\n",
      "Iteration 46843 => Loss: 6.69855295820618490410\n",
      "Iteration 46844 => Loss: 6.69855290766438393746\n",
      "Iteration 46845 => Loss: 6.69855285712938908205\n",
      "Iteration 46846 => Loss: 6.69855280660120921965\n",
      "Iteration 46847 => Loss: 6.69855275607984612662\n",
      "Iteration 46848 => Loss: 6.69855270556527671033\n",
      "Iteration 46849 => Loss: 6.69855265505752228705\n",
      "Iteration 46850 => Loss: 6.69855260455657042229\n",
      "Iteration 46851 => Loss: 6.69855255406242200422\n",
      "Iteration 46852 => Loss: 6.69855250357508413828\n",
      "Iteration 46853 => Loss: 6.69855245309454350178\n",
      "Iteration 46854 => Loss: 6.69855240262081341740\n",
      "Iteration 46855 => Loss: 6.69855235215386990433\n",
      "Iteration 46856 => Loss: 6.69855230169373871973\n",
      "Iteration 46857 => Loss: 6.69855225124040565277\n",
      "Iteration 46858 => Loss: 6.69855220079386626253\n",
      "Iteration 46859 => Loss: 6.69855215035413298352\n",
      "Iteration 46860 => Loss: 6.69855209992119071671\n",
      "Iteration 46861 => Loss: 6.69855204949504123846\n",
      "Iteration 46862 => Loss: 6.69855199907569165418\n",
      "Iteration 46863 => Loss: 6.69855194866313308211\n",
      "Iteration 46864 => Loss: 6.69855189825736996312\n",
      "Iteration 46865 => Loss: 6.69855184785839607997\n",
      "Iteration 46866 => Loss: 6.69855179746621498538\n",
      "Iteration 46867 => Loss: 6.69855174708082490298\n",
      "Iteration 46868 => Loss: 6.69855169670222405642\n",
      "Iteration 46869 => Loss: 6.69855164633041066935\n",
      "Iteration 46870 => Loss: 6.69855159596538385358\n",
      "Iteration 46871 => Loss: 6.69855154560715071455\n",
      "Iteration 46872 => Loss: 6.69855149525569881774\n",
      "Iteration 46873 => Loss: 6.69855144491103171589\n",
      "Iteration 46874 => Loss: 6.69855139457314763263\n",
      "Iteration 46875 => Loss: 6.69855134424204479160\n",
      "Iteration 46876 => Loss: 6.69855129391773207459\n",
      "Iteration 46877 => Loss: 6.69855124360019438257\n",
      "Iteration 46878 => Loss: 6.69855119328942993917\n",
      "Iteration 46879 => Loss: 6.69855114298546094886\n",
      "Iteration 46880 => Loss: 6.69855109268826165447\n",
      "Iteration 46881 => Loss: 6.69855104239783472053\n",
      "Iteration 46882 => Loss: 6.69855099211419258154\n",
      "Iteration 46883 => Loss: 6.69855094183732280300\n",
      "Iteration 46884 => Loss: 6.69855089156722360855\n",
      "Iteration 46885 => Loss: 6.69855084130391009722\n",
      "Iteration 46886 => Loss: 6.69855079104736184092\n",
      "Iteration 46887 => Loss: 6.69855074079758949779\n",
      "Iteration 46888 => Loss: 6.69855069055458418603\n",
      "Iteration 46889 => Loss: 6.69855064031834768201\n",
      "Iteration 46890 => Loss: 6.69855059008888620298\n",
      "Iteration 46891 => Loss: 6.69855053986618997897\n",
      "Iteration 46892 => Loss: 6.69855048965026078633\n",
      "Iteration 46893 => Loss: 6.69855043944109773690\n",
      "Iteration 46894 => Loss: 6.69855038923870349521\n",
      "Iteration 46895 => Loss: 6.69855033904307006765\n",
      "Iteration 46896 => Loss: 6.69855028885420278328\n",
      "Iteration 46897 => Loss: 6.69855023867209808941\n",
      "Iteration 46898 => Loss: 6.69855018849675865056\n",
      "Iteration 46899 => Loss: 6.69855013832817380859\n",
      "Iteration 46900 => Loss: 6.69855008816634889257\n",
      "Iteration 46901 => Loss: 6.69855003801129278429\n",
      "Iteration 46902 => Loss: 6.69854998786298505564\n",
      "Iteration 46903 => Loss: 6.69854993772143902930\n",
      "Iteration 46904 => Loss: 6.69854988758665292892\n",
      "Iteration 46905 => Loss: 6.69854983745861520816\n",
      "Iteration 46906 => Loss: 6.69854978733733652518\n",
      "Iteration 46907 => Loss: 6.69854973722281599180\n",
      "Iteration 46908 => Loss: 6.69854968711503939716\n",
      "Iteration 46909 => Loss: 6.69854963701401917575\n",
      "Iteration 46910 => Loss: 6.69854958691975266305\n",
      "Iteration 46911 => Loss: 6.69854953683223541816\n",
      "Iteration 46912 => Loss: 6.69854948675146388837\n",
      "Iteration 46913 => Loss: 6.69854943667743718549\n",
      "Iteration 46914 => Loss: 6.69854938661016952040\n",
      "Iteration 46915 => Loss: 6.69854933654964135314\n",
      "Iteration 46916 => Loss: 6.69854928649585978917\n",
      "Iteration 46917 => Loss: 6.69854923644882216394\n",
      "Iteration 46918 => Loss: 6.69854918640852936562\n",
      "Iteration 46919 => Loss: 6.69854913637497695333\n",
      "Iteration 46920 => Loss: 6.69854908634816581525\n",
      "Iteration 46921 => Loss: 6.69854903632810128045\n",
      "Iteration 46922 => Loss: 6.69854898631477535531\n",
      "Iteration 46923 => Loss: 6.69854893630818626349\n",
      "Iteration 46924 => Loss: 6.69854888630833933405\n",
      "Iteration 46925 => Loss: 6.69854883631523101428\n",
      "Iteration 46926 => Loss: 6.69854878632885153422\n",
      "Iteration 46927 => Loss: 6.69854873634921244019\n",
      "Iteration 46928 => Loss: 6.69854868637630929129\n",
      "Iteration 46929 => Loss: 6.69854863641013942299\n",
      "Iteration 46930 => Loss: 6.69854858645070017076\n",
      "Iteration 46931 => Loss: 6.69854853649799508730\n",
      "Iteration 46932 => Loss: 6.69854848655202328445\n",
      "Iteration 46933 => Loss: 6.69854843661277943312\n",
      "Iteration 46934 => Loss: 6.69854838668026264514\n",
      "Iteration 46935 => Loss: 6.69854833675447913777\n",
      "Iteration 46936 => Loss: 6.69854828683541825285\n",
      "Iteration 46937 => Loss: 6.69854823692307910221\n",
      "Iteration 46938 => Loss: 6.69854818701747678489\n",
      "Iteration 46939 => Loss: 6.69854813711860064274\n",
      "Iteration 46940 => Loss: 6.69854808722643735308\n",
      "Iteration 46941 => Loss: 6.69854803734100023860\n",
      "Iteration 46942 => Loss: 6.69854798746229018747\n",
      "Iteration 46943 => Loss: 6.69854793759029387701\n",
      "Iteration 46944 => Loss: 6.69854788772502640626\n",
      "Iteration 46945 => Loss: 6.69854783786647178800\n",
      "Iteration 46946 => Loss: 6.69854778801463979221\n",
      "Iteration 46947 => Loss: 6.69854773816952153709\n",
      "Iteration 46948 => Loss: 6.69854768833112323989\n",
      "Iteration 46949 => Loss: 6.69854763849944223608\n",
      "Iteration 46950 => Loss: 6.69854758867446964388\n",
      "Iteration 46951 => Loss: 6.69854753885621523324\n",
      "Iteration 46952 => Loss: 6.69854748904467278692\n",
      "Iteration 46953 => Loss: 6.69854743923983786402\n",
      "Iteration 46954 => Loss: 6.69854738944172378723\n",
      "Iteration 46955 => Loss: 6.69854733965031012843\n",
      "Iteration 46956 => Loss: 6.69854728986561376303\n",
      "Iteration 46957 => Loss: 6.69854724008762225651\n",
      "Iteration 46958 => Loss: 6.69854719031634271431\n",
      "Iteration 46959 => Loss: 6.69854714055176625465\n",
      "Iteration 46960 => Loss: 6.69854709079389287751\n",
      "Iteration 46961 => Loss: 6.69854704104272791199\n",
      "Iteration 46962 => Loss: 6.69854699129826602899\n",
      "Iteration 46963 => Loss: 6.69854694156050989307\n",
      "Iteration 46964 => Loss: 6.69854689182945151060\n",
      "Iteration 46965 => Loss: 6.69854684210509088160\n",
      "Iteration 46966 => Loss: 6.69854679238743866421\n",
      "Iteration 46967 => Loss: 6.69854674267648508845\n",
      "Iteration 46968 => Loss: 6.69854669297223015434\n",
      "Iteration 46969 => Loss: 6.69854664327467119733\n",
      "Iteration 46970 => Loss: 6.69854659358380377654\n",
      "Iteration 46971 => Loss: 6.69854654389964032646\n",
      "Iteration 46972 => Loss: 6.69854649422217196530\n",
      "Iteration 46973 => Loss: 6.69854644455139069947\n",
      "Iteration 46974 => Loss: 6.69854639488730452257\n",
      "Iteration 46975 => Loss: 6.69854634522991698731\n",
      "Iteration 46976 => Loss: 6.69854629557921388283\n",
      "Iteration 46977 => Loss: 6.69854624593520586728\n",
      "Iteration 46978 => Loss: 6.69854619629787872981\n",
      "Iteration 46979 => Loss: 6.69854614666725023397\n",
      "Iteration 46980 => Loss: 6.69854609704330528075\n",
      "Iteration 46981 => Loss: 6.69854604742604653467\n",
      "Iteration 46982 => Loss: 6.69854599781547221937\n",
      "Iteration 46983 => Loss: 6.69854594821158766393\n",
      "Iteration 46984 => Loss: 6.69854589861438842746\n",
      "Iteration 46985 => Loss: 6.69854584902387006906\n",
      "Iteration 46986 => Loss: 6.69854579944003258873\n",
      "Iteration 46987 => Loss: 6.69854574986287332194\n",
      "Iteration 46988 => Loss: 6.69854570029239759776\n",
      "Iteration 46989 => Loss: 6.69854565072860275166\n",
      "Iteration 46990 => Loss: 6.69854560117148789544\n",
      "Iteration 46991 => Loss: 6.69854555162104770005\n",
      "Iteration 46992 => Loss: 6.69854550207728305367\n",
      "Iteration 46993 => Loss: 6.69854545254019928535\n",
      "Iteration 46994 => Loss: 6.69854540300978928968\n",
      "Iteration 46995 => Loss: 6.69854535348604862577\n",
      "Iteration 46996 => Loss: 6.69854530396898795175\n",
      "Iteration 46997 => Loss: 6.69854525445859483312\n",
      "Iteration 46998 => Loss: 6.69854520495487193443\n",
      "Iteration 46999 => Loss: 6.69854515545782813746\n",
      "Iteration 47000 => Loss: 6.69854510596744923134\n",
      "Iteration 47001 => Loss: 6.69854505648373788063\n",
      "Iteration 47002 => Loss: 6.69854500700669674984\n",
      "Iteration 47003 => Loss: 6.69854495753632317445\n",
      "Iteration 47004 => Loss: 6.69854490807261004903\n",
      "Iteration 47005 => Loss: 6.69854485861556891990\n",
      "Iteration 47006 => Loss: 6.69854480916518912892\n",
      "Iteration 47007 => Loss: 6.69854475972146978791\n",
      "Iteration 47008 => Loss: 6.69854471028441356140\n",
      "Iteration 47009 => Loss: 6.69854466085402666664\n",
      "Iteration 47010 => Loss: 6.69854461143029489278\n",
      "Iteration 47011 => Loss: 6.69854456201322268072\n",
      "Iteration 47012 => Loss: 6.69854451260280647773\n",
      "Iteration 47013 => Loss: 6.69854446319905960650\n",
      "Iteration 47014 => Loss: 6.69854441380195897437\n",
      "Iteration 47015 => Loss: 6.69854436441151968040\n",
      "Iteration 47016 => Loss: 6.69854431502773195461\n",
      "Iteration 47017 => Loss: 6.69854426565060112608\n",
      "Iteration 47018 => Loss: 6.69854421628012275391\n",
      "Iteration 47019 => Loss: 6.69854416691629772629\n",
      "Iteration 47020 => Loss: 6.69854411755912604320\n",
      "Iteration 47021 => Loss: 6.69854406820860415195\n",
      "Iteration 47022 => Loss: 6.69854401886472494709\n",
      "Iteration 47023 => Loss: 6.69854396952751063310\n",
      "Iteration 47024 => Loss: 6.69854392019693456461\n",
      "Iteration 47025 => Loss: 6.69854387087300562342\n",
      "Iteration 47026 => Loss: 6.69854382155572647406\n",
      "Iteration 47027 => Loss: 6.69854377224508734656\n",
      "Iteration 47028 => Loss: 6.69854372294110156361\n",
      "Iteration 47029 => Loss: 6.69854367364374958527\n",
      "Iteration 47030 => Loss: 6.69854362435304206969\n",
      "Iteration 47031 => Loss: 6.69854357506898345775\n",
      "Iteration 47032 => Loss: 6.69854352579155776226\n",
      "Iteration 47033 => Loss: 6.69854347652077830588\n",
      "Iteration 47034 => Loss: 6.69854342725663354230\n",
      "Iteration 47035 => Loss: 6.69854337799913235330\n",
      "Iteration 47036 => Loss: 6.69854332874826852162\n",
      "Iteration 47037 => Loss: 6.69854327950404027092\n",
      "Iteration 47038 => Loss: 6.69854323026644404848\n",
      "Iteration 47039 => Loss: 6.69854318103547807794\n",
      "Iteration 47040 => Loss: 6.69854313181115834652\n",
      "Iteration 47041 => Loss: 6.69854308259346886700\n",
      "Iteration 47042 => Loss: 6.69854303338240164578\n",
      "Iteration 47043 => Loss: 6.69854298417797622278\n",
      "Iteration 47044 => Loss: 6.69854293498018105169\n",
      "Iteration 47045 => Loss: 6.69854288578900902706\n",
      "Iteration 47046 => Loss: 6.69854283660446547799\n",
      "Iteration 47047 => Loss: 6.69854278742655129264\n",
      "Iteration 47048 => Loss: 6.69854273825526469466\n",
      "Iteration 47049 => Loss: 6.69854268909060746040\n",
      "Iteration 47050 => Loss: 6.69854263993257248444\n",
      "Iteration 47051 => Loss: 6.69854259078115532589\n",
      "Iteration 47052 => Loss: 6.69854254163636930741\n",
      "Iteration 47053 => Loss: 6.69854249249820554724\n",
      "Iteration 47054 => Loss: 6.69854244336665249904\n",
      "Iteration 47055 => Loss: 6.69854239424173325546\n",
      "Iteration 47056 => Loss: 6.69854234512343094110\n",
      "Iteration 47057 => Loss: 6.69854229601174200326\n",
      "Iteration 47058 => Loss: 6.69854224690667354736\n",
      "Iteration 47059 => Loss: 6.69854219780822202068\n",
      "Iteration 47060 => Loss: 6.69854214871638831141\n",
      "Iteration 47061 => Loss: 6.69854209963116531412\n",
      "Iteration 47062 => Loss: 6.69854205055256279877\n",
      "Iteration 47063 => Loss: 6.69854200148056477815\n",
      "Iteration 47064 => Loss: 6.69854195241518457493\n",
      "Iteration 47065 => Loss: 6.69854190335641686005\n",
      "Iteration 47066 => Loss: 6.69854185430425630443\n",
      "Iteration 47067 => Loss: 6.69854180525871090168\n",
      "Iteration 47068 => Loss: 6.69854175621977265820\n",
      "Iteration 47069 => Loss: 6.69854170718743624491\n",
      "Iteration 47070 => Loss: 6.69854165816171320813\n",
      "Iteration 47071 => Loss: 6.69854160914259644244\n",
      "Iteration 47072 => Loss: 6.69854156013008683601\n",
      "Iteration 47073 => Loss: 6.69854151112417817160\n",
      "Iteration 47074 => Loss: 6.69854146212487044920\n",
      "Iteration 47075 => Loss: 6.69854141313216988607\n",
      "Iteration 47076 => Loss: 6.69854136414606937677\n",
      "Iteration 47077 => Loss: 6.69854131516656714496\n",
      "Iteration 47078 => Loss: 6.69854126619367118423\n",
      "Iteration 47079 => Loss: 6.69854121722736195466\n",
      "Iteration 47080 => Loss: 6.69854116826766876613\n",
      "Iteration 47081 => Loss: 6.69854111931456053242\n",
      "Iteration 47082 => Loss: 6.69854107036805324071\n",
      "Iteration 47083 => Loss: 6.69854102142813800924\n",
      "Iteration 47084 => Loss: 6.69854097249482105525\n",
      "Iteration 47085 => Loss: 6.69854092356809438513\n",
      "Iteration 47086 => Loss: 6.69854087464796243978\n",
      "Iteration 47087 => Loss: 6.69854082573442610737\n",
      "Iteration 47088 => Loss: 6.69854077682747739431\n",
      "Iteration 47089 => Loss: 6.69854072792712251783\n",
      "Iteration 47090 => Loss: 6.69854067903334904344\n",
      "Iteration 47091 => Loss: 6.69854063014616851746\n",
      "Iteration 47092 => Loss: 6.69854058126557649899\n",
      "Iteration 47093 => Loss: 6.69854053239157565258\n",
      "Iteration 47094 => Loss: 6.69854048352415532008\n",
      "Iteration 47095 => Loss: 6.69854043466331461332\n",
      "Iteration 47096 => Loss: 6.69854038580905708500\n",
      "Iteration 47097 => Loss: 6.69854033696139516962\n",
      "Iteration 47098 => Loss: 6.69854028812031287998\n",
      "Iteration 47099 => Loss: 6.69854023928580311065\n",
      "Iteration 47100 => Loss: 6.69854019045787918429\n",
      "Iteration 47101 => Loss: 6.69854014163652955460\n",
      "Iteration 47102 => Loss: 6.69854009282176665607\n",
      "Iteration 47103 => Loss: 6.69854004401357983056\n",
      "Iteration 47104 => Loss: 6.69853999521196818989\n",
      "Iteration 47105 => Loss: 6.69853994641692729317\n",
      "Iteration 47106 => Loss: 6.69853989762847490397\n",
      "Iteration 47107 => Loss: 6.69853984884658260057\n",
      "Iteration 47108 => Loss: 6.69853980007127169927\n",
      "Iteration 47109 => Loss: 6.69853975130253420645\n",
      "Iteration 47110 => Loss: 6.69853970254036212850\n",
      "Iteration 47111 => Loss: 6.69853965378476612358\n",
      "Iteration 47112 => Loss: 6.69853960503573464536\n",
      "Iteration 47113 => Loss: 6.69853955629327302290\n",
      "Iteration 47114 => Loss: 6.69853950755738214440\n",
      "Iteration 47115 => Loss: 6.69853945882806378620\n",
      "Iteration 47116 => Loss: 6.69853941010530373745\n",
      "Iteration 47117 => Loss: 6.69853936138911443265\n",
      "Iteration 47118 => Loss: 6.69853931267948254913\n",
      "Iteration 47119 => Loss: 6.69853926397642318591\n",
      "Iteration 47120 => Loss: 6.69853921527991857943\n",
      "Iteration 47121 => Loss: 6.69853916658997583511\n",
      "Iteration 47122 => Loss: 6.69853911790659672931\n",
      "Iteration 47123 => Loss: 6.69853906922977682115\n",
      "Iteration 47124 => Loss: 6.69853902055952055150\n",
      "Iteration 47125 => Loss: 6.69853897189581726224\n",
      "Iteration 47126 => Loss: 6.69853892323867761149\n",
      "Iteration 47127 => Loss: 6.69853887458808650024\n",
      "Iteration 47128 => Loss: 6.69853882594405991568\n",
      "Iteration 47129 => Loss: 6.69853877730658364698\n",
      "Iteration 47130 => Loss: 6.69853872867565947047\n",
      "Iteration 47131 => Loss: 6.69853868005129182706\n",
      "Iteration 47132 => Loss: 6.69853863143346917042\n",
      "Iteration 47133 => Loss: 6.69853858282220304687\n",
      "Iteration 47134 => Loss: 6.69853853421749168007\n",
      "Iteration 47135 => Loss: 6.69853848561932263550\n",
      "Iteration 47136 => Loss: 6.69853843702770390678\n",
      "Iteration 47137 => Loss: 6.69853838844263460572\n",
      "Iteration 47138 => Loss: 6.69853833986411206780\n",
      "Iteration 47139 => Loss: 6.69853829129213185212\n",
      "Iteration 47140 => Loss: 6.69853824272670284046\n",
      "Iteration 47141 => Loss: 6.69853819416781171014\n",
      "Iteration 47142 => Loss: 6.69853814561546823114\n",
      "Iteration 47143 => Loss: 6.69853809706966618620\n",
      "Iteration 47144 => Loss: 6.69853804853040646350\n",
      "Iteration 47145 => Loss: 6.69853799999768373397\n",
      "Iteration 47146 => Loss: 6.69853795147149977396\n",
      "Iteration 47147 => Loss: 6.69853790295186080073\n",
      "Iteration 47148 => Loss: 6.69853785443876059702\n",
      "Iteration 47149 => Loss: 6.69853780593219561013\n",
      "Iteration 47150 => Loss: 6.69853775743216139915\n",
      "Iteration 47151 => Loss: 6.69853770893866951042\n",
      "Iteration 47152 => Loss: 6.69853766045171017396\n",
      "Iteration 47153 => Loss: 6.69853761197128516613\n",
      "Iteration 47154 => Loss: 6.69853756349739093423\n",
      "Iteration 47155 => Loss: 6.69853751503002481371\n",
      "Iteration 47156 => Loss: 6.69853746656919746272\n",
      "Iteration 47157 => Loss: 6.69853741811489644675\n",
      "Iteration 47158 => Loss: 6.69853736966712531853\n",
      "Iteration 47159 => Loss: 6.69853732122587697262\n",
      "Iteration 47160 => Loss: 6.69853727279116650806\n",
      "Iteration 47161 => Loss: 6.69853722436297793763\n",
      "Iteration 47162 => Loss: 6.69853717594131126134\n",
      "Iteration 47163 => Loss: 6.69853712752617180826\n",
      "Iteration 47164 => Loss: 6.69853707911755869020\n",
      "Iteration 47165 => Loss: 6.69853703071546746628\n",
      "Iteration 47166 => Loss: 6.69853698231989724832\n",
      "Iteration 47167 => Loss: 6.69853693393085158903\n",
      "Iteration 47168 => Loss: 6.69853688554832515933\n",
      "Iteration 47169 => Loss: 6.69853683717231618289\n",
      "Iteration 47170 => Loss: 6.69853678880282377150\n",
      "Iteration 47171 => Loss: 6.69853674043985325426\n",
      "Iteration 47172 => Loss: 6.69853669208339486119\n",
      "Iteration 47173 => Loss: 6.69853664373345925043\n",
      "Iteration 47174 => Loss: 6.69853659539003665202\n",
      "Iteration 47175 => Loss: 6.69853654705312973050\n",
      "Iteration 47176 => Loss: 6.69853649872273226862\n",
      "Iteration 47177 => Loss: 6.69853645039884693091\n",
      "Iteration 47178 => Loss: 6.69853640208147727009\n",
      "Iteration 47179 => Loss: 6.69853635377061618073\n",
      "Iteration 47180 => Loss: 6.69853630546626810371\n",
      "Iteration 47181 => Loss: 6.69853625716842238091\n",
      "Iteration 47182 => Loss: 6.69853620887708611775\n",
      "Iteration 47183 => Loss: 6.69853616059226020241\n",
      "Iteration 47184 => Loss: 6.69853611231394197034\n",
      "Iteration 47185 => Loss: 6.69853606404212786885\n",
      "Iteration 47186 => Loss: 6.69853601577682145063\n",
      "Iteration 47187 => Loss: 6.69853596751800761666\n",
      "Iteration 47188 => Loss: 6.69853591926571301229\n",
      "Iteration 47189 => Loss: 6.69853587101990743946\n",
      "Iteration 47190 => Loss: 6.69853582278060422084\n",
      "Iteration 47191 => Loss: 6.69853577454780513278\n",
      "Iteration 47192 => Loss: 6.69853572632150395805\n",
      "Iteration 47193 => Loss: 6.69853567810170424934\n",
      "Iteration 47194 => Loss: 6.69853562988839890124\n",
      "Iteration 47195 => Loss: 6.69853558168158613739\n",
      "Iteration 47196 => Loss: 6.69853553348127572775\n",
      "Iteration 47197 => Loss: 6.69853548528746145507\n",
      "Iteration 47198 => Loss: 6.69853543710013088486\n",
      "Iteration 47199 => Loss: 6.69853538891930444521\n",
      "Iteration 47200 => Loss: 6.69853534074496970163\n",
      "Iteration 47201 => Loss: 6.69853529257712398959\n",
      "Iteration 47202 => Loss: 6.69853524441576642090\n",
      "Iteration 47203 => Loss: 6.69853519626090321282\n",
      "Iteration 47204 => Loss: 6.69853514811252992445\n",
      "Iteration 47205 => Loss: 6.69853509997063767401\n",
      "Iteration 47206 => Loss: 6.69853505183523356692\n",
      "Iteration 47207 => Loss: 6.69853500370632470862\n",
      "Iteration 47208 => Loss: 6.69853495558389244735\n",
      "Iteration 47209 => Loss: 6.69853490746794566491\n",
      "Iteration 47210 => Loss: 6.69853485935848258492\n",
      "Iteration 47211 => Loss: 6.69853481125550853648\n",
      "Iteration 47212 => Loss: 6.69853476315900930871\n",
      "Iteration 47213 => Loss: 6.69853471506899200705\n",
      "Iteration 47214 => Loss: 6.69853466698545485514\n",
      "Iteration 47215 => Loss: 6.69853461890840140569\n",
      "Iteration 47216 => Loss: 6.69853457083781744785\n",
      "Iteration 47217 => Loss: 6.69853452277371896884\n",
      "Iteration 47218 => Loss: 6.69853447471609797503\n",
      "Iteration 47219 => Loss: 6.69853442666494913738\n",
      "Iteration 47220 => Loss: 6.69853437862027245586\n",
      "Iteration 47221 => Loss: 6.69853433058207770046\n",
      "Iteration 47222 => Loss: 6.69853428255034533123\n",
      "Iteration 47223 => Loss: 6.69853423452509044722\n",
      "Iteration 47224 => Loss: 6.69853418650631038389\n",
      "Iteration 47225 => Loss: 6.69853413849399981217\n",
      "Iteration 47226 => Loss: 6.69853409048815517934\n",
      "Iteration 47227 => Loss: 6.69853404248878447902\n",
      "Iteration 47228 => Loss: 6.69853399449587794123\n",
      "Iteration 47229 => Loss: 6.69853394650944178323\n",
      "Iteration 47230 => Loss: 6.69853389852946357053\n",
      "Iteration 47231 => Loss: 6.69853385055596017850\n",
      "Iteration 47232 => Loss: 6.69853380258891739629\n",
      "Iteration 47233 => Loss: 6.69853375462834321752\n",
      "Iteration 47234 => Loss: 6.69853370667422520768\n",
      "Iteration 47235 => Loss: 6.69853365872656514313\n",
      "Iteration 47236 => Loss: 6.69853361078537812290\n",
      "Iteration 47237 => Loss: 6.69853356285063927800\n",
      "Iteration 47238 => Loss: 6.69853351492236637199\n",
      "Iteration 47239 => Loss: 6.69853346700054697038\n",
      "Iteration 47240 => Loss: 6.69853341908519261949\n",
      "Iteration 47241 => Loss: 6.69853337117628822028\n",
      "Iteration 47242 => Loss: 6.69853332327384709544\n",
      "Iteration 47243 => Loss: 6.69853327537785148138\n",
      "Iteration 47244 => Loss: 6.69853322748832002986\n",
      "Iteration 47245 => Loss: 6.69853317960523231278\n",
      "Iteration 47246 => Loss: 6.69853313172859632374\n",
      "Iteration 47247 => Loss: 6.69853308385842005634\n",
      "Iteration 47248 => Loss: 6.69853303599468663521\n",
      "Iteration 47249 => Loss: 6.69853298813740583029\n",
      "Iteration 47250 => Loss: 6.69853294028657852976\n",
      "Iteration 47251 => Loss: 6.69853289244219141096\n",
      "Iteration 47252 => Loss: 6.69853284460424980296\n",
      "Iteration 47253 => Loss: 6.69853279677276880477\n",
      "Iteration 47254 => Loss: 6.69853274894771821835\n",
      "Iteration 47255 => Loss: 6.69853270112911935996\n",
      "Iteration 47256 => Loss: 6.69853265331695890694\n",
      "Iteration 47257 => Loss: 6.69853260551125018196\n",
      "Iteration 47258 => Loss: 6.69853255771197009238\n",
      "Iteration 47259 => Loss: 6.69853250991914528356\n",
      "Iteration 47260 => Loss: 6.69853246213275177467\n",
      "Iteration 47261 => Loss: 6.69853241435279400662\n",
      "Iteration 47262 => Loss: 6.69853236657928352571\n",
      "Iteration 47263 => Loss: 6.69853231881220434474\n",
      "Iteration 47264 => Loss: 6.69853227105156534549\n",
      "Iteration 47265 => Loss: 6.69853222329736119889\n",
      "Iteration 47266 => Loss: 6.69853217554959279312\n",
      "Iteration 47267 => Loss: 6.69853212780825302275\n",
      "Iteration 47268 => Loss: 6.69853208007335876317\n",
      "Iteration 47269 => Loss: 6.69853203234489047446\n",
      "Iteration 47270 => Loss: 6.69853198462284460390\n",
      "Iteration 47271 => Loss: 6.69853193690724246778\n",
      "Iteration 47272 => Loss: 6.69853188919805919710\n",
      "Iteration 47273 => Loss: 6.69853184149530633817\n",
      "Iteration 47274 => Loss: 6.69853179379898744372\n",
      "Iteration 47275 => Loss: 6.69853174610908652653\n",
      "Iteration 47276 => Loss: 6.69853169842562579106\n",
      "Iteration 47277 => Loss: 6.69853165074858836192\n",
      "Iteration 47278 => Loss: 6.69853160307796180462\n",
      "Iteration 47279 => Loss: 6.69853155541376832360\n",
      "Iteration 47280 => Loss: 6.69853150775599814892\n",
      "Iteration 47281 => Loss: 6.69853146010464417515\n",
      "Iteration 47282 => Loss: 6.69853141245971617224\n",
      "Iteration 47283 => Loss: 6.69853136482120525841\n",
      "Iteration 47284 => Loss: 6.69853131718911409820\n",
      "Iteration 47285 => Loss: 6.69853126956344624432\n",
      "Iteration 47286 => Loss: 6.69853122194419192681\n",
      "Iteration 47287 => Loss: 6.69853117433135292202\n",
      "Iteration 47288 => Loss: 6.69853112672493633539\n",
      "Iteration 47289 => Loss: 6.69853107912492440335\n",
      "Iteration 47290 => Loss: 6.69853103153133933034\n",
      "Iteration 47291 => Loss: 6.69853098394415624739\n",
      "Iteration 47292 => Loss: 6.69853093636339202988\n",
      "Iteration 47293 => Loss: 6.69853088878903424330\n",
      "Iteration 47294 => Loss: 6.69853084122109354581\n",
      "Iteration 47295 => Loss: 6.69853079365955395019\n",
      "Iteration 47296 => Loss: 6.69853074610443499637\n",
      "Iteration 47297 => Loss: 6.69853069855571092717\n",
      "Iteration 47298 => Loss: 6.69853065101340483523\n",
      "Iteration 47299 => Loss: 6.69853060347749895698\n",
      "Iteration 47300 => Loss: 6.69853055594799950967\n",
      "Iteration 47301 => Loss: 6.69853050842490560512\n",
      "Iteration 47302 => Loss: 6.69853046090821813152\n",
      "Iteration 47303 => Loss: 6.69853041339792465436\n",
      "Iteration 47304 => Loss: 6.69853036589404293721\n",
      "Iteration 47305 => Loss: 6.69853031839655788104\n",
      "Iteration 47306 => Loss: 6.69853027090547392675\n",
      "Iteration 47307 => Loss: 6.69853022342079373885\n",
      "Iteration 47308 => Loss: 6.69853017594250488287\n",
      "Iteration 47309 => Loss: 6.69853012847061624058\n",
      "Iteration 47310 => Loss: 6.69853008100512870016\n",
      "Iteration 47311 => Loss: 6.69853003354603249164\n",
      "Iteration 47312 => Loss: 6.69852998609332939139\n",
      "Iteration 47313 => Loss: 6.69852993864702384030\n",
      "Iteration 47314 => Loss: 6.69852989120711050930\n",
      "Iteration 47315 => Loss: 6.69852984377358673385\n",
      "Iteration 47316 => Loss: 6.69852979634646317209\n",
      "Iteration 47317 => Loss: 6.69852974892572294863\n",
      "Iteration 47318 => Loss: 6.69852970151137583343\n",
      "Iteration 47319 => Loss: 6.69852965410341916197\n",
      "Iteration 47320 => Loss: 6.69852960670184494063\n",
      "Iteration 47321 => Loss: 6.69852955930666826845\n",
      "Iteration 47322 => Loss: 6.69852951191786605278\n",
      "Iteration 47323 => Loss: 6.69852946453545783356\n",
      "Iteration 47324 => Loss: 6.69852941715943206447\n",
      "Iteration 47325 => Loss: 6.69852936978978785731\n",
      "Iteration 47326 => Loss: 6.69852932242653142936\n",
      "Iteration 47327 => Loss: 6.69852927506965389881\n",
      "Iteration 47328 => Loss: 6.69852922771915793021\n",
      "Iteration 47329 => Loss: 6.69852918037504263538\n",
      "Iteration 47330 => Loss: 6.69852913303730534977\n",
      "Iteration 47331 => Loss: 6.69852908570594873794\n",
      "Iteration 47332 => Loss: 6.69852903838097279987\n",
      "Iteration 47333 => Loss: 6.69852899106236865379\n",
      "Iteration 47334 => Loss: 6.69852894375014518147\n",
      "Iteration 47335 => Loss: 6.69852889644428994842\n",
      "Iteration 47336 => Loss: 6.69852884914482071821\n",
      "Iteration 47337 => Loss: 6.69852880185171351002\n",
      "Iteration 47338 => Loss: 6.69852875456498608742\n",
      "Iteration 47339 => Loss: 6.69852870728463578587\n",
      "Iteration 47340 => Loss: 6.69852866001064750634\n",
      "Iteration 47341 => Loss: 6.69852861274302391337\n",
      "Iteration 47342 => Loss: 6.69852856548177744145\n",
      "Iteration 47343 => Loss: 6.69852851822689920880\n",
      "Iteration 47344 => Loss: 6.69852847097838743906\n",
      "Iteration 47345 => Loss: 6.69852842373624479677\n",
      "Iteration 47346 => Loss: 6.69852837650046684104\n",
      "Iteration 47347 => Loss: 6.69852832927105268368\n",
      "Iteration 47348 => Loss: 6.69852828204800587741\n",
      "Iteration 47349 => Loss: 6.69852823483131576410\n",
      "Iteration 47350 => Loss: 6.69852818762099033734\n",
      "Iteration 47351 => Loss: 6.69852814041703670256\n",
      "Iteration 47352 => Loss: 6.69852809321942910259\n",
      "Iteration 47353 => Loss: 6.69852804602819063007\n",
      "Iteration 47354 => Loss: 6.69852799884331151503\n",
      "Iteration 47355 => Loss: 6.69852795166478465205\n",
      "Iteration 47356 => Loss: 6.69852790449261359385\n",
      "Iteration 47357 => Loss: 6.69852785732680633402\n",
      "Iteration 47358 => Loss: 6.69852781016735754349\n",
      "Iteration 47359 => Loss: 6.69852776301425567596\n",
      "Iteration 47360 => Loss: 6.69852771586750783683\n",
      "Iteration 47361 => Loss: 6.69852766872711491430\n",
      "Iteration 47362 => Loss: 6.69852762159307602019\n",
      "Iteration 47363 => Loss: 6.69852757446538316088\n",
      "Iteration 47364 => Loss: 6.69852752734404965906\n",
      "Iteration 47365 => Loss: 6.69852748022905775116\n",
      "Iteration 47366 => Loss: 6.69852743312041543078\n",
      "Iteration 47367 => Loss: 6.69852738601812536245\n",
      "Iteration 47368 => Loss: 6.69852733892217688805\n",
      "Iteration 47369 => Loss: 6.69852729183257622481\n",
      "Iteration 47370 => Loss: 6.69852724474932070819\n",
      "Iteration 47371 => Loss: 6.69852719767241033821\n",
      "Iteration 47372 => Loss: 6.69852715060184511486\n",
      "Iteration 47373 => Loss: 6.69852710353762414996\n",
      "Iteration 47374 => Loss: 6.69852705647974566716\n",
      "Iteration 47375 => Loss: 6.69852700942820344920\n",
      "Iteration 47376 => Loss: 6.69852696238300371334\n",
      "Iteration 47377 => Loss: 6.69852691534413757779\n",
      "Iteration 47378 => Loss: 6.69852686831161747705\n",
      "Iteration 47379 => Loss: 6.69852682128543186479\n",
      "Iteration 47380 => Loss: 6.69852677426558518192\n",
      "Iteration 47381 => Loss: 6.69852672725207565207\n",
      "Iteration 47382 => Loss: 6.69852668024489705800\n",
      "Iteration 47383 => Loss: 6.69852663324405295242\n",
      "Iteration 47384 => Loss: 6.69852658624954155897\n",
      "Iteration 47385 => Loss: 6.69852653926136731855\n",
      "Iteration 47386 => Loss: 6.69852649227952490207\n",
      "Iteration 47387 => Loss: 6.69852644530400720413\n",
      "Iteration 47388 => Loss: 6.69852639833482133014\n",
      "Iteration 47389 => Loss: 6.69852635137196816828\n",
      "Iteration 47390 => Loss: 6.69852630441544327766\n",
      "Iteration 47391 => Loss: 6.69852625746524399375\n",
      "Iteration 47392 => Loss: 6.69852621052136765201\n",
      "Iteration 47393 => Loss: 6.69852616358382135786\n",
      "Iteration 47394 => Loss: 6.69852611665259889406\n",
      "Iteration 47395 => Loss: 6.69852606972770026061\n",
      "Iteration 47396 => Loss: 6.69852602280912901023\n",
      "Iteration 47397 => Loss: 6.69852597589687182023\n",
      "Iteration 47398 => Loss: 6.69852592899093846057\n",
      "Iteration 47399 => Loss: 6.69852588209132715491\n",
      "Iteration 47400 => Loss: 6.69852583519804234413\n",
      "Iteration 47401 => Loss: 6.69852578831106715285\n",
      "Iteration 47402 => Loss: 6.69852574143041046284\n",
      "Iteration 47403 => Loss: 6.69852569455606872140\n",
      "Iteration 47404 => Loss: 6.69852564768804992212\n",
      "Iteration 47405 => Loss: 6.69852560082634607141\n",
      "Iteration 47406 => Loss: 6.69852555397095184020\n",
      "Iteration 47407 => Loss: 6.69852550712187788662\n",
      "Iteration 47408 => Loss: 6.69852546027911266435\n",
      "Iteration 47409 => Loss: 6.69852541344266061429\n",
      "Iteration 47410 => Loss: 6.69852536661252084826\n",
      "Iteration 47411 => Loss: 6.69852531978868892537\n",
      "Iteration 47412 => Loss: 6.69852527297117017469\n",
      "Iteration 47413 => Loss: 6.69852522615995926714\n",
      "Iteration 47414 => Loss: 6.69852517935505353819\n",
      "Iteration 47415 => Loss: 6.69852513255645387602\n",
      "Iteration 47416 => Loss: 6.69852508576416294517\n",
      "Iteration 47417 => Loss: 6.69852503897817985745\n",
      "Iteration 47418 => Loss: 6.69852499219849040202\n",
      "Iteration 47419 => Loss: 6.69852494542511589515\n",
      "Iteration 47420 => Loss: 6.69852489865803857327\n",
      "Iteration 47421 => Loss: 6.69852485189727175907\n",
      "Iteration 47422 => Loss: 6.69852480514279058355\n",
      "Iteration 47423 => Loss: 6.69852475839462080387\n",
      "Iteration 47424 => Loss: 6.69852471165274820919\n",
      "Iteration 47425 => Loss: 6.69852466491717279951\n",
      "Iteration 47426 => Loss: 6.69852461818789368664\n",
      "Iteration 47427 => Loss: 6.69852457146491442330\n",
      "Iteration 47428 => Loss: 6.69852452474822879225\n",
      "Iteration 47429 => Loss: 6.69852447803784034619\n",
      "Iteration 47430 => Loss: 6.69852443133374375606\n",
      "Iteration 47431 => Loss: 6.69852438463594612728\n",
      "Iteration 47432 => Loss: 6.69852433794443591353\n",
      "Iteration 47433 => Loss: 6.69852429125921755571\n",
      "Iteration 47434 => Loss: 6.69852424458028661292\n",
      "Iteration 47435 => Loss: 6.69852419790765374330\n",
      "Iteration 47436 => Loss: 6.69852415124130207147\n",
      "Iteration 47437 => Loss: 6.69852410458124758463\n",
      "Iteration 47438 => Loss: 6.69852405792747696012\n",
      "Iteration 47439 => Loss: 6.69852401127998842156\n",
      "Iteration 47440 => Loss: 6.69852396463879440347\n",
      "Iteration 47441 => Loss: 6.69852391800387625409\n",
      "Iteration 47442 => Loss: 6.69852387137524551974\n",
      "Iteration 47443 => Loss: 6.69852382475289509500\n",
      "Iteration 47444 => Loss: 6.69852377813683386165\n",
      "Iteration 47445 => Loss: 6.69852373152704672066\n",
      "Iteration 47446 => Loss: 6.69852368492354344198\n",
      "Iteration 47447 => Loss: 6.69852363832632402563\n",
      "Iteration 47448 => Loss: 6.69852359173538225434\n",
      "Iteration 47449 => Loss: 6.69852354515071457541\n",
      "Iteration 47450 => Loss: 6.69852349857232187702\n",
      "Iteration 47451 => Loss: 6.69852345200021392913\n",
      "Iteration 47452 => Loss: 6.69852340543438007359\n",
      "Iteration 47453 => Loss: 6.69852335887481853405\n",
      "Iteration 47454 => Loss: 6.69852331232152664597\n",
      "Iteration 47455 => Loss: 6.69852326577451329115\n",
      "Iteration 47456 => Loss: 6.69852321923376514690\n",
      "Iteration 47457 => Loss: 6.69852317269929997678\n",
      "Iteration 47458 => Loss: 6.69852312617109291182\n",
      "Iteration 47459 => Loss: 6.69852307964916615646\n",
      "Iteration 47460 => Loss: 6.69852303313350816438\n",
      "Iteration 47461 => Loss: 6.69852298662411271835\n",
      "Iteration 47462 => Loss: 6.69852294012098958831\n",
      "Iteration 47463 => Loss: 6.69852289362412722795\n",
      "Iteration 47464 => Loss: 6.69852284713353007817\n",
      "Iteration 47465 => Loss: 6.69852280064919725078\n",
      "Iteration 47466 => Loss: 6.69852275417113141032\n",
      "Iteration 47467 => Loss: 6.69852270769932811589\n",
      "Iteration 47468 => Loss: 6.69852266123378470297\n",
      "Iteration 47469 => Loss: 6.69852261477450472427\n",
      "Iteration 47470 => Loss: 6.69852256832147840981\n",
      "Iteration 47471 => Loss: 6.69852252187472441136\n",
      "Iteration 47472 => Loss: 6.69852247543422762988\n",
      "Iteration 47473 => Loss: 6.69852242899997385450\n",
      "Iteration 47474 => Loss: 6.69852238257198617788\n",
      "Iteration 47475 => Loss: 6.69852233615025394187\n",
      "Iteration 47476 => Loss: 6.69852228973478336371\n",
      "Iteration 47477 => Loss: 6.69852224332556200892\n",
      "Iteration 47478 => Loss: 6.69852219692259609474\n",
      "Iteration 47479 => Loss: 6.69852215052588118027\n",
      "Iteration 47480 => Loss: 6.69852210413542081824\n",
      "Iteration 47481 => Loss: 6.69852205775120790321\n",
      "Iteration 47482 => Loss: 6.69852201137323977065\n",
      "Iteration 47483 => Loss: 6.69852196500153329595\n",
      "Iteration 47484 => Loss: 6.69852191863606982736\n",
      "Iteration 47485 => Loss: 6.69852187227685291759\n",
      "Iteration 47486 => Loss: 6.69852182592388611937\n",
      "Iteration 47487 => Loss: 6.69852177957716055090\n",
      "Iteration 47488 => Loss: 6.69852173323667976490\n",
      "Iteration 47489 => Loss: 6.69852168690245264315\n",
      "Iteration 47490 => Loss: 6.69852164057446142209\n",
      "Iteration 47491 => Loss: 6.69852159425271231896\n",
      "Iteration 47492 => Loss: 6.69852154793721510373\n",
      "Iteration 47493 => Loss: 6.69852150162794490740\n",
      "Iteration 47494 => Loss: 6.69852145532492571078\n",
      "Iteration 47495 => Loss: 6.69852140902814241485\n",
      "Iteration 47496 => Loss: 6.69852136273759324325\n",
      "Iteration 47497 => Loss: 6.69852131645328352505\n",
      "Iteration 47498 => Loss: 6.69852127017521414842\n",
      "Iteration 47499 => Loss: 6.69852122390337623159\n",
      "Iteration 47500 => Loss: 6.69852117763778132087\n",
      "Iteration 47501 => Loss: 6.69852113137841520540\n",
      "Iteration 47502 => Loss: 6.69852108512527788520\n",
      "Iteration 47503 => Loss: 6.69852103887837380114\n",
      "Iteration 47504 => Loss: 6.69852099263771094684\n",
      "Iteration 47505 => Loss: 6.69852094640327511144\n",
      "Iteration 47506 => Loss: 6.69852090017506807129\n",
      "Iteration 47507 => Loss: 6.69852085395308982640\n",
      "Iteration 47508 => Loss: 6.69852080773734126495\n",
      "Iteration 47509 => Loss: 6.69852076152781439333\n",
      "Iteration 47510 => Loss: 6.69852071532452431057\n",
      "Iteration 47511 => Loss: 6.69852066912744881222\n",
      "Iteration 47512 => Loss: 6.69852062293661187908\n",
      "Iteration 47513 => Loss: 6.69852057675198864217\n",
      "Iteration 47514 => Loss: 6.69852053057359420052\n",
      "Iteration 47515 => Loss: 6.69852048440141878416\n",
      "Iteration 47516 => Loss: 6.69852043823545972856\n",
      "Iteration 47517 => Loss: 6.69852039207573213275\n",
      "Iteration 47518 => Loss: 6.69852034592222267406\n",
      "Iteration 47519 => Loss: 6.69852029977493135249\n",
      "Iteration 47520 => Loss: 6.69852025363385639167\n",
      "Iteration 47521 => Loss: 6.69852020749900134433\n",
      "Iteration 47522 => Loss: 6.69852016137036443411\n",
      "Iteration 47523 => Loss: 6.69852011524793677921\n",
      "Iteration 47524 => Loss: 6.69852006913172903779\n",
      "Iteration 47525 => Loss: 6.69852002302173321624\n",
      "Iteration 47526 => Loss: 6.69851997691795375545\n",
      "Iteration 47527 => Loss: 6.69851993082038088545\n",
      "Iteration 47528 => Loss: 6.69851988472902437621\n",
      "Iteration 47529 => Loss: 6.69851983864387889867\n",
      "Iteration 47530 => Loss: 6.69851979256493734738\n",
      "Iteration 47531 => Loss: 6.69851974649221837410\n",
      "Iteration 47532 => Loss: 6.69851970042569533348\n",
      "Iteration 47533 => Loss: 6.69851965436538687726\n",
      "Iteration 47534 => Loss: 6.69851960831127613005\n",
      "Iteration 47535 => Loss: 6.69851956226338351996\n",
      "Iteration 47536 => Loss: 6.69851951622168595435\n",
      "Iteration 47537 => Loss: 6.69851947018619497953\n",
      "Iteration 47538 => Loss: 6.69851942415690704280\n",
      "Iteration 47539 => Loss: 6.69851937813382303233\n",
      "Iteration 47540 => Loss: 6.69851933211693939541\n",
      "Iteration 47541 => Loss: 6.69851928610626323746\n",
      "Iteration 47542 => Loss: 6.69851924010177590674\n",
      "Iteration 47543 => Loss: 6.69851919410349783135\n",
      "Iteration 47544 => Loss: 6.69851914811141035955\n",
      "Iteration 47545 => Loss: 6.69851910212552770219\n",
      "Iteration 47546 => Loss: 6.69851905614583564841\n",
      "Iteration 47547 => Loss: 6.69851901017233775093\n",
      "Iteration 47548 => Loss: 6.69851896420504022700\n",
      "Iteration 47549 => Loss: 6.69851891824393330666\n",
      "Iteration 47550 => Loss: 6.69851887228901965443\n",
      "Iteration 47551 => Loss: 6.69851882634030193486\n",
      "Iteration 47552 => Loss: 6.69851878039777126617\n",
      "Iteration 47553 => Loss: 6.69851873446143120105\n",
      "Iteration 47554 => Loss: 6.69851868853128440406\n",
      "Iteration 47555 => Loss: 6.69851864260732732248\n",
      "Iteration 47556 => Loss: 6.69851859668955817995\n",
      "Iteration 47557 => Loss: 6.69851855077797253557\n",
      "Iteration 47558 => Loss: 6.69851850487257571842\n",
      "Iteration 47559 => Loss: 6.69851845897336772850\n",
      "Iteration 47560 => Loss: 6.69851841308034146039\n",
      "Iteration 47561 => Loss: 6.69851836719350135496\n",
      "Iteration 47562 => Loss: 6.69851832131284297134\n",
      "Iteration 47563 => Loss: 6.69851827543836542134\n",
      "Iteration 47564 => Loss: 6.69851822957006781678\n",
      "Iteration 47565 => Loss: 6.69851818370795548674\n",
      "Iteration 47566 => Loss: 6.69851813785202221396\n",
      "Iteration 47567 => Loss: 6.69851809200226888663\n",
      "Iteration 47568 => Loss: 6.69851804615869284021\n",
      "Iteration 47569 => Loss: 6.69851800032129496287\n",
      "Iteration 47570 => Loss: 6.69851795449006726102\n",
      "Iteration 47571 => Loss: 6.69851790866502661004\n",
      "Iteration 47572 => Loss: 6.69851786284615169365\n",
      "Iteration 47573 => Loss: 6.69851781703345583452\n",
      "Iteration 47574 => Loss: 6.69851777122693103905\n",
      "Iteration 47575 => Loss: 6.69851772542657997178\n",
      "Iteration 47576 => Loss: 6.69851767963239996817\n",
      "Iteration 47577 => Loss: 6.69851763384438747551\n",
      "Iteration 47578 => Loss: 6.69851758806254693468\n",
      "Iteration 47579 => Loss: 6.69851754228688189841\n",
      "Iteration 47580 => Loss: 6.69851749651737637947\n",
      "Iteration 47581 => Loss: 6.69851745075403925966\n",
      "Iteration 47582 => Loss: 6.69851740499687497987\n",
      "Iteration 47583 => Loss: 6.69851735924587199378\n",
      "Iteration 47584 => Loss: 6.69851731350103118956\n",
      "Iteration 47585 => Loss: 6.69851726776236144900\n",
      "Iteration 47586 => Loss: 6.69851722202985300214\n",
      "Iteration 47587 => Loss: 6.69851717630350584898\n",
      "Iteration 47588 => Loss: 6.69851713058332176587\n",
      "Iteration 47589 => Loss: 6.69851708486929453557\n",
      "Iteration 47590 => Loss: 6.69851703916142682260\n",
      "Iteration 47591 => Loss: 6.69851699345972306787\n",
      "Iteration 47592 => Loss: 6.69851694776417705413\n",
      "Iteration 47593 => Loss: 6.69851690207478611683\n",
      "Iteration 47594 => Loss: 6.69851685639154759144\n",
      "Iteration 47595 => Loss: 6.69851681071447213611\n",
      "Iteration 47596 => Loss: 6.69851676504354998087\n",
      "Iteration 47597 => Loss: 6.69851671937877934937\n",
      "Iteration 47598 => Loss: 6.69851667372017001156\n",
      "Iteration 47599 => Loss: 6.69851662806770598024\n",
      "Iteration 47600 => Loss: 6.69851658242139080812\n",
      "Iteration 47601 => Loss: 6.69851653678123248881\n",
      "Iteration 47602 => Loss: 6.69851649114722746958\n",
      "Iteration 47603 => Loss: 6.69851644551935976324\n",
      "Iteration 47604 => Loss: 6.69851639989765157424\n",
      "Iteration 47605 => Loss: 6.69851635428208691536\n",
      "Iteration 47606 => Loss: 6.69851630867266312208\n",
      "Iteration 47607 => Loss: 6.69851626306939085254\n",
      "Iteration 47608 => Loss: 6.69851621747226388948\n",
      "Iteration 47609 => Loss: 6.69851617188128400926\n",
      "Iteration 47610 => Loss: 6.69851612629644765917\n",
      "Iteration 47611 => Loss: 6.69851608071774418107\n",
      "Iteration 47612 => Loss: 6.69851603514518512128\n",
      "Iteration 47613 => Loss: 6.69851598957877403251\n",
      "Iteration 47614 => Loss: 6.69851594401849848026\n",
      "Iteration 47615 => Loss: 6.69851589846436645814\n",
      "Iteration 47616 => Loss: 6.69851585291636908437\n",
      "Iteration 47617 => Loss: 6.69851580737450991165\n",
      "Iteration 47618 => Loss: 6.69851576183878272275\n",
      "Iteration 47619 => Loss: 6.69851571630920084033\n",
      "Iteration 47620 => Loss: 6.69851567078575005354\n",
      "Iteration 47621 => Loss: 6.69851562526843213874\n",
      "Iteration 47622 => Loss: 6.69851557975724265503\n",
      "Iteration 47623 => Loss: 6.69851553425219581328\n",
      "Iteration 47624 => Loss: 6.69851548875327562627\n",
      "Iteration 47625 => Loss: 6.69851544326049008760\n",
      "Iteration 47626 => Loss: 6.69851539777383031549\n",
      "Iteration 47627 => Loss: 6.69851535229329986265\n",
      "Iteration 47628 => Loss: 6.69851530681890139363\n",
      "Iteration 47629 => Loss: 6.69851526135062602663\n",
      "Iteration 47630 => Loss: 6.69851521588847820254\n",
      "Iteration 47631 => Loss: 6.69851517043246680316\n",
      "Iteration 47632 => Loss: 6.69851512498256784767\n",
      "Iteration 47633 => Loss: 6.69851507953879732327\n",
      "Iteration 47634 => Loss: 6.69851503410114812453\n",
      "Iteration 47635 => Loss: 6.69851498866962558054\n",
      "Iteration 47636 => Loss: 6.69851494324421992133\n",
      "Iteration 47637 => Loss: 6.69851489782494002867\n",
      "Iteration 47638 => Loss: 6.69851485241178590258\n",
      "Iteration 47639 => Loss: 6.69851480700474333219\n",
      "Iteration 47640 => Loss: 6.69851476160381853475\n",
      "Iteration 47641 => Loss: 6.69851471620901239845\n",
      "Iteration 47642 => Loss: 6.69851467082032936418\n",
      "Iteration 47643 => Loss: 6.69851462543775166836\n",
      "Iteration 47644 => Loss: 6.69851458006129707456\n",
      "Iteration 47645 => Loss: 6.69851453469095226012\n",
      "Iteration 47646 => Loss: 6.69851448932672699499\n",
      "Iteration 47647 => Loss: 6.69851444396860973285\n",
      "Iteration 47648 => Loss: 6.69851439861660846731\n",
      "Iteration 47649 => Loss: 6.69851435327071342840\n",
      "Iteration 47650 => Loss: 6.69851430793093527427\n",
      "Iteration 47651 => Loss: 6.69851426259726689949\n",
      "Iteration 47652 => Loss: 6.69851421726969409320\n",
      "Iteration 47653 => Loss: 6.69851417194823550716\n",
      "Iteration 47654 => Loss: 6.69851412663289647043\n",
      "Iteration 47655 => Loss: 6.69851408132364944947\n",
      "Iteration 47656 => Loss: 6.69851403602051043151\n",
      "Iteration 47657 => Loss: 6.69851399072347497565\n",
      "Iteration 47658 => Loss: 6.69851394543255285186\n",
      "Iteration 47659 => Loss: 6.69851390014772984927\n",
      "Iteration 47660 => Loss: 6.69851385486900507971\n",
      "Iteration 47661 => Loss: 6.69851380959638209589\n",
      "Iteration 47662 => Loss: 6.69851376432985556875\n",
      "Iteration 47663 => Loss: 6.69851371906943882095\n",
      "Iteration 47664 => Loss: 6.69851367381511408894\n",
      "Iteration 47665 => Loss: 6.69851362856688403724\n",
      "Iteration 47666 => Loss: 6.69851358332476198854\n",
      "Iteration 47667 => Loss: 6.69851353808872396201\n",
      "Iteration 47668 => Loss: 6.69851349285879305029\n",
      "Iteration 47669 => Loss: 6.69851344763495237800\n",
      "Iteration 47670 => Loss: 6.69851340241719750423\n",
      "Iteration 47671 => Loss: 6.69851335720554708075\n",
      "Iteration 47672 => Loss: 6.69851331199998867305\n",
      "Iteration 47673 => Loss: 6.69851326680051162299\n",
      "Iteration 47674 => Loss: 6.69851322160713902321\n",
      "Iteration 47675 => Loss: 6.69851317641984511653\n",
      "Iteration 47676 => Loss: 6.69851313123864144927\n",
      "Iteration 47677 => Loss: 6.69851308606352890962\n",
      "Iteration 47678 => Loss: 6.69851304089450483303\n",
      "Iteration 47679 => Loss: 6.69851299573156300227\n",
      "Iteration 47680 => Loss: 6.69851295057470963457\n",
      "Iteration 47681 => Loss: 6.69851290542394117722\n",
      "Iteration 47682 => Loss: 6.69851286027926118294\n",
      "Iteration 47683 => Loss: 6.69851281514065899358\n",
      "Iteration 47684 => Loss: 6.69851277000813549733\n",
      "Iteration 47685 => Loss: 6.69851272488170135233\n",
      "Iteration 47686 => Loss: 6.69851267976134590043\n",
      "Iteration 47687 => Loss: 6.69851263464706558892\n",
      "Iteration 47688 => Loss: 6.69851258953886841141\n",
      "Iteration 47689 => Loss: 6.69851254443675081518\n",
      "Iteration 47690 => Loss: 6.69851249934071635295\n",
      "Iteration 47691 => Loss: 6.69851245425074459661\n",
      "Iteration 47692 => Loss: 6.69851240916685330973\n",
      "Iteration 47693 => Loss: 6.69851236408904338049\n",
      "Iteration 47694 => Loss: 6.69851231901730326257\n",
      "Iteration 47695 => Loss: 6.69851227395163473233\n",
      "Iteration 47696 => Loss: 6.69851222889204578337\n",
      "Iteration 47697 => Loss: 6.69851218383851954030\n",
      "Iteration 47698 => Loss: 6.69851213879107021398\n",
      "Iteration 47699 => Loss: 6.69851209374968981081\n",
      "Iteration 47700 => Loss: 6.69851204871437566624\n",
      "Iteration 47701 => Loss: 6.69851200368513222116\n",
      "Iteration 47702 => Loss: 6.69851195866195858741\n",
      "Iteration 47703 => Loss: 6.69851191364485387680\n",
      "Iteration 47704 => Loss: 6.69851186863380654302\n",
      "Iteration 47705 => Loss: 6.69851182362883434962\n",
      "Iteration 47706 => Loss: 6.69851177862992130940\n",
      "Iteration 47707 => Loss: 6.69851173363706831054\n",
      "Iteration 47708 => Loss: 6.69851168865028689936\n",
      "Iteration 47709 => Loss: 6.69851164366956197682\n",
      "Iteration 47710 => Loss: 6.69851159869489620746\n",
      "Iteration 47711 => Loss: 6.69851155372629669671\n",
      "Iteration 47712 => Loss: 6.69851150876375012189\n",
      "Iteration 47713 => Loss: 6.69851146380726802931\n",
      "Iteration 47714 => Loss: 6.69851141885684064903\n",
      "Iteration 47715 => Loss: 6.69851137391247153374\n",
      "Iteration 47716 => Loss: 6.69851132897416157164\n",
      "Iteration 47717 => Loss: 6.69851128404190099275\n",
      "Iteration 47718 => Loss: 6.69851123911569779068\n",
      "Iteration 47719 => Loss: 6.69851119419554308365\n",
      "Iteration 47720 => Loss: 6.69851114928145197069\n",
      "Iteration 47721 => Loss: 6.69851110437340135917\n",
      "Iteration 47722 => Loss: 6.69851105947141256536\n",
      "Iteration 47723 => Loss: 6.69851101457547226659\n",
      "Iteration 47724 => Loss: 6.69851096968557957467\n",
      "Iteration 47725 => Loss: 6.69851092480173981869\n",
      "Iteration 47726 => Loss: 6.69851087992393967596\n",
      "Iteration 47727 => Loss: 6.69851083505219513370\n",
      "Iteration 47728 => Loss: 6.69851079018648665198\n",
      "Iteration 47729 => Loss: 6.69851074532683821161\n",
      "Iteration 47730 => Loss: 6.69851070047321872636\n",
      "Iteration 47731 => Loss: 6.69851065562565572975\n",
      "Iteration 47732 => Loss: 6.69851061078413234640\n",
      "Iteration 47733 => Loss: 6.69851056594864857630\n",
      "Iteration 47734 => Loss: 6.69851052111921507759\n",
      "Iteration 47735 => Loss: 6.69851047629580964582\n",
      "Iteration 47736 => Loss: 6.69851043147845537362\n",
      "Iteration 47737 => Loss: 6.69851038666713183289\n",
      "Iteration 47738 => Loss: 6.69851034186185056996\n",
      "Iteration 47739 => Loss: 6.69851029706260536756\n",
      "Iteration 47740 => Loss: 6.69851025226939622570\n",
      "Iteration 47741 => Loss: 6.69851020748223024981\n",
      "Iteration 47742 => Loss: 6.69851016270108878814\n",
      "Iteration 47743 => Loss: 6.69851011792598693972\n",
      "Iteration 47744 => Loss: 6.69851007315691493460\n",
      "Iteration 47745 => Loss: 6.69851002839387987819\n",
      "Iteration 47746 => Loss: 6.69850998363687555326\n",
      "Iteration 47747 => Loss: 6.69850993888590462433\n",
      "Iteration 47748 => Loss: 6.69850989414096087415\n",
      "Iteration 47749 => Loss: 6.69850984940204696727\n",
      "Iteration 47750 => Loss: 6.69850980466916379186\n",
      "Iteration 47751 => Loss: 6.69850975994230246613\n",
      "Iteration 47752 => Loss: 6.69850971522147187187\n",
      "Iteration 47753 => Loss: 6.69850967050667112090\n",
      "Iteration 47754 => Loss: 6.69850962579789133144\n",
      "Iteration 47755 => Loss: 6.69850958109513694438\n",
      "Iteration 47756 => Loss: 6.69850953639840795972\n",
      "Iteration 47757 => Loss: 6.69850949170769638386\n",
      "Iteration 47758 => Loss: 6.69850944702301109857\n",
      "Iteration 47759 => Loss: 6.69850940234434766296\n",
      "Iteration 47760 => Loss: 6.69850935767170252433\n",
      "Iteration 47761 => Loss: 6.69850931300507568267\n",
      "Iteration 47762 => Loss: 6.69850926834446980251\n",
      "Iteration 47763 => Loss: 6.69850922368988221933\n",
      "Iteration 47764 => Loss: 6.69850917904131559766\n",
      "Iteration 47765 => Loss: 6.69850913439875572664\n",
      "Iteration 47766 => Loss: 6.69850908976222214619\n",
      "Iteration 47767 => Loss: 6.69850904513169442822\n",
      "Iteration 47768 => Loss: 6.69850900050718855994\n",
      "Iteration 47769 => Loss: 6.69850895588869388320\n",
      "Iteration 47770 => Loss: 6.69850891127620329257\n",
      "Iteration 47771 => Loss: 6.69850886666973011074\n",
      "Iteration 47772 => Loss: 6.69850882206926545592\n",
      "Iteration 47773 => Loss: 6.69850877747481643354\n",
      "Iteration 47774 => Loss: 6.69850873288637504999\n",
      "Iteration 47775 => Loss: 6.69850868830394574616\n",
      "Iteration 47776 => Loss: 6.69850864372751253484\n",
      "Iteration 47777 => Loss: 6.69850859915709140324\n",
      "Iteration 47778 => Loss: 6.69850855459267702230\n",
      "Iteration 47779 => Loss: 6.69850851003426672747\n",
      "Iteration 47780 => Loss: 6.69850846548186051876\n",
      "Iteration 47781 => Loss: 6.69850842093546017253\n",
      "Iteration 47782 => Loss: 6.69850837639506302423\n",
      "Iteration 47783 => Loss: 6.69850833186066640934\n",
      "Iteration 47784 => Loss: 6.69850828733226855149\n",
      "Iteration 47785 => Loss: 6.69850824280987122705\n",
      "Iteration 47786 => Loss: 6.69850819829347088330\n",
      "Iteration 47787 => Loss: 6.69850815378307640202\n",
      "Iteration 47788 => Loss: 6.69850810927867268418\n",
      "Iteration 47789 => Loss: 6.69850806478026861157\n",
      "Iteration 47790 => Loss: 6.69850802028786329601\n",
      "Iteration 47791 => Loss: 6.69850797580145052024\n",
      "Iteration 47792 => Loss: 6.69850793132103650152\n",
      "Iteration 47793 => Loss: 6.69850788684661235806\n",
      "Iteration 47794 => Loss: 6.69850784237818253075\n",
      "Iteration 47795 => Loss: 6.69850779791573724964\n",
      "Iteration 47796 => Loss: 6.69850775345929339011\n",
      "Iteration 47797 => Loss: 6.69850770900883674130\n",
      "Iteration 47798 => Loss: 6.69850766456436907959\n",
      "Iteration 47799 => Loss: 6.69850762012589040495\n",
      "Iteration 47800 => Loss: 6.69850757569339894104\n",
      "Iteration 47801 => Loss: 6.69850753126689735240\n",
      "Iteration 47802 => Loss: 6.69850748684637675723\n",
      "Iteration 47803 => Loss: 6.69850744243185580729\n",
      "Iteration 47804 => Loss: 6.69850739802330341632\n",
      "Iteration 47805 => Loss: 6.69850735362074090062\n",
      "Iteration 47806 => Loss: 6.69850730922416470747\n",
      "Iteration 47807 => Loss: 6.69850726483356861962\n",
      "Iteration 47808 => Loss: 6.69850722044895618978\n",
      "Iteration 47809 => Loss: 6.69850717607031764800\n",
      "Iteration 47810 => Loss: 6.69850713169766542876\n",
      "Iteration 47811 => Loss: 6.69850708733099242664\n",
      "Iteration 47812 => Loss: 6.69850704297029331258\n",
      "Iteration 47813 => Loss: 6.69850699861557696835\n",
      "Iteration 47814 => Loss: 6.69850695426683717670\n",
      "Iteration 47815 => Loss: 6.69850690992406949675\n",
      "Iteration 47816 => Loss: 6.69850686558727748121\n",
      "Iteration 47817 => Loss: 6.69850682125646379461\n",
      "Iteration 47818 => Loss: 6.69850677693162221971\n",
      "Iteration 47819 => Loss: 6.69850673261275364467\n",
      "Iteration 47820 => Loss: 6.69850668829985629316\n",
      "Iteration 47821 => Loss: 6.69850664399292838880\n",
      "Iteration 47822 => Loss: 6.69850659969196815524\n",
      "Iteration 47823 => Loss: 6.69850655539698447427\n",
      "Iteration 47824 => Loss: 6.69850651110796757592\n",
      "Iteration 47825 => Loss: 6.69850646682491746020\n",
      "Iteration 47826 => Loss: 6.69850642254783590346\n",
      "Iteration 47827 => Loss: 6.69850637827671402391\n",
      "Iteration 47828 => Loss: 6.69850633401156425606\n",
      "Iteration 47829 => Loss: 6.69850628975237594176\n",
      "Iteration 47830 => Loss: 6.69850624549915973915\n",
      "Iteration 47831 => Loss: 6.69850620125189966103\n",
      "Iteration 47832 => Loss: 6.69850615701060725371\n",
      "Iteration 47833 => Loss: 6.69850611277526652998\n",
      "Iteration 47834 => Loss: 6.69850606854589347705\n",
      "Iteration 47835 => Loss: 6.69850602432248010132\n",
      "Iteration 47836 => Loss: 6.69850598010502107371\n",
      "Iteration 47837 => Loss: 6.69850593589351994694\n",
      "Iteration 47838 => Loss: 6.69850589168798649098\n",
      "Iteration 47839 => Loss: 6.69850584748840383043\n",
      "Iteration 47840 => Loss: 6.69850580329477462982\n",
      "Iteration 47841 => Loss: 6.69850575910710155370\n",
      "Iteration 47842 => Loss: 6.69850571492538726659\n",
      "Iteration 47843 => Loss: 6.69850567074961755765\n",
      "Iteration 47844 => Loss: 6.69850562657980752590\n",
      "Iteration 47845 => Loss: 6.69850558241594828957\n",
      "Iteration 47846 => Loss: 6.69850553825804251318\n",
      "Iteration 47847 => Loss: 6.69850549410608220313\n",
      "Iteration 47848 => Loss: 6.69850544996007180032\n",
      "Iteration 47849 => Loss: 6.69850540582001308110\n",
      "Iteration 47850 => Loss: 6.69850536168589894004\n",
      "Iteration 47851 => Loss: 6.69850531755773381803\n",
      "Iteration 47852 => Loss: 6.69850527343551505055\n",
      "Iteration 47853 => Loss: 6.69850522931923908487\n",
      "Iteration 47854 => Loss: 6.69850518520891391461\n",
      "Iteration 47855 => Loss: 6.69850514110453065797\n",
      "Iteration 47856 => Loss: 6.69850509700608132135\n",
      "Iteration 47857 => Loss: 6.69850505291359077376\n",
      "Iteration 47858 => Loss: 6.69850500882703503436\n",
      "Iteration 47859 => Loss: 6.69850496474640877409\n",
      "Iteration 47860 => Loss: 6.69850492067173330923\n",
      "Iteration 47861 => Loss: 6.69850487660299620529\n",
      "Iteration 47862 => Loss: 6.69850483254019479773\n",
      "Iteration 47863 => Loss: 6.69850478848332908655\n",
      "Iteration 47864 => Loss: 6.69850474443240617717\n",
      "Iteration 47865 => Loss: 6.69850470038741718781\n",
      "Iteration 47866 => Loss: 6.69850465634835767759\n",
      "Iteration 47867 => Loss: 6.69850461231523830463\n",
      "Iteration 47868 => Loss: 6.69850456828805196352\n",
      "Iteration 47869 => Loss: 6.69850452426679598972\n",
      "Iteration 47870 => Loss: 6.69850448025146860687\n",
      "Iteration 47871 => Loss: 6.69850443624208313764\n",
      "Iteration 47872 => Loss: 6.69850439223862181848\n",
      "Iteration 47873 => Loss: 6.69850434824108642573\n",
      "Iteration 47874 => Loss: 6.69850430424948850572\n",
      "Iteration 47875 => Loss: 6.69850426026380940669\n",
      "Iteration 47876 => Loss: 6.69850421628405978680\n",
      "Iteration 47877 => Loss: 6.69850417231023786968\n",
      "Iteration 47878 => Loss: 6.69850412834234454351\n",
      "Iteration 47879 => Loss: 6.69850408438036826198\n",
      "Iteration 47880 => Loss: 6.69850404042432057139\n",
      "Iteration 47881 => Loss: 6.69850399647419969540\n",
      "Iteration 47882 => Loss: 6.69850395252999231133\n",
      "Iteration 47883 => Loss: 6.69850390859171174185\n",
      "Iteration 47884 => Loss: 6.69850386465934999336\n",
      "Iteration 47885 => Loss: 6.69850382073291328311\n",
      "Iteration 47886 => Loss: 6.69850377681238651206\n",
      "Iteration 47887 => Loss: 6.69850373289778833197\n",
      "Iteration 47888 => Loss: 6.69850368898909742654\n",
      "Iteration 47889 => Loss: 6.69850364508633067118\n",
      "Iteration 47890 => Loss: 6.69850360118948096044\n",
      "Iteration 47891 => Loss: 6.69850355729854030074\n",
      "Iteration 47892 => Loss: 6.69850351341351579748\n",
      "Iteration 47893 => Loss: 6.69850346953441189157\n",
      "Iteration 47894 => Loss: 6.69850342566120904308\n",
      "Iteration 47895 => Loss: 6.69850338179391879834\n",
      "Iteration 47896 => Loss: 6.69850333793254826276\n",
      "Iteration 47897 => Loss: 6.69850329407708322549\n",
      "Iteration 47898 => Loss: 6.69850325022752723925\n",
      "Iteration 47899 => Loss: 6.69850320638388119221\n",
      "Iteration 47900 => Loss: 6.69850316254614419620\n",
      "Iteration 47901 => Loss: 6.69850311871431269850\n",
      "Iteration 47902 => Loss: 6.69850307488838669912\n",
      "Iteration 47903 => Loss: 6.69850303106837063893\n",
      "Iteration 47904 => Loss: 6.69850298725425297164\n",
      "Iteration 47905 => Loss: 6.69850294344604701990\n",
      "Iteration 47906 => Loss: 6.69850289964374034923\n",
      "Iteration 47907 => Loss: 6.69850285584733384781\n",
      "Iteration 47908 => Loss: 6.69850281205682840380\n",
      "Iteration 47909 => Loss: 6.69850276827222224085\n",
      "Iteration 47910 => Loss: 6.69850272449352512893\n",
      "Iteration 47911 => Loss: 6.69850268072072108083\n",
      "Iteration 47912 => Loss: 6.69850263695381542561\n",
      "Iteration 47913 => Loss: 6.69850259319280993964\n",
      "Iteration 47914 => Loss: 6.69850254943769751748\n",
      "Iteration 47915 => Loss: 6.69850250568848171184\n",
      "Iteration 47916 => Loss: 6.69850246194516429910\n",
      "Iteration 47917 => Loss: 6.69850241820774172652\n",
      "Iteration 47918 => Loss: 6.69850237447620777687\n",
      "Iteration 47919 => Loss: 6.69850233075056866738\n",
      "Iteration 47920 => Loss: 6.69850228703081995718\n",
      "Iteration 47921 => Loss: 6.69850224331696519897\n",
      "Iteration 47922 => Loss: 6.69850219960899817551\n",
      "Iteration 47923 => Loss: 6.69850215590692865675\n",
      "Iteration 47924 => Loss: 6.69850211221073799095\n",
      "Iteration 47925 => Loss: 6.69850206852044927075\n",
      "Iteration 47926 => Loss: 6.69850202483603407444\n",
      "Iteration 47927 => Loss: 6.69850198115750661287\n",
      "Iteration 47928 => Loss: 6.69850193748486955059\n",
      "Iteration 47929 => Loss: 6.69850189381811311762\n",
      "Iteration 47930 => Loss: 6.69850185015724530757\n",
      "Iteration 47931 => Loss: 6.69850180650226079138\n",
      "Iteration 47932 => Loss: 6.69850176285315068725\n",
      "Iteration 47933 => Loss: 6.69850171920993453512\n",
      "Iteration 47934 => Loss: 6.69850167557259545958\n",
      "Iteration 47935 => Loss: 6.69850163194112901976\n",
      "Iteration 47936 => Loss: 6.69850158831554320926\n",
      "Iteration 47937 => Loss: 6.69850154469584513350\n",
      "Iteration 47938 => Loss: 6.69850150108201702892\n",
      "Iteration 47939 => Loss: 6.69850145747406688912\n",
      "Iteration 47940 => Loss: 6.69850141387199560228\n",
      "Iteration 47941 => Loss: 6.69850137027580050386\n",
      "Iteration 47942 => Loss: 6.69850132668547715298\n",
      "Iteration 47943 => Loss: 6.69850128310103265505\n",
      "Iteration 47944 => Loss: 6.69850123952245457559\n",
      "Iteration 47945 => Loss: 6.69850119594975002002\n",
      "Iteration 47946 => Loss: 6.69850115238291454745\n",
      "Iteration 47947 => Loss: 6.69850110882195970419\n",
      "Iteration 47948 => Loss: 6.69850106526686683850\n",
      "Iteration 47949 => Loss: 6.69850102171764660852\n",
      "Iteration 47950 => Loss: 6.69850097817428657976\n",
      "Iteration 47951 => Loss: 6.69850093463680362760\n",
      "Iteration 47952 => Loss: 6.69850089110518531754\n",
      "Iteration 47953 => Loss: 6.69850084757942898506\n",
      "Iteration 47954 => Loss: 6.69850080405953818286\n",
      "Iteration 47955 => Loss: 6.69850076054551557547\n",
      "Iteration 47956 => Loss: 6.69850071703736205109\n",
      "Iteration 47957 => Loss: 6.69850067353505451706\n",
      "Iteration 47958 => Loss: 6.69850063003862228328\n",
      "Iteration 47959 => Loss: 6.69850058654803959257\n",
      "Iteration 47960 => Loss: 6.69850054306332420850\n",
      "Iteration 47961 => Loss: 6.69850049958446991383\n",
      "Iteration 47962 => Loss: 6.69850045611147049129\n",
      "Iteration 47963 => Loss: 6.69850041264433571087\n",
      "Iteration 47964 => Loss: 6.69850036918305402622\n",
      "Iteration 47965 => Loss: 6.69850032572763165462\n",
      "Iteration 47966 => Loss: 6.69850028227805882608\n",
      "Iteration 47967 => Loss: 6.69850023883434531058\n",
      "Iteration 47968 => Loss: 6.69850019539648133815\n",
      "Iteration 47969 => Loss: 6.69850015196448200783\n",
      "Iteration 47970 => Loss: 6.69850010853832333879\n",
      "Iteration 47971 => Loss: 6.69850006511801776554\n",
      "Iteration 47972 => Loss: 6.69850002170356795261\n",
      "Iteration 47973 => Loss: 6.69849997829496768276\n",
      "Iteration 47974 => Loss: 6.69849993489221606779\n",
      "Iteration 47975 => Loss: 6.69849989149531399590\n",
      "Iteration 47976 => Loss: 6.69849984810424903259\n",
      "Iteration 47977 => Loss: 6.69849980471904249413\n",
      "Iteration 47978 => Loss: 6.69849976133968016967\n",
      "Iteration 47979 => Loss: 6.69849971796615673014\n",
      "Iteration 47980 => Loss: 6.69849967459848283369\n",
      "Iteration 47981 => Loss: 6.69849963123665670395\n",
      "Iteration 47982 => Loss: 6.69849958788067212367\n",
      "Iteration 47983 => Loss: 6.69849954453052820469\n",
      "Iteration 47984 => Loss: 6.69849950118622139428\n",
      "Iteration 47985 => Loss: 6.69849945784775968605\n",
      "Iteration 47986 => Loss: 6.69849941451513597457\n",
      "Iteration 47987 => Loss: 6.69849937118835647709\n",
      "Iteration 47988 => Loss: 6.69849932786741053548\n",
      "Iteration 47989 => Loss: 6.69849928455230259061\n",
      "Iteration 47990 => Loss: 6.69849924124303086614\n",
      "Iteration 47991 => Loss: 6.69849919793959447389\n",
      "Iteration 47992 => Loss: 6.69849915464199430204\n",
      "Iteration 47993 => Loss: 6.69849911135022679787\n",
      "Iteration 47994 => Loss: 6.69849906806429551409\n",
      "Iteration 47995 => Loss: 6.69849902478419867435\n",
      "Iteration 47996 => Loss: 6.69849898150993094958\n",
      "Iteration 47997 => Loss: 6.69849893824149766886\n",
      "Iteration 47998 => Loss: 6.69849889497888906220\n",
      "Iteration 47999 => Loss: 6.69849885172211045870\n",
      "Iteration 48000 => Loss: 6.69849880847115919380\n",
      "Iteration 48001 => Loss: 6.69849876522604859019\n",
      "Iteration 48002 => Loss: 6.69849872198675377888\n",
      "Iteration 48003 => Loss: 6.69849867875328719435\n",
      "Iteration 48004 => Loss: 6.69849863552565238933\n",
      "Iteration 48005 => Loss: 6.69849859230383426478\n",
      "Iteration 48006 => Loss: 6.69849854908784525520\n",
      "Iteration 48007 => Loss: 6.69849850587767559063\n",
      "Iteration 48008 => Loss: 6.69849846267333681737\n",
      "Iteration 48009 => Loss: 6.69849841947481117188\n",
      "Iteration 48010 => Loss: 6.69849837628210931229\n",
      "Iteration 48011 => Loss: 6.69849833309523212677\n",
      "Iteration 48012 => Loss: 6.69849828991416540447\n",
      "Iteration 48013 => Loss: 6.69849824673892246807\n",
      "Iteration 48014 => Loss: 6.69849820356950065303\n",
      "Iteration 48015 => Loss: 6.69849816040589196575\n",
      "Iteration 48016 => Loss: 6.69849811724809374169\n",
      "Iteration 48017 => Loss: 6.69849807409611752718\n",
      "Iteration 48018 => Loss: 6.69849803094995444042\n",
      "Iteration 48019 => Loss: 6.69849798780960892230\n",
      "Iteration 48020 => Loss: 6.69849794467507653195\n",
      "Iteration 48021 => Loss: 6.69849790154635282846\n",
      "Iteration 48022 => Loss: 6.69849785842344047637\n",
      "Iteration 48023 => Loss: 6.69849781530634214022\n",
      "Iteration 48024 => Loss: 6.69849777219505426729\n",
      "Iteration 48025 => Loss: 6.69849772908957330486\n",
      "Iteration 48026 => Loss: 6.69849768598990014112\n",
      "Iteration 48027 => Loss: 6.69849764289603832879\n",
      "Iteration 48028 => Loss: 6.69849759980797632153\n",
      "Iteration 48029 => Loss: 6.69849755672572655385\n",
      "Iteration 48030 => Loss: 6.69849751364928192032\n",
      "Iteration 48031 => Loss: 6.69849747057863886823\n",
      "Iteration 48032 => Loss: 6.69849742751379828576\n",
      "Iteration 48033 => Loss: 6.69849738445476283744\n",
      "Iteration 48034 => Loss: 6.69849734140153252326\n",
      "Iteration 48035 => Loss: 6.69849729835410467871\n",
      "Iteration 48036 => Loss: 6.69849725531246953381\n",
      "Iteration 48037 => Loss: 6.69849721227663863488\n",
      "Iteration 48038 => Loss: 6.69849716924660754103\n",
      "Iteration 48039 => Loss: 6.69849712622237358772\n",
      "Iteration 48040 => Loss: 6.69849708320393499861\n",
      "Iteration 48041 => Loss: 6.69849704019129976729\n",
      "Iteration 48042 => Loss: 6.69849699718445457108\n",
      "Iteration 48043 => Loss: 6.69849695418340385089\n",
      "Iteration 48044 => Loss: 6.69849691118815293578\n",
      "Iteration 48045 => Loss: 6.69849686819868583854\n",
      "Iteration 48046 => Loss: 6.69849682521502121091\n",
      "Iteration 48047 => Loss: 6.69849678223715372383\n",
      "Iteration 48048 => Loss: 6.69849673926506827826\n",
      "Iteration 48049 => Loss: 6.69849669629877642052\n",
      "Iteration 48050 => Loss: 6.69849665333827104519\n",
      "Iteration 48051 => Loss: 6.69849661038355304044\n",
      "Iteration 48052 => Loss: 6.69849656743463039987\n",
      "Iteration 48053 => Loss: 6.69849652449149246536\n",
      "Iteration 48054 => Loss: 6.69849648155413568418\n",
      "Iteration 48055 => Loss: 6.69849643862256538540\n",
      "Iteration 48056 => Loss: 6.69849639569679222717\n",
      "Iteration 48057 => Loss: 6.69849635277678778777\n",
      "Iteration 48058 => Loss: 6.69849630986257782439\n",
      "Iteration 48059 => Loss: 6.69849626695414546163\n",
      "Iteration 48060 => Loss: 6.69849622405149869309\n",
      "Iteration 48061 => Loss: 6.69849618115462686063\n",
      "Iteration 48062 => Loss: 6.69849613826353706969\n",
      "Iteration 48063 => Loss: 6.69849609537823287297\n",
      "Iteration 48064 => Loss: 6.69849605249870361234\n",
      "Iteration 48065 => Loss: 6.69849600962494839962\n",
      "Iteration 48066 => Loss: 6.69849596675697522841\n",
      "Iteration 48067 => Loss: 6.69849592389477876964\n",
      "Iteration 48068 => Loss: 6.69849588103835813513\n",
      "Iteration 48069 => Loss: 6.69849583818771154853\n",
      "Iteration 48070 => Loss: 6.69849579534283812166\n",
      "Iteration 48071 => Loss: 6.69849575250373696633\n",
      "Iteration 48072 => Loss: 6.69849570967040630620\n",
      "Iteration 48073 => Loss: 6.69849566684285591123\n",
      "Iteration 48074 => Loss: 6.69849562402107423509\n",
      "Iteration 48075 => Loss: 6.69849558120505150782\n",
      "Iteration 48076 => Loss: 6.69849553839481348660\n",
      "Iteration 48077 => Loss: 6.69849549559033352608\n",
      "Iteration 48078 => Loss: 6.69849545279162761346\n",
      "Iteration 48079 => Loss: 6.69849540999868420244\n",
      "Iteration 48080 => Loss: 6.69849536721151039842\n",
      "Iteration 48081 => Loss: 6.69849532443009820781\n",
      "Iteration 48082 => Loss: 6.69849528165445740058\n",
      "Iteration 48083 => Loss: 6.69849523888457376586\n",
      "Iteration 48084 => Loss: 6.69849519612046151451\n",
      "Iteration 48085 => Loss: 6.69849515336210732386\n",
      "Iteration 48086 => Loss: 6.69849511060951208208\n",
      "Iteration 48087 => Loss: 6.69849506786268378278\n",
      "Iteration 48088 => Loss: 6.69849502512160999146\n",
      "Iteration 48089 => Loss: 6.69849498238629781355\n",
      "Iteration 48090 => Loss: 6.69849493965674103180\n",
      "Iteration 48091 => Loss: 6.69849489693294408710\n",
      "Iteration 48092 => Loss: 6.69849485421490342674\n",
      "Iteration 48093 => Loss: 6.69849481150262437978\n",
      "Iteration 48094 => Loss: 6.69849476879609806446\n",
      "Iteration 48095 => Loss: 6.69849472609532270440\n",
      "Iteration 48096 => Loss: 6.69849468340030895774\n",
      "Iteration 48097 => Loss: 6.69849464071104439000\n",
      "Iteration 48098 => Loss: 6.69849459802753255389\n",
      "Iteration 48099 => Loss: 6.69849455534976900850\n",
      "Iteration 48100 => Loss: 6.69849451267775730656\n",
      "Iteration 48101 => Loss: 6.69849447001150011261\n",
      "Iteration 48102 => Loss: 6.69849442735098410395\n",
      "Iteration 48103 => Loss: 6.69849438469622260328\n",
      "Iteration 48104 => Loss: 6.69849434204720850516\n",
      "Iteration 48105 => Loss: 6.69849429940394092142\n",
      "Iteration 48106 => Loss: 6.69849425676641541116\n",
      "Iteration 48107 => Loss: 6.69849421413463819164\n",
      "Iteration 48108 => Loss: 6.69849417150861281556\n",
      "Iteration 48109 => Loss: 6.69849412888832418389\n",
      "Iteration 48110 => Loss: 6.69849408627377140846\n",
      "Iteration 48111 => Loss: 6.69849404366497136465\n",
      "Iteration 48112 => Loss: 6.69849400106191339432\n",
      "Iteration 48113 => Loss: 6.69849395846459128023\n",
      "Iteration 48114 => Loss: 6.69849391587301212780\n",
      "Iteration 48115 => Loss: 6.69849387328717416068\n",
      "Iteration 48116 => Loss: 6.69849383070706849708\n",
      "Iteration 48117 => Loss: 6.69849378813270845967\n",
      "Iteration 48118 => Loss: 6.69849374556407717307\n",
      "Iteration 48119 => Loss: 6.69849370300118085453\n",
      "Iteration 48120 => Loss: 6.69849366044402838583\n",
      "Iteration 48121 => Loss: 6.69849361789260466793\n",
      "Iteration 48122 => Loss: 6.69849357534691325355\n",
      "Iteration 48123 => Loss: 6.69849353280696302448\n",
      "Iteration 48124 => Loss: 6.69849349027273621715\n",
      "Iteration 48125 => Loss: 6.69849344774424881876\n",
      "Iteration 48126 => Loss: 6.69849340522148839483\n",
      "Iteration 48127 => Loss: 6.69849336270445583352\n",
      "Iteration 48128 => Loss: 6.69849332019315646392\n",
      "Iteration 48129 => Loss: 6.69849327768758318058\n",
      "Iteration 48130 => Loss: 6.69849323518773509534\n",
      "Iteration 48131 => Loss: 6.69849319269361753726\n",
      "Iteration 48132 => Loss: 6.69849315020521718367\n",
      "Iteration 48133 => Loss: 6.69849310772255357449\n",
      "Iteration 48134 => Loss: 6.69849306524561338705\n",
      "Iteration 48135 => Loss: 6.69849302277439750952\n",
      "Iteration 48136 => Loss: 6.69849298030889794831\n",
      "Iteration 48137 => Loss: 6.69849293784913069061\n",
      "Iteration 48138 => Loss: 6.69849289539507264379\n",
      "Iteration 48139 => Loss: 6.69849285294674068325\n",
      "Iteration 48140 => Loss: 6.69849281050412947991\n",
      "Iteration 48141 => Loss: 6.69849276806723814559\n",
      "Iteration 48142 => Loss: 6.69849272563606668029\n",
      "Iteration 48143 => Loss: 6.69849268321060709042\n",
      "Iteration 48144 => Loss: 6.69849264079086292867\n",
      "Iteration 48145 => Loss: 6.69849259837684041230\n",
      "Iteration 48146 => Loss: 6.69849255596853421224\n",
      "Iteration 48147 => Loss: 6.69849251356594432849\n",
      "Iteration 48148 => Loss: 6.69849247116906099109\n",
      "Iteration 48149 => Loss: 6.69849242877789574635\n",
      "Iteration 48150 => Loss: 6.69849238639243882432\n",
      "Iteration 48151 => Loss: 6.69849234401269644223\n",
      "Iteration 48152 => Loss: 6.69849230163865971832\n",
      "Iteration 48153 => Loss: 6.69849225927033753436\n",
      "Iteration 48154 => Loss: 6.69849221690772633764\n",
      "Iteration 48155 => Loss: 6.69849217455082257544\n",
      "Iteration 48156 => Loss: 6.69849213219962269505\n",
      "Iteration 48157 => Loss: 6.69849208985412758466\n",
      "Iteration 48158 => Loss: 6.69849204751434701421\n",
      "Iteration 48159 => Loss: 6.69849200518026943740\n",
      "Iteration 48160 => Loss: 6.69849196285189218969\n",
      "Iteration 48161 => Loss: 6.69849192052921971197\n",
      "Iteration 48162 => Loss: 6.69849187821225200423\n",
      "Iteration 48163 => Loss: 6.69849183590099173102\n",
      "Iteration 48164 => Loss: 6.69849179359542379331\n",
      "Iteration 48165 => Loss: 6.69849175129556151376\n",
      "Iteration 48166 => Loss: 6.69849170900139334606\n",
      "Iteration 48167 => Loss: 6.69849166671292461928\n",
      "Iteration 48168 => Loss: 6.69849162443016243884\n",
      "Iteration 48169 => Loss: 6.69849158215309437026\n",
      "Iteration 48170 => Loss: 6.69849153988172307805\n",
      "Iteration 48171 => Loss: 6.69849149761604323317\n",
      "Iteration 48172 => Loss: 6.69849145535606549373\n",
      "Iteration 48173 => Loss: 6.69849141310177653708\n",
      "Iteration 48174 => Loss: 6.69849137085319146223\n",
      "Iteration 48175 => Loss: 6.69849132861029250563\n",
      "Iteration 48176 => Loss: 6.69849128637308144363\n",
      "Iteration 48177 => Loss: 6.69849124414156715801\n",
      "Iteration 48178 => Loss: 6.69849120191573721428\n",
      "Iteration 48179 => Loss: 6.69849115969560049422\n",
      "Iteration 48180 => Loss: 6.69849111748115877418\n",
      "Iteration 48181 => Loss: 6.69849107527240317239\n",
      "Iteration 48182 => Loss: 6.69849103306933191249\n",
      "Iteration 48183 => Loss: 6.69849099087194677082\n",
      "Iteration 48184 => Loss: 6.69849094868024863558\n",
      "Iteration 48185 => Loss: 6.69849090649424194766\n",
      "Iteration 48186 => Loss: 6.69849086431391249619\n",
      "Iteration 48187 => Loss: 6.69849082213926827478\n",
      "Iteration 48188 => Loss: 6.69849077997030928344\n",
      "Iteration 48189 => Loss: 6.69849073780702930492\n",
      "Iteration 48190 => Loss: 6.69849069564943100374\n",
      "Iteration 48191 => Loss: 6.69849065349751970899\n",
      "Iteration 48192 => Loss: 6.69849061135128032163\n",
      "Iteration 48193 => Loss: 6.69849056921072527615\n",
      "Iteration 48194 => Loss: 6.69849052707584746713\n",
      "Iteration 48195 => Loss: 6.69849048494664600639\n",
      "Iteration 48196 => Loss: 6.69849044282312711118\n",
      "Iteration 48197 => Loss: 6.69849040070527124158\n",
      "Iteration 48198 => Loss: 6.69849035859310681928\n",
      "Iteration 48199 => Loss: 6.69849031648660719895\n",
      "Iteration 48200 => Loss: 6.69849027438578481508\n",
      "Iteration 48201 => Loss: 6.69849023229063345042\n",
      "Iteration 48202 => Loss: 6.69849019020115665768\n",
      "Iteration 48203 => Loss: 6.69849014811734999597\n",
      "Iteration 48204 => Loss: 6.69849010603921524165\n",
      "Iteration 48205 => Loss: 6.69849006396674617747\n",
      "Iteration 48206 => Loss: 6.69849002189995168521\n",
      "Iteration 48207 => Loss: 6.69848997983882288310\n",
      "Iteration 48208 => Loss: 6.69848993778336510019\n",
      "Iteration 48209 => Loss: 6.69848989573356856653\n",
      "Iteration 48210 => Loss: 6.69848985368944216390\n",
      "Iteration 48211 => Loss: 6.69848981165098145141\n",
      "Iteration 48212 => Loss: 6.69848976961818998177\n",
      "Iteration 48213 => Loss: 6.69848972759105265595\n",
      "Iteration 48214 => Loss: 6.69848968556958190845\n",
      "Iteration 48215 => Loss: 6.69848964355377329838\n",
      "Iteration 48216 => Loss: 6.69848960154362860209\n",
      "Iteration 48217 => Loss: 6.69848955953914870776\n",
      "Iteration 48218 => Loss: 6.69848951754032118089\n",
      "Iteration 48219 => Loss: 6.69848947554715490327\n",
      "Iteration 48220 => Loss: 6.69848943355964543400\n",
      "Iteration 48221 => Loss: 6.69848939157779721398\n",
      "Iteration 48222 => Loss: 6.69848934960160580232\n",
      "Iteration 48223 => Loss: 6.69848930763107031083\n",
      "Iteration 48224 => Loss: 6.69848926566619162770\n",
      "Iteration 48225 => Loss: 6.69848922370696531203\n",
      "Iteration 48226 => Loss: 6.69848918175339225201\n",
      "Iteration 48227 => Loss: 6.69848913980547422398\n",
      "Iteration 48228 => Loss: 6.69848909786321211612\n",
      "Iteration 48229 => Loss: 6.69848905592660148756\n",
      "Iteration 48230 => Loss: 6.69848901399563700920\n",
      "Iteration 48231 => Loss: 6.69848897207032045742\n",
      "Iteration 48232 => Loss: 6.69848893015065627310\n",
      "Iteration 48233 => Loss: 6.69848888823664267989\n",
      "Iteration 48234 => Loss: 6.69848884632827701324\n",
      "Iteration 48235 => Loss: 6.69848880442555572046\n",
      "Iteration 48236 => Loss: 6.69848876252848413060\n",
      "Iteration 48237 => Loss: 6.69848872063706046731\n",
      "Iteration 48238 => Loss: 6.69848867875127407245\n",
      "Iteration 48239 => Loss: 6.69848863687113738052\n",
      "Iteration 48240 => Loss: 6.69848859499664239792\n",
      "Iteration 48241 => Loss: 6.69848855312778823645\n",
      "Iteration 48242 => Loss: 6.69848851126457578431\n",
      "Iteration 48243 => Loss: 6.69848846940700681785\n",
      "Iteration 48244 => Loss: 6.69848842755507689617\n",
      "Iteration 48245 => Loss: 6.69848838570878690746\n",
      "Iteration 48246 => Loss: 6.69848834386813507535\n",
      "Iteration 48247 => Loss: 6.69848830203312495257\n",
      "Iteration 48248 => Loss: 6.69848826020374765733\n",
      "Iteration 48249 => Loss: 6.69848821838000851869\n",
      "Iteration 48250 => Loss: 6.69848817656190487213\n",
      "Iteration 48251 => Loss: 6.69848813474943849400\n",
      "Iteration 48252 => Loss: 6.69848809294260227887\n",
      "Iteration 48253 => Loss: 6.69848805114140510852\n",
      "Iteration 48254 => Loss: 6.69848800934583099576\n",
      "Iteration 48255 => Loss: 6.69848796755590125684\n",
      "Iteration 48256 => Loss: 6.69848792577159368733\n",
      "Iteration 48257 => Loss: 6.69848788399291983353\n",
      "Iteration 48258 => Loss: 6.69848784221987525456\n",
      "Iteration 48259 => Loss: 6.69848780045245906223\n",
      "Iteration 48260 => Loss: 6.69848775869066681565\n",
      "Iteration 48261 => Loss: 6.69848771693450917297\n",
      "Iteration 48262 => Loss: 6.69848767518397725240\n",
      "Iteration 48263 => Loss: 6.69848763343907016576\n",
      "Iteration 48264 => Loss: 6.69848759169978702488\n",
      "Iteration 48265 => Loss: 6.69848754996612782975\n",
      "Iteration 48266 => Loss: 6.69848750823809346855\n",
      "Iteration 48267 => Loss: 6.69848746651568127675\n",
      "Iteration 48268 => Loss: 6.69848742479889036616\n",
      "Iteration 48269 => Loss: 6.69848738308772340133\n",
      "Iteration 48270 => Loss: 6.69848734138217949408\n",
      "Iteration 48271 => Loss: 6.69848729968224709808\n",
      "Iteration 48272 => Loss: 6.69848725798794397690\n",
      "Iteration 48273 => Loss: 6.69848721629924792609\n",
      "Iteration 48274 => Loss: 6.69848717461617315649\n",
      "Iteration 48275 => Loss: 6.69848713293871789176\n",
      "Iteration 48276 => Loss: 6.69848709126687769100\n",
      "Iteration 48277 => Loss: 6.69848704960065344238\n",
      "Iteration 48278 => Loss: 6.69848700794004248138\n",
      "Iteration 48279 => Loss: 6.69848696628504836070\n",
      "Iteration 48280 => Loss: 6.69848692463566308675\n",
      "Iteration 48281 => Loss: 6.69848688299189642947\n",
      "Iteration 48282 => Loss: 6.69848684135373151349\n",
      "Iteration 48283 => Loss: 6.69848679972118254966\n",
      "Iteration 48284 => Loss: 6.69848675809424243255\n",
      "Iteration 48285 => Loss: 6.69848671647291915576\n",
      "Iteration 48286 => Loss: 6.69848667485719762027\n",
      "Iteration 48287 => Loss: 6.69848663324708137878\n",
      "Iteration 48288 => Loss: 6.69848659164257131948\n",
      "Iteration 48289 => Loss: 6.69848655004367277144\n",
      "Iteration 48290 => Loss: 6.69848650845037507651\n",
      "Iteration 48291 => Loss: 6.69848646686269333372\n",
      "Iteration 48292 => Loss: 6.69848642528060178591\n",
      "Iteration 48293 => Loss: 6.69848638370411730847\n",
      "Iteration 48294 => Loss: 6.69848634213323723685\n",
      "Iteration 48295 => Loss: 6.69848630056796423560\n",
      "Iteration 48296 => Loss: 6.69848625900828320567\n",
      "Iteration 48297 => Loss: 6.69848621745419858797\n",
      "Iteration 48298 => Loss: 6.69848617590572459335\n",
      "Iteration 48299 => Loss: 6.69848613436284878730\n",
      "Iteration 48300 => Loss: 6.69848609282556317623\n",
      "Iteration 48301 => Loss: 6.69848605129387930646\n",
      "Iteration 48302 => Loss: 6.69848600976779007254\n",
      "Iteration 48303 => Loss: 6.69848596824730080357\n",
      "Iteration 48304 => Loss: 6.69848592673239373596\n",
      "Iteration 48305 => Loss: 6.69848588522309906779\n",
      "Iteration 48306 => Loss: 6.69848584371938304827\n",
      "Iteration 48307 => Loss: 6.69848580222126610551\n",
      "Iteration 48308 => Loss: 6.69848576072874113407\n",
      "Iteration 48309 => Loss: 6.69848571924180635762\n",
      "Iteration 48310 => Loss: 6.69848567776046355249\n",
      "Iteration 48311 => Loss: 6.69848563628470916598\n",
      "Iteration 48312 => Loss: 6.69848559481454230990\n",
      "Iteration 48313 => Loss: 6.69848555334996387245\n",
      "Iteration 48314 => Loss: 6.69848551189097474179\n",
      "Iteration 48315 => Loss: 6.69848547043756692432\n",
      "Iteration 48316 => Loss: 6.69848542898975107818\n",
      "Iteration 48317 => Loss: 6.69848538754751565705\n",
      "Iteration 48318 => Loss: 6.69848534611086776636\n",
      "Iteration 48319 => Loss: 6.69848530467980562975\n",
      "Iteration 48320 => Loss: 6.69848526325432480633\n",
      "Iteration 48321 => Loss: 6.69848522183442351974\n",
      "Iteration 48322 => Loss: 6.69848518042010621087\n",
      "Iteration 48323 => Loss: 6.69848513901136755067\n",
      "Iteration 48324 => Loss: 6.69848509760821109182\n",
      "Iteration 48325 => Loss: 6.69848505621063150528\n",
      "Iteration 48326 => Loss: 6.69848501481862790286\n",
      "Iteration 48327 => Loss: 6.69848497343220383726\n",
      "Iteration 48328 => Loss: 6.69848493205135842032\n",
      "Iteration 48329 => Loss: 6.69848489067608898750\n",
      "Iteration 48330 => Loss: 6.69848484930639287427\n",
      "Iteration 48331 => Loss: 6.69848480794227096879\n",
      "Iteration 48332 => Loss: 6.69848476658372415926\n",
      "Iteration 48333 => Loss: 6.69848472523075599838\n",
      "Iteration 48334 => Loss: 6.69848468388335582802\n",
      "Iteration 48335 => Loss: 6.69848464254152364816\n",
      "Iteration 48336 => Loss: 6.69848460120526301154\n",
      "Iteration 48337 => Loss: 6.69848455987456770089\n",
      "Iteration 48338 => Loss: 6.69848451854945992068\n",
      "Iteration 48339 => Loss: 6.69848447722989792652\n",
      "Iteration 48340 => Loss: 6.69848443591591813373\n",
      "Iteration 48341 => Loss: 6.69848439460750633145\n",
      "Iteration 48342 => Loss: 6.69848435330465452608\n",
      "Iteration 48343 => Loss: 6.69848431200736715851\n",
      "Iteration 48344 => Loss: 6.69848427071564866964\n",
      "Iteration 48345 => Loss: 6.69848422942949461856\n",
      "Iteration 48346 => Loss: 6.69848418814890500528\n",
      "Iteration 48347 => Loss: 6.69848414687387272437\n",
      "Iteration 48348 => Loss: 6.69848410560440576944\n",
      "Iteration 48349 => Loss: 6.69848406434049969960\n",
      "Iteration 48350 => Loss: 6.69848402308215007395\n",
      "Iteration 48351 => Loss: 6.69848398182936666245\n",
      "Iteration 48352 => Loss: 6.69848394058213614244\n",
      "Iteration 48353 => Loss: 6.69848389934047094840\n",
      "Iteration 48354 => Loss: 6.69848385810435598131\n",
      "Iteration 48355 => Loss: 6.69848381687379923477\n",
      "Iteration 48356 => Loss: 6.69848377564880514967\n",
      "Iteration 48357 => Loss: 6.69848373442935418609\n",
      "Iteration 48358 => Loss: 6.69848369321546055488\n",
      "Iteration 48359 => Loss: 6.69848365200712425604\n",
      "Iteration 48360 => Loss: 6.69848361080434351322\n",
      "Iteration 48361 => Loss: 6.69848356960711210917\n",
      "Iteration 48362 => Loss: 6.69848352841543359659\n",
      "Iteration 48363 => Loss: 6.69848348722929731736\n",
      "Iteration 48364 => Loss: 6.69848344604872103503\n",
      "Iteration 48365 => Loss: 6.69848340487368520968\n",
      "Iteration 48366 => Loss: 6.69848336370420138763\n",
      "Iteration 48367 => Loss: 6.69848332254026956889\n",
      "Iteration 48368 => Loss: 6.69848328138187909531\n",
      "Iteration 48369 => Loss: 6.69848324022903707231\n",
      "Iteration 48370 => Loss: 6.69848319908173994719\n",
      "Iteration 48371 => Loss: 6.69848315793998771994\n",
      "Iteration 48372 => Loss: 6.69848311680377417332\n",
      "Iteration 48373 => Loss: 6.69848307567311351818\n",
      "Iteration 48374 => Loss: 6.69848303454799332002\n",
      "Iteration 48375 => Loss: 6.69848299342840824977\n",
      "Iteration 48376 => Loss: 6.69848295231436985375\n",
      "Iteration 48377 => Loss: 6.69848291120586925018\n",
      "Iteration 48378 => Loss: 6.69848287010290288634\n",
      "Iteration 48379 => Loss: 6.69848282900548142038\n",
      "Iteration 48380 => Loss: 6.69848278791359685869\n",
      "Iteration 48381 => Loss: 6.69848274682725097762\n",
      "Iteration 48382 => Loss: 6.69848270574643755992\n",
      "Iteration 48383 => Loss: 6.69848266467116015832\n",
      "Iteration 48384 => Loss: 6.69848262360142054916\n",
      "Iteration 48385 => Loss: 6.69848258253720985067\n",
      "Iteration 48386 => Loss: 6.69848254147854316187\n",
      "Iteration 48387 => Loss: 6.69848250042539827831\n",
      "Iteration 48388 => Loss: 6.69848245937779651626\n",
      "Iteration 48389 => Loss: 6.69848241833571567128\n",
      "Iteration 48390 => Loss: 6.69848237729917084238\n",
      "Iteration 48391 => Loss: 6.69848233626814781871\n",
      "Iteration 48392 => Loss: 6.69848229524266347568\n",
      "Iteration 48393 => Loss: 6.69848225422270626694\n",
      "Iteration 48394 => Loss: 6.69848221320827086345\n",
      "Iteration 48395 => Loss: 6.69848217219936437061\n",
      "Iteration 48396 => Loss: 6.69848213119598412391\n",
      "Iteration 48397 => Loss: 6.69848209019813101150\n",
      "Iteration 48398 => Loss: 6.69848204920579792798\n",
      "Iteration 48399 => Loss: 6.69848200821899109059\n",
      "Iteration 48400 => Loss: 6.69848196723771138750\n",
      "Iteration 48401 => Loss: 6.69848192626194638422\n",
      "Iteration 48402 => Loss: 6.69848188529170762706\n",
      "Iteration 48403 => Loss: 6.69848184432698889879\n",
      "Iteration 48404 => Loss: 6.69848180336778753485\n",
      "Iteration 48405 => Loss: 6.69848176241411241705\n",
      "Iteration 48406 => Loss: 6.69848172146594400544\n",
      "Iteration 48407 => Loss: 6.69848168052330716904\n",
      "Iteration 48408 => Loss: 6.69848163958617881519\n",
      "Iteration 48409 => Loss: 6.69848159865456782569\n",
      "Iteration 48410 => Loss: 6.69848155772847064782\n",
      "Iteration 48411 => Loss: 6.69848151680788816975\n",
      "Iteration 48412 => Loss: 6.69848147589282572056\n",
      "Iteration 48413 => Loss: 6.69848143498327441847\n",
      "Iteration 48414 => Loss: 6.69848139407922982258\n",
      "Iteration 48415 => Loss: 6.69848135318070880828\n",
      "Iteration 48416 => Loss: 6.69848131228768739476\n",
      "Iteration 48417 => Loss: 6.69848127140017890468\n",
      "Iteration 48418 => Loss: 6.69848123051818156171\n",
      "Iteration 48419 => Loss: 6.69848118964169536582\n",
      "Iteration 48420 => Loss: 6.69848114877071143525\n",
      "Iteration 48421 => Loss: 6.69848110790523865177\n",
      "Iteration 48422 => Loss: 6.69848106704527257449\n",
      "Iteration 48423 => Loss: 6.69848102619081675613\n",
      "Iteration 48424 => Loss: 6.69848098534185609765\n",
      "Iteration 48425 => Loss: 6.69848094449840480991\n",
      "Iteration 48426 => Loss: 6.69848090366045667565\n",
      "Iteration 48427 => Loss: 6.69848086282801435942\n",
      "Iteration 48428 => Loss: 6.69848082200107253215\n",
      "Iteration 48429 => Loss: 6.69848078117963208200\n",
      "Iteration 48430 => Loss: 6.69848074036368856810\n",
      "Iteration 48431 => Loss: 6.69848069955325176039\n",
      "Iteration 48432 => Loss: 6.69848065874830922439\n",
      "Iteration 48433 => Loss: 6.69848061794886984188\n",
      "Iteration 48434 => Loss: 6.69848057715492473108\n",
      "Iteration 48435 => Loss: 6.69848053636647122744\n",
      "Iteration 48436 => Loss: 6.69848049558352354182\n",
      "Iteration 48437 => Loss: 6.69848045480607012792\n",
      "Iteration 48438 => Loss: 6.69848041403410476846\n",
      "Iteration 48439 => Loss: 6.69848037326764345067\n",
      "Iteration 48440 => Loss: 6.69848033250666485827\n",
      "Iteration 48441 => Loss: 6.69848029175118320211\n",
      "Iteration 48442 => Loss: 6.69848025100118960040\n",
      "Iteration 48443 => Loss: 6.69848021025669471129\n",
      "Iteration 48444 => Loss: 6.69848016951768787663\n",
      "Iteration 48445 => Loss: 6.69848012878417264915\n",
      "Iteration 48446 => Loss: 6.69848008805613837069\n",
      "Iteration 48447 => Loss: 6.69848004733359481122\n",
      "Iteration 48448 => Loss: 6.69848000661654374710\n",
      "Iteration 48449 => Loss: 6.69847996590497718472\n",
      "Iteration 48450 => Loss: 6.69847992519889601226\n",
      "Iteration 48451 => Loss: 6.69847988449829667701\n",
      "Iteration 48452 => Loss: 6.69847984380318450803\n",
      "Iteration 48453 => Loss: 6.69847980311355417626\n",
      "Iteration 48454 => Loss: 6.69847976242941278713\n",
      "Iteration 48455 => Loss: 6.69847972175074346524\n",
      "Iteration 48456 => Loss: 6.69847968107756663869\n",
      "Iteration 48457 => Loss: 6.69847964040986543210\n",
      "Iteration 48458 => Loss: 6.69847959974764251001\n",
      "Iteration 48459 => Loss: 6.69847955909089964877\n",
      "Iteration 48460 => Loss: 6.69847951843963951291\n",
      "Iteration 48461 => Loss: 6.69847947779385233247\n",
      "Iteration 48462 => Loss: 6.69847943715354254834\n",
      "Iteration 48463 => Loss: 6.69847939651871016054\n",
      "Iteration 48464 => Loss: 6.69847935588935516904\n",
      "Iteration 48465 => Loss: 6.69847931526546958025\n",
      "Iteration 48466 => Loss: 6.69847927464705961142\n",
      "Iteration 48467 => Loss: 6.69847923403412970345\n",
      "Iteration 48468 => Loss: 6.69847919342667008635\n",
      "Iteration 48469 => Loss: 6.69847915282467809561\n",
      "Iteration 48470 => Loss: 6.69847911222815728394\n",
      "Iteration 48471 => Loss: 6.69847907163710587497\n",
      "Iteration 48472 => Loss: 6.69847903105153275050\n",
      "Iteration 48473 => Loss: 6.69847899047141659423\n",
      "Iteration 48474 => Loss: 6.69847894989677694610\n",
      "Iteration 48475 => Loss: 6.69847890932760137161\n",
      "Iteration 48476 => Loss: 6.69847886876389431166\n",
      "Iteration 48477 => Loss: 6.69847882820565398987\n",
      "Iteration 48478 => Loss: 6.69847878765287152447\n",
      "Iteration 48479 => Loss: 6.69847874710556023814\n",
      "Iteration 48480 => Loss: 6.69847870656371480180\n",
      "Iteration 48481 => Loss: 6.69847866602732722185\n",
      "Iteration 48482 => Loss: 6.69847862549640549190\n",
      "Iteration 48483 => Loss: 6.69847858497094428287\n",
      "Iteration 48484 => Loss: 6.69847854445094093023\n",
      "Iteration 48485 => Loss: 6.69847850393640253941\n",
      "Iteration 48486 => Loss: 6.69847846342731756408\n",
      "Iteration 48487 => Loss: 6.69847842292370110329\n",
      "Iteration 48488 => Loss: 6.69847838242553539345\n",
      "Iteration 48489 => Loss: 6.69847834193282754001\n",
      "Iteration 48490 => Loss: 6.69847830144557576659\n",
      "Iteration 48491 => Loss: 6.69847826096377829685\n",
      "Iteration 48492 => Loss: 6.69847822048743868351\n",
      "Iteration 48493 => Loss: 6.69847818001655603837\n",
      "Iteration 48494 => Loss: 6.69847813955111703876\n",
      "Iteration 48495 => Loss: 6.69847809909113856008\n",
      "Iteration 48496 => Loss: 6.69847805863661083237\n",
      "Iteration 48497 => Loss: 6.69847801818753296743\n",
      "Iteration 48498 => Loss: 6.69847797774390496528\n",
      "Iteration 48499 => Loss: 6.69847793730572682591\n",
      "Iteration 48500 => Loss: 6.69847789687300032568\n",
      "Iteration 48501 => Loss: 6.69847785644572191188\n",
      "Iteration 48502 => Loss: 6.69847781602388625544\n",
      "Iteration 48503 => Loss: 6.69847777560750667902\n",
      "Iteration 48504 => Loss: 6.69847773519656453090\n",
      "Iteration 48505 => Loss: 6.69847769479107491009\n",
      "Iteration 48506 => Loss: 6.69847765439102449392\n",
      "Iteration 48507 => Loss: 6.69847761399641861146\n",
      "Iteration 48508 => Loss: 6.69847757360725815090\n",
      "Iteration 48509 => Loss: 6.69847753322354133587\n",
      "Iteration 48510 => Loss: 6.69847749284526194913\n",
      "Iteration 48511 => Loss: 6.69847745247241910249\n",
      "Iteration 48512 => Loss: 6.69847741210502700682\n",
      "Iteration 48513 => Loss: 6.69847737174306878671\n",
      "Iteration 48514 => Loss: 6.69847733138655154761\n",
      "Iteration 48515 => Loss: 6.69847729103546551954\n",
      "Iteration 48516 => Loss: 6.69847725068982313701\n",
      "Iteration 48517 => Loss: 6.69847721034961818276\n",
      "Iteration 48518 => Loss: 6.69847717001485065680\n",
      "Iteration 48519 => Loss: 6.69847712968551345369\n",
      "Iteration 48520 => Loss: 6.69847708936161101434\n",
      "Iteration 48521 => Loss: 6.69847704904314777963\n",
      "Iteration 48522 => Loss: 6.69847700873011309142\n",
      "Iteration 48523 => Loss: 6.69847696842250872606\n",
      "Iteration 48524 => Loss: 6.69847692812033823628\n",
      "Iteration 48525 => Loss: 6.69847688782359984572\n",
      "Iteration 48526 => Loss: 6.69847684753228467258\n",
      "Iteration 48527 => Loss: 6.69847680724640515137\n",
      "Iteration 48528 => Loss: 6.69847676696595506485\n",
      "Iteration 48529 => Loss: 6.69847672669093263664\n",
      "Iteration 48530 => Loss: 6.69847668642133164951\n",
      "Iteration 48531 => Loss: 6.69847664615716276160\n",
      "Iteration 48532 => Loss: 6.69847660589841531475\n",
      "Iteration 48533 => Loss: 6.69847656564510085531\n",
      "Iteration 48534 => Loss: 6.69847652539719540243\n",
      "Iteration 48535 => Loss: 6.69847648515472293695\n",
      "Iteration 48536 => Loss: 6.69847644491767546526\n",
      "Iteration 48537 => Loss: 6.69847640468604588193\n",
      "Iteration 48538 => Loss: 6.69847636445984040421\n",
      "Iteration 48539 => Loss: 6.69847632423905281485\n",
      "Iteration 48540 => Loss: 6.69847628402368311384\n",
      "Iteration 48541 => Loss: 6.69847624381373929481\n",
      "Iteration 48542 => Loss: 6.69847620360920981142\n",
      "Iteration 48543 => Loss: 6.69847616341009999275\n",
      "Iteration 48544 => Loss: 6.69847612321640628608\n",
      "Iteration 48545 => Loss: 6.69847608302812158598\n",
      "Iteration 48546 => Loss: 6.69847604284526010332\n",
      "Iteration 48547 => Loss: 6.69847600266781117995\n",
      "Iteration 48548 => Loss: 6.69847596249577303951\n",
      "Iteration 48549 => Loss: 6.69847592232915278743\n",
      "Iteration 48550 => Loss: 6.69847588216794331828\n",
      "Iteration 48551 => Loss: 6.69847584201214640842\n",
      "Iteration 48552 => Loss: 6.69847580186176028150\n",
      "Iteration 48553 => Loss: 6.69847576171678227297\n",
      "Iteration 48554 => Loss: 6.69847572157721682373\n",
      "Iteration 48555 => Loss: 6.69847568144306126925\n",
      "Iteration 48556 => Loss: 6.69847564131430583956\n",
      "Iteration 48557 => Loss: 6.69847560119096208098\n",
      "Iteration 48558 => Loss: 6.69847556107302821715\n",
      "Iteration 48559 => Loss: 6.69847552096049891901\n",
      "Iteration 48560 => Loss: 6.69847548085337241019\n",
      "Iteration 48561 => Loss: 6.69847544075164869071\n",
      "Iteration 48562 => Loss: 6.69847540065533397780\n",
      "Iteration 48563 => Loss: 6.69847536056442027785\n",
      "Iteration 48564 => Loss: 6.69847532047890581453\n",
      "Iteration 48565 => Loss: 6.69847528039879769324\n",
      "Iteration 48566 => Loss: 6.69847524032408614403\n",
      "Iteration 48567 => Loss: 6.69847520025477827232\n",
      "Iteration 48568 => Loss: 6.69847516019086697270\n",
      "Iteration 48569 => Loss: 6.69847512013235224515\n",
      "Iteration 48570 => Loss: 6.69847508007924385964\n",
      "Iteration 48571 => Loss: 6.69847504003152582897\n",
      "Iteration 48572 => Loss: 6.69847499998921147579\n",
      "Iteration 48573 => Loss: 6.69847495995228126020\n",
      "Iteration 48574 => Loss: 6.69847491992075561029\n",
      "Iteration 48575 => Loss: 6.69847487989460965707\n",
      "Iteration 48576 => Loss: 6.69847483987387981585\n",
      "Iteration 48577 => Loss: 6.69847479985853233586\n",
      "Iteration 48578 => Loss: 6.69847475984857076980\n",
      "Iteration 48579 => Loss: 6.69847471984400488765\n",
      "Iteration 48580 => Loss: 6.69847467984483468939\n",
      "Iteration 48581 => Loss: 6.69847463985104596418\n",
      "Iteration 48582 => Loss: 6.69847459986265114651\n",
      "Iteration 48583 => Loss: 6.69847455987964401913\n",
      "Iteration 48584 => Loss: 6.69847451990202191752\n",
      "Iteration 48585 => Loss: 6.69847447992978839437\n",
      "Iteration 48586 => Loss: 6.69847443996295144331\n",
      "Iteration 48587 => Loss: 6.69847440000148708350\n",
      "Iteration 48588 => Loss: 6.69847436004541130217\n",
      "Iteration 48589 => Loss: 6.69847432009471788206\n",
      "Iteration 48590 => Loss: 6.69847428014941392860\n",
      "Iteration 48591 => Loss: 6.69847424020948611911\n",
      "Iteration 48592 => Loss: 6.69847420027494422357\n",
      "Iteration 48593 => Loss: 6.69847416034577669564\n",
      "Iteration 48594 => Loss: 6.69847412042199152893\n",
      "Iteration 48595 => Loss: 6.69847408050359227616\n",
      "Iteration 48596 => Loss: 6.69847404059056739101\n",
      "Iteration 48597 => Loss: 6.69847400068292664344\n",
      "Iteration 48598 => Loss: 6.69847396078065404623\n",
      "Iteration 48599 => Loss: 6.69847392088376736297\n",
      "Iteration 48600 => Loss: 6.69847388099225149460\n",
      "Iteration 48601 => Loss: 6.69847384110611532293\n",
      "Iteration 48602 => Loss: 6.69847380122534552527\n",
      "Iteration 48603 => Loss: 6.69847376134995275976\n",
      "Iteration 48604 => Loss: 6.69847372147993702640\n",
      "Iteration 48605 => Loss: 6.69847368161529654884\n",
      "Iteration 48606 => Loss: 6.69847364175602510983\n",
      "Iteration 48607 => Loss: 6.69847360190211826847\n",
      "Iteration 48608 => Loss: 6.69847356205358401837\n",
      "Iteration 48609 => Loss: 6.69847352221042502407\n",
      "Iteration 48610 => Loss: 6.69847348237263062742\n",
      "Iteration 48611 => Loss: 6.69847344254020793386\n",
      "Iteration 48612 => Loss: 6.69847340271314628524\n",
      "Iteration 48613 => Loss: 6.69847336289145722787\n",
      "Iteration 48614 => Loss: 6.69847332307513187999\n",
      "Iteration 48615 => Loss: 6.69847328326417290612\n",
      "Iteration 48616 => Loss: 6.69847324345857497718\n",
      "Iteration 48617 => Loss: 6.69847320365834875133\n",
      "Iteration 48618 => Loss: 6.69847316386347824135\n",
      "Iteration 48619 => Loss: 6.69847312407396966449\n",
      "Iteration 48620 => Loss: 6.69847308428982834982\n",
      "Iteration 48621 => Loss: 6.69847304451103919831\n",
      "Iteration 48622 => Loss: 6.69847300473761819717\n",
      "Iteration 48623 => Loss: 6.69847296496955380007\n",
      "Iteration 48624 => Loss: 6.69847292520685222428\n",
      "Iteration 48625 => Loss: 6.69847288544950458800\n",
      "Iteration 48626 => Loss: 6.69847284569751089123\n",
      "Iteration 48627 => Loss: 6.69847280595087646304\n",
      "Iteration 48628 => Loss: 6.69847276620960396798\n",
      "Iteration 48629 => Loss: 6.69847272647368097154\n",
      "Iteration 48630 => Loss: 6.69847268674311013825\n",
      "Iteration 48631 => Loss: 6.69847264701789857355\n",
      "Iteration 48632 => Loss: 6.69847260729803473112\n",
      "Iteration 48633 => Loss: 6.69847256758352571637\n",
      "Iteration 48634 => Loss: 6.69847252787436442389\n",
      "Iteration 48635 => Loss: 6.69847248817056506454\n",
      "Iteration 48636 => Loss: 6.69847244847210454566\n",
      "Iteration 48637 => Loss: 6.69847240877899885447\n",
      "Iteration 48638 => Loss: 6.69847236909124088555\n",
      "Iteration 48639 => Loss: 6.69847232940882975072\n",
      "Iteration 48640 => Loss: 6.69847228973176633815\n",
      "Iteration 48641 => Loss: 6.69847225006005242420\n",
      "Iteration 48642 => Loss: 6.69847221039367646256\n",
      "Iteration 48643 => Loss: 6.69847217073265355225\n",
      "Iteration 48644 => Loss: 6.69847213107697481149\n",
      "Iteration 48645 => Loss: 6.69847209142663668757\n",
      "Iteration 48646 => Loss: 6.69847205178163829231\n",
      "Iteration 48647 => Loss: 6.69847201214198229025\n",
      "Iteration 48648 => Loss: 6.69847197250767756316\n",
      "Iteration 48649 => Loss: 6.69847193287870457112\n",
      "Iteration 48650 => Loss: 6.69847189325507486046\n",
      "Iteration 48651 => Loss: 6.69847185363677777303\n",
      "Iteration 48652 => Loss: 6.69847181402382929605\n",
      "Iteration 48653 => Loss: 6.69847177441621166594\n",
      "Iteration 48654 => Loss: 6.69847173481393198813\n",
      "Iteration 48655 => Loss: 6.69847169521699292716\n",
      "Iteration 48656 => Loss: 6.69847165562538826578\n",
      "Iteration 48657 => Loss: 6.69847161603911800398\n",
      "Iteration 48658 => Loss: 6.69847157645818658267\n",
      "Iteration 48659 => Loss: 6.69847153688257979098\n",
      "Iteration 48660 => Loss: 6.69847149731230739889\n",
      "Iteration 48661 => Loss: 6.69847145774737207091\n",
      "Iteration 48662 => Loss: 6.69847141818775959621\n",
      "Iteration 48663 => Loss: 6.69847137863348418563\n",
      "Iteration 48664 => Loss: 6.69847133908453606921\n",
      "Iteration 48665 => Loss: 6.69847129954092057602\n",
      "Iteration 48666 => Loss: 6.69847126000262971246\n",
      "Iteration 48667 => Loss: 6.69847122046966969577\n",
      "Iteration 48668 => Loss: 6.69847118094203874961\n",
      "Iteration 48669 => Loss: 6.69847114141973065671\n",
      "Iteration 48670 => Loss: 6.69847110190274896979\n",
      "Iteration 48671 => Loss: 6.69847106239109368886\n",
      "Iteration 48672 => Loss: 6.69847102288476392573\n",
      "Iteration 48673 => Loss: 6.69847098338375079862\n",
      "Iteration 48674 => Loss: 6.69847094388806407750\n",
      "Iteration 48675 => Loss: 6.69847090439769932146\n",
      "Iteration 48676 => Loss: 6.69847086491265386599\n",
      "Iteration 48677 => Loss: 6.69847082543293392831\n",
      "Iteration 48678 => Loss: 6.69847078595852796212\n",
      "Iteration 48679 => Loss: 6.69847074648945106645\n",
      "Iteration 48680 => Loss: 6.69847070702568370137\n",
      "Iteration 48681 => Loss: 6.69847066756723830139\n",
      "Iteration 48682 => Loss: 6.69847062811410687289\n",
      "Iteration 48683 => Loss: 6.69847058866629474494\n",
      "Iteration 48684 => Loss: 6.69847054922379303576\n",
      "Iteration 48685 => Loss: 6.69847050978660885079\n",
      "Iteration 48686 => Loss: 6.69847047035474041365\n",
      "Iteration 48687 => Loss: 6.69847043092818683618\n",
      "Iteration 48688 => Loss: 6.69847039150694190113\n",
      "Iteration 48689 => Loss: 6.69847035209101271391\n",
      "Iteration 48690 => Loss: 6.69847031268039483365\n",
      "Iteration 48691 => Loss: 6.69847027327508293126\n",
      "Iteration 48692 => Loss: 6.69847023387508588854\n",
      "Iteration 48693 => Loss: 6.69847019448039038281\n",
      "Iteration 48694 => Loss: 6.69847015509101595399\n",
      "Iteration 48695 => Loss: 6.69847011570694395033\n",
      "Iteration 48696 => Loss: 6.69847007632816993095\n",
      "Iteration 48697 => Loss: 6.69847003695471432394\n",
      "Iteration 48698 => Loss: 6.69846999758655226032\n",
      "Iteration 48699 => Loss: 6.69846995822370772089\n",
      "Iteration 48700 => Loss: 6.69846991886616294209\n",
      "Iteration 48701 => Loss: 6.69846987951391881211\n",
      "Iteration 48702 => Loss: 6.69846984016697977182\n",
      "Iteration 48703 => Loss: 6.69846980082534670942\n",
      "Iteration 48704 => Loss: 6.69846976148900363768\n",
      "Iteration 48705 => Loss: 6.69846972215796743200\n",
      "Iteration 48706 => Loss: 6.69846968283223631602\n",
      "Iteration 48707 => Loss: 6.69846964351179430253\n",
      "Iteration 48708 => Loss: 6.69846960419665649056\n",
      "Iteration 48709 => Loss: 6.69846956488681843922\n",
      "Iteration 48710 => Loss: 6.69846952558227304309\n",
      "Iteration 48711 => Loss: 6.69846948628302829576\n",
      "Iteration 48712 => Loss: 6.69846944698907886817\n",
      "Iteration 48713 => Loss: 6.69846940770041765489\n",
      "Iteration 48714 => Loss: 6.69846936841705442589\n",
      "Iteration 48715 => Loss: 6.69846932913898562845\n",
      "Iteration 48716 => Loss: 6.69846928986620859803\n",
      "Iteration 48717 => Loss: 6.69846925059872244645\n",
      "Iteration 48718 => Loss: 6.69846921133653250280\n",
      "Iteration 48719 => Loss: 6.69846917207962899710\n",
      "Iteration 48720 => Loss: 6.69846913282801192935\n",
      "Iteration 48721 => Loss: 6.69846909358169018134\n",
      "Iteration 48722 => Loss: 6.69846905434065220675\n",
      "Iteration 48723 => Loss: 6.69846901510490688736\n",
      "Iteration 48724 => Loss: 6.69846897587444445321\n",
      "Iteration 48725 => Loss: 6.69846893664926668066\n",
      "Iteration 48726 => Loss: 6.69846889742938689238\n",
      "Iteration 48727 => Loss: 6.69846885821477666667\n",
      "Iteration 48728 => Loss: 6.69846881900545909616\n",
      "Iteration 48729 => Loss: 6.69846877980142174636\n",
      "Iteration 48730 => Loss: 6.69846874060267172268\n",
      "Iteration 48731 => Loss: 6.69846870140919925518\n",
      "Iteration 48732 => Loss: 6.69846866222100789656\n",
      "Iteration 48733 => Loss: 6.69846862303810208772\n",
      "Iteration 48734 => Loss: 6.69846858386047117051\n",
      "Iteration 48735 => Loss: 6.69846854468811692129\n",
      "Iteration 48736 => Loss: 6.69846850552105088639\n",
      "Iteration 48737 => Loss: 6.69846846635925530222\n",
      "Iteration 48738 => Loss: 6.69846842720273638605\n",
      "Iteration 48739 => Loss: 6.69846838805149680240\n",
      "Iteration 48740 => Loss: 6.69846834890553122221\n",
      "Iteration 48741 => Loss: 6.69846830976483875730\n",
      "Iteration 48742 => Loss: 6.69846827062942029585\n",
      "Iteration 48743 => Loss: 6.69846823149927583785\n",
      "Iteration 48744 => Loss: 6.69846819237441160055\n",
      "Iteration 48745 => Loss: 6.69846815325481603765\n",
      "Iteration 48746 => Loss: 6.69846811414048559641\n",
      "Iteration 48747 => Loss: 6.69846807503143093498\n",
      "Iteration 48748 => Loss: 6.69846803592764228341\n",
      "Iteration 48749 => Loss: 6.69846799682912763529\n",
      "Iteration 48750 => Loss: 6.69846795773588343792\n",
      "Iteration 48751 => Loss: 6.69846791864790436222\n",
      "Iteration 48752 => Loss: 6.69846787956518507912\n",
      "Iteration 48753 => Loss: 6.69846784048774157583\n",
      "Iteration 48754 => Loss: 6.69846780141556141785\n",
      "Iteration 48755 => Loss: 6.69846776234864726973\n",
      "Iteration 48756 => Loss: 6.69846772328699735510\n",
      "Iteration 48757 => Loss: 6.69846768423061078579\n",
      "Iteration 48758 => Loss: 6.69846764517948045636\n",
      "Iteration 48759 => Loss: 6.69846760613361968950\n",
      "Iteration 48760 => Loss: 6.69846756709301516253\n",
      "Iteration 48761 => Loss: 6.69846752805767664540\n",
      "Iteration 48762 => Loss: 6.69846748902758992728\n",
      "Iteration 48763 => Loss: 6.69846745000277188353\n",
      "Iteration 48764 => Loss: 6.69846741098320475061\n",
      "Iteration 48765 => Loss: 6.69846737196890629207\n",
      "Iteration 48766 => Loss: 6.69846733295985963252\n",
      "Iteration 48767 => Loss: 6.69846729395606832469\n",
      "Iteration 48768 => Loss: 6.69846725495752881585\n",
      "Iteration 48769 => Loss: 6.69846721596425176415\n",
      "Iteration 48770 => Loss: 6.69846717697622118237\n",
      "Iteration 48771 => Loss: 6.69846713799344950502\n",
      "Iteration 48772 => Loss: 6.69846709901593051484\n",
      "Iteration 48773 => Loss: 6.69846706004366065912\n",
      "Iteration 48774 => Loss: 6.69846702107665059600\n",
      "Iteration 48775 => Loss: 6.69846698211487723285\n",
      "Iteration 48776 => Loss: 6.69846694315836810318\n",
      "Iteration 48777 => Loss: 6.69846690420710277891\n",
      "Iteration 48778 => Loss: 6.69846686526107948367\n",
      "Iteration 48779 => Loss: 6.69846682632031598104\n",
      "Iteration 48780 => Loss: 6.69846678738479628379\n",
      "Iteration 48781 => Loss: 6.69846674845451950375\n",
      "Iteration 48782 => Loss: 6.69846670952948919364\n",
      "Iteration 48783 => Loss: 6.69846667060970357710\n",
      "Iteration 48784 => Loss: 6.69846663169516620684\n",
      "Iteration 48785 => Loss: 6.69846659278587264197\n",
      "Iteration 48786 => Loss: 6.69846655388181932977\n",
      "Iteration 48787 => Loss: 6.69846651498300627026\n",
      "Iteration 48788 => Loss: 6.69846647608943523977\n",
      "Iteration 48789 => Loss: 6.69846643720110712650\n",
      "Iteration 48790 => Loss: 6.69846639831802459497\n",
      "Iteration 48791 => Loss: 6.69846635944017076980\n",
      "Iteration 48792 => Loss: 6.69846632056756252638\n",
      "Iteration 48793 => Loss: 6.69846628170018920656\n",
      "Iteration 48794 => Loss: 6.69846624283805258671\n",
      "Iteration 48795 => Loss: 6.69846620398116066042\n",
      "Iteration 48796 => Loss: 6.69846616512949655231\n",
      "Iteration 48797 => Loss: 6.69846612628307092052\n",
      "Iteration 48798 => Loss: 6.69846608744188110052\n",
      "Iteration 48799 => Loss: 6.69846604860592265140\n",
      "Iteration 48800 => Loss: 6.69846600977519734954\n",
      "Iteration 48801 => Loss: 6.69846597094970608310\n",
      "Iteration 48802 => Loss: 6.69846593212944263485\n",
      "Iteration 48803 => Loss: 6.69846589331441588655\n",
      "Iteration 48804 => Loss: 6.69846585450461429190\n",
      "Iteration 48805 => Loss: 6.69846581570004673267\n",
      "Iteration 48806 => Loss: 6.69846577690070521527\n",
      "Iteration 48807 => Loss: 6.69846573810659151604\n",
      "Iteration 48808 => Loss: 6.69846569931771185225\n",
      "Iteration 48809 => Loss: 6.69846566053405290120\n",
      "Iteration 48810 => Loss: 6.69846562175562088015\n",
      "Iteration 48811 => Loss: 6.69846558298241401275\n",
      "Iteration 48812 => Loss: 6.69846554421443229899\n",
      "Iteration 48813 => Loss: 6.69846550545167485069\n",
      "Iteration 48814 => Loss: 6.69846546669413722697\n",
      "Iteration 48815 => Loss: 6.69846542794182653324\n",
      "Iteration 48816 => Loss: 6.69846538919473744045\n",
      "Iteration 48817 => Loss: 6.69846535045287172494\n",
      "Iteration 48818 => Loss: 6.69846531171621872858\n",
      "Iteration 48819 => Loss: 6.69846527298479532675\n",
      "Iteration 48820 => Loss: 6.69846523425858375589\n",
      "Iteration 48821 => Loss: 6.69846519553758845689\n",
      "Iteration 48822 => Loss: 6.69846515682181919971\n",
      "Iteration 48823 => Loss: 6.69846511811126266167\n",
      "Iteration 48824 => Loss: 6.69846507940592061914\n",
      "Iteration 48825 => Loss: 6.69846504070579218393\n",
      "Iteration 48826 => Loss: 6.69846500201087913240\n",
      "Iteration 48827 => Loss: 6.69846496332119123451\n",
      "Iteration 48828 => Loss: 6.69846492463670628581\n",
      "Iteration 48829 => Loss: 6.69846488595743938532\n",
      "Iteration 48830 => Loss: 6.69846484728337721037\n",
      "Iteration 48831 => Loss: 6.69846480861452953093\n",
      "Iteration 48832 => Loss: 6.69846476995089723516\n",
      "Iteration 48833 => Loss: 6.69846473129247232947\n",
      "Iteration 48834 => Loss: 6.69846469263925303750\n",
      "Iteration 48835 => Loss: 6.69846465399124735285\n",
      "Iteration 48836 => Loss: 6.69846461534844639374\n",
      "Iteration 48837 => Loss: 6.69846457671084749563\n",
      "Iteration 48838 => Loss: 6.69846453807846753392\n",
      "Iteration 48839 => Loss: 6.69846449945128785686\n",
      "Iteration 48840 => Loss: 6.69846446082930579990\n",
      "Iteration 48841 => Loss: 6.69846442221253646210\n",
      "Iteration 48842 => Loss: 6.69846438360096652076\n",
      "Iteration 48843 => Loss: 6.69846434499460485767\n",
      "Iteration 48844 => Loss: 6.69846430639343815017\n",
      "Iteration 48845 => Loss: 6.69846426779747350366\n",
      "Iteration 48846 => Loss: 6.69846422920672157630\n",
      "Iteration 48847 => Loss: 6.69846419062116016363\n",
      "Iteration 48848 => Loss: 6.69846415204079814742\n",
      "Iteration 48849 => Loss: 6.69846411346563641587\n",
      "Iteration 48850 => Loss: 6.69846407489567408078\n",
      "Iteration 48851 => Loss: 6.69846403633090670127\n",
      "Iteration 48852 => Loss: 6.69846399777133516551\n",
      "Iteration 48853 => Loss: 6.69846395921695680897\n",
      "Iteration 48854 => Loss: 6.69846392066778140162\n",
      "Iteration 48855 => Loss: 6.69846388212379295624\n",
      "Iteration 48856 => Loss: 6.69846384358500746004\n",
      "Iteration 48857 => Loss: 6.69846380505140714945\n",
      "Iteration 48858 => Loss: 6.69846376652300268262\n",
      "Iteration 48859 => Loss: 6.69846372799978428958\n",
      "Iteration 48860 => Loss: 6.69846368948176618119\n",
      "Iteration 48861 => Loss: 6.69846365096893503477\n",
      "Iteration 48862 => Loss: 6.69846361246128996214\n",
      "Iteration 48863 => Loss: 6.69846357395883629238\n",
      "Iteration 48864 => Loss: 6.69846353546157402548\n",
      "Iteration 48865 => Loss: 6.69846349696949250330\n",
      "Iteration 48866 => Loss: 6.69846345848259616673\n",
      "Iteration 48867 => Loss: 6.69846342000089212121\n",
      "Iteration 48868 => Loss: 6.69846338152437148494\n",
      "Iteration 48869 => Loss: 6.69846334305303692247\n",
      "Iteration 48870 => Loss: 6.69846330458688932197\n",
      "Iteration 48871 => Loss: 6.69846326612592157801\n",
      "Iteration 48872 => Loss: 6.69846322767013546695\n",
      "Iteration 48873 => Loss: 6.69846318921953187697\n",
      "Iteration 48874 => Loss: 6.69846315077410903172\n",
      "Iteration 48875 => Loss: 6.69846311233386781936\n",
      "Iteration 48876 => Loss: 6.69846307389880823990\n",
      "Iteration 48877 => Loss: 6.69846303546892141156\n",
      "Iteration 48878 => Loss: 6.69846299704421799248\n",
      "Iteration 48879 => Loss: 6.69846295862469798266\n",
      "Iteration 48880 => Loss: 6.69846292021034805941\n",
      "Iteration 48881 => Loss: 6.69846288180117444000\n",
      "Iteration 48882 => Loss: 6.69846284339717623624\n",
      "Iteration 48883 => Loss: 6.69846280499835167177\n",
      "Iteration 48884 => Loss: 6.69846276660470429931\n",
      "Iteration 48885 => Loss: 6.69846272821623234250\n",
      "Iteration 48886 => Loss: 6.69846268983292514321\n",
      "Iteration 48887 => Loss: 6.69846265145479424774\n",
      "Iteration 48888 => Loss: 6.69846261308183343886\n",
      "Iteration 48889 => Loss: 6.69846257471405337469\n",
      "Iteration 48890 => Loss: 6.69846253635143451532\n",
      "Iteration 48891 => Loss: 6.69846249799398396618\n",
      "Iteration 48892 => Loss: 6.69846245964170439180\n",
      "Iteration 48893 => Loss: 6.69846242129459223946\n",
      "Iteration 48894 => Loss: 6.69846238295264750917\n",
      "Iteration 48895 => Loss: 6.69846234461586842457\n",
      "Iteration 48896 => Loss: 6.69846230628426209108\n",
      "Iteration 48897 => Loss: 6.69846226795781429786\n",
      "Iteration 48898 => Loss: 6.69846222963653215032\n",
      "Iteration 48899 => Loss: 6.69846219132041031941\n",
      "Iteration 48900 => Loss: 6.69846215300945768689\n",
      "Iteration 48901 => Loss: 6.69846211470366892371\n",
      "Iteration 48902 => Loss: 6.69846207640303781261\n",
      "Iteration 48903 => Loss: 6.69846203810756612995\n",
      "Iteration 48904 => Loss: 6.69846199981725654027\n",
      "Iteration 48905 => Loss: 6.69846196153210993174\n",
      "Iteration 48906 => Loss: 6.69846192325211386986\n",
      "Iteration 48907 => Loss: 6.69846188497727901279\n",
      "Iteration 48908 => Loss: 6.69846184670760713686\n",
      "Iteration 48909 => Loss: 6.69846180844308758395\n",
      "Iteration 48910 => Loss: 6.69846177018372213041\n",
      "Iteration 48911 => Loss: 6.69846173192951876985\n",
      "Iteration 48912 => Loss: 6.69846169368046773229\n",
      "Iteration 48913 => Loss: 6.69846165543656635322\n",
      "Iteration 48914 => Loss: 6.69846161719781907351\n",
      "Iteration 48915 => Loss: 6.69846157896422944589\n",
      "Iteration 48916 => Loss: 6.69846154073578947674\n",
      "Iteration 48917 => Loss: 6.69846150251250360697\n",
      "Iteration 48918 => Loss: 6.69846146429436739567\n",
      "Iteration 48919 => Loss: 6.69846142608137729013\n",
      "Iteration 48920 => Loss: 6.69846138787353062582\n",
      "Iteration 48921 => Loss: 6.69846134967084783085\n",
      "Iteration 48922 => Loss: 6.69846131147329959532\n",
      "Iteration 48923 => Loss: 6.69846127328091256459\n",
      "Iteration 48924 => Loss: 6.69846123509365387605\n",
      "Iteration 48925 => Loss: 6.69846119691155283959\n",
      "Iteration 48926 => Loss: 6.69846115873460234980\n",
      "Iteration 48927 => Loss: 6.69846112056278109037\n",
      "Iteration 48928 => Loss: 6.69846108239611393032\n",
      "Iteration 48929 => Loss: 6.69846104423458132970\n",
      "Iteration 48930 => Loss: 6.69846100607819483486\n",
      "Iteration 48931 => Loss: 6.69846096792695444577\n",
      "Iteration 48932 => Loss: 6.69846092978085128067\n",
      "Iteration 48933 => Loss: 6.69846089163989155679\n",
      "Iteration 48934 => Loss: 6.69846085350406816872\n",
      "Iteration 48935 => Loss: 6.69846081537338289280\n",
      "Iteration 48936 => Loss: 6.69846077724783750540\n",
      "Iteration 48937 => Loss: 6.69846073912742578926\n",
      "Iteration 48938 => Loss: 6.69846070101215662618\n",
      "Iteration 48939 => Loss: 6.69846066290201935800\n",
      "Iteration 48940 => Loss: 6.69846062479702375470\n",
      "Iteration 48941 => Loss: 6.69846058669715649359\n",
      "Iteration 48942 => Loss: 6.69846054860241846285\n",
      "Iteration 48943 => Loss: 6.69846051051282209698\n",
      "Iteration 48944 => Loss: 6.69846047242835762603\n",
      "Iteration 48945 => Loss: 6.69846043434902238545\n",
      "Iteration 48946 => Loss: 6.69846039627481992795\n",
      "Iteration 48947 => Loss: 6.69846035820574758901\n",
      "Iteration 48948 => Loss: 6.69846032014180270409\n",
      "Iteration 48949 => Loss: 6.69846028208299149043\n",
      "Iteration 48950 => Loss: 6.69846024402930328989\n",
      "Iteration 48951 => Loss: 6.69846020598075142516\n",
      "Iteration 48952 => Loss: 6.69846016793731813266\n",
      "Iteration 48953 => Loss: 6.69846012989901318235\n",
      "Iteration 48954 => Loss: 6.69846009186583035699\n",
      "Iteration 48955 => Loss: 6.69846005383777942654\n",
      "Iteration 48956 => Loss: 6.69846001581485861465\n",
      "Iteration 48957 => Loss: 6.69845997779704838138\n",
      "Iteration 48958 => Loss: 6.69845993978436737848\n",
      "Iteration 48959 => Loss: 6.69845990177680494781\n",
      "Iteration 48960 => Loss: 6.69845986377436197756\n",
      "Iteration 48961 => Loss: 6.69845982577704823768\n",
      "Iteration 48962 => Loss: 6.69845978778484685279\n",
      "Iteration 48963 => Loss: 6.69845974979777114555\n",
      "Iteration 48964 => Loss: 6.69845971181581134601\n",
      "Iteration 48965 => Loss: 6.69845967383896301328\n",
      "Iteration 48966 => Loss: 6.69845963586724035821\n",
      "Iteration 48967 => Loss: 6.69845959790063272266\n",
      "Iteration 48968 => Loss: 6.69845955993914454751\n",
      "Iteration 48969 => Loss: 6.69845952198276961553\n",
      "Iteration 48970 => Loss: 6.69845948403150348582\n",
      "Iteration 48971 => Loss: 6.69845944608535681652\n",
      "Iteration 48972 => Loss: 6.69845940814432783128\n",
      "Iteration 48973 => Loss: 6.69845937020840231924\n",
      "Iteration 48974 => Loss: 6.69845933227759271489\n",
      "Iteration 48975 => Loss: 6.69845929435189457735\n",
      "Iteration 48976 => Loss: 6.69845925643130524207\n",
      "Iteration 48977 => Loss: 6.69845921851582737361\n",
      "Iteration 48978 => Loss: 6.69845918060546274830\n",
      "Iteration 48979 => Loss: 6.69845914270019537895\n",
      "Iteration 48980 => Loss: 6.69845910480004302912\n",
      "Iteration 48981 => Loss: 6.69845906690499592884\n",
      "Iteration 48982 => Loss: 6.69845902901505496629\n",
      "Iteration 48983 => Loss: 6.69845899113022102966\n",
      "Iteration 48984 => Loss: 6.69845895325049500713\n",
      "Iteration 48985 => Loss: 6.69845891537586712872\n",
      "Iteration 48986 => Loss: 6.69845887750634894076\n",
      "Iteration 48987 => Loss: 6.69845883964192800875\n",
      "Iteration 48988 => Loss: 6.69845880178261054994\n",
      "Iteration 48989 => Loss: 6.69845876392839922886\n",
      "Iteration 48990 => Loss: 6.69845872607928338738\n",
      "Iteration 48991 => Loss: 6.69845868823526835456\n",
      "Iteration 48992 => Loss: 6.69845865039635324223\n",
      "Iteration 48993 => Loss: 6.69845861256253627403\n",
      "Iteration 48994 => Loss: 6.69845857473382100267\n",
      "Iteration 48995 => Loss: 6.69845853691020387544\n",
      "Iteration 48996 => Loss: 6.69845849909167601055\n",
      "Iteration 48997 => Loss: 6.69845846127825605976\n",
      "Iteration 48998 => Loss: 6.69845842346991915406\n",
      "Iteration 48999 => Loss: 6.69845838566668572156\n",
      "Iteration 49000 => Loss: 6.69845834786853888687\n",
      "Iteration 49001 => Loss: 6.69845831007548841995\n",
      "Iteration 49002 => Loss: 6.69845827228753432081\n",
      "Iteration 49003 => Loss: 6.69845823450466770765\n",
      "Iteration 49004 => Loss: 6.69845819672689302138\n",
      "Iteration 49005 => Loss: 6.69845815895421115016\n",
      "Iteration 49006 => Loss: 6.69845812118661854129\n",
      "Iteration 49007 => Loss: 6.69845808342411608294\n",
      "Iteration 49008 => Loss: 6.69845804566669933422\n",
      "Iteration 49009 => Loss: 6.69845800791437095967\n",
      "Iteration 49010 => Loss: 6.69845797016712740657\n",
      "Iteration 49011 => Loss: 6.69845793242497844489\n",
      "Iteration 49012 => Loss: 6.69845789468790542287\n",
      "Iteration 49013 => Loss: 6.69845785695592077502\n",
      "Iteration 49014 => Loss: 6.69845781922902361316\n",
      "Iteration 49015 => Loss: 6.69845778150720772004\n",
      "Iteration 49016 => Loss: 6.69845774379047309566\n",
      "Iteration 49017 => Loss: 6.69845770607882773362\n",
      "Iteration 49018 => Loss: 6.69845766837225564672\n",
      "Iteration 49019 => Loss: 6.69845763067077015762\n",
      "Iteration 49020 => Loss: 6.69845759297436060820\n",
      "Iteration 49021 => Loss: 6.69845755528303410387\n",
      "Iteration 49022 => Loss: 6.69845751759678975645\n",
      "Iteration 49023 => Loss: 6.69845747991561957235\n",
      "Iteration 49024 => Loss: 6.69845744223952355156\n",
      "Iteration 49025 => Loss: 6.69845740456850702316\n",
      "Iteration 49026 => Loss: 6.69845736690256821078\n",
      "Iteration 49027 => Loss: 6.69845732924170444988\n",
      "Iteration 49028 => Loss: 6.69845729158591485231\n",
      "Iteration 49029 => Loss: 6.69845725393519941804\n",
      "Iteration 49030 => Loss: 6.69845721628955903526\n",
      "Iteration 49031 => Loss: 6.69845717864898571037\n",
      "Iteration 49032 => Loss: 6.69845714101348832514\n",
      "Iteration 49033 => Loss: 6.69845710338306599141\n",
      "Iteration 49034 => Loss: 6.69845706575770538649\n",
      "Iteration 49035 => Loss: 6.69845702813742693849\n",
      "Iteration 49036 => Loss: 6.69845699052221288383\n",
      "Iteration 49037 => Loss: 6.69845695291205789346\n",
      "Iteration 49038 => Loss: 6.69845691530697884275\n",
      "Iteration 49039 => Loss: 6.69845687770696596175\n",
      "Iteration 49040 => Loss: 6.69845684011202369135\n",
      "Iteration 49041 => Loss: 6.69845680252214048522\n",
      "Iteration 49042 => Loss: 6.69845676493732966605\n",
      "Iteration 49043 => Loss: 6.69845672735757879934\n",
      "Iteration 49044 => Loss: 6.69845668978289587869\n",
      "Iteration 49045 => Loss: 6.69845665221327113414\n",
      "Iteration 49046 => Loss: 6.69845661464871788837\n",
      "Iteration 49047 => Loss: 6.69845657708921304874\n",
      "Iteration 49048 => Loss: 6.69845653953477615516\n",
      "Iteration 49049 => Loss: 6.69845650198540099041\n",
      "Iteration 49050 => Loss: 6.69845646444108577811\n",
      "Iteration 49051 => Loss: 6.69845642690181986012\n",
      "Iteration 49052 => Loss: 6.69845638936762188820\n",
      "Iteration 49053 => Loss: 6.69845635183848475691\n",
      "Iteration 49054 => Loss: 6.69845631431439869630\n",
      "Iteration 49055 => Loss: 6.69845627679536903543\n",
      "Iteration 49056 => Loss: 6.69845623928140199155\n",
      "Iteration 49057 => Loss: 6.69845620177248513016\n",
      "Iteration 49058 => Loss: 6.69845616426861933945\n",
      "Iteration 49059 => Loss: 6.69845612676981261302\n",
      "Iteration 49060 => Loss: 6.69845608927605606908\n",
      "Iteration 49061 => Loss: 6.69845605178735237217\n",
      "Iteration 49062 => Loss: 6.69845601430369708140\n",
      "Iteration 49063 => Loss: 6.69845597682508930859\n",
      "Iteration 49064 => Loss: 6.69845593935154504095\n",
      "Iteration 49065 => Loss: 6.69845590188303940948\n",
      "Iteration 49066 => Loss: 6.69845586441958751323\n",
      "Iteration 49067 => Loss: 6.69845582696118579946\n",
      "Iteration 49068 => Loss: 6.69845578950782893912\n",
      "Iteration 49069 => Loss: 6.69845575205951604403\n",
      "Iteration 49070 => Loss: 6.69845571461625954868\n",
      "Iteration 49071 => Loss: 6.69845567717803103136\n",
      "Iteration 49072 => Loss: 6.69845563974486157832\n",
      "Iteration 49073 => Loss: 6.69845560231672898510\n",
      "Iteration 49074 => Loss: 6.69845556489364568620\n",
      "Iteration 49075 => Loss: 6.69845552747560013529\n",
      "Iteration 49076 => Loss: 6.69845549006259943781\n",
      "Iteration 49077 => Loss: 6.69845545265463737650\n",
      "Iteration 49078 => Loss: 6.69845541525172283315\n",
      "Iteration 49079 => Loss: 6.69845537785383893237\n",
      "Iteration 49080 => Loss: 6.69845534046099722048\n",
      "Iteration 49081 => Loss: 6.69845530307320125019\n",
      "Iteration 49082 => Loss: 6.69845526569043592247\n",
      "Iteration 49083 => Loss: 6.69845522831271278363\n",
      "Iteration 49084 => Loss: 6.69845519094002295191\n",
      "Iteration 49085 => Loss: 6.69845515357236553911\n",
      "Iteration 49086 => Loss: 6.69845511620975120337\n",
      "Iteration 49087 => Loss: 6.69845507885216484567\n",
      "Iteration 49088 => Loss: 6.69845504149961623597\n",
      "Iteration 49089 => Loss: 6.69845500415209649248\n",
      "Iteration 49090 => Loss: 6.69845496680961716152\n",
      "Iteration 49091 => Loss: 6.69845492947216403223\n",
      "Iteration 49092 => Loss: 6.69845489213974154552\n",
      "Iteration 49093 => Loss: 6.69845485481235680680\n",
      "Iteration 49094 => Loss: 6.69845481748999205251\n",
      "Iteration 49095 => Loss: 6.69845478017266415804\n",
      "Iteration 49096 => Loss: 6.69845474286036068889\n",
      "Iteration 49097 => Loss: 6.69845470555309230321\n",
      "Iteration 49098 => Loss: 6.69845466825083502016\n",
      "Iteration 49099 => Loss: 6.69845463095361726147\n",
      "Iteration 49100 => Loss: 6.69845459366142659263\n",
      "Iteration 49101 => Loss: 6.69845455637425413187\n",
      "Iteration 49102 => Loss: 6.69845451909211142549\n",
      "Iteration 49103 => Loss: 6.69845448181498248630\n",
      "Iteration 49104 => Loss: 6.69845444454288596603\n",
      "Iteration 49105 => Loss: 6.69845440727581387108\n",
      "Iteration 49106 => Loss: 6.69845437001375554331\n",
      "Iteration 49107 => Loss: 6.69845433275672519358\n",
      "Iteration 49108 => Loss: 6.69845429550470949920\n",
      "Iteration 49109 => Loss: 6.69845425825771645378\n",
      "Iteration 49110 => Loss: 6.69845422101573984008\n",
      "Iteration 49111 => Loss: 6.69845418377878765170\n",
      "Iteration 49112 => Loss: 6.69845414654684923050\n",
      "Iteration 49113 => Loss: 6.69845410931992457648\n",
      "Iteration 49114 => Loss: 6.69845407209801990689\n",
      "Iteration 49115 => Loss: 6.69845403488112989265\n",
      "Iteration 49116 => Loss: 6.69845399766925631013\n",
      "Iteration 49117 => Loss: 6.69845396046239738297\n",
      "Iteration 49118 => Loss: 6.69845392326055399934\n",
      "Iteration 49119 => Loss: 6.69845388606371816564\n",
      "Iteration 49120 => Loss: 6.69845384887189521095\n",
      "Iteration 49121 => Loss: 6.69845381168508691161\n",
      "Iteration 49122 => Loss: 6.69845377450328882674\n",
      "Iteration 49123 => Loss: 6.69845373732650184451\n",
      "Iteration 49124 => Loss: 6.69845370015472330039\n",
      "Iteration 49125 => Loss: 6.69845366298795319437\n",
      "Iteration 49126 => Loss: 6.69845362582618886194\n",
      "Iteration 49127 => Loss: 6.69845358866943474396\n",
      "Iteration 49128 => Loss: 6.69845355151769172863\n",
      "Iteration 49129 => Loss: 6.69845351437095271052\n",
      "Iteration 49130 => Loss: 6.69845347722921236056\n",
      "Iteration 49131 => Loss: 6.69845344009248044870\n",
      "Iteration 49132 => Loss: 6.69845340296075786313\n",
      "Iteration 49133 => Loss: 6.69845336583403128117\n",
      "Iteration 49134 => Loss: 6.69845332871231313732\n",
      "Iteration 49135 => Loss: 6.69845329159559188525\n",
      "Iteration 49136 => Loss: 6.69845325448387907130\n",
      "Iteration 49137 => Loss: 6.69845321737716048460\n",
      "Iteration 49138 => Loss: 6.69845318027544944783\n",
      "Iteration 49139 => Loss: 6.69845314317873263832\n",
      "Iteration 49140 => Loss: 6.69845310608701716149\n",
      "Iteration 49141 => Loss: 6.69845306900029413555\n",
      "Iteration 49142 => Loss: 6.69845303191857244229\n",
      "Iteration 49143 => Loss: 6.69845299484184408811\n",
      "Iteration 49144 => Loss: 6.69845295777011617844\n",
      "Iteration 49145 => Loss: 6.69845292070338516055\n",
      "Iteration 49146 => Loss: 6.69845288364164659356\n",
      "Iteration 49147 => Loss: 6.69845284658490047747\n",
      "Iteration 49148 => Loss: 6.69845280953314858863\n",
      "Iteration 49149 => Loss: 6.69845277248639003886\n",
      "Iteration 49150 => Loss: 6.69845273544462305182\n",
      "Iteration 49151 => Loss: 6.69845269840785029203\n",
      "Iteration 49152 => Loss: 6.69845266137606287771\n",
      "Iteration 49153 => Loss: 6.69845262434927324335\n",
      "Iteration 49154 => Loss: 6.69845258732746984265\n",
      "Iteration 49155 => Loss: 6.69845255031064823470\n",
      "Iteration 49156 => Loss: 6.69845251329882263036\n",
      "Iteration 49157 => Loss: 6.69845247629198503603\n",
      "Iteration 49158 => Loss: 6.69845243929012568174\n",
      "Iteration 49159 => Loss: 6.69845240229325966652\n",
      "Iteration 49160 => Loss: 6.69845236530137366771\n",
      "Iteration 49161 => Loss: 6.69845232831447390254\n",
      "Iteration 49162 => Loss: 6.69845229133256392373\n",
      "Iteration 49163 => Loss: 6.69845225435563218497\n",
      "Iteration 49164 => Loss: 6.69845221738368490350\n",
      "Iteration 49165 => Loss: 6.69845218041671941478\n",
      "Iteration 49166 => Loss: 6.69845214345473038975\n",
      "Iteration 49167 => Loss: 6.69845210649772848654\n",
      "Iteration 49168 => Loss: 6.69845206954570304703\n",
      "Iteration 49169 => Loss: 6.69845203259865584755\n",
      "Iteration 49170 => Loss: 6.69845199565659221719\n",
      "Iteration 49171 => Loss: 6.69845195871950060962\n",
      "Iteration 49172 => Loss: 6.69845192178739257116\n",
      "Iteration 49173 => Loss: 6.69845188486025922003\n",
      "Iteration 49174 => Loss: 6.69845184793809789170\n",
      "Iteration 49175 => Loss: 6.69845181102091302705\n",
      "Iteration 49176 => Loss: 6.69845177410870551427\n",
      "Iteration 49177 => Loss: 6.69845173720146735974\n",
      "Iteration 49178 => Loss: 6.69845170029920655708\n",
      "Iteration 49179 => Loss: 6.69845166340192310628\n",
      "Iteration 49180 => Loss: 6.69845162650960368467\n",
      "Iteration 49181 => Loss: 6.69845158962225628585\n",
      "Iteration 49182 => Loss: 6.69845155273987646893\n",
      "Iteration 49183 => Loss: 6.69845151586247489206\n",
      "Iteration 49184 => Loss: 6.69845147899003912073\n",
      "Iteration 49185 => Loss: 6.69845144212257270766\n",
      "Iteration 49186 => Loss: 6.69845140526007387649\n",
      "Iteration 49187 => Loss: 6.69845136840253640997\n",
      "Iteration 49188 => Loss: 6.69845133154997096625\n",
      "Iteration 49189 => Loss: 6.69845129470237221625\n",
      "Iteration 49190 => Loss: 6.69845125785973927179\n",
      "Iteration 49191 => Loss: 6.69845122102206591563\n",
      "Iteration 49192 => Loss: 6.69845118418935570048\n",
      "Iteration 49193 => Loss: 6.69845114736161484359\n",
      "Iteration 49194 => Loss: 6.69845111053883446317\n",
      "Iteration 49195 => Loss: 6.69845107372101811194\n",
      "Iteration 49196 => Loss: 6.69845103690815868447\n",
      "Iteration 49197 => Loss: 6.69845100010026328619\n",
      "Iteration 49198 => Loss: 6.69845096329732392348\n",
      "Iteration 49199 => Loss: 6.69845092649934592544\n",
      "Iteration 49200 => Loss: 6.69845088970632662750\n",
      "Iteration 49201 => Loss: 6.69845085291826691787\n",
      "Iteration 49202 => Loss: 6.69845081613515791474\n",
      "Iteration 49203 => Loss: 6.69845077935701205263\n",
      "Iteration 49204 => Loss: 6.69845074258382044974\n",
      "Iteration 49205 => Loss: 6.69845070581558399425\n",
      "Iteration 49206 => Loss: 6.69845066905230446253\n",
      "Iteration 49207 => Loss: 6.69845063229396942006\n",
      "Iteration 49208 => Loss: 6.69845059554059840679\n",
      "Iteration 49209 => Loss: 6.69845055879217543549\n",
      "Iteration 49210 => Loss: 6.69845052204870139434\n",
      "Iteration 49211 => Loss: 6.69845048531018338878\n",
      "Iteration 49212 => Loss: 6.69845044857660898430\n",
      "Iteration 49213 => Loss: 6.69845041184799416811\n",
      "Iteration 49214 => Loss: 6.69845037512432206483\n",
      "Iteration 49215 => Loss: 6.69845033840559800353\n",
      "Iteration 49216 => Loss: 6.69845030169182908963\n",
      "Iteration 49217 => Loss: 6.69845026498299844775\n",
      "Iteration 49218 => Loss: 6.69845022827912206509\n",
      "Iteration 49219 => Loss: 6.69845019158018306626\n",
      "Iteration 49220 => Loss: 6.69845015488619743849\n",
      "Iteration 49221 => Loss: 6.69845011819715274726\n",
      "Iteration 49222 => Loss: 6.69845008151305520983\n",
      "Iteration 49223 => Loss: 6.69845004483389239169\n",
      "Iteration 49224 => Loss: 6.69845000815967761554\n",
      "Iteration 49225 => Loss: 6.69844997149040377593\n",
      "Iteration 49226 => Loss: 6.69844993482607353741\n",
      "Iteration 49227 => Loss: 6.69844989816668245908\n",
      "Iteration 49228 => Loss: 6.69844986151223054094\n",
      "Iteration 49229 => Loss: 6.69844982486271955935\n",
      "Iteration 49230 => Loss: 6.69844978821814507342\n",
      "Iteration 49231 => Loss: 6.69844975157851063585\n",
      "Iteration 49232 => Loss: 6.69844971494380470034\n",
      "Iteration 49233 => Loss: 6.69844967831404503045\n",
      "Iteration 49234 => Loss: 6.69844964168922007985\n",
      "Iteration 49235 => Loss: 6.69844960506933162492\n",
      "Iteration 49236 => Loss: 6.69844956845437344839\n",
      "Iteration 49237 => Loss: 6.69844953184434377391\n",
      "Iteration 49238 => Loss: 6.69844949523925770052\n",
      "Iteration 49239 => Loss: 6.69844945863910545825\n",
      "Iteration 49240 => Loss: 6.69844942204387550078\n",
      "Iteration 49241 => Loss: 6.69844938545358026261\n",
      "Iteration 49242 => Loss: 6.69844934886821796738\n",
      "Iteration 49243 => Loss: 6.69844931228778595056\n",
      "Iteration 49244 => Loss: 6.69844927571228243579\n",
      "Iteration 49245 => Loss: 6.69844923914170298218\n",
      "Iteration 49246 => Loss: 6.69844920257605824787\n",
      "Iteration 49247 => Loss: 6.69844916601534023926\n",
      "Iteration 49248 => Loss: 6.69844912945954096273\n",
      "Iteration 49249 => Loss: 6.69844909290867640550\n",
      "Iteration 49250 => Loss: 6.69844905636272880400\n",
      "Iteration 49251 => Loss: 6.69844901982170970456\n",
      "Iteration 49252 => Loss: 6.69844898328561377809\n",
      "Iteration 49253 => Loss: 6.69844894675444191279\n",
      "Iteration 49254 => Loss: 6.69844891022819677318\n",
      "Iteration 49255 => Loss: 6.69844887370686414840\n",
      "Iteration 49256 => Loss: 6.69844883719046357839\n",
      "Iteration 49257 => Loss: 6.69844880067898085230\n",
      "Iteration 49258 => Loss: 6.69844876417241152922\n",
      "Iteration 49259 => Loss: 6.69844872767076893183\n",
      "Iteration 49260 => Loss: 6.69844869117403973746\n",
      "Iteration 49261 => Loss: 6.69844865468223371607\n",
      "Iteration 49262 => Loss: 6.69844861819534109770\n",
      "Iteration 49263 => Loss: 6.69844858171336277053\n",
      "Iteration 49264 => Loss: 6.69844854523630939269\n",
      "Iteration 49265 => Loss: 6.69844850876416408880\n",
      "Iteration 49266 => Loss: 6.69844847229693129975\n",
      "Iteration 49267 => Loss: 6.69844843583461546643\n",
      "Iteration 49268 => Loss: 6.69844839937721303613\n",
      "Iteration 49269 => Loss: 6.69844836292471867978\n",
      "Iteration 49270 => Loss: 6.69844832647714394369\n",
      "Iteration 49271 => Loss: 6.69844829003447994609\n",
      "Iteration 49272 => Loss: 6.69844825359672402243\n",
      "Iteration 49273 => Loss: 6.69844821716387350818\n",
      "Iteration 49274 => Loss: 6.69844818073594172603\n",
      "Iteration 49275 => Loss: 6.69844814431291002421\n",
      "Iteration 49276 => Loss: 6.69844810789479172541\n",
      "Iteration 49277 => Loss: 6.69844807148157972421\n",
      "Iteration 49278 => Loss: 6.69844803507327135605\n",
      "Iteration 49279 => Loss: 6.69844799866987461456\n",
      "Iteration 49280 => Loss: 6.69844796227138150613\n",
      "Iteration 49281 => Loss: 6.69844792587779558346\n",
      "Iteration 49282 => Loss: 6.69844788948910618842\n",
      "Iteration 49283 => Loss: 6.69844785310532131462\n",
      "Iteration 49284 => Loss: 6.69844781672644273840\n",
      "Iteration 49285 => Loss: 6.69844778035246690706\n",
      "Iteration 49286 => Loss: 6.69844774398338849153\n",
      "Iteration 49287 => Loss: 6.69844770761921193269\n",
      "Iteration 49288 => Loss: 6.69844767125994078327\n",
      "Iteration 49289 => Loss: 6.69844763490556260876\n",
      "Iteration 49290 => Loss: 6.69844759855608806731\n",
      "Iteration 49291 => Loss: 6.69844756221150738895\n",
      "Iteration 49292 => Loss: 6.69844752587182679093\n",
      "Iteration 49293 => Loss: 6.69844748953704804961\n",
      "Iteration 49294 => Loss: 6.69844745320715517778\n",
      "Iteration 49295 => Loss: 6.69844741688216149811\n",
      "Iteration 49296 => Loss: 6.69844738056205990517\n",
      "Iteration 49297 => Loss: 6.69844734424686105712\n",
      "Iteration 49298 => Loss: 6.69844730793654186130\n",
      "Iteration 49299 => Loss: 6.69844727163112985124\n",
      "Iteration 49300 => Loss: 6.69844723533060548704\n",
      "Iteration 49301 => Loss: 6.69844719903496255142\n",
      "Iteration 49302 => Loss: 6.69844716274422591340\n",
      "Iteration 49303 => Loss: 6.69844712645837603304\n",
      "Iteration 49304 => Loss: 6.69844709017741202217\n",
      "Iteration 49305 => Loss: 6.69844705390132943990\n",
      "Iteration 49306 => Loss: 6.69844701763014427343\n",
      "Iteration 49307 => Loss: 6.69844698136384320009\n",
      "Iteration 49308 => Loss: 6.69844694510243510166\n",
      "Iteration 49309 => Loss: 6.69844690884590576729\n",
      "Iteration 49310 => Loss: 6.69844687259426674331\n",
      "Iteration 49311 => Loss: 6.69844683634751358881\n",
      "Iteration 49312 => Loss: 6.69844680010564452743\n",
      "Iteration 49313 => Loss: 6.69844676386865867102\n",
      "Iteration 49314 => Loss: 6.69844672763654713776\n",
      "Iteration 49315 => Loss: 6.69844669140932591489\n",
      "Iteration 49316 => Loss: 6.69844665518698523243\n",
      "Iteration 49317 => Loss: 6.69844661896952597857\n",
      "Iteration 49318 => Loss: 6.69844658275694460059\n",
      "Iteration 49319 => Loss: 6.69844654654924731574\n",
      "Iteration 49320 => Loss: 6.69844651034642435405\n",
      "Iteration 49321 => Loss: 6.69844647414847838007\n",
      "Iteration 49322 => Loss: 6.69844643795541738740\n",
      "Iteration 49323 => Loss: 6.69844640176722716518\n",
      "Iteration 49324 => Loss: 6.69844636558391659520\n",
      "Iteration 49325 => Loss: 6.69844632940548123656\n",
      "Iteration 49326 => Loss: 6.69844629323192020109\n",
      "Iteration 49327 => Loss: 6.69844625706323348879\n",
      "Iteration 49328 => Loss: 6.69844622089941932330\n",
      "Iteration 49329 => Loss: 6.69844618474048125734\n",
      "Iteration 49330 => Loss: 6.69844614858641662636\n",
      "Iteration 49331 => Loss: 6.69844611243721921312\n",
      "Iteration 49332 => Loss: 6.69844607629289701123\n",
      "Iteration 49333 => Loss: 6.69844604015344380343\n",
      "Iteration 49334 => Loss: 6.69844600401885781338\n",
      "Iteration 49335 => Loss: 6.69844596788914170560\n",
      "Iteration 49336 => Loss: 6.69844593176429459191\n",
      "Iteration 49337 => Loss: 6.69844589564431647233\n",
      "Iteration 49338 => Loss: 6.69844585952920912320\n",
      "Iteration 49339 => Loss: 6.69844582341896188638\n",
      "Iteration 49340 => Loss: 6.69844578731357831458\n",
      "Iteration 49341 => Loss: 6.69844575121306728960\n",
      "Iteration 49342 => Loss: 6.69844571511741637693\n",
      "Iteration 49343 => Loss: 6.69844567902663179382\n",
      "Iteration 49344 => Loss: 6.69844564294071265209\n",
      "Iteration 49345 => Loss: 6.69844560685965184632\n",
      "Iteration 49346 => Loss: 6.69844557078345381740\n",
      "Iteration 49347 => Loss: 6.69844553471211945350\n",
      "Iteration 49348 => Loss: 6.69844549864564520192\n",
      "Iteration 49349 => Loss: 6.69844546258402218086\n",
      "Iteration 49350 => Loss: 6.69844542652726904208\n",
      "Iteration 49351 => Loss: 6.69844539047536979837\n",
      "Iteration 49352 => Loss: 6.69844535442832889061\n",
      "Iteration 49353 => Loss: 6.69844531838614720698\n",
      "Iteration 49354 => Loss: 6.69844528234882385931\n",
      "Iteration 49355 => Loss: 6.69844524631635351852\n",
      "Iteration 49356 => Loss: 6.69844521028873440827\n",
      "Iteration 49357 => Loss: 6.69844517426597896304\n",
      "Iteration 49358 => Loss: 6.69844513824807386015\n",
      "Iteration 49359 => Loss: 6.69844510223502087598\n",
      "Iteration 49360 => Loss: 6.69844506622682445141\n",
      "Iteration 49361 => Loss: 6.69844503022347659282\n",
      "Iteration 49362 => Loss: 6.69844499422497641206\n",
      "Iteration 49363 => Loss: 6.69844495823133279089\n",
      "Iteration 49364 => Loss: 6.69844492224253418300\n",
      "Iteration 49365 => Loss: 6.69844488625858680564\n",
      "Iteration 49366 => Loss: 6.69844485027949243516\n",
      "Iteration 49367 => Loss: 6.69844481430524574250\n",
      "Iteration 49368 => Loss: 6.69844477833584850401\n",
      "Iteration 49369 => Loss: 6.69844474237129183791\n",
      "Iteration 49370 => Loss: 6.69844470641158284963\n",
      "Iteration 49371 => Loss: 6.69844467045671887462\n",
      "Iteration 49372 => Loss: 6.69844463450670435378\n",
      "Iteration 49373 => Loss: 6.69844459856152774080\n",
      "Iteration 49374 => Loss: 6.69844456262120413470\n",
      "Iteration 49375 => Loss: 6.69844452668571843645\n",
      "Iteration 49376 => Loss: 6.69844449075507153424\n",
      "Iteration 49377 => Loss: 6.69844445482926786894\n",
      "Iteration 49378 => Loss: 6.69844441890830477604\n",
      "Iteration 49379 => Loss: 6.69844438299218136734\n",
      "Iteration 49380 => Loss: 6.69844434708090208375\n",
      "Iteration 49381 => Loss: 6.69844431117446337254\n",
      "Iteration 49382 => Loss: 6.69844427527285635193\n",
      "Iteration 49383 => Loss: 6.69844423937609168007\n",
      "Iteration 49384 => Loss: 6.69844420348416047517\n",
      "Iteration 49385 => Loss: 6.69844416759706806630\n",
      "Iteration 49386 => Loss: 6.69844413171481090075\n",
      "Iteration 49387 => Loss: 6.69844409583738897851\n",
      "Iteration 49388 => Loss: 6.69844405996480585230\n",
      "Iteration 49389 => Loss: 6.69844402409704464674\n",
      "Iteration 49390 => Loss: 6.69844398823413023081\n",
      "Iteration 49391 => Loss: 6.69844395237604040005\n",
      "Iteration 49392 => Loss: 6.69844391652278314808\n",
      "Iteration 49393 => Loss: 6.69844388067435669853\n",
      "Iteration 49394 => Loss: 6.69844384483076638048\n",
      "Iteration 49395 => Loss: 6.69844380899199798307\n",
      "Iteration 49396 => Loss: 6.69844377315807104623\n",
      "Iteration 49397 => Loss: 6.69844373732895981277\n",
      "Iteration 49398 => Loss: 6.69844370150468026992\n",
      "Iteration 49399 => Loss: 6.69844366568523330585\n",
      "Iteration 49400 => Loss: 6.69844362987060648607\n",
      "Iteration 49401 => Loss: 6.69844359406081135688\n",
      "Iteration 49402 => Loss: 6.69844355825583637198\n",
      "Iteration 49403 => Loss: 6.69844352245568863680\n",
      "Iteration 49404 => Loss: 6.69844348666036282225\n",
      "Iteration 49405 => Loss: 6.69844345086985804016\n",
      "Iteration 49406 => Loss: 6.69844341508418317233\n",
      "Iteration 49407 => Loss: 6.69844337930332578424\n",
      "Iteration 49408 => Loss: 6.69844334352729564586\n",
      "Iteration 49409 => Loss: 6.69844330775608121087\n",
      "Iteration 49410 => Loss: 6.69844327198968869652\n",
      "Iteration 49411 => Loss: 6.69844323622811366192\n",
      "Iteration 49412 => Loss: 6.69844320047136143614\n",
      "Iteration 49413 => Loss: 6.69844316471942224922\n",
      "Iteration 49414 => Loss: 6.69844312897230143022\n",
      "Iteration 49415 => Loss: 6.69844309323000075551\n",
      "Iteration 49416 => Loss: 6.69844305749251578419\n",
      "Iteration 49417 => Loss: 6.69844302175984385173\n",
      "Iteration 49418 => Loss: 6.69844298603199028719\n",
      "Iteration 49419 => Loss: 6.69844295030894798515\n",
      "Iteration 49420 => Loss: 6.69844291459071961015\n",
      "Iteration 49421 => Loss: 6.69844287887730338582\n",
      "Iteration 49422 => Loss: 6.69844284316870375307\n",
      "Iteration 49423 => Loss: 6.69844280746491005374\n",
      "Iteration 49424 => Loss: 6.69844277176594005141\n",
      "Iteration 49425 => Loss: 6.69844273607176798890\n",
      "Iteration 49426 => Loss: 6.69844270038240630072\n",
      "Iteration 49427 => Loss: 6.69844266469785853957\n",
      "Iteration 49428 => Loss: 6.69844262901812026456\n",
      "Iteration 49429 => Loss: 6.69844259334318348209\n",
      "Iteration 49430 => Loss: 6.69844255767305796212\n",
      "Iteration 49431 => Loss: 6.69844252200773482286\n",
      "Iteration 49432 => Loss: 6.69844248634722294611\n",
      "Iteration 49433 => Loss: 6.69844245069151256189\n",
      "Iteration 49434 => Loss: 6.69844241504061432835\n",
      "Iteration 49435 => Loss: 6.69844237939451137009\n",
      "Iteration 49436 => Loss: 6.69844234375321700981\n",
      "Iteration 49437 => Loss: 6.69844230811672147752\n",
      "Iteration 49438 => Loss: 6.69844227248502388505\n",
      "Iteration 49439 => Loss: 6.69844223685813400238\n",
      "Iteration 49440 => Loss: 6.69844220123605182948\n",
      "Iteration 49441 => Loss: 6.69844216561875605009\n",
      "Iteration 49442 => Loss: 6.69844213000626531596\n",
      "Iteration 49443 => Loss: 6.69844209439858051525\n",
      "Iteration 49444 => Loss: 6.69844205879568299622\n",
      "Iteration 49445 => Loss: 6.69844202319758874609\n",
      "Iteration 49446 => Loss: 6.69844198760428977124\n",
      "Iteration 49447 => Loss: 6.69844195201578429533\n",
      "Iteration 49448 => Loss: 6.69844191643207764741\n",
      "Iteration 49449 => Loss: 6.69844188085316716297\n",
      "Iteration 49450 => Loss: 6.69844184527905284199\n",
      "Iteration 49451 => Loss: 6.69844180970972491451\n",
      "Iteration 49452 => Loss: 6.69844177414519581504\n",
      "Iteration 49453 => Loss: 6.69844173858545488542\n",
      "Iteration 49454 => Loss: 6.69844170303050745474\n",
      "Iteration 49455 => Loss: 6.69844166748035085845\n",
      "Iteration 49456 => Loss: 6.69844163193498776110\n",
      "Iteration 49457 => Loss: 6.69844159639440306364\n",
      "Iteration 49458 => Loss: 6.69844156085861719419\n",
      "Iteration 49459 => Loss: 6.69844152532762038277\n",
      "Iteration 49460 => Loss: 6.69844148980141174121\n",
      "Iteration 49461 => Loss: 6.69844145427999126952\n",
      "Iteration 49462 => Loss: 6.69844141876335097407\n",
      "Iteration 49463 => Loss: 6.69844138325149440760\n",
      "Iteration 49464 => Loss: 6.69844134774442689917\n",
      "Iteration 49465 => Loss: 6.69844131224215377785\n",
      "Iteration 49466 => Loss: 6.69844127674464928646\n",
      "Iteration 49467 => Loss: 6.69844124125194362307\n",
      "Iteration 49468 => Loss: 6.69844120576400392508\n",
      "Iteration 49469 => Loss: 6.69844117028085683785\n",
      "Iteration 49470 => Loss: 6.69844113480249170323\n",
      "Iteration 49471 => Loss: 6.69844109932889875125\n",
      "Iteration 49472 => Loss: 6.69844106386009396914\n",
      "Iteration 49473 => Loss: 6.69844102839606492239\n",
      "Iteration 49474 => Loss: 6.69844099293681516372\n",
      "Iteration 49475 => Loss: 6.69844095748234735765\n",
      "Iteration 49476 => Loss: 6.69844092203264640517\n",
      "Iteration 49477 => Loss: 6.69844088658773451073\n",
      "Iteration 49478 => Loss: 6.69844085114759835164\n",
      "Iteration 49479 => Loss: 6.69844081571222638161\n",
      "Iteration 49480 => Loss: 6.69844078028163814054\n",
      "Iteration 49481 => Loss: 6.69844074485582385847\n",
      "Iteration 49482 => Loss: 6.69844070943477998270\n",
      "Iteration 49483 => Loss: 6.69844067401850917776\n",
      "Iteration 49484 => Loss: 6.69844063860701677271\n",
      "Iteration 49485 => Loss: 6.69844060320028233946\n",
      "Iteration 49486 => Loss: 6.69844056779833518789\n",
      "Iteration 49487 => Loss: 6.69844053240115311354\n",
      "Iteration 49488 => Loss: 6.69844049700873522823\n",
      "Iteration 49489 => Loss: 6.69844046162109041376\n",
      "Iteration 49490 => Loss: 6.69844042623821511739\n",
      "Iteration 49491 => Loss: 6.69844039086010401007\n",
      "Iteration 49492 => Loss: 6.69844035548676064451\n",
      "Iteration 49493 => Loss: 6.69844032011818590888\n",
      "Iteration 49494 => Loss: 6.69844028475438246772\n",
      "Iteration 49495 => Loss: 6.69844024939533611018\n",
      "Iteration 49496 => Loss: 6.69844021404105749440\n",
      "Iteration 49497 => Loss: 6.69844017869154040312\n",
      "Iteration 49498 => Loss: 6.69844014334679105360\n",
      "Iteration 49499 => Loss: 6.69844010800680056406\n",
      "Iteration 49500 => Loss: 6.69844007267157515173\n",
      "Iteration 49501 => Loss: 6.69844003734110948756\n",
      "Iteration 49502 => Loss: 6.69844000201540623607\n",
      "Iteration 49503 => Loss: 6.69843996669445918002\n",
      "Iteration 49504 => Loss: 6.69843993137827453666\n",
      "Iteration 49505 => Loss: 6.69843989606684786509\n",
      "Iteration 49506 => Loss: 6.69843986076018360620\n",
      "Iteration 49507 => Loss: 6.69843982545826577280\n",
      "Iteration 49508 => Loss: 6.69843979016112012204\n",
      "Iteration 49509 => Loss: 6.69843975486872622582\n",
      "Iteration 49510 => Loss: 6.69843971958107342601\n",
      "Iteration 49511 => Loss: 6.69843968429819192067\n",
      "Iteration 49512 => Loss: 6.69843964902005772899\n",
      "Iteration 49513 => Loss: 6.69843961374668150910\n",
      "Iteration 49514 => Loss: 6.69843957847805970829\n",
      "Iteration 49515 => Loss: 6.69843954321418433295\n",
      "Iteration 49516 => Loss: 6.69843950795506071216\n",
      "Iteration 49517 => Loss: 6.69843947270069417499\n",
      "Iteration 49518 => Loss: 6.69843943745107583965\n",
      "Iteration 49519 => Loss: 6.69843940220620215342\n",
      "Iteration 49520 => Loss: 6.69843936696608199810\n",
      "Iteration 49521 => Loss: 6.69843933173071448550\n",
      "Iteration 49522 => Loss: 6.69843929650008895749\n",
      "Iteration 49523 => Loss: 6.69843926127421074312\n",
      "Iteration 49524 => Loss: 6.69843922605307984242\n",
      "Iteration 49525 => Loss: 6.69843919083670247261\n",
      "Iteration 49526 => Loss: 6.69843915562505820560\n",
      "Iteration 49527 => Loss: 6.69843912041816569314\n",
      "Iteration 49528 => Loss: 6.69843908521601516526\n",
      "Iteration 49529 => Loss: 6.69843905001861461557\n",
      "Iteration 49530 => Loss: 6.69843901482594983321\n",
      "Iteration 49531 => Loss: 6.69843897963802703543\n",
      "Iteration 49532 => Loss: 6.69843894445485421585\n",
      "Iteration 49533 => Loss: 6.69843890927641005817\n",
      "Iteration 49534 => Loss: 6.69843887410270966143\n",
      "Iteration 49535 => Loss: 6.69843883893375569016\n",
      "Iteration 49536 => Loss: 6.69843880376953926259\n",
      "Iteration 49537 => Loss: 6.69843876861005504963\n",
      "Iteration 49538 => Loss: 6.69843873345531548580\n",
      "Iteration 49539 => Loss: 6.69843869830530902476\n",
      "Iteration 49540 => Loss: 6.69843866316003921924\n",
      "Iteration 49541 => Loss: 6.69843862801950429287\n",
      "Iteration 49542 => Loss: 6.69843859288370513383\n",
      "Iteration 49543 => Loss: 6.69843855775264351848\n",
      "Iteration 49544 => Loss: 6.69843852262630523597\n",
      "Iteration 49545 => Loss: 6.69843848750471071440\n",
      "Iteration 49546 => Loss: 6.69843845238784751928\n",
      "Iteration 49547 => Loss: 6.69843841727571387423\n",
      "Iteration 49548 => Loss: 6.69843838216831866106\n",
      "Iteration 49549 => Loss: 6.69843834706564855708\n",
      "Iteration 49550 => Loss: 6.69843831196770800318\n",
      "Iteration 49551 => Loss: 6.69843827687449788755\n",
      "Iteration 49552 => Loss: 6.69843824178601554564\n",
      "Iteration 49553 => Loss: 6.69843820670225831293\n",
      "Iteration 49554 => Loss: 6.69843817162323418302\n",
      "Iteration 49555 => Loss: 6.69843813654893782683\n",
      "Iteration 49556 => Loss: 6.69843810147936480348\n",
      "Iteration 49557 => Loss: 6.69843806641451511297\n",
      "Iteration 49558 => Loss: 6.69843803135439674890\n",
      "Iteration 49559 => Loss: 6.69843799629899727677\n",
      "Iteration 49560 => Loss: 6.69843796124832646655\n",
      "Iteration 49561 => Loss: 6.69843792620236921920\n",
      "Iteration 49562 => Loss: 6.69843789116114329829\n",
      "Iteration 49563 => Loss: 6.69843785612463893386\n",
      "Iteration 49564 => Loss: 6.69843782109285257320\n",
      "Iteration 49565 => Loss: 6.69843778606578510448\n",
      "Iteration 49566 => Loss: 6.69843775104344008042\n",
      "Iteration 49567 => Loss: 6.69843771602581661284\n",
      "Iteration 49568 => Loss: 6.69843768101290493178\n",
      "Iteration 49569 => Loss: 6.69843764600471747173\n",
      "Iteration 49570 => Loss: 6.69843761100124801544\n",
      "Iteration 49571 => Loss: 6.69843757600248679296\n",
      "Iteration 49572 => Loss: 6.69843754100845512056\n",
      "Iteration 49573 => Loss: 6.69843750601912812925\n",
      "Iteration 49574 => Loss: 6.69843747103452091807\n",
      "Iteration 49575 => Loss: 6.69843743605462105251\n",
      "Iteration 49576 => Loss: 6.69843740107944363160\n",
      "Iteration 49577 => Loss: 6.69843736610897266814\n",
      "Iteration 49578 => Loss: 6.69843733114321882027\n",
      "Iteration 49579 => Loss: 6.69843729618217142985\n",
      "Iteration 49580 => Loss: 6.69843726122584293137\n",
      "Iteration 49581 => Loss: 6.69843722627422089033\n",
      "Iteration 49582 => Loss: 6.69843719132730441856\n",
      "Iteration 49583 => Loss: 6.69843715638510683874\n",
      "Iteration 49584 => Loss: 6.69843712144760061733\n",
      "Iteration 49585 => Loss: 6.69843708651482039329\n",
      "Iteration 49586 => Loss: 6.69843705158673685673\n",
      "Iteration 49587 => Loss: 6.69843701666336599487\n",
      "Iteration 49588 => Loss: 6.69843698174469270867\n",
      "Iteration 49589 => Loss: 6.69843694683073476170\n",
      "Iteration 49590 => Loss: 6.69843691192147527858\n",
      "Iteration 49591 => Loss: 6.69843687701692314107\n",
      "Iteration 49592 => Loss: 6.69843684211707302012\n",
      "Iteration 49593 => Loss: 6.69843680722192313937\n",
      "Iteration 49594 => Loss: 6.69843677233147172245\n",
      "Iteration 49595 => Loss: 6.69843673744572765116\n",
      "Iteration 49596 => Loss: 6.69843670256469092550\n",
      "Iteration 49597 => Loss: 6.69843666768834555825\n",
      "Iteration 49598 => Loss: 6.69843663281669954301\n",
      "Iteration 49599 => Loss: 6.69843659794975554433\n",
      "Iteration 49600 => Loss: 6.69843656308751000950\n",
      "Iteration 49601 => Loss: 6.69843652822995938578\n",
      "Iteration 49602 => Loss: 6.69843649337711077862\n",
      "Iteration 49603 => Loss: 6.69843645852895885895\n",
      "Iteration 49604 => Loss: 6.69843642368549563315\n",
      "Iteration 49605 => Loss: 6.69843638884673175937\n",
      "Iteration 49606 => Loss: 6.69843635401265657947\n",
      "Iteration 49607 => Loss: 6.69843631918328430430\n",
      "Iteration 49608 => Loss: 6.69843628435860338755\n",
      "Iteration 49609 => Loss: 6.69843624953860850013\n",
      "Iteration 49610 => Loss: 6.69843621472331651745\n",
      "Iteration 49611 => Loss: 6.69843617991270079415\n",
      "Iteration 49612 => Loss: 6.69843614510679064011\n",
      "Iteration 49613 => Loss: 6.69843611030556296271\n",
      "Iteration 49614 => Loss: 6.69843607550902842007\n",
      "Iteration 49615 => Loss: 6.69843604071718257131\n",
      "Iteration 49616 => Loss: 6.69843600593002452825\n",
      "Iteration 49617 => Loss: 6.69843597114754896182\n",
      "Iteration 49618 => Loss: 6.69843593636976564198\n",
      "Iteration 49619 => Loss: 6.69843590159666391060\n",
      "Iteration 49620 => Loss: 6.69843586682825442580\n",
      "Iteration 49621 => Loss: 6.69843583206452652945\n",
      "Iteration 49622 => Loss: 6.69843579730548288609\n",
      "Iteration 49623 => Loss: 6.69843576255112527207\n",
      "Iteration 49624 => Loss: 6.69843572780144658196\n",
      "Iteration 49625 => Loss: 6.69843569305645125667\n",
      "Iteration 49626 => Loss: 6.69843565831614373707\n",
      "Iteration 49627 => Loss: 6.69843562358050981231\n",
      "Iteration 49628 => Loss: 6.69843558884955925237\n",
      "Iteration 49629 => Loss: 6.69843555412329028087\n",
      "Iteration 49630 => Loss: 6.69843551940170023329\n",
      "Iteration 49631 => Loss: 6.69843548468478555691\n",
      "Iteration 49632 => Loss: 6.69843544997255424533\n",
      "Iteration 49633 => Loss: 6.69843541526499475225\n",
      "Iteration 49634 => Loss: 6.69843538056212128851\n",
      "Iteration 49635 => Loss: 6.69843534586391697871\n",
      "Iteration 49636 => Loss: 6.69843531117038626377\n",
      "Iteration 49637 => Loss: 6.69843527648153447274\n",
      "Iteration 49638 => Loss: 6.69843524179735538837\n",
      "Iteration 49639 => Loss: 6.69843520711785078703\n",
      "Iteration 49640 => Loss: 6.69843517244301622782\n",
      "Iteration 49641 => Loss: 6.69843513777285970434\n",
      "Iteration 49642 => Loss: 6.69843510310737055846\n",
      "Iteration 49643 => Loss: 6.69843506844655767196\n",
      "Iteration 49644 => Loss: 6.69843503379040861034\n",
      "Iteration 49645 => Loss: 6.69843499913893136721\n",
      "Iteration 49646 => Loss: 6.69843496449213038346\n",
      "Iteration 49647 => Loss: 6.69843492984998878370\n",
      "Iteration 49648 => Loss: 6.69843489521251989061\n",
      "Iteration 49649 => Loss: 6.69843486057972192782\n",
      "Iteration 49650 => Loss: 6.69843482595158068449\n",
      "Iteration 49651 => Loss: 6.69843479132811214782\n",
      "Iteration 49652 => Loss: 6.69843475670931010058\n",
      "Iteration 49653 => Loss: 6.69843472209517010185\n",
      "Iteration 49654 => Loss: 6.69843468748569481619\n",
      "Iteration 49655 => Loss: 6.69843465288088779630\n",
      "Iteration 49656 => Loss: 6.69843461828073216680\n",
      "Iteration 49657 => Loss: 6.69843458368524746760\n",
      "Iteration 49658 => Loss: 6.69843454909443103418\n",
      "Iteration 49659 => Loss: 6.69843451450826155025\n",
      "Iteration 49660 => Loss: 6.69843447992675677938\n",
      "Iteration 49661 => Loss: 6.69843444534991583339\n",
      "Iteration 49662 => Loss: 6.69843441077773160686\n",
      "Iteration 49663 => Loss: 6.69843437621021298156\n",
      "Iteration 49664 => Loss: 6.69843434164734130576\n",
      "Iteration 49665 => Loss: 6.69843430708913611937\n",
      "Iteration 49666 => Loss: 6.69843427253557521794\n",
      "Iteration 49667 => Loss: 6.69843423798668080593\n",
      "Iteration 49668 => Loss: 6.69843420344244133702\n",
      "Iteration 49669 => Loss: 6.69843416890285592302\n",
      "Iteration 49670 => Loss: 6.69843413436791923488\n",
      "Iteration 49671 => Loss: 6.69843409983764548343\n",
      "Iteration 49672 => Loss: 6.69843406531202045784\n",
      "Iteration 49673 => Loss: 6.69843403079104326991\n",
      "Iteration 49674 => Loss: 6.69843399627472368962\n",
      "Iteration 49675 => Loss: 6.69843396176305194700\n",
      "Iteration 49676 => Loss: 6.69843392725603425930\n",
      "Iteration 49677 => Loss: 6.69843389275365996838\n",
      "Iteration 49678 => Loss: 6.69843385825594150873\n",
      "Iteration 49679 => Loss: 6.69843382376286911040\n",
      "Iteration 49680 => Loss: 6.69843378927444277338\n",
      "Iteration 49681 => Loss: 6.69843375479066693856\n",
      "Iteration 49682 => Loss: 6.69843372031154071777\n",
      "Iteration 49683 => Loss: 6.69843368583704990016\n",
      "Iteration 49684 => Loss: 6.69843365136720869657\n",
      "Iteration 49685 => Loss: 6.69843361690201888337\n",
      "Iteration 49686 => Loss: 6.69843358244147069058\n",
      "Iteration 49687 => Loss: 6.69843354798556411822\n",
      "Iteration 49688 => Loss: 6.69843351353430094264\n",
      "Iteration 49689 => Loss: 6.69843347908767405841\n",
      "Iteration 49690 => Loss: 6.69843344464569767638\n",
      "Iteration 49691 => Loss: 6.69843341020836113842\n",
      "Iteration 49692 => Loss: 6.69843337577565822727\n",
      "Iteration 49693 => Loss: 6.69843334134760404197\n",
      "Iteration 49694 => Loss: 6.69843330692418437167\n",
      "Iteration 49695 => Loss: 6.69843327250540276907\n",
      "Iteration 49696 => Loss: 6.69843323809125745782\n",
      "Iteration 49697 => Loss: 6.69843320368175643154\n",
      "Iteration 49698 => Loss: 6.69843316927688814388\n",
      "Iteration 49699 => Loss: 6.69843313487665259487\n",
      "Iteration 49700 => Loss: 6.69843310048105955445\n",
      "Iteration 49701 => Loss: 6.69843306609009481178\n",
      "Iteration 49702 => Loss: 6.69843303170376547229\n",
      "Iteration 49703 => Loss: 6.69843299732207242414\n",
      "Iteration 49704 => Loss: 6.69843296294501033827\n",
      "Iteration 49705 => Loss: 6.69843292857258543194\n",
      "Iteration 49706 => Loss: 6.69843289420478615881\n",
      "Iteration 49707 => Loss: 6.69843285984162051250\n",
      "Iteration 49708 => Loss: 6.69843282548308582847\n",
      "Iteration 49709 => Loss: 6.69843279112917944218\n",
      "Iteration 49710 => Loss: 6.69843275677989868910\n",
      "Iteration 49711 => Loss: 6.69843272243524889831\n",
      "Iteration 49712 => Loss: 6.69843268809522651708\n",
      "Iteration 49713 => Loss: 6.69843265375983953902\n",
      "Iteration 49714 => Loss: 6.69843261942907108875\n",
      "Iteration 49715 => Loss: 6.69843258510292915986\n",
      "Iteration 49716 => Loss: 6.69843255078141552872\n",
      "Iteration 49717 => Loss: 6.69843251646452486625\n",
      "Iteration 49718 => Loss: 6.69843248215226250153\n",
      "Iteration 49719 => Loss: 6.69843244784461511188\n",
      "Iteration 49720 => Loss: 6.69843241354159868450\n",
      "Iteration 49721 => Loss: 6.69843237924320433763\n",
      "Iteration 49722 => Loss: 6.69843234494942230128\n",
      "Iteration 49723 => Loss: 6.69843231066026945086\n",
      "Iteration 49724 => Loss: 6.69843227637573424005\n",
      "Iteration 49725 => Loss: 6.69843224209582288609\n",
      "Iteration 49726 => Loss: 6.69843220782052917173\n",
      "Iteration 49727 => Loss: 6.69843217354985309697\n",
      "Iteration 49728 => Loss: 6.69843213928379199729\n",
      "Iteration 49729 => Loss: 6.69843210502235120174\n",
      "Iteration 49730 => Loss: 6.69843207076552982215\n",
      "Iteration 49731 => Loss: 6.69843203651332608217\n",
      "Iteration 49732 => Loss: 6.69843200226572843548\n",
      "Iteration 49733 => Loss: 6.69843196802275109292\n",
      "Iteration 49734 => Loss: 6.69843193378438872543\n",
      "Iteration 49735 => Loss: 6.69843189955063511576\n",
      "Iteration 49736 => Loss: 6.69843186532149648116\n",
      "Iteration 49737 => Loss: 6.69843183109697282163\n",
      "Iteration 49738 => Loss: 6.69843179687706236081\n",
      "Iteration 49739 => Loss: 6.69843176266176065781\n",
      "Iteration 49740 => Loss: 6.69843172845107304170\n",
      "Iteration 49741 => Loss: 6.69843169424499684794\n",
      "Iteration 49742 => Loss: 6.69843166004351875387\n",
      "Iteration 49743 => Loss: 6.69843162584665474668\n",
      "Iteration 49744 => Loss: 6.69843159165439594460\n",
      "Iteration 49745 => Loss: 6.69843155746675034123\n",
      "Iteration 49746 => Loss: 6.69843152328371260751\n",
      "Iteration 49747 => Loss: 6.69843148910527474982\n",
      "Iteration 49748 => Loss: 6.69843145493144209723\n",
      "Iteration 49749 => Loss: 6.69843142076221909065\n",
      "Iteration 49750 => Loss: 6.69843138659760484188\n",
      "Iteration 49751 => Loss: 6.69843135243758514008\n",
      "Iteration 49752 => Loss: 6.69843131828217597246\n",
      "Iteration 49753 => Loss: 6.69843128413135868726\n",
      "Iteration 49754 => Loss: 6.69843124998515193624\n",
      "Iteration 49755 => Loss: 6.69843121584354506126\n",
      "Iteration 49756 => Loss: 6.69843118170654161503\n",
      "Iteration 49757 => Loss: 6.69843114757413538030\n",
      "Iteration 49758 => Loss: 6.69843111344632635706\n",
      "Iteration 49759 => Loss: 6.69843107932311276898\n",
      "Iteration 49760 => Loss: 6.69843104520449905692\n",
      "Iteration 49761 => Loss: 6.69843101109048966180\n",
      "Iteration 49762 => Loss: 6.69843097698107658999\n",
      "Iteration 49763 => Loss: 6.69843094287625184791\n",
      "Iteration 49764 => Loss: 6.69843090877603053457\n",
      "Iteration 49765 => Loss: 6.69843087468039755095\n",
      "Iteration 49766 => Loss: 6.69843084058935822611\n",
      "Iteration 49767 => Loss: 6.69843080650291877731\n",
      "Iteration 49768 => Loss: 6.69843077242107032276\n",
      "Iteration 49769 => Loss: 6.69843073834381641518\n",
      "Iteration 49770 => Loss: 6.69843070427114817278\n",
      "Iteration 49771 => Loss: 6.69843067020307714188\n",
      "Iteration 49772 => Loss: 6.69843063613959532887\n",
      "Iteration 49773 => Loss: 6.69843060208070628647\n",
      "Iteration 49774 => Loss: 6.69843056802640113290\n",
      "Iteration 49775 => Loss: 6.69843053397668786175\n",
      "Iteration 49776 => Loss: 6.69843049993156647304\n",
      "Iteration 49777 => Loss: 6.69843046589102630861\n",
      "Iteration 49778 => Loss: 6.69843043185507802662\n",
      "Iteration 49779 => Loss: 6.69843039782370830437\n",
      "Iteration 49780 => Loss: 6.69843036379693401727\n",
      "Iteration 49781 => Loss: 6.69843032977473651357\n",
      "Iteration 49782 => Loss: 6.69843029575712822776\n",
      "Iteration 49783 => Loss: 6.69843026174410116624\n",
      "Iteration 49784 => Loss: 6.69843022773566421080\n",
      "Iteration 49785 => Loss: 6.69843019373180315057\n",
      "Iteration 49786 => Loss: 6.69843015973252420281\n",
      "Iteration 49787 => Loss: 6.69843012573782470298\n",
      "Iteration 49788 => Loss: 6.69843009174771530922\n",
      "Iteration 49789 => Loss: 6.69843005776217470526\n",
      "Iteration 49790 => Loss: 6.69843002378122154283\n",
      "Iteration 49791 => Loss: 6.69842998980484516380\n",
      "Iteration 49792 => Loss: 6.69842995583304823271\n",
      "Iteration 49793 => Loss: 6.69842992186582364411\n",
      "Iteration 49794 => Loss: 6.69842988790317850345\n",
      "Iteration 49795 => Loss: 6.69842985394511636343\n",
      "Iteration 49796 => Loss: 6.69842981999162212503\n",
      "Iteration 49797 => Loss: 6.69842978604270911092\n",
      "Iteration 49798 => Loss: 6.69842975209836843931\n",
      "Iteration 49799 => Loss: 6.69842971815859744567\n",
      "Iteration 49800 => Loss: 6.69842968422340323542\n",
      "Iteration 49801 => Loss: 6.69842965029278136768\n",
      "Iteration 49802 => Loss: 6.69842961636673273063\n",
      "Iteration 49803 => Loss: 6.69842958244525732425\n",
      "Iteration 49804 => Loss: 6.69842954852834715496\n",
      "Iteration 49805 => Loss: 6.69842951461601021634\n",
      "Iteration 49806 => Loss: 6.69842948070824650841\n",
      "Iteration 49807 => Loss: 6.69842944680504981392\n",
      "Iteration 49808 => Loss: 6.69842941290642013286\n",
      "Iteration 49809 => Loss: 6.69842937901236101794\n",
      "Iteration 49810 => Loss: 6.69842934512286891646\n",
      "Iteration 49811 => Loss: 6.69842931123793317028\n",
      "Iteration 49812 => Loss: 6.69842927735757598384\n",
      "Iteration 49813 => Loss: 6.69842924348177515270\n",
      "Iteration 49814 => Loss: 6.69842920961054577589\n",
      "Iteration 49815 => Loss: 6.69842917574388074797\n",
      "Iteration 49816 => Loss: 6.69842914188177918078\n",
      "Iteration 49817 => Loss: 6.69842910802423752159\n",
      "Iteration 49818 => Loss: 6.69842907417125843494\n",
      "Iteration 49819 => Loss: 6.69842904032284103266\n",
      "Iteration 49820 => Loss: 6.69842900647898531474\n",
      "Iteration 49821 => Loss: 6.69842897263969039301\n",
      "Iteration 49822 => Loss: 6.69842893880495537928\n",
      "Iteration 49823 => Loss: 6.69842890497478116174\n",
      "Iteration 49824 => Loss: 6.69842887114916152314\n",
      "Iteration 49825 => Loss: 6.69842883732810712161\n",
      "Iteration 49826 => Loss: 6.69842880351159930541\n",
      "Iteration 49827 => Loss: 6.69842876969965406175\n",
      "Iteration 49828 => Loss: 6.69842873589226872610\n",
      "Iteration 49829 => Loss: 6.69842870208942908761\n",
      "Iteration 49830 => Loss: 6.69842866829115735072\n",
      "Iteration 49831 => Loss: 6.69842863449742953463\n",
      "Iteration 49832 => Loss: 6.69842860070825807384\n",
      "Iteration 49833 => Loss: 6.69842856692364208016\n",
      "Iteration 49834 => Loss: 6.69842853314357888905\n",
      "Iteration 49835 => Loss: 6.69842849936806139510\n",
      "Iteration 49836 => Loss: 6.69842846559709848009\n",
      "Iteration 49837 => Loss: 6.69842843183069280855\n",
      "Iteration 49838 => Loss: 6.69842839806882395237\n",
      "Iteration 49839 => Loss: 6.69842836431151678056\n",
      "Iteration 49840 => Loss: 6.69842833055875530590\n",
      "Iteration 49841 => Loss: 6.69842829681053775204\n",
      "Iteration 49842 => Loss: 6.69842826306686678350\n",
      "Iteration 49843 => Loss: 6.69842822932774861755\n",
      "Iteration 49844 => Loss: 6.69842819559317526057\n",
      "Iteration 49845 => Loss: 6.69842816186315026528\n",
      "Iteration 49846 => Loss: 6.69842812813766652624\n",
      "Iteration 49847 => Loss: 6.69842809441672937254\n",
      "Iteration 49848 => Loss: 6.69842806070032903420\n",
      "Iteration 49849 => Loss: 6.69842802698847972209\n",
      "Iteration 49850 => Loss: 6.69842799328117699531\n",
      "Iteration 49851 => Loss: 6.69842795957840753118\n",
      "Iteration 49852 => Loss: 6.69842792588017665878\n",
      "Iteration 49853 => Loss: 6.69842789218649681260\n",
      "Iteration 49854 => Loss: 6.69842785849735555814\n",
      "Iteration 49855 => Loss: 6.69842782481274578998\n",
      "Iteration 49856 => Loss: 6.69842779113268260716\n",
      "Iteration 49857 => Loss: 6.69842775745715623970\n",
      "Iteration 49858 => Loss: 6.69842772378616579942\n",
      "Iteration 49859 => Loss: 6.69842769011971572723\n",
      "Iteration 49860 => Loss: 6.69842765645779625316\n",
      "Iteration 49861 => Loss: 6.69842762280042069989\n",
      "Iteration 49862 => Loss: 6.69842758914757396838\n",
      "Iteration 49863 => Loss: 6.69842755549926405223\n",
      "Iteration 49864 => Loss: 6.69842752185548295785\n",
      "Iteration 49865 => Loss: 6.69842748821624933697\n",
      "Iteration 49866 => Loss: 6.69842745458153210336\n",
      "Iteration 49867 => Loss: 6.69842742095135523783\n",
      "Iteration 49868 => Loss: 6.69842738732571429949\n",
      "Iteration 49869 => Loss: 6.69842735370460395927\n",
      "Iteration 49870 => Loss: 6.69842732008801888810\n",
      "Iteration 49871 => Loss: 6.69842728647596441505\n",
      "Iteration 49872 => Loss: 6.69842725286843787558\n",
      "Iteration 49873 => Loss: 6.69842721926544548694\n",
      "Iteration 49874 => Loss: 6.69842718566697747917\n",
      "Iteration 49875 => Loss: 6.69842715207303385228\n",
      "Iteration 49876 => Loss: 6.69842711848361904714\n",
      "Iteration 49877 => Loss: 6.69842708489873395195\n",
      "Iteration 49878 => Loss: 6.69842705131837323762\n",
      "Iteration 49879 => Loss: 6.69842701774253157510\n",
      "Iteration 49880 => Loss: 6.69842698417122228705\n",
      "Iteration 49881 => Loss: 6.69842695060443737987\n",
      "Iteration 49882 => Loss: 6.69842691704216619542\n",
      "Iteration 49883 => Loss: 6.69842688348442116819\n",
      "Iteration 49884 => Loss: 6.69842684993119785730\n",
      "Iteration 49885 => Loss: 6.69842681638249892728\n",
      "Iteration 49886 => Loss: 6.69842678283832260178\n",
      "Iteration 49887 => Loss: 6.69842674929866177536\n",
      "Iteration 49888 => Loss: 6.69842671576351644802\n",
      "Iteration 49889 => Loss: 6.69842668223290083063\n",
      "Iteration 49890 => Loss: 6.69842664870679627143\n",
      "Iteration 49891 => Loss: 6.69842661518521520492\n",
      "Iteration 49892 => Loss: 6.69842658166814519660\n",
      "Iteration 49893 => Loss: 6.69842654815559246373\n",
      "Iteration 49894 => Loss: 6.69842651464755522994\n",
      "Iteration 49895 => Loss: 6.69842648114403704795\n",
      "Iteration 49896 => Loss: 6.69842644764502992416\n",
      "Iteration 49897 => Loss: 6.69842641415054362852\n",
      "Iteration 49898 => Loss: 6.69842638066056217383\n",
      "Iteration 49899 => Loss: 6.69842634717509977094\n",
      "Iteration 49900 => Loss: 6.69842631369414309717\n",
      "Iteration 49901 => Loss: 6.69842628021770014612\n",
      "Iteration 49902 => Loss: 6.69842624674577002963\n",
      "Iteration 49903 => Loss: 6.69842621327834919498\n",
      "Iteration 49904 => Loss: 6.69842617981543764216\n",
      "Iteration 49905 => Loss: 6.69842614635703714754\n",
      "Iteration 49906 => Loss: 6.69842611290314060568\n",
      "Iteration 49907 => Loss: 6.69842607945375245748\n",
      "Iteration 49908 => Loss: 6.69842604600887714383\n",
      "Iteration 49909 => Loss: 6.69842601256850311842\n",
      "Iteration 49910 => Loss: 6.69842597913263837484\n",
      "Iteration 49911 => Loss: 6.69842594570127758402\n",
      "Iteration 49912 => Loss: 6.69842591227442518687\n",
      "Iteration 49913 => Loss: 6.69842587885207585430\n",
      "Iteration 49914 => Loss: 6.69842584543422692178\n",
      "Iteration 49915 => Loss: 6.69842581202088016568\n",
      "Iteration 49916 => Loss: 6.69842577861203736234\n",
      "Iteration 49917 => Loss: 6.69842574520770206448\n",
      "Iteration 49918 => Loss: 6.69842571180786361396\n",
      "Iteration 49919 => Loss: 6.69842567841252023442\n",
      "Iteration 49920 => Loss: 6.69842564502168524854\n",
      "Iteration 49921 => Loss: 6.69842561163534266910\n",
      "Iteration 49922 => Loss: 6.69842557825350581879\n",
      "Iteration 49923 => Loss: 6.69842554487616492764\n",
      "Iteration 49924 => Loss: 6.69842551150332266019\n",
      "Iteration 49925 => Loss: 6.69842547813497279918\n",
      "Iteration 49926 => Loss: 6.69842544477112245005\n",
      "Iteration 49927 => Loss: 6.69842541141177338915\n",
      "Iteration 49928 => Loss: 6.69842537805690962927\n",
      "Iteration 49929 => Loss: 6.69842534470654538126\n",
      "Iteration 49930 => Loss: 6.69842531136067798059\n",
      "Iteration 49931 => Loss: 6.69842527801929588094\n",
      "Iteration 49932 => Loss: 6.69842524468240974045\n",
      "Iteration 49933 => Loss: 6.69842521135001778276\n",
      "Iteration 49934 => Loss: 6.69842517802211645517\n",
      "Iteration 49935 => Loss: 6.69842514469870575766\n",
      "Iteration 49936 => Loss: 6.69842511137978480207\n",
      "Iteration 49937 => Loss: 6.69842507806535536474\n",
      "Iteration 49938 => Loss: 6.69842504475541655751\n",
      "Iteration 49939 => Loss: 6.69842501144996482765\n",
      "Iteration 49940 => Loss: 6.69842497814900461606\n",
      "Iteration 49941 => Loss: 6.69842494485251904734\n",
      "Iteration 49942 => Loss: 6.69842491156053299051\n",
      "Iteration 49943 => Loss: 6.69842487827302690562\n",
      "Iteration 49944 => Loss: 6.69842484499000434539\n",
      "Iteration 49945 => Loss: 6.69842481171147419161\n",
      "Iteration 49946 => Loss: 6.69842477843742312160\n",
      "Iteration 49947 => Loss: 6.69842474516785468808\n",
      "Iteration 49948 => Loss: 6.69842471190277599646\n",
      "Iteration 49949 => Loss: 6.69842467864217283591\n",
      "Iteration 49950 => Loss: 6.69842464538605497637\n",
      "Iteration 49951 => Loss: 6.69842461213441620060\n",
      "Iteration 49952 => Loss: 6.69842457888726094950\n",
      "Iteration 49953 => Loss: 6.69842454564458389399\n",
      "Iteration 49954 => Loss: 6.69842451240638769860\n",
      "Iteration 49955 => Loss: 6.69842447917266436974\n",
      "Iteration 49956 => Loss: 6.69842444594342456554\n",
      "Iteration 49957 => Loss: 6.69842441271866206876\n",
      "Iteration 49958 => Loss: 6.69842437949838043210\n",
      "Iteration 49959 => Loss: 6.69842434628256988560\n",
      "Iteration 49960 => Loss: 6.69842431307123398199\n",
      "Iteration 49961 => Loss: 6.69842427986437272125\n",
      "Iteration 49962 => Loss: 6.69842424666199587335\n",
      "Iteration 49963 => Loss: 6.69842421346408567473\n",
      "Iteration 49964 => Loss: 6.69842418027065100716\n",
      "Iteration 49965 => Loss: 6.69842414708168476523\n",
      "Iteration 49966 => Loss: 6.69842411389720027159\n",
      "Iteration 49967 => Loss: 6.69842408071717709817\n",
      "Iteration 49968 => Loss: 6.69842404754162679126\n",
      "Iteration 49969 => Loss: 6.69842401437054757452\n",
      "Iteration 49970 => Loss: 6.69842398120394211247\n",
      "Iteration 49971 => Loss: 6.69842394804179797063\n",
      "Iteration 49972 => Loss: 6.69842391488413202438\n",
      "Iteration 49973 => Loss: 6.69842388173093183923\n",
      "Iteration 49974 => Loss: 6.69842384858219475063\n",
      "Iteration 49975 => Loss: 6.69842381543793230492\n",
      "Iteration 49976 => Loss: 6.69842378229812585033\n",
      "Iteration 49977 => Loss: 6.69842374916278959773\n",
      "Iteration 49978 => Loss: 6.69842371603191555351\n",
      "Iteration 49979 => Loss: 6.69842368290551082310\n",
      "Iteration 49980 => Loss: 6.69842364978356741290\n",
      "Iteration 49981 => Loss: 6.69842361666608798743\n",
      "Iteration 49982 => Loss: 6.69842358355306810580\n",
      "Iteration 49983 => Loss: 6.69842355044451309709\n",
      "Iteration 49984 => Loss: 6.69842351734041940858\n",
      "Iteration 49985 => Loss: 6.69842348424078615210\n",
      "Iteration 49986 => Loss: 6.69842345114560622221\n",
      "Iteration 49987 => Loss: 6.69842341805490004703\n",
      "Iteration 49988 => Loss: 6.69842338496864453390\n",
      "Iteration 49989 => Loss: 6.69842335188684590008\n",
      "Iteration 49990 => Loss: 6.69842331880951213918\n",
      "Iteration 49991 => Loss: 6.69842328573663259306\n",
      "Iteration 49992 => Loss: 6.69842325266820370899\n",
      "Iteration 49993 => Loss: 6.69842321960423259242\n",
      "Iteration 49994 => Loss: 6.69842318654472368422\n",
      "Iteration 49995 => Loss: 6.69842315348966277355\n",
      "Iteration 49996 => Loss: 6.69842312043905874219\n",
      "Iteration 49997 => Loss: 6.69842308739290803743\n",
      "Iteration 49998 => Loss: 6.69842305435121065926\n",
      "Iteration 49999 => Loss: 6.69842302131397104858\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:09:05.295022Z",
     "start_time": "2025-05-27T16:09:05.286899Z"
    }
   },
   "cell_type": "code",
   "source": "print(w)",
   "id": "831693f1e3ee97ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.32966172]\n",
      " [ 1.23135282]\n",
      " [-0.02111842]\n",
      " [ 3.12372705]]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T16:09:05.524700Z",
     "start_time": "2025-05-27T16:09:05.299044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x1 = X[:, 2]  # Temperature\n",
    "x2 = X[:, 1]  # Reservations\n",
    "y = Y[:, 0]   # Pizzas (flattened for plotting)\n",
    "\n",
    "# Plot the axes\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"figure.facecolor\": \"white\"})\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_xlabel(\"Temperature\", labelpad=15, fontsize=15)\n",
    "ax.set_ylabel(\"Reservations\", labelpad=15, fontsize=15)\n",
    "ax.set_zlabel(\"Pizzas\", labelpad=10, fontsize=15)\n",
    "\n",
    "# Plot the data points\n",
    "ax.scatter3D(x1, x2, y, color='blue', label='Data Points')\n",
    "\n",
    "# Plot the regression plane\n",
    "MARGIN = 5\n",
    "edges_x1 = np.linspace(np.min(x1) - MARGIN, np.max(x1) + MARGIN, 2)\n",
    "edges_x2 = np.linspace(np.min(x2) - MARGIN, np.max(x2) + MARGIN, 2)\n",
    "xs, ys = np.meshgrid(edges_x1, edges_x2)\n",
    "\n",
    "# Calculate zs using the model: z = w0 + w1*x2 + w2*x1\n",
    "# (Reservations = x2, Temperature = x1)\n",
    "zs = w[0] + w[1] * ys + w[2] * xs + w[3] * np.mean(X[:, 3])  # Keep 'Tourists' constant for 3D plot\n",
    "\n",
    "ax.plot_surface(xs, ys, zs, alpha=0.3, cmap=cm.viridis)\n",
    "ax.legend()\n",
    "plt.title(\"Linear Regression Plane over Pizza Data\", fontsize=15)\n",
    "plt.show()"
   ],
   "id": "99d30ff208b302e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
